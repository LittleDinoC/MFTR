[
    {
        "id": "gorilla_huggingface_query_0",
        "query": "You are building a virtual global tour guide that can identify languages from the audio of people speaking. Use a model to identify which language is being spoken.",
        "instruction": "Given a `language identification` task, retrieve tools that process audio inputs to accurately determine the language being spoken, utilizing models designed for audio classification and language detection purposes.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Language Identification",
                    "api_call": "AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')",
                    "api_arguments": [
                        "model = AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')",
                        "processor = Wav2Vec2Processor.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.27.0.dev0",
                        "pytorch==1.13.1",
                        "datasets==2.9.0",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "google/xtreme_s",
                        "accuracy": 0.8805
                    },
                    "description": "This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset. It is used for language identification in audio classification tasks.",
                    "id": "huggingface_api_820",
                    "name": "sanchit-gandhi/whisper-medium-fleurs-lang-id"
                },
                "id": "gorilla_huggingface_tool_815",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_1",
        "query": "We want to implement a customer service chatbot to engage with website visitors and provide support.",
        "instruction": "Given a `chatbot implementation` task, retrieve tools that facilitate the creation of a conversational agent by processing text-based interactions and generating suitable responses to engage with users on a website, while ensuring support and continuous engagement.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Conversational",
                    "api_call": "BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')",
                    "api_arguments": "['message']",
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "Input a message to start chatting with facebook/blenderbot-400M-distill.",
                    "performance": {
                        "dataset": "blended_skill_talk",
                        "accuracy": "Not specified"
                    },
                    "description": "BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.",
                    "id": "huggingface_api_572",
                    "name": "facebook/blenderbot-400M-distill"
                },
                "id": "gorilla_huggingface_tool_568",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_2",
        "query": "We are a news agency that wants to extract useful features from Korean news articles for a content recommendation service.",
        "instruction": "Given a `feature extraction` task, retrieve tools that process Korean text inputs, such as news articles, to extract useful features for content recommendation services using language models capable of handling Korean language datasets.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "PyTorch Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "api_arguments": {
                        "tokenizer": "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "tokenizers": "latest"
                    },
                    "example_code": "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "performance": {
                        "dataset": "NSMC",
                        "accuracy": 0.901
                    },
                    "description": "KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.",
                    "id": "huggingface_api_8",
                    "name": "kobart-base-v2"
                },
                "id": "gorilla_huggingface_tool_8",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_3",
        "query": "We need to create automated transcripts from recorded podcasts that include punctuation for better readability.",
        "instruction": "Given an `automated transcription with punctuation` task, retrieve tools that process audio inputs, specifically recorded podcasts, to generate accurate and readable textual transcriptions that include punctuation marks, enhancing the text's clarity and prosody.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "https://github.com/neonbjb/ocotillo",
                    "performance": {
                        "dataset": "librispeech validation set",
                        "accuracy": "4.45%"
                    },
                    "description": "This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.",
                    "id": "huggingface_api_748",
                    "name": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli"
                },
                "id": "gorilla_huggingface_tool_743",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_4",
        "query": "We need to generate a motivational quote related to sports.",
        "instruction": "Given a `text generation` task, retrieve tools that specialize in producing motivational content by generating coherent and contextually relevant text outputs aligned with the query's theme of sports.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.",
                    "id": "huggingface_api_613",
                    "name": "TehVenom/PPO_Pygway-V8p4_Dev-6b"
                },
                "id": "gorilla_huggingface_tool_608",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_5",
        "query": "As a toy company, we are designing a new toy line. We'd like you to create an image of a toy robot using relevant text prompts as control input.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that utilize text prompts to create visual representations, specifically focusing on generating images of toy robots using models like ControlNet and Stable Diffusion which process text inputs and control conditions for image creation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Diffusion Models",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')",
                    "api_arguments": [
                        "checkpoint",
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import MLSDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\ncontrol_image = processor(image)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "MLSD",
                        "accuracy": "Not provided"
                    },
                    "description": "Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.",
                    "id": "huggingface_api_278",
                    "name": "lllyasviel/control_v11p_sd15_mlsd"
                },
                "id": "gorilla_huggingface_tool_274",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_6",
        "query": "You are an Indian in Telugu pandit teaching kids how to pronounce conventional holy prayers.. Teach me mantras in Telugu synthesized by human like voice pronunciation.",
        "instruction": "Given a `text-to-speech synthesis` task, retrieve tools that convert textual representations of Telugu mantras into audio outputs with natural and human-like pronunciation, tailored to the query's language and cultural context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "pipeline('text-to-speech', model='SYSPIN/Telugu_Male_TTS')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.",
                    "id": "huggingface_api_734",
                    "name": "SYSPIN/Telugu_Male_TTS"
                },
                "id": "gorilla_huggingface_tool_729",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_7",
        "query": "We have been asked to predict future criminal re-offense from a given dataset. What model should we adopt and how do we proceed?",
        "instruction": "Given a `predictive modeling` task, retrieve tools that are designed for tabular classification to assess future criminal reoffense risks by leveraging appropriate datasets and machine learning frameworks to provide predictions and insights aligned with the query's objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('imodels/figs-compas-recidivism', 'sklearn_model.joblib')))",
                    "api_arguments": [
                        "REPO_ID",
                        "FILENAME"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "huggingface_hub",
                        "pandas",
                        "numpy",
                        "datasets",
                        "imodels",
                        "sklearn.model_selection"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = imodels/figs-compas-recidivism\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\npreds = model.predict(X_test)\nprint('accuracy', np.mean(preds==y_test))",
                    "performance": {
                        "dataset": "imodels/compas-recidivism",
                        "accuracy": 0.6759165485112416
                    },
                    "description": "A tabular classification model for predicting recidivism using the COMPAS dataset. The model is an imodels.FIGSClassifier trained with Scikit-learn and can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_865",
                    "name": "imodels/figs-compas-recidivism"
                },
                "id": "gorilla_huggingface_tool_858",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_8",
        "query": "We received a customer feedback document which is very lengthy. We need a summarization of it.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy textual content to generate concise summaries, effectively condensing the main points and maintaining the essence of the original document.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text-to-Text Generation",
                    "api_call": "pipeline('summarization', model='philschmid/bart-large-cnn-samsum')",
                    "api_arguments": {
                        "model": "philschmid/bart-large-cnn-samsum"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\nconversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation? \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "samsum",
                        "accuracy": {
                            "eval_rouge1": 42.621,
                            "eval_rouge2": 21.9825,
                            "eval_rougeL": 33.034,
                            "eval_rougeLsum": 39.6783,
                            "test_rouge1": 41.3174,
                            "test_rouge2": 20.8716,
                            "test_rougeL": 32.1337,
                            "test_rougeLsum": 38.4149
                        }
                    },
                    "description": "philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.",
                    "id": "huggingface_api_549",
                    "name": "philschmid/bart-large-cnn-samsum"
                },
                "id": "gorilla_huggingface_tool_545",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_9",
        "query": "Estimate the depth of a pool using computational depth estimation, given an underwater photo.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to analyze underwater images and compute the depth based on visual data, optimizing accuracy through specialized algorithms and models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.",
                    "id": "huggingface_api_146",
                    "name": "hf-tiny-model-private/tiny-random-GLPNForDepthEstimation"
                },
                "id": "gorilla_huggingface_tool_142",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_10",
        "query": "Create an API that processes large data sets of financial transactions and can deliver information on the number of transactions and their monetary value, based on a date range.",
        "instruction": "Given a `data processing and analysis` task, retrieve tools capable of handling large datasets, particularly financial transactions, by processing inputs based on date ranges and delivering insights on the transactions' count and monetary value.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "api_arguments": "model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import TapasTokenizer, TapasForQuestionAnswering\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "performance": {
                        "dataset": "wikisql",
                        "accuracy": "Not specified"
                    },
                    "description": "TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.",
                    "id": "huggingface_api_451",
                    "name": "google/tapas-small-finetuned-wikisql-supervised"
                },
                "id": "gorilla_huggingface_tool_447",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_11",
        "query": "A product is built that analyzes book reviews in order to determine how similar two examples from multiple books are to each other.",
        "instruction": "Given a `sentence similarity analysis` task, retrieve tools that can assess the similarity between text samples by utilizing models designed for natural language processing, specifically those capable of feature extraction and embedding generation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.",
                    "id": "huggingface_api_2",
                    "name": "princeton-nlp/unsup-simcse-roberta-base"
                },
                "id": "gorilla_huggingface_tool_2",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_12",
        "query": "Translate the following text from French to English: â€œLe systÃ¨me Ã©ducatif franÃ§ais est composÃ© d'Ã©coles maternelles, d'Ã©coles Ã©lÃ©mentaires, de collÃ¨ges et de lycÃ©es.â€",
        "instruction": "Given a `translation` task, retrieve tools that convert text from French to English by processing bilingual textual inputs to produce accurate translations aligned with the query's linguistic and contextual requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment Ã§a va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_13",
        "query": "Assit me to process and segment an image for further analysis.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process and analyze images to segment them into meaningful parts for further analysis, utilizing advanced computer vision techniques and model architectures.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')",
                    "api_arguments": [
                        "images",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "ADE20K",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_234",
                    "name": "nvidia/segformer-b5-finetuned-ade-640-640"
                },
                "id": "gorilla_huggingface_tool_230",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_14",
        "query": "Our client is a legal firm that needs assistance in extracting specific information from a large number of legal documents. Automate the process of answering questions related to these documents.",
        "instruction": "Given a `document question answering` task, retrieve tools that automate the extraction of information from legal documents by leveraging models capable of processing textual and visual inputs to answer queries accurately based on document content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": "question, image",
                    "python_environment_requirements": "transformers, torch, datasets, tokenizers",
                    "example_code": "",
                    "performance": {
                        "dataset": "None",
                        "accuracy": {
                            "Loss": 4.3167
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.",
                    "id": "huggingface_api_136",
                    "name": "layoutlmv2-base-uncased_finetuned_docvqa"
                },
                "id": "gorilla_huggingface_tool_129",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_15",
        "query": "I want to analyze the text and images in a document and extract answers to questions based on the content.",
        "instruction": "Given a `multimodal document analysis` task, retrieve tools that analyze both text and images within a document to extract answers to questions, utilizing advanced machine learning models designed for document question answering.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A LayoutLMv2 model for document question answering.",
                    "id": "huggingface_api_129",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023"
                },
                "id": "gorilla_huggingface_tool_128",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_16",
        "query": "I have a list of Olympic Game host cities and their corresponding years. I want to know which year the games were held in Beijing.",
        "instruction": "Given a `year lookup` task for historical events in a tabular format, retrieve tools that perform table question answering by processing structured data to determine the specific year an event occurred in a specified location, aligning with the query's requirements for detailed information extraction.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "pandas"
                        ]
                    },
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "arxiv:2107.07653",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.",
                    "id": "huggingface_api_443",
                    "name": "microsoft/tapex-base"
                },
                "id": "gorilla_huggingface_tool_439",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_17",
        "query": "We need to prepare some sample conversations featuring frequently asked questions for helping customers with our products.",
        "instruction": "Given a `conversational sample generation` task, retrieve tools that utilize natural language processing capabilities to generate dialogue-based content by processing frequent customer questions and producing interactive and coherent conversational responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')",
                    "api_arguments": [
                        "message"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "conversational_pipeline('Hello, how are you?')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A GPT-4 model for generating conversational responses in a dialogue setting.",
                    "id": "huggingface_api_588",
                    "name": "ingen51/DialoGPT-medium-GPT4"
                },
                "id": "gorilla_huggingface_tool_583",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_18",
        "query": "We have a news article and we need to extract all the entities like the names of people, organizations, and locations.",
        "instruction": "Given an `entity extraction` task, retrieve tools that perform Named Entity Recognition (NER) by processing text inputs to identify and extract entities such as names of people, organizations, and locations from the provided content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')",
                    "api_arguments": {
                        "model": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
                        "tokenizer": "Davlan/distilbert-base-multilingual-cased-ner-hrl"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "ANERcorp",
                                "language": "Arabic"
                            },
                            {
                                "name": "conll 2003",
                                "language": "German"
                            },
                            {
                                "name": "conll 2003",
                                "language": "English"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Spanish"
                            },
                            {
                                "name": "Europeana Newspapers",
                                "language": "French"
                            },
                            {
                                "name": "Italian I-CAB",
                                "language": "Italian"
                            },
                            {
                                "name": "Latvian NER",
                                "language": "Latvian"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Dutch"
                            },
                            {
                                "name": "Paramopama + Second Harem",
                                "language": "Portuguese"
                            },
                            {
                                "name": "MSRA",
                                "language": "Chinese"
                            }
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).",
                    "id": "huggingface_api_408",
                    "name": "distilbert-base-multilingual-cased-ner-hrl"
                },
                "id": "gorilla_huggingface_tool_404",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_19",
        "query": "Design a text-based interface where you can input text and get an image description from the given text.",
        "instruction": "Given a `text-based interface design` task, retrieve tools that transform text descriptions into visual outputs by generating images from the provided textual inputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "pipeline('text-to-image', model='prompthero/openjourney-v4')",
                    "api_arguments": {
                        "text": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "generate_image('your text here')",
                    "performance": {
                        "dataset": "Midjourney v4 images",
                        "accuracy": "Not provided"
                    },
                    "description": "Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.",
                    "id": "huggingface_api_42",
                    "name": "prompthero/openjourney-v4"
                },
                "id": "gorilla_huggingface_tool_42",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_20",
        "query": "Our company receives invoices in different formats. We need to extract specific information from these documents to process payments and keep records.",
        "instruction": "Given a `document information extraction` task, retrieve tools designed to process multimodal documents, extracting specific data for payment processing and record-keeping by utilizing machine learning models capable of understanding various document formats.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.12.2",
                        "torch==1.8.0+cu101",
                        "datasets==1.14.0",
                        "tokenizers==0.10.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 1.194
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_115",
                    "name": "layoutlmv2-base-uncased-finetuned-docvqa"
                },
                "id": "gorilla_huggingface_tool_114",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_21",
        "query": "We are building an audio assistant. Apply noise suppression to our new voice commands.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that specialize in processing audio inputs to suppress noise and enhance the quality of voice commands, aligning with the requirements for developing an audio assistant.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_22",
        "query": "I have a diary entry and want to identify the names of people and locations mentioned in it.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and categorize named entities such as people and locations within the text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english')\n\n# make example sentence\nsentence = Sentence('George Washington went to Washington')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": "93.06"
                    },
                    "description": "This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_426",
                    "name": "flair/ner-english"
                },
                "id": "gorilla_huggingface_tool_422",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_23",
        "query": "I want to build an AI model that can analyze images and answer questions about the content of the image.",
        "instruction": "Given an `AI model development` task, retrieve tools that facilitate the creation and training of artificial intelligence models capable of multimodal analysis, specifically those that process images to answer questions regarding their content, by employing advanced frameworks and pre-trained architectures to enhance usability and performance in image-based question-answering scenarios.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')",
                    "api_arguments": "image, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "TextVQA",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_106",
                    "name": "git-large-textvqa"
                },
                "id": "gorilla_huggingface_tool_105",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_24",
        "query": "A product manager needs an explanation related to model conversion. They are confused why it is important. Can you please help them by providing an answer?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing to interpret and provide answers to specific questions based on provided context or information.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))",
                    "api_arguments": {
                        "model_name": "deepset/bert-large-uncased-whole-word-masking-squad2",
                        "tokenizer": "deepset/bert-large-uncased-whole-word-masking-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "QA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "Exact Match": 80.885,
                            "F1": 83.876
                        }
                    },
                    "description": "This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.",
                    "id": "huggingface_api_467",
                    "name": "bert-large-uncased-whole-word-masking-squad2"
                },
                "id": "gorilla_huggingface_tool_463",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_25",
        "query": "I am making a keyword search engine that ranks text passages based on their importance regarding a given keyword.",
        "instruction": "Given a `keyword-based ranking` task, retrieve tools that can process text passages by evaluating their relevance to a specified keyword, and rank them accordingly based on importance and content relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')",
                    "api_arguments": {
                        "padding": "True",
                        "truncation": "True",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForSequenceClassification",
                        "torch": "import torch"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)",
                    "performance": {
                        "dataset": {
                            "TREC Deep Learning 2019": {
                                "NDCG@10": 74.31
                            },
                            "MS Marco Passage Reranking": {
                                "MRR@10": 39.02,
                                "accuracy": "960 Docs / Sec"
                            }
                        }
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco",
                    "id": "huggingface_api_391",
                    "name": "cross-encoder/ms-marco-MiniLM-L-12-v2"
                },
                "id": "gorilla_huggingface_tool_387",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_26",
        "query": "Our company is working on a news website. We want to present summaries of news articles written in French to the users.",
        "instruction": "Given a `news summarization` task, retrieve tools that can summarize content by processing French news articles to provide concise and coherent summaries aligned with the query's linguistic and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "orangeSum",
                        "accuracy": ""
                    },
                    "description": "Barthez model finetuned on orangeSum for abstract generation in French language",
                    "id": "huggingface_api_546",
                    "name": "moussaKam/barthez-orangesum-abstract"
                },
                "id": "gorilla_huggingface_tool_542",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_27",
        "query": "We are developing an app to classify food images. We have a set of images and want to use a pre-trained model for classification.",
        "instruction": "Given an `image classification` task, retrieve tools that leverage pre-trained models for classifying images by processing image inputs and returning classification labels, ensuring the appropriate application of zero-shot or standard classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')",
                    "api_arguments": [
                        "image",
                        "possible_class_names"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "80.1"
                    },
                    "description": "A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.",
                    "id": "huggingface_api_350",
                    "name": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
                },
                "id": "gorilla_huggingface_tool_346",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_28",
        "query": "We have a company that processes loan applications. Give us the answer for a question, based on a document.",
        "instruction": "Given a `document question answering` task, retrieve tools that specialize in extracting answers from documents by processing textual queries and leveraging advanced language models to provide accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3",
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 8.5806
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.",
                    "id": "huggingface_api_140",
                    "name": "tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa"
                },
                "id": "gorilla_huggingface_tool_136",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_29",
        "query": "Our gaming company is looking for a reinforcement learning solution to implement an artificial agent that can play SoccerTwos proficiently.",
        "instruction": "Given a `reinforcement learning implementation` task, retrieve tools that facilitate the training and deployment of artificial agents via reinforcement learning frameworks, specifically for applications like SoccerTwos, by leveraging model training libraries and environment configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Unity ML-Agents Library",
                    "functionality": "Train and play SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-SoccerTwosv2' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "ml-agents"
                    ],
                    "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_908",
                    "name": "poca-SoccerTwosv2"
                },
                "id": "gorilla_huggingface_tool_901",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_30",
        "query": "Design a pipeline to create artistic variations of an input image.",
        "instruction": "Given an `image variation design` task, retrieve tools that process image inputs to generate artistic variations by utilizing stable diffusion techniques and ensure compatibility with specified frameworks and environment requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Variations",
                    "api_call": "StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')",
                    "api_arguments": {
                        "revision": "v2.0"
                    },
                    "python_environment_requirements": "Diffusers >=0.8.0",
                    "example_code": "from diffusers import StableDiffusionImageVariationPipeline\nfrom PIL import Image\ndevice = cuda:0\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n lambdalabs/sd-image-variations-diffusers,\n revision=v2.0,\n)\nsd_pipe = sd_pipe.to(device)\nim = Image.open(path/to/image.jpg)\ntform = transforms.Compose([\n transforms.ToTensor(),\n transforms.Resize(\n  (224, 224),\n  interpolation=transforms.InterpolationMode.BICUBIC,\n  antialias=False,\n ),\n transforms.Normalize(\n  [0.48145466, 0.4578275, 0.40821073],\n  [0.26862954, 0.26130258, 0.27577711]),\n])\ninp = tform(im).to(device).unsqueeze(0)\nout = sd_pipe(inp, guidance_scale=3)\nout[images][0].save(result.jpg)",
                    "performance": {
                        "dataset": "ChristophSchuhmann/improved_aesthetics_6plus",
                        "accuracy": "N/A"
                    },
                    "description": "This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.",
                    "id": "huggingface_api_260",
                    "name": "lambdalabs/sd-image-variations-diffusers"
                },
                "id": "gorilla_huggingface_tool_256",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_31",
        "query": "Write an overview for astronomers about how Jupiter became the largest planet in our solar system.",
        "instruction": "Given an `overview writing` task, retrieve tools that assist in generating comprehensive and informative overviews by utilizing natural language processing capabilities to synthesize information aligned with the topic's focus and the query's needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')",
                    "api_arguments": {
                        "input_ids": "encoding['input_ids']",
                        "attention_mask": "encoding['attention_mask']"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\ntokenizer = AutoTokenizer.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\nmodel = AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\ntext = 'Huggingface has democratized NLP. Huge thanks to Huggingface for this.'\nquestion = 'What has Huggingface done ?'\nencoding = tokenizer(question, text, return_tensors='pt')\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\nstart_scores, end_scores = model(input_ids, attention_mask=attention_mask)\nall_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\nanswer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))",
                    "performance": {
                        "dataset": "squad_v1",
                        "accuracy": {
                            "Exact Match": 85.1466,
                            "F1": 91.5415
                        }
                    },
                    "description": "This is longformer-base-4096 model fine-tuned on SQuAD v1 dataset for question answering task. Longformer model created by Iz Beltagy, Matthew E. Peters, Arman Coha from AllenAI. As the paper explains it, Longformer is a BERT-like model for long documents. The pre-trained model can handle sequences with up to 4096 tokens.",
                    "id": "huggingface_api_473",
                    "name": "valhalla/longformer-base-4096-finetuned-squadv1"
                },
                "id": "gorilla_huggingface_tool_469",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_32",
        "query": "We are building a platform to compare and contrast user input sentences with existing sentences in our database. It should return similar results.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that utilize natural language processing techniques to compare user input sentences with a database, returning results based on semantic similarity.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Automated evaluation"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_705",
                    "name": "sentence-transformers/paraphrase-distilroberta-base-v2"
                },
                "id": "gorilla_huggingface_tool_700",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_33",
        "query": "We are a company manufacturing AI-based toothbrushes for children. We want to analyze the emotion of children while they brush their teeth.",
        "instruction": "Given an `emotion analysis` task in an audio context, retrieve tools that process audio inputs to recognize and classify emotions, focusing on extracting emotional attributes from children's interactions as they brush their teeth.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')",
                    "api_arguments": {
                        "path": "/path/to/russian_audio_speech.wav",
                        "sampling_rate": 16000
                    },
                    "python_environment_requirements": [
                        "torch",
                        "torchaudio",
                        "transformers",
                        "librosa",
                        "numpy"
                    ],
                    "example_code": "result = predict('/path/to/russian_audio_speech.wav', 16000)\nprint(result)",
                    "performance": {
                        "dataset": "Russian Emotional Speech Dialogs",
                        "accuracy": "72%"
                    },
                    "description": "A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.",
                    "id": "huggingface_api_826",
                    "name": "wav2vec2-xlsr-53-russian-emotion-recognition"
                },
                "id": "gorilla_huggingface_tool_821",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_34",
        "query": "Our client has a traffic camera system and wants to detect vehicles in the images captured. Implement a solution.",
        "instruction": "Given an `object detection` task, retrieve tools that process image inputs to identify and locate objects, specifically vehicles in this case, by leveraging models trained for object detection tasks that provide output including bounding boxes, scores, and categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Transformers",
                    "functionality": "Object Detection",
                    "api_call": "yolov5.load('fcakyon/yolov5s-v7.0')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": "import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')",
                    "performance": {
                        "dataset": "detection-datasets/coco",
                        "accuracy": null
                    },
                    "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.",
                    "id": "huggingface_api_226",
                    "name": "fcakyon/yolov5s-v7.0"
                },
                "id": "gorilla_huggingface_tool_222",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_35",
        "query": "We are developing an AI chatbot to interact with users. We need the bot to recognize the user's emotions based on their text input.",
        "instruction": "Given a `emotion recognition` task, retrieve tools that process text inputs to classify and identify emotions, leveraging models capable of analyzing and returning emotion-related insights relevant to the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "classifier(I love this!)",
                    "performance": {
                        "dataset": [
                            "Crowdflower (2016)",
                            "Emotion Dataset, Elvis et al. (2018)",
                            "GoEmotions, Demszky et al. (2020)",
                            "ISEAR, Vikash (2018)",
                            "MELD, Poria et al. (2019)",
                            "SemEval-2018, EI-reg, Mohammad et al. (2018)",
                            "Emotion Lines (Friends)"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.",
                    "id": "huggingface_api_400",
                    "name": "michellejieli/emotion_text_classifier"
                },
                "id": "gorilla_huggingface_tool_396",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_36",
        "query": "Design a model to classify the following image: a city park with a playground and a lake, surrounded by trees and skyscrapers.",
        "instruction": "Given an `image classification` task, retrieve tools that perform zero-shot image classification by processing image inputs and using pre-defined categories to assign the most relevant labels, specifically within a city park context.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')",
                    "api_arguments": {
                        "text": [
                            "a photo of a residential area",
                            "a photo of a playground",
                            "a photo of a stadium",
                            "a photo of a forest",
                            "a photo of an airport"
                        ],
                        "images": "image",
                        "return_tensors": "pt",
                        "padding": "True"
                    },
                    "python_environment_requirements": [
                        "PIL",
                        "requests",
                        "transformers"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nlabels = [residential area, playground, stadium, forest, airport]\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nfor l, p in zip(labels, probs[0]):\n print(f{l:&lt;16} {p:.4f})",
                    "performance": {
                        "dataset": {
                            "RSICD": {
                                "original CLIP": {
                                    "k=1": 0.572,
                                    "k=3": 0.745,
                                    "k=5": 0.837,
                                    "k=10": 0.939
                                },
                                "clip-rsicd-v2 (this model)": {
                                    "k=1": 0.883,
                                    "k=3": 0.968,
                                    "k=5": 0.982,
                                    "k=10": 0.998
                                }
                            }
                        }
                    },
                    "description": "This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.",
                    "id": "huggingface_api_360",
                    "name": "flax-community/clip-rsicd-v2"
                },
                "id": "gorilla_huggingface_tool_356",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_37",
        "query": "There's a collection of texts that we want to be able to analyze for their part-of-speech tags to better understand the structure of the sentences within the texts.",
        "instruction": "Given a `text analysis` task, retrieve tools that perform part-of-speech tagging to analyze the syntactic structure of text by processing sentence inputs and returning tagged data to facilitate further linguistic understanding.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Part-of-Speech Tagging",
                    "api_call": "SequenceTagger.load('flair/pos-english')",
                    "api_arguments": "sentence",
                    "python_environment_requirements": "flair (pip install flair)",
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger.load(flair/pos-english)\n\nsentence = Sentence(I love Berlin.)\n\ntagger.predict(sentence)\n\nprint(sentence)\n\nfor entity in sentence.get_spans('pos'):\n print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "98.19"
                    },
                    "description": "This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_425",
                    "name": "flair/pos-english"
                },
                "id": "gorilla_huggingface_tool_421",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_38",
        "query": "We are developing a program to teach French to English speakers. The program should complete a sentence with a missing word in French.",
        "instruction": "Given a `language learning assistance` task, retrieve tools that utilize natural language processing to complete sentences in French by identifying and suggesting suitable words for fill-mask tasks, designed to enhance language learning for English speakers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')",
                    "api_arguments": [
                        "model",
                        "tokenizer"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')",
                    "performance": {
                        "dataset": "oscar",
                        "accuracy": "N/A"
                    },
                    "description": "CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.",
                    "id": "huggingface_api_669",
                    "name": "camembert-base"
                },
                "id": "gorilla_huggingface_tool_664",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_39",
        "query": "Create a poem about nature for a school assignment.",
        "instruction": "Given a `poem creation` task, retrieve tools that specialize in generating creative text, specifically aimed at crafting poems, by processing input prompts related to the query's topic of nature to produce coherent and expressive outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')",
                    "api_arguments": {
                        "model": "sshleifer/tiny-gpt2"
                    },
                    "python_environment_requirements": {
                        "huggingface_transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\nresult = nlp('Once upon a time')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.",
                    "id": "huggingface_api_622",
                    "name": "sshleifer/tiny-gpt2"
                },
                "id": "gorilla_huggingface_tool_617",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_40",
        "query": "We're working with a voiceover company, and they're looking for a tool to help them change the voice style of voiceovers while keeping the same content.",
        "instruction": "Given a `voice style transformation` task, retrieve tools that specialize in modifying voice attributes while maintaining the original content by processing audio inputs and utilizing advanced speech-to-speech conversion techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')",
                    "api_arguments": {
                        "audio": "example_speech",
                        "sampling_rate": "sampling_rate",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "numpy",
                        "torch",
                        "soundfile"
                    ],
                    "example_code": "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\ndataset = dataset.sort('id')\nsampling_rate = dataset.features['audio'].sampling_rate\nexample_speech = dataset[0]['audio']['array']\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nimport numpy as np\nimport torch\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\nimport soundfile as sf\nsf.write('speech.wav', speech.numpy(), samplerate=16000)",
                    "performance": {
                        "dataset": "CMU ARCTIC",
                        "accuracy": "Not specified"
                    },
                    "description": "SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.",
                    "id": "huggingface_api_784",
                    "name": "microsoft/speecht5_vc"
                },
                "id": "gorilla_huggingface_tool_779",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_41",
        "query": "Our team wants to create a new app for autonomous vehicles. For that, we need to estimate the depth of the field from images.",
        "instruction": "Given a `depth estimation` task, retrieve tools that facilitate the analysis of images to estimate the depth of the field, specifically for use cases like autonomous vehicle applications, leveraging computer vision models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3533,
                            "Mae": 0.2668,
                            "Rmse": 0.3716,
                            "Abs Rel": 0.3427,
                            "Log Mae": 0.1167,
                            "Log Rmse": 0.1703,
                            "Delta1": 0.5522,
                            "Delta2": 0.8362,
                            "Delta3": 0.9382
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.",
                    "id": "huggingface_api_163",
                    "name": "glpn-nyu-finetuned-diode-221121-063504"
                },
                "id": "gorilla_huggingface_tool_159",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_42",
        "query": "Develop an AI character that can play the SoccerTwos game with advanced strategies.",
        "instruction": "Given a `AI character development` task for a specific game, retrieve tools that harness machine learning and reinforcement learning frameworks to design and implement advanced strategies for gameplay, ensuring effective interaction within the game's environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "ML-Agents",
                    "functionality": "SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "unity-ml-agents",
                        "deep-reinforcement-learning",
                        "ML-Agents-SoccerTwos"
                    ],
                    "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ðŸ‘€",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_907",
                    "name": "Raiden-1001/poca-Soccerv7"
                },
                "id": "gorilla_huggingface_tool_900",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_43",
        "query": "I am developing software that needs to retrieve relevant information from a collection of documents based on a user's query.",
        "instruction": "Given an `information retrieval` task, retrieve tools that process user queries to extract relevant information from document collections efficiently, utilizing natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                    "api_arguments": {
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('model_name')",
                        "features": "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": {
                        "import": "from transformers import AutoTokenizer, AutoModelForSequenceClassification",
                        "model": "model = AutoModelForSequenceClassification.from_pretrained('model_name')",
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('model_name')",
                        "features": "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')",
                        "scores": "with torch.no_grad():\n    scores = model(**features).logits\n    print(scores)"
                    },
                    "performance": {
                        "dataset": "TREC Deep Learning 2019",
                        "accuracy": "69.84 (NDCG@10)"
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.",
                    "id": "huggingface_api_401",
                    "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2"
                },
                "id": "gorilla_huggingface_tool_397",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_44",
        "query": "In order to have a better understanding of our clients, I'd like to identify the names of people and organizations mentioned in the following customer review.",
        "instruction": "Given a `named entity recognition` task, retrieve tools proficient in natural language processing to identify and classify entities such as names of people and organizations within text data by leveraging advanced models designed for token classification and entity recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')",
                        "tokenizer": "AutoTokenizer.from_pretrained('dslim/bert-large-NER')"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.1"
                    },
                    "example_code": {
                        "example": "My name is Wolfgang and I live in Berlin",
                        "ner_results": "nlp(example)"
                    },
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": {
                            "f1": 0.92,
                            "precision": 0.92,
                            "recall": 0.919
                        }
                    },
                    "description": "bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
                    "id": "huggingface_api_412",
                    "name": "dslim/bert-large-NER"
                },
                "id": "gorilla_huggingface_tool_408",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_45",
        "query": "We are a content marketing agency and we are focusing on promoting our clients' products. We want to use a tool to generate interesting marketing messages.",
        "instruction": "Given a `marketing message generation` task, retrieve tools that support natural language processing to generate compelling and creative marketing content by processing text inputs, utilizing advanced models such as BART for effective text generation and manipulation aligned with the query's goals.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('facebook/bart-large')",
                    "api_arguments": {
                        "pretrained_model_name": "facebook/bart-large"
                    },
                    "python_environment_requirements": {
                        "library": "transformers",
                        "version": "latest"
                    },
                    "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "Not provided"
                    },
                    "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).",
                    "id": "huggingface_api_5",
                    "name": "facebook/bart-large"
                },
                "id": "gorilla_huggingface_tool_5",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_46",
        "query": "A travel agency needs a system to predict whether a client's vacation will be successful based on their chosen destination, accommodation, and travel style. We want to provide suggestions for clients who may need additional support.",
        "instruction": "Given a `vacation success prediction` task, retrieve tools that perform binary classification using tabular data related to vacation attributes such as destination, accommodation, and travel style to determine the likelihood of a successful trip and provide suggestions for additional support.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Binary Classification",
                    "api_call": "load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))",
                    "api_arguments": [
                        "new_data"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas",
                        "numpy",
                        "tensorflow"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nREPO_ID = 'danupurnomo/dummy-titanic'\nPIPELINE_FILENAME = 'final_pipeline.pkl'\nTF_FILENAME = 'titanic_model.h5'\nmodel_pipeline = joblib.load(cached_download(\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\n))\nmodel_seq = load_model(cached_download(\n hf_hub_url(REPO_ID, TF_FILENAME)\n))",
                    "performance": {
                        "dataset": "Titanic",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.",
                    "id": "huggingface_api_851",
                    "name": "danupurnomo/dummy-titanic"
                },
                "id": "gorilla_huggingface_tool_845",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_47",
        "query": "We are building a location recommendation system that identifies possible locations for new stores based on images from potential locations. Use the StreetCLIP model to generate probabilities for various cities.",
        "instruction": "Given a `location recommendation` task, retrieve tools that leverage image geolocalization capabilities to analyze images from potential store locations, using models like StreetCLIP, to generate location probabilities for various cities.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Geolocalization",
                    "api_call": "CLIPModel.from_pretrained('geolocal/StreetCLIP')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "geolocal/StreetCLIP"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "IM2GPS",
                                "accuracy": {
                                    "25km": 28.3,
                                    "200km": 45.1,
                                    "750km": 74.7,
                                    "2500km": 88.2
                                }
                            },
                            {
                                "name": "IM2GPS3K",
                                "accuracy": {
                                    "25km": 22.4,
                                    "200km": 37.4,
                                    "750km": 61.3,
                                    "2500km": 80.4
                                }
                            }
                        ]
                    },
                    "description": "StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.",
                    "id": "huggingface_api_374",
                    "name": "geolocal/StreetCLIP"
                },
                "id": "gorilla_huggingface_tool_370",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_48",
        "query": "We are developing an app that can help mediate virtual therapy sessions. We need a feature that can detect emotions in the voice of the user.",
        "instruction": "Given an `emotion detection` task, retrieve tools that analyze voice inputs to identify and classify emotions, specifically leveraging advanced audio processing capabilities to assist in virtual therapy sessions.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')",
                    "api_arguments": {
                        "audio_file": "string"
                    },
                    "python_environment_requirements": [
                        "audio_models",
                        "transformers",
                        "torch",
                        "numpy",
                        "pydub"
                    ],
                    "example_code": "def predict_emotion_hubert(audio_file):\n from audio_models import HubertForSpeechClassification\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\n import torch.nn.functional as F\n import torch\n import numpy as np\n from pydub import AudioSegment\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\nsampling_rate=16000\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\ndef speech_file_to_array(path, sampling_rate):\n sound = AudioSegment.from_file(path)\n sound = sound.set_frame_rate(sampling_rate)\n sound_array = np.array(sound.get_array_of_samples())\n return sound_array\nsound_array = speech_file_to_array(audio_file, sampling_rate)\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\nwith torch.no_grad():\n logits = model(**inputs).logits\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\noutputs = [{\n emo: config.id2label[i],\n score: round(score * 100, 1)}\n for i, score in enumerate(scores)\n]\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\nresult = predict_emotion_hubert(male-crying.mp3)\nresult",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "A pretrained model for predicting emotion in local audio files using Hubert.",
                    "id": "huggingface_api_811",
                    "name": "Rajaram1996/Hubert_emotion"
                },
                "id": "gorilla_huggingface_tool_806",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_49",
        "query": "I am developing a news analysis platform. I need to predict the named entities from the articles.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform entity extraction by processing text inputs to identify and label named entities within the content of news articles.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "pipeline('ner')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "nlp('My name is John and I live in New York.')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.",
                    "id": "huggingface_api_928",
                    "name": "dslim/bert-base-NER-uncased"
                },
                "id": "gorilla_huggingface_tool_405",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_50",
        "query": "Explain how to use GPT-3 to create a slogan for an e-commerce website that sells eco-friendly products.",
        "instruction": "Given a `slogan creation` task, retrieve tools that assist in generating creative text content by leveraging language models like GPT-3 to produce slogans tailored for specific themes or industries, such as eco-friendly e-commerce websites.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Synthesis",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').",
                    "api_arguments": "None",
                    "python_environment_requirements": "diffusers",
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-cifar10-32\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception_score": 9.46,
                            "FID_score": 3.17
                        }
                    },
                    "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.",
                    "id": "huggingface_api_286",
                    "name": "google/ddpm-cifar10-32"
                },
                "id": "gorilla_huggingface_tool_282",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_51",
        "query": "We are creating an analysis platform for Counter-Strike: Global Offensive. Detect and locate players in the given image.",
        "instruction": "Given an `object detection` task, retrieve tools that process image inputs to identify and locate specific objects, such as players, within the content and provide precise detection results aligned with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                    "api_arguments": {
                        "image": "URL or local path to image"
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "csgo-object-detection",
                        "accuracy": 0.844
                    },
                    "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].",
                    "id": "huggingface_api_229",
                    "name": "keremberke/yolov8n-csgo-player-detection"
                },
                "id": "gorilla_huggingface_tool_225",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_52",
        "query": "I am reading a book and, whenever I engage with an interesting topic, I write a short sentence summarizing that topic. I would like to have an assistant that, given a sentence, automatically classifies that topic among 'technology', 'literature', and 'science'.",
        "instruction": "Given a `topic classification` task, retrieve tools that automatically categorize text based on its content into predefined categories such as 'technology', 'literature', and 'science' using natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')",
                    "api_arguments": [
                        "sent",
                        "candidate_labels"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\nsent = 'Apple just announced the newest iPhone X'\ncandidate_labels = ['technology', 'sports', 'politics']\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.64",
                            "MNLI_mismatched": "87.77"
                        }
                    },
                    "description": "This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.",
                    "id": "huggingface_api_513",
                    "name": "cross-encoder/nli-deberta-v3-xsmall"
                },
                "id": "gorilla_huggingface_tool_509",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_53",
        "query": "Your company is developing a chatbot and requires accurate summaries of lengthy dialogues without losing context.",
        "instruction": "Given a `dialogue summarization` task, retrieve tools that specialize in processing long dialogue inputs to generate concise and coherent summaries without losing context. These tools should leverage advanced natural language processing architectures to handle extensive dialogue data effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "2109.02492"
                    },
                    "description": "DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.",
                    "id": "huggingface_api_642",
                    "name": "DialogLED-base-16384"
                },
                "id": "gorilla_huggingface_tool_637",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_54",
        "query": "I am an interior designer and want to showcase a modern living room with a fireplace and a large window overlooking a forest. Create an image according to this description.",
        "instruction": "Given an `image generation` task, retrieve tools that create images based on detailed text descriptions by processing the textual inputs to generate visuals that align with the specified features and style in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image generation and modification based on text prompts",
                    "api_call": "StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')",
                    "api_arguments": [
                        "prompt",
                        "image",
                        "mask_image"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\n\npipe.to(cuda)\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(./yellow_cat_on_park_bench.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.",
                    "id": "huggingface_api_38",
                    "name": "stabilityai/stable-diffusion-2-inpainting"
                },
                "id": "gorilla_huggingface_tool_38",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_55",
        "query": "Design an AI algorithm to answer questions from scanned documents.",
        "instruction": "Given a `document question answering` task, retrieve tools that leverage AI algorithms to process multimodal inputs, specifically scanned documents, to generate accurate responses to the questions related to the content of the documents.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": "4.12.2",
                        "pytorch": "1.8.0+cu101",
                        "datasets": "1.14.0",
                        "tokenizers": "0.10.3"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 2.087
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_131",
                    "name": "layoutlmv2-base-uncased-finetuned-infovqa"
                },
                "id": "gorilla_huggingface_tool_130",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_56",
        "query": "###Instruction:",
        "instruction": "Given a `audio separation` task, retrieve tools that process audio inputs to isolate and separate different audio sources within a single audio file, enhancing clarity and individual sound component identification.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')",
                    "api_arguments": "audio",
                    "python_environment_requirements": "Asteroid",
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_786",
                    "name": "Awais/Audio_Source_Separation"
                },
                "id": "gorilla_huggingface_tool_781",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_57",
        "query": "As the project manager of a company who receives long emails, my goal is to summarize them and extract the most important information.",
        "instruction": "Given a `summarization` task, retrieve tools that can process lengthy text inputs to produce concise summaries by extracting the most significant information, effectively meeting the query's objective of information distillation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_summarizer')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "pip install sentencepiece"
                    ],
                    "example_code": "context = \nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play.You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England's score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\n\nget_response(context)",
                    "performance": {
                        "dataset": "cnn_dailymail",
                        "accuracy": {
                            "ROUGE-1": 36.604,
                            "ROUGE-2": 14.64,
                            "ROUGE-L": 23.884,
                            "ROUGE-LSUM": 32.902,
                            "loss": 2.576,
                            "gen_len": 76.398
                        }
                    },
                    "description": "PEGASUS fine-tuned for summarization",
                    "id": "huggingface_api_574",
                    "name": "tuner007/pegasus_summarizer"
                },
                "id": "gorilla_huggingface_tool_570",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_58",
        "query": "We are building an AI-based camera system to ensure safety on a construction site. Detect workers wearing hard hats in a given image.",
        "instruction": "Given an `object detection` task, retrieve tools that analyze images to identify specific objects, such as safety equipment, by processing image inputs and returning detected objects and their attributes, ensuring compliance and safety in specified environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-hard-hat-detection')",
                    "api_arguments": {
                        "image": "URL or local path to the image"
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "hard-hat-detection",
                        "accuracy": 0.811
                    },
                    "description": "A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.",
                    "id": "huggingface_api_213",
                    "name": "keremberke/yolov8m-hard-hat-detection"
                },
                "id": "gorilla_huggingface_tool_209",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_59",
        "query": "How do I separate the speakers from an audio file using the pre-trained ConvTasNet_Libri2Mix_sepclean_8k model from Hugging Face?",
        "instruction": "Given a `speaker separation` task, retrieve tools that utilize pre-trained models to process audio files, specifically to distinguish and separate individual speakers, in alignment with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_60",
        "query": "I'm developing a game that needs more Minecraft skins for some characters. How can I generete them with this model?",
        "instruction": "Given an `image generation` task for character customization, retrieve tools that utilize diffusion models to generate Minecraft skin images by processing model-specific parameters and techniques to create unique visual content aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Diffusers",
                    "api_call": "DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\nimage = pipeline().images[0].convert('RGBA')\nimage",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unconditional image generation model for generating Minecraft skin images using the diffusion model.",
                    "id": "huggingface_api_302",
                    "name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2"
                },
                "id": "gorilla_huggingface_tool_298",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_61",
        "query": "An educational software company needs an advanced method to separate voices from background noise. Develop a solution.",
        "instruction": "Given a `voice separation` task, retrieve tools that employ advanced audio processing techniques to effectively isolate vocal tracks from background noise by utilizing specific models or algorithms tailored for this purpose.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_62",
        "query": "We want to build a product to classify images of pets into different categories.",
        "instruction": "Given an `image classification` task, retrieve tools that efficiently categorize images by processing visual inputs and assigning them to predefined categories, particularly in a zero-shot learning context aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('openai/clip-vit-large-patch14')",
                    "api_arguments": {
                        "text": [
                            "a photo of a cat",
                            "a photo of a dog"
                        ],
                        "images": "image",
                        "return_tensors": "pt",
                        "padding": "True"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "PIL",
                            "requests",
                            "transformers"
                        ]
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            "Food101",
                            "CIFAR10",
                            "CIFAR100",
                            "Birdsnap",
                            "SUN397",
                            "Stanford Cars",
                            "FGVC Aircraft",
                            "VOC2007",
                            "DTD",
                            "Oxford-IIIT Pet dataset",
                            "Caltech101",
                            "Flowers102",
                            "MNIST",
                            "SVHN",
                            "IIIT5K",
                            "Hateful Memes",
                            "SST-2",
                            "UCF101",
                            "Kinetics700",
                            "Country211",
                            "CLEVR Counting",
                            "KITTI Distance",
                            "STL-10",
                            "RareAct",
                            "Flickr30",
                            "MSCOCO",
                            "ImageNet",
                            "ImageNet-A",
                            "ImageNet-R",
                            "ImageNet Sketch",
                            "ObjectNet (ImageNet Overlap)",
                            "Youtube-BB",
                            "ImageNet-Vid"
                        ],
                        "accuracy": "varies depending on the dataset"
                    },
                    "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.",
                    "id": "huggingface_api_347",
                    "name": "openai/clip-vit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_343",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_63",
        "query": "A company is having an ad campaign with a superhero theme. They want to generate a video of Spiderman surfing to showcase as a social media ad.",
        "instruction": "Given a `video creation` task with a thematic and character-based focus, retrieve tools that leverage text-to-video synthesis capabilities by processing descriptive text inputs to generate video content consistent with the campaign's superhero theme.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video Synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate"
                    ],
                    "example_code": "import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": [
                            "LAION5B",
                            "ImageNet",
                            "Webvid"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.",
                    "id": "huggingface_api_94",
                    "name": "damo-vilab/text-to-video-ms-1.7b-legacy"
                },
                "id": "gorilla_huggingface_tool_94",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_64",
        "query": "We want to evaluate the carbon footprint of a construction project based on the tabular data of material consumption.",
        "instruction": "Given a `carbon footprint evaluation` task, retrieve tools that analyze tabular data to predict or assess the carbon emissions of construction projects, employing models designed for tabular data inputs and capable of providing emission estimations.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Validation Metrics",
                        "accuracy": 0.831
                    },
                    "description": "A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.",
                    "id": "huggingface_api_863",
                    "name": "tejas23/autotrain-amx2-1702259728"
                },
                "id": "gorilla_huggingface_tool_856",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_65",
        "query": "Find a model that can be used to predict the properties of molecules based on their graph representations.",
        "instruction": "Given a `molecule property prediction` task, retrieve tools that utilize graph-based machine learning models capable of analyzing molecular graph representations to predict molecular properties, ensuring compatibility with the specified framework and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Graph Machine Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('graphormer-base-pcqm4mv1')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "See the Graph Classification with Transformers tutorial",
                    "performance": {
                        "dataset": "PCQM4M-LSC",
                        "accuracy": "1st place on the KDD CUP 2021 (quantum prediction track)"
                    },
                    "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.",
                    "id": "huggingface_api_142",
                    "name": "graphormer-base-pcqm4mv1"
                },
                "id": "gorilla_huggingface_tool_138",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_66",
        "query": "Develop a voice command security system that distinguishes between specific command phrases like \"disarm security\" or \"activate alarm\".",
        "instruction": "Given a `voice command recognition` task, retrieve tools that specialize in audio classification to accurately process distinct spoken command phrases and identify specific commands such as \"disarm security\" and \"activate alarm\" based on their audio characteristics.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='superb/hubert-base-superb-ks')",
                    "api_arguments": [
                        "file",
                        "top_k"
                    ],
                    "python_environment_requirements": [
                        "datasets",
                        "transformers",
                        "torchaudio"
                    ],
                    "example_code": "from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-ks)\nlabels = classifier(dataset[0][file], top_k=5)",
                    "performance": {
                        "dataset": "Speech Commands dataset v1.0",
                        "accuracy": 0.9672
                    },
                    "description": "This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_806",
                    "name": "superb/hubert-base-superb-ks"
                },
                "id": "gorilla_huggingface_tool_801",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_67",
        "query": "Write a summary of a conference held by the World Health Organization discussing the impacts of climate change on human health.",
        "instruction": "Given a `text summarization` task, retrieve tools that can process detailed textual content to produce concise and coherent summaries, reflecting the core aspects and findings of the discussed topic, using models designed for abstractive summarization.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-xsum')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": {
                                    "ROUGE-1": 46.862,
                                    "ROUGE-2": 24.453,
                                    "ROUGE-L": 39.055,
                                    "ROUGE-LSUM": 39.099
                                }
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": {
                                    "ROUGE-1": 22.206,
                                    "ROUGE-2": 7.67,
                                    "ROUGE-L": 15.405,
                                    "ROUGE-LSUM": 19.218
                                }
                            },
                            {
                                "name": "samsum",
                                "accuracy": {
                                    "ROUGE-1": 21.81,
                                    "ROUGE-2": 4.253,
                                    "ROUGE-L": 17.447,
                                    "ROUGE-LSUM": 18.891
                                }
                            }
                        ]
                    },
                    "description": "PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.",
                    "id": "huggingface_api_554",
                    "name": "google/pegasus-xsum"
                },
                "id": "gorilla_huggingface_tool_550",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_68",
        "query": "I am writing an article on the history of technology companies, and I want to extract the names of companies and people mentioned in the text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that analyze text to identify and extract entities such as names of companies and people, based on the content and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                        "tokenizer": "AutoTokenizer.from_pretrained('Jean-Baptiste/roberta-large-ner-english')",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "nlp(Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": {
                            "PER": {
                                "precision": 0.9914,
                                "recall": 0.9927,
                                "f1": 0.992
                            },
                            "ORG": {
                                "precision": 0.9627,
                                "recall": 0.9661,
                                "f1": 0.9644
                            },
                            "LOC": {
                                "precision": 0.9795,
                                "recall": 0.9862,
                                "f1": 0.9828
                            },
                            "MISC": {
                                "precision": 0.9292,
                                "recall": 0.9262,
                                "f1": 0.9277
                            },
                            "Overall": {
                                "precision": 0.974,
                                "recall": 0.9766,
                                "f1": 0.9753
                            }
                        }
                    },
                    "description": "roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case.",
                    "id": "huggingface_api_416",
                    "name": "Jean-Baptiste/roberta-large-ner-english"
                },
                "id": "gorilla_huggingface_tool_412",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_69",
        "query": "We need a quick summary of a news article we found online. Can you help us with that?",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise and coherent summaries, effectively capturing the main points and essential information of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_70",
        "query": "I am developing a home security software which can detect intruders entering the house. In case any door or object is tampered, the application will ask, \"Who entered the room?\", to assist quick analysis of the CCTV recordings.",
        "instruction": "Given a `home security visual query` task, retrieve tools that enable visual question answering by processing visual inputs and questions to provide relevant answers or insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')",
                    "api_arguments": {
                        "raw_image": "RGB image",
                        "question": "string"
                    },
                    "python_environment_requirements": {
                        "transformers": "BlipProcessor, BlipForQuestionAnswering"
                    },
                    "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = how many dogs are in the picture?\ninputs = processor(raw_image, question, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "VQA",
                        "accuracy": "+1.6% in VQA score"
                    },
                    "description": "BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.",
                    "id": "huggingface_api_105",
                    "name": "Salesforce/blip-vqa-capfilt-large"
                },
                "id": "gorilla_huggingface_tool_104",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_71",
        "query": "Calculate the distance between the objects in an image for an autonomous vehicle driving in a parking lot.",
        "instruction": "Given a `depth estimation for autonomous vehicles` task, retrieve tools that perform computer vision analysis by processing image inputs to calculate object distances, leveraging models designed for depth estimation tasks in automotive environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3497,
                            "Mae": 0.2847,
                            "Rmse": 0.3977,
                            "Abs Rel": 0.3477,
                            "Log Mae": 0.1203,
                            "Log Rmse": 0.1726,
                            "Delta1": 0.5217,
                            "Delta2": 0.8246,
                            "Delta3": 0.9436
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.",
                    "id": "huggingface_api_169",
                    "name": "glpn-kitti-finetuned-diode-221214-123047"
                },
                "id": "gorilla_huggingface_tool_165",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_72",
        "query": "Our company is an environmental consultancy firm. Determine the carbon emissions of different facilities based on the provided data.",
        "instruction": "Given a `carbon emissions estimation` task, retrieve tools that can process environmental data inputs using tabular regression models to predict carbon emissions of different facilities, aiming for accuracy and alignment with the provided data parameters.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "kochetkovIT/autotrain-data-ironhack",
                        "accuracy": {
                            "Loss": 2.603,
                            "R2": 0.013,
                            "MSE": 6.776,
                            "MAE": 1.666,
                            "RMSLE": 0.502
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.",
                    "id": "huggingface_api_870",
                    "name": "kochetkovIT/autotrain-ironhack-49741119788"
                },
                "id": "gorilla_huggingface_tool_863",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_73",
        "query": "We need to create a dialogue in Russian for our educational app. It should cover a general greeting and asking about the users' well-being.",
        "instruction": "Given a `conversational dialogue creation` task, retrieve tools that generate and process dialogue content in the specified language (Russian) for applications by utilizing pre-trained language models designed to produce coherent and contextually relevant exchanges.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "tinkoff-ai/ruDialoGPT-medium"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\nmodel = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\ninputs = tokenizer('@@ÐŸÐ•Ð Ð’Ð«Ð™@@ Ð¿Ñ€Ð¸Ð²ÐµÑ‚ @@Ð’Ð¢ÐžÐ ÐžÐ™@@ Ð¿Ñ€Ð¸Ð²ÐµÑ‚ @@ÐŸÐ•Ð Ð’Ð«Ð™@@ ÐºÐ°Ðº Ð´ÐµÐ»Ð°? @@Ð’Ð¢ÐžÐ ÐžÐ™@@', return_tensors='pt')\ngenerated_token_ids = model.generate(\n **inputs,\n top_k=10,\n top_p=0.95,\n num_beams=3,\n num_return_sequences=3,\n do_sample=True,\n no_repeat_ngram_size=2,\n temperature=1.2,\n repetition_penalty=1.2,\n length_penalty=1.0,\n eos_token_id=50257,\n max_new_tokens=40\n)\ncontext_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\ncontext_with_response",
                    "performance": {
                        "dataset": "Private Validation Set",
                        "sensibleness": 0.78,
                        "specificity": 0.69,
                        "SSA": 0.735
                    },
                    "description": "This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents. The model was trained with context size 3.",
                    "id": "huggingface_api_599",
                    "name": "tinkoff-ai/ruDialoGPT-medium"
                },
                "id": "gorilla_huggingface_tool_594",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_74",
        "query": "Extract entities from a provided sentence mentioning various companies and their CEOs.",
        "instruction": "Given an `entity extraction` task, retrieve tools that specialize in processing text inputs to identify and extract named entities, such as companies and their CEOs, from sentences based on natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification",
                        "tokenizer": "AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-name_all",
                        "accuracy": 0.9989316041363876
                    },
                    "description": "This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.",
                    "id": "huggingface_api_413",
                    "name": "904029577"
                },
                "id": "gorilla_huggingface_tool_409",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_75",
        "query": "Our business is growing rapidly, and we've received an increasing number of questions related to product images. We need the model to provide answers based on images.",
        "instruction": "Given a `visual question answering` task, retrieve tools that process visual inputs to provide answers derived from product images, ensuring that the tools are capable of interpreting images in conjunction with textual queries to address the increasing number of image-related questions.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')",
                    "api_arguments": {
                        "model": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "tokenizer": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "vqa(image_path, question)",
                    "performance": {
                        "dataset": "JosephusCheung/GuanacoVQADataset",
                        "accuracy": "unknown"
                    },
                    "description": "A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.",
                    "id": "huggingface_api_113",
                    "name": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                },
                "id": "gorilla_huggingface_tool_112",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_76",
        "query": "We need a tool to help us generate textual descriptions for images and videos related to our product.",
        "instruction": "Given an `image and video captioning` task, retrieve tools that generate descriptive text by processing visual inputs, such as images or videos, to produce accurate and contextually relevant textual descriptions associated with the products depicted.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')",
                    "api_arguments": "image, text",
                    "python_environment_requirements": "transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "TextCaps",
                        "accuracy": "Refer to the paper"
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_84",
                    "name": "git-large-textcaps"
                },
                "id": "gorilla_huggingface_tool_84",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_77",
        "query": "Our school needs to create fill-in-the-blank quizzes for students. Is it possible to generate a fill-in-the-blank question from the following sentence: \"The cat chased the mouse and then climbed the tree.\"",
        "instruction": "Given a `fill-in-the-blank generation` task, retrieve tools that create fill-in-the-blank questions by processing sentences and providing options for missing words or phrases aligned with the educational context and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')",
                    "api_arguments": {
                        "model_name_or_path": "microsoft/deberta-v2-xxlarge"
                    },
                    "python_environment_requirements": {
                        "pip_install": [
                            "datasets",
                            "deepspeed"
                        ]
                    },
                    "example_code": "python -m torch.distributed.launch --nproc_per_node=${num_gpus} run_glue.py --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME --do_train --do_eval --max_seq_length 256 --per_device_train_batch_size ${batch_size} --learning_rate 3e-6 --num_train_epochs 3 --output_dir $output_dir --overwrite_output_dir --logging_steps 10 --logging_dir $output_dir --deepspeed ds_config.json",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQuAD 1.1",
                                "accuracy": "F1/EM: 96.1/91.4"
                            },
                            {
                                "name": "SQuAD 2.0",
                                "accuracy": "F1/EM: 92.2/89.7"
                            },
                            {
                                "name": "MNLI-m/mm",
                                "accuracy": "Acc: 91.7/91.9"
                            },
                            {
                                "name": "SST-2",
                                "accuracy": "Acc: 97.2"
                            },
                            {
                                "name": "QNLI",
                                "accuracy": "Acc: 96.0"
                            },
                            {
                                "name": "CoLA",
                                "accuracy": "MCC: 72.0"
                            },
                            {
                                "name": "RTE",
                                "accuracy": "Acc: 93.5"
                            },
                            {
                                "name": "MRPC",
                                "accuracy": "Acc/F1: 93.1/94.9"
                            },
                            {
                                "name": "QQP",
                                "accuracy": "Acc/F1: 92.7/90.3"
                            },
                            {
                                "name": "STS-B",
                                "accuracy": "P/S: 93.2/93.1"
                            }
                        ]
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data.",
                    "id": "huggingface_api_686",
                    "name": "microsoft/deberta-v2-xxlarge"
                },
                "id": "gorilla_huggingface_tool_681",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_78",
        "query": "Make a summary video for our last team meeting. The audio from the video must identify who is speaking and when.",
        "instruction": "Given a `video summary creation` task, retrieve tools with audio analysis capabilities to identify speakers and their speaking times during a meeting, then generate a concise video summary reflecting the discussions accurately and efficiently.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker diarization",
                    "api_call": "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": {
                        "load_pipeline": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "apply_pipeline": "diarization = pipeline(audio.wav)",
                        "save_output": "with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)"
                    },
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.",
                    "id": "huggingface_api_846",
                    "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_840",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_79",
        "query": "Our robotics team needs a way to stabilize a two-wheeled self-balancing robot. Would you suggest and apply any RL algorithm for this purpose?",
        "instruction": "Given a `robot stabilization` task, retrieve tools that apply reinforcement learning algorithms to balance robotic systems by utilizing frameworks like Stable-Baselines3 for effective self-balancing control and support continuous learning and adaptation.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_901",
                    "name": "sb3/ppo-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_894",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_80",
        "query": "I am working for the review section of a book company. I want to convert a book summary into a positive book review.",
        "instruction": "Given a `text conversion` task, retrieve tools that perform sentiment analysis and text generation by processing a book summary to transform it into a positively embellished book review.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('t5-3b')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.",
                    "id": "huggingface_api_535",
                    "name": "t5-3b"
                },
                "id": "gorilla_huggingface_tool_531",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_81",
        "query": "A business collaboration project requires staff to translate Russian documents into English.",
        "instruction": "Given a `document translation` task, retrieve tools that facilitate the process of translating text from one language to another, specifically from Russian to English, by employing advanced natural language processing models to deliver accurate translations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-ru-en')",
                    "api_arguments": {
                        "from_pretrained": "Helsinki-NLP/opus-mt-ru-en"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForSeq2SeqLM"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Helsinki-NLP/opus-mt-ru-en)",
                    "performance": {
                        "dataset": "newstest2019-ruen.ru.en",
                        "accuracy": 31.4
                    },
                    "description": "A Russian to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Transformer-align architecture and trained on the OPUS dataset. The model can be used for translation and text-to-text generation tasks.",
                    "id": "huggingface_api_527",
                    "name": "Helsinki-NLP/opus-mt-ru-en"
                },
                "id": "gorilla_huggingface_tool_523",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_82",
        "query": "We'd like our chatbot to act as a fictional character for engaging with our users.",
        "instruction": "Given a `conversational character simulation` task, retrieve tools that facilitate text generation for dialogue by processing inputs that define a character's persona and dialogue history to engage users interactively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')",
                    "api_arguments": [
                        "input_ids",
                        "max_length",
                        "num_return_sequences"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained('waifu-workshop/pygmalion-6b')\nmodel = AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\n\ninput_text = [CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\n<START>\\n[DIALOGUE HISTORY]\\nYou: [Your input message here]\\n[CHARACTER]:\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\n\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)",
                    "performance": {
                        "dataset": "56MB of dialogue data gathered from multiple sources",
                        "accuracy": "Not specified"
                    },
                    "description": "Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.",
                    "id": "huggingface_api_580",
                    "name": "pygmalion-6b"
                },
                "id": "gorilla_huggingface_tool_567",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_83",
        "query": "Detect the location of players in an image from a Counter-Strike: Global Offensive (CS:GO) game.",
        "instruction": "Given an `object detection` task, retrieve tools that are capable of detecting and identifying players in a game image by processing visual inputs and providing detailed location data of the targets.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                    "api_arguments": {
                        "image": "URL or local path to image"
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "csgo-object-detection",
                        "accuracy": 0.844
                    },
                    "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].",
                    "id": "huggingface_api_229",
                    "name": "keremberke/yolov8n-csgo-player-detection"
                },
                "id": "gorilla_huggingface_tool_225",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_84",
        "query": "I want to make a conference call app which differentiates between the times when the user is speaking and when there is no voice activity.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that distinguish between periods of speech and silence in audio inputs to develop a conference call application that accurately identifies user speaking activity.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face",
                    "functionality": "Voice Activity Detection",
                    "api_call": "pipeline('voice-activity-detection', model='funasr/FSMN-VAD')",
                    "api_arguments": {},
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library.",
                    "id": "huggingface_api_838",
                    "name": "FSMN-VAD"
                },
                "id": "gorilla_huggingface_tool_833",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_85",
        "query": "I want you to create a function that generates captions for a list of images.",
        "instruction": "Given an `image captioning` task, retrieve tools that process image inputs to generate descriptive textual captions, leveraging advanced models designed for translating visual content into coherent text.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Captioning",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')",
                    "api_arguments": {
                        "model": "nlpconnect/vit-gpt2-image-captioning"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "PIL"
                    ],
                    "example_code": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']",
                    "performance": {
                        "dataset": "Not provided",
                        "accuracy": "Not provided"
                    },
                    "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.",
                    "id": "huggingface_api_58",
                    "name": "nlpconnect/vit-gpt2-image-captioning"
                },
                "id": "gorilla_huggingface_tool_58",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_86",
        "query": "I need a system that extracts all the well-known named entities such as person names, locations, and organizations from news articles.",
        "instruction": "Given a `named entity extraction` task, retrieve tools capable of processing text inputs to reliably identify and categorize well-known entities such as person names, locations, and organizations from the provided content, ensuring accurate and contextually appropriate entity recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english')\n\n# make example sentence\nsentence = Sentence('George Washington went to Washington')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "conll2003",
                        "accuracy": "93.06"
                    },
                    "description": "This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_426",
                    "name": "flair/ner-english"
                },
                "id": "gorilla_huggingface_tool_422",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_87",
        "query": "Our company manages a restaurant. We need to analyze customers' tips and predict how much tip a new customer would give based on their total bill, sex, smoker, day, time, and party size. We should use a pre-trained model.",
        "instruction": "Given a `customer tipping prediction` task, retrieve tools that utilize pre-trained regression models to analyze input parameters such as total bill, customer demographics, and visit details to predict the tip amount for new customers.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "baseline-trainer",
                    "api_call": "joblib.load(hf_hub_download('merve/tips5wx_sbh5-tip-regression', 'sklearn_model.joblib'))",
                    "api_arguments": {
                        "alpha": 10
                    },
                    "python_environment_requirements": [
                        "dabl"
                    ],
                    "example_code": "Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\ntotal_bill True False False ... False False False\nsex False False False ... False False False\nsmoker False False False ... False False False\nday False False False ... False False False\ntime False False False ... False False False\nsize False False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])",
                    "performance": {
                        "dataset": "tips5wx_sbh5",
                        "r2": 0.389363,
                        "neg_mean_squared_error": -1.092356
                    },
                    "description": "Baseline Model trained on tips5wx_sbh5 to apply regression on tip",
                    "id": "huggingface_api_885",
                    "name": "merve/tips5wx_sbh5-tip-regression"
                },
                "id": "gorilla_huggingface_tool_878",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_88",
        "query": "Can you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?",
        "instruction": "Given a `conversational AI initiation` task, retrieve tools designed for conversational interactions that emulate a specified role. These tools should generate dialogue based on contextual narratives, role instructions, and conversation history to engage realistically with users.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('allenai/cosmo-xl')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "allenai/cosmo-xl"
                    },
                    "python_environment_requirements": {
                        "torch": "latest",
                        "transformers": "latest"
                    },
                    "example_code": {
                        "import": [
                            "import torch",
                            "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
                        ],
                        "initialize": [
                            "device = torch.device(cuda if torch.cuda.is_available() else cpu)",
                            "tokenizer = AutoTokenizer.from_pretrained(allenai/cosmo-xl)",
                            "model = AutoModelForSeq2SeqLM.from_pretrained(allenai/cosmo-xl).to(device)"
                        ],
                        "example": [
                            "def set_input(situation_narrative, role_instruction, conversation_history):",
                            " input_text =  <turn> .join(conversation_history)",
                            "if role_instruction != :",
                            " input_text = {} &lt;sep&gt; {}'.format(role_instruction, input_text)",
                            "if situation_narrative != :",
                            " input_text = {} &lt;sep&gt; {}'.format(situation_narrative, input_text)",
                            "return input_text",
                            "def generate(situation_narrative, role_instruction, conversation_history):",
                            " input_text = set_input(situation_narrative, role_instruction, conversation_history)",
                            " inputs = tokenizer([input_text], return_tensors=pt).to(device)",
                            " outputs = model.generate(inputs[input_ids], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)",
                            " response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)",
                            " return response",
                            "situation = Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.",
                            "instruction = You are Cosmo and you are talking to a friend.",
                            "conversation = [",
                            " Hey, how was your trip to Abu Dhabi?",
                            "]",
                            "response = generate(situation, instruction, conversation)",
                            "print(response)"
                        ]
                    },
                    "performance": {
                        "dataset": {
                            "allenai/soda": "",
                            "allenai/prosocial-dialog": ""
                        },
                        "accuracy": ""
                    },
                    "description": "COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation.",
                    "id": "huggingface_api_604",
                    "name": "allenai/cosmo-xl"
                },
                "id": "gorilla_huggingface_tool_599",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_89",
        "query": "As an art director, generating ideas from descriptions can be difficult. Produce an image of a serene lake at sunset.",
        "instruction": "Given an `image generation` task from text descriptions, retrieve tools that can create visuals by processing textual prompts to produce realistic or artistic images aligned with the specified theme or description.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')",
                    "api_arguments": [
                        "prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "scipy"
                    ],
                    "example_code": "import torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = CompVis/stable-diffusion-v1-4\ndevice = cuda\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.",
                    "id": "huggingface_api_30",
                    "name": "CompVis/stable-diffusion-v1-4"
                },
                "id": "gorilla_huggingface_tool_30",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_90",
        "query": "The company director needs a summary of a recent financial report. You should provide the answer of following question: What were the total revenues for the last quarter?",
        "instruction": "Given a `financial report analysis` task, retrieve tools that can extract and summarize key financial data by processing document inputs and addressing specific queries regarding financial metrics.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A LayoutLM model for document question answering.",
                    "id": "huggingface_api_135",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023"
                },
                "id": "gorilla_huggingface_tool_133",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_91",
        "query": "Our travel agency needs to build a chatbot that helps tourists find the best attractions in various destinations. The chatbot should answer questions related to tourist attractions.",
        "instruction": "Given a `chatbot development` task, retrieve tools that facilitate the creation of question-answering systems by leveraging natural language processing techniques to address inquiries about tourist attractions, ensuring relevant and precise responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')",
                    "api_arguments": [
                        "input_ids"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\nembeddings = model(input_ids).pooler_output",
                    "performance": {
                        "dataset": [
                            {
                                "name": "NQ",
                                "accuracy": {
                                    "top_20": 78.4,
                                    "top_100": 85.4
                                }
                            },
                            {
                                "name": "TriviaQA",
                                "accuracy": {
                                    "top_20": 79.4,
                                    "top_100": 85.0
                                }
                            },
                            {
                                "name": "WQ",
                                "accuracy": {
                                    "top_20": 73.2,
                                    "top_100": 81.4
                                }
                            },
                            {
                                "name": "TREC",
                                "accuracy": {
                                    "top_20": 79.8,
                                    "top_100": 89.1
                                }
                            },
                            {
                                "name": "SQuAD",
                                "accuracy": {
                                    "top_20": 63.2,
                                    "top_100": 77.2
                                }
                            }
                        ]
                    },
                    "description": "Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).",
                    "id": "huggingface_api_19",
                    "name": "facebook/dpr-question_encoder-single-nq-base"
                },
                "id": "gorilla_huggingface_tool_19",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_92",
        "query": "We want to automatically generate hashtags for the provided image URL to improve social media post performance.",
        "instruction": "Given an `image hashtag generation` task, retrieve tools that analyze images and extract features to generate relevant hashtags, enhancing the visibility and engagement of social media posts by leveraging computer vision techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/vit-base-patch16-224-in21k"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "ImageNet-21k",
                        "accuracy": "Refer to tables 2 and 5 of the original paper"
                    },
                    "description": "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.",
                    "id": "huggingface_api_17",
                    "name": "google/vit-base-patch16-224-in21k"
                },
                "id": "gorilla_huggingface_tool_17",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_93",
        "query": "Build a simple application to predict the survival status of passengers on the Titanic based on their age, gender, and passenger class.",
        "instruction": "Given a `predictive modeling` task, retrieve tools that facilitate the creation of a binary classification application using tabular data, specifically designed to predict outcomes based on characteristics like age, gender, and class for the Titanic survival dataset.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face",
                    "functionality": "Binary Classification",
                    "api_call": "AutoModel.from_pretrained('harithapliyal/autotrain-tatanic-survival-51030121311')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "harithapliyal/autotrain-data-tatanic-survival",
                        "accuracy": 0.872
                    },
                    "description": "A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.",
                    "id": "huggingface_api_852",
                    "name": "harithapliyal/autotrain-tatanic-survival-51030121311"
                },
                "id": "gorilla_huggingface_tool_846",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_94",
        "query": "We are building a quiz application where the image will be shown, and we have to choose a dressings matching that image. Please help in classifying the image.",
        "instruction": "Given an `image classification` task, retrieve tools that can classify images by matching visual inputs to specified categories or labels to align with the query's requirements for the quiz application.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "AlignModel.from_pretrained('kakaobrain/align-base')",
                    "api_arguments": [
                        "text",
                        "images",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "torch",
                        "PIL",
                        "transformers"
                    ],
                    "example_code": "import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\nprocessor = AlignProcessor.from_pretrained(kakaobrain/align-base)\nmodel = AlignModel.from_pretrained(kakaobrain/align-base)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [an image of a cat, an image of a dog]\ninputs = processor(text=candidate_labels, images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)",
                    "performance": {
                        "dataset": "COYO-700M",
                        "accuracy": "on-par or outperforms Google ALIGN's reported metrics"
                    },
                    "description": "The ALIGN model is a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder. It learns to align visual and text representations with contrastive learning. This implementation is trained on the open source COYO dataset and can be used for zero-shot image classification and multi-modal embedding retrieval.",
                    "id": "huggingface_api_361",
                    "name": "kakaobrain/align-base"
                },
                "id": "gorilla_huggingface_tool_357",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_95",
        "query": "The company wants to create a social media application like Pinterest that generates captions for users' images. Show us how to create this functionality.",
        "instruction": "Given an `image captioning` task, retrieve tools that enable the generation of textual captions by processing user-provided images and, optionally, accompanying text. These tools should effectively leverage pre-trained models to produce descriptive and coherent captions aligned with the application's functionality and user requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')",
                    "api_arguments": "image, optional text",
                    "python_environment_requirements": "transformers",
                    "example_code": "Refer to the documentation",
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_74",
                    "name": "blip2-opt-6.7b"
                },
                "id": "gorilla_huggingface_tool_74",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_96",
        "query": "We are developing a customer support platform for our telecommunication company in Spain. We want to know if they are happy or unhappy with our services, based on the content of their message.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that assess customer sentiment by processing textual content in their messages, identifying emotions or attitudes such as happiness or dissatisfaction, to provide insights into service perception.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_97",
        "query": "I am organizing a virtual party and want to create a short, autogenerated video based on a text description (e.g., \"cats playing with laser pointer\"). Can you accomplish this with a text-to-video generation API?",
        "instruction": "Given a `text-to-video generation` task, retrieve tools that transform descriptive text inputs into autogenerated videos by processing text prompts and generating video content that aligns with the input description and query requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-video-synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps",
                        "num_frames"
                    ],
                    "python_environment_requirements": [
                        "pip install git+https://github.com/huggingface/diffusers transformers accelerate"
                    ],
                    "example_code": "pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": "Webvid",
                        "accuracy": "Not specified"
                    },
                    "description": "A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.",
                    "id": "huggingface_api_95",
                    "name": "damo-vilab/text-to-video-ms-1.7b"
                },
                "id": "gorilla_huggingface_tool_90",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_98",
        "query": "We are creating an online video conference service, and we need to detect when two or more speakers are speaking at the same time in the audio.",
        "instruction": "Given an `overlapped speech detection` task, retrieve tools that analyze audio inputs to identify instances where multiple speakers are speaking simultaneously, ensuring accurate and efficient detection tailored to the query's specific requirements and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "pyannote.audio",
                    "functionality": "overlapped-speech-detection",
                    "api_call": "pipeline.from_pretrained('pyannote/overlapped-speech-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # two or more speakers are active between speech.start and speech.end\n  ...",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": null
                    },
                    "description": "Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.",
                    "id": "huggingface_api_750",
                    "name": "pyannote/overlapped-speech-detection"
                },
                "id": "gorilla_huggingface_tool_745",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_99",
        "query": "I want to build an AI that identifies the best marketing strategies for my website by trying different combinations of headlines and images.",
        "instruction": "Given an `AI strategy optimization` task, retrieve tools that leverage AI and machine learning, particularly models and frameworks capable of exploring various combinations of digital marketing elements like headlines and images to identify effective strategies for enhancing website performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "MountainCar-v0",
                    "api_call": "load_from_hub(repo_id='sb3/dqn-MountainCar-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "RL Zoo",
                        "SB3",
                        "SB3 Contrib"
                    ],
                    "example_code": [
                        "python -m rl_zoo3.load_from_hub --algo dqn --env MountainCar-v0 -orga sb3 -f logs/",
                        "python train.py --algo dqn --env MountainCar-v0 -f logs/",
                        "python -m rl_zoo3.push_to_hub --algo dqn --env MountainCar-v0 -f logs/ -orga sb3"
                    ],
                    "performance": {
                        "dataset": "MountainCar-v0",
                        "accuracy": "-103.40 +/- 7.49"
                    },
                    "description": "This is a trained model of a DQN agent playing MountainCar-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_895",
                    "name": "sb3/dqn-MountainCar-v0"
                },
                "id": "gorilla_huggingface_tool_888",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_100",
        "query": "My daughter left her laptop logged in, and I found her English essay unfinished. There's a sentence that goes like \"In the story, the antagonist represents the <mask> nature of humanity.\" Can you help me complete her sentence with an appropriate word?",
        "instruction": "Given a `fill-in-the-blank` task, retrieve tools that utilize masked language modeling to process text inputs with missing elements and provide contextually appropriate word completions to enhance the narrative coherence.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MNLI",
                                "accuracy": 87.6
                            },
                            {
                                "name": "QQP",
                                "accuracy": 91.9
                            },
                            {
                                "name": "QNLI",
                                "accuracy": 92.8
                            },
                            {
                                "name": "SST-2",
                                "accuracy": 94.8
                            },
                            {
                                "name": "CoLA",
                                "accuracy": 63.6
                            },
                            {
                                "name": "STS-B",
                                "accuracy": 91.2
                            },
                            {
                                "name": "MRPC",
                                "accuracy": 90.2
                            },
                            {
                                "name": "RTE",
                                "accuracy": 78.7
                            }
                        ]
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.",
                    "id": "huggingface_api_659",
                    "name": "roberta-base"
                },
                "id": "gorilla_huggingface_tool_654",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_101",
        "query": "My company is working on a children's storybook. I need to generate images based on the text descriptions of scenes in the story.",
        "instruction": "Given an `image generation` task, retrieve tools that facilitate the creation of visual content by processing text descriptions and generating corresponding images, aligning the outputs with the narrative themes and details provided in the story.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')",
                    "api_arguments": {
                        "prompt": "a photo of an astronaut riding a horse on mars"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.",
                    "id": "huggingface_api_36",
                    "name": "stabilityai/stable-diffusion-2-1"
                },
                "id": "gorilla_huggingface_tool_36",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_102",
        "query": "We have a road safety app that helps identify potholes. Can we use an image segmentation model to detect if there are potholes in the pictures of roads?",
        "instruction": "Given an `image segmentation` task, retrieve tools that use image processing capabilities to detect specific objects, such as potholes, within road images by providing detailed segmented outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "YOLO('keremberke/yolov8s-pothole-segmentation')",
                    "api_arguments": {
                        "image": "URL or local path to the image"
                    },
                    "python_environment_requirements": {
                        "ultralyticsplus": "0.0.23",
                        "ultralytics": "8.0.21"
                    },
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nprint(results[0].masks)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "pothole-segmentation",
                        "accuracy": {
                            "mAP@0.5(box)": 0.928,
                            "mAP@0.5(mask)": 0.928
                        }
                    },
                    "description": "A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.",
                    "id": "huggingface_api_256",
                    "name": "keremberke/yolov8s-pothole-segmentation"
                },
                "id": "gorilla_huggingface_tool_252",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_103",
        "query": "Use a zero-shot classifier to classify news headlines into three categories: sports, technology, and politics.",
        "instruction": "Given a `zero-shot classification` task, retrieve tools that leverage natural language processing capabilities to categorize text into specified categories without prior training, using a defined set of candidate labels to process the input query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')",
                    "api_arguments": [
                        "sent",
                        "candidate_labels"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\nsent = 'Apple just announced the newest iPhone X'\ncandidate_labels = ['technology', 'sports', 'politics']\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.64",
                            "MNLI_mismatched": "87.77"
                        }
                    },
                    "description": "This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.",
                    "id": "huggingface_api_513",
                    "name": "cross-encoder/nli-deberta-v3-xsmall"
                },
                "id": "gorilla_huggingface_tool_509",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_104",
        "query": "Extract a conclusion from the following text: \"Studies have been shown that owning a dog is good for you. Having a dog can help decrease stress levels, improve your mood, and increase physical activity.\"",
        "instruction": "Given a `text summarization` task, retrieve tools that can process text inputs to generate concise conclusions, capturing the essence of the provided content in alignment with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Text Classification",
                    "api_call": "T5Model.from_pretrained('t5-base')",
                    "api_arguments": [
                        "input_ids",
                        "decoder_input_ids"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\nmodel = T5Model.from_pretrained('t5-base')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.",
                    "id": "huggingface_api_519",
                    "name": "t5-base"
                },
                "id": "gorilla_huggingface_tool_515",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_105",
        "query": "There is a news article stating, \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU.\" We need to determine which category this article should fall under.",
        "instruction": "Given a `category classification` task, retrieve tools that perform text classification by analyzing the content and matching it to predefined categories using natural language processing techniques, specifically zero-shot classification models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary')",
                    "api_arguments": {
                        "sequence_to_classify": "Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU",
                        "candidate_labels": [
                            "politics",
                            "economy",
                            "entertainment",
                            "environment"
                        ],
                        "multi_label": false
                    },
                    "python_environment_requirements": [
                        "transformers==4.13"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)\nsequence_to_classify = Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\ncandidate_labels = [politics, economy, entertainment, environment]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MultiNLI-matched",
                                "accuracy": 0.857
                            },
                            {
                                "name": "MultiNLI-mismatched",
                                "accuracy": 0.856
                            },
                            {
                                "name": "ANLI-all",
                                "accuracy": 0.537
                            },
                            {
                                "name": "ANLI-r3",
                                "accuracy": 0.497
                            },
                            {
                                "name": "WANLI",
                                "accuracy": 0.732
                            },
                            {
                                "name": "LingNLI",
                                "accuracy": 0.788
                            },
                            {
                                "name": "fever-nli",
                                "accuracy": 0.761
                            }
                        ]
                    },
                    "description": "This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people.",
                    "id": "huggingface_api_512",
                    "name": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
                },
                "id": "gorilla_huggingface_tool_508",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_106",
        "query": "We are developing an AI-powered code review system. Our model should provide a short summary of the provided code snippet.",
        "instruction": "Given a `code summarization` task, retrieve tools that specialize in code understanding and text generation by processing input code snippets to produce concise summaries that capture the essential semantics and purposes of the code.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Code Understanding and Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')",
                    "api_arguments": [
                        "text",
                        "return_tensors",
                        "input_ids",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import RobertaTokenizer, T5ForConditionalGeneration\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\ntext = def greet(user): print(f'hello <extra_id_0>!')\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "code_search_net",
                        "accuracy": "Refer to the paper for evaluation results on several downstream benchmarks"
                    },
                    "description": "CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.",
                    "id": "huggingface_api_656",
                    "name": "Salesforce/codet5-base"
                },
                "id": "gorilla_huggingface_tool_651",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_107",
        "query": "We have a French news agency and we want to categorize the news articles based on sports, politics, and science.",
        "instruction": "Given a `news categorization` task, retrieve tools that utilize natural language processing to classify news articles into specified categories such as sports, politics, and science, leveraging models capable of handling French text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')",
                    "api_arguments": {
                        "sequence": "str",
                        "candidate_labels": "List[str]",
                        "hypothesis_template": "str"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "sequence = L'Ã©quipe de France joue aujourd'hui au Parc des Princes\ncandidate_labels = [sport,politique,science]\nhypothesis_template = Ce texte parle de {}.\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "performance": {
                        "dataset": "xnli",
                        "accuracy": {
                            "validation": 81.4,
                            "test": 81.7
                        }
                    },
                    "description": "Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.",
                    "id": "huggingface_api_492",
                    "name": "BaptisteDoyen/camembert-base-xnli"
                },
                "id": "gorilla_huggingface_tool_488",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_108",
        "query": "The company has just received a document written in French, and they need it translated into English.",
        "instruction": "Given a `translation` task, retrieve tools that perform language translation by processing text inputs in French and generating accurate English translations, aligning with the query's linguistic and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment Ã§a va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_109",
        "query": "I recently interviewed a person in Japanese. I need to transcribe the interview in order to find relevant quotes for my article.",
        "instruction": "Given an `audio transcription` task, retrieve tools specialized in transcribing audio content in Japanese by processing audio inputs to produce textual transcriptions that capture the interview dialogue accurately for use in an article.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Recognition",
                    "api_call": "SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')",
                    "api_arguments": [
                        "audio_paths"
                    ],
                    "python_environment_requirements": [
                        "huggingsound",
                        "torch",
                        "librosa",
                        "datasets",
                        "transformers"
                    ],
                    "example_code": "from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\ntranscriptions = model.transcribe(audio_paths)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": {
                            "WER": 81.8,
                            "CER": 20.16
                        }
                    },
                    "description": "Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.",
                    "id": "huggingface_api_761",
                    "name": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese"
                },
                "id": "gorilla_huggingface_tool_756",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_110",
        "query": "I need a text analysis tool that can automatically predict the most plausible missing text in a given sentence.",
        "instruction": "Given a `text prediction` task, retrieve tools that leverage natural language processing capabilities to automatically predict and fill in missing text within a sentence, ensuring the predictions are contextually appropriate and coherent with the existing text.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='albert-base-v2')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='albert-base-v2')\nunmasker(Hello I'm a [MASK] model.)",
                    "performance": {
                        "dataset": {
                            "SQuAD1.1": "90.2/83.2",
                            "SQuAD2.0": "82.1/79.3",
                            "MNLI": "84.6",
                            "SST-2": "92.9",
                            "RACE": "66.8"
                        },
                        "accuracy": "82.3"
                    },
                    "description": "ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.",
                    "id": "huggingface_api_666",
                    "name": "albert-base-v2"
                },
                "id": "gorilla_huggingface_tool_661",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_111",
        "query": "The publisher has sent us a draft of an article, but some of the words have been masked. We need to identify the masked words.",
        "instruction": "Given a `fill-mask language modeling` task, retrieve tools that identify and replace masked words in text using advanced natural language processing techniques to complete and enhance the text content accurately and coherently.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='xlm-roberta-large')",
                    "api_arguments": {
                        "model": "xlm-roberta-large"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='xlm-roberta-large')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "CommonCrawl",
                        "accuracy": "N/A"
                    },
                    "description": "XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering.",
                    "id": "huggingface_api_663",
                    "name": "xlm-roberta-large"
                },
                "id": "gorilla_huggingface_tool_658",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_112",
        "query": "As a book store owner, I want to classify customer reviews into positive and negative sentiments.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that classify text data into categories such as positive and negative sentiments by processing input text and utilizing pre-trained language models to evaluate and assign sentiment labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Text Classification",
                    "api_call": "DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]",
                    "performance": {
                        "dataset": "glue",
                        "accuracy": 0.911
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.",
                    "id": "huggingface_api_370",
                    "name": "distilbert-base-uncased-finetuned-sst-2-english"
                },
                "id": "gorilla_huggingface_tool_366",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_113",
        "query": "Tell me the day of the game when it was played given the following context: \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing to analyze and interpret textual context in order to extract specific information, such as dates, from the provided content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')",
                    "api_arguments": {
                        "model": "csarron/bert-base-uncased-squad-v1",
                        "tokenizer": "csarron/bert-base-uncased-squad-v1"
                    },
                    "python_environment_requirements": "Python 3.7.5",
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline(\n question-answering,\n model=csarron/bert-base-uncased-squad-v1,\n tokenizer=csarron/bert-base-uncased-squad-v1\n)\npredictions = qa_pipeline({\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\n 'question': What day was the game played on?\n})\nprint(predictions)",
                    "performance": {
                        "dataset": "SQuAD1.1",
                        "accuracy": {
                            "EM": 80.9,
                            "F1": 88.2
                        }
                    },
                    "description": "BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.",
                    "id": "huggingface_api_483",
                    "name": "csarron/bert-base-uncased-squad-v1"
                },
                "id": "gorilla_huggingface_tool_479",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_114",
        "query": "A chat service needs a way to compare and cluster similar sentences from users in different languages. Find a suitable feature extraction method to achieve this.",
        "instruction": "Given a `multilingual sentence clustering` task, retrieve tools that perform feature extraction to compare and cluster similar sentences by processing multilingual text inputs and generating language-agnostic embeddings.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('rasa/LaBSE')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.",
                    "id": "huggingface_api_21",
                    "name": "rasa/LaBSE"
                },
                "id": "gorilla_huggingface_tool_21",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_115",
        "query": "I am working in a dating chatapp development team. We want to generate sentences to make it more interactive.",
        "instruction": "Given a `sentence generation` task, retrieve tools that facilitate interactive and engaging sentence creation by utilizing natural language processing capabilities, tailored for applications such as chat applications, to enhance user interaction.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='albert-base-v2')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='albert-base-v2')\nunmasker(Hello I'm a [MASK] model.)",
                    "performance": {
                        "dataset": {
                            "SQuAD1.1": "90.2/83.2",
                            "SQuAD2.0": "82.1/79.3",
                            "MNLI": "84.6",
                            "SST-2": "92.9",
                            "RACE": "66.8"
                        },
                        "accuracy": "82.3"
                    },
                    "description": "ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.",
                    "id": "huggingface_api_666",
                    "name": "albert-base-v2"
                },
                "id": "gorilla_huggingface_tool_661",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_116",
        "query": "We are developing a medical records analysis software that automatically recognizes biomedical entities from physicians' case reports.",
        "instruction": "Given a `biomedical entity recognition` task, retrieve tools that perform Named Entity Recognition specifically for biomedical texts by processing the case reports to automatically identify and classify biomedical entities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)",
                        "tokenizer": "AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
                    },
                    "example_code": "pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)",
                    "performance": {
                        "dataset": "Maccrobat",
                        "accuracy": "Not provided"
                    },
                    "description": "An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.",
                    "id": "huggingface_api_404",
                    "name": "d4data/biomedical-ner-all"
                },
                "id": "gorilla_huggingface_tool_400",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_117",
        "query": "We need a system to control the access into parking lot. Analyze an image and find out the license plate numbers to detect whether it's an authorized vehicle or not.",
        "instruction": "Given a `license plate recognition` task, retrieve tools that analyze images to identify and extract license plate numbers, allowing for the verification of vehicle authorization based on these attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "License Plate Detection",
                    "api_call": "yolov5.load('keremberke/yolov5m-license-plate')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": [
                        "import yolov5",
                        "model = yolov5.load('keremberke/yolov5m-license-plate')",
                        "model.conf = 0.25",
                        "model.iou = 0.45",
                        "model.agnostic = False",
                        "model.multi_label = False",
                        "model.max_det = 1000",
                        "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model(img, size=640)",
                        "results = model(img, augment=True)",
                        "predictions = results.pred[0]",
                        "boxes = predictions[:, :4]",
                        "scores = predictions[:, 4]",
                        "categories = predictions[:, 5]",
                        "results.show()",
                        "results.save(save_dir='results/')"
                    ],
                    "performance": {
                        "dataset": "keremberke/license-plate-object-detection",
                        "accuracy": 0.988
                    },
                    "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.",
                    "id": "huggingface_api_214",
                    "name": "keremberke/yolov5m-license-plate"
                },
                "id": "gorilla_huggingface_tool_210",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_118",
        "query": "Build a system for detecting hate speech from social media comments in Korean.",
        "instruction": "Given a `hate speech detection` system development task, retrieve tools that enable natural language processing for identifying harmful content in social media comments, specifically utilizing models capable of processing Korean text to extract relevant features and assist in classification tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "PyTorch Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "api_arguments": {
                        "tokenizer": "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "tokenizers": "latest"
                    },
                    "example_code": "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "performance": {
                        "dataset": "NSMC",
                        "accuracy": 0.901
                    },
                    "description": "KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.",
                    "id": "huggingface_api_8",
                    "name": "kobart-base-v2"
                },
                "id": "gorilla_huggingface_tool_8",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_119",
        "query": "I need to estimate CO2 emissions from vehicles based on their characteristics, such as engine size, transmission type, and miles traveled.",
        "instruction": "Given a `CO2 emissions estimation` task, retrieve tools that process vehicle characteristics to estimate emissions by leveraging models capable of analyzing features like engine size, transmission type, and miles traveled to provide accurate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Tabular Classification",
                    "api_call": "AutoModel.from_pretrained('datadmg/autotrain-test-news-44534112235')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "datadmg/autotrain-data-test-news",
                        "accuracy": 0.333
                    },
                    "description": "This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.",
                    "id": "huggingface_api_857",
                    "name": "datadmg/autotrain-test-news-44534112235"
                },
                "id": "gorilla_huggingface_tool_850",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_120",
        "query": "We received a scanned document with a lot of information. We need to go through it and find answers to specific questions. Create a program to extract information from the document.",
        "instruction": "Given a `document information extraction` task, retrieve tools that specialize in document question answering by processing scanned documents and leveraging multimodal capabilities to accurately provide answers to specific queries.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')",
                    "api_arguments": {
                        "image": "path/to/image/file"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_125",
                    "name": "tiny-random-LayoutLMv3ForQuestionAnswering"
                },
                "id": "gorilla_huggingface_tool_124",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_121",
        "query": "I have few picture from my album, and I would like to find out whether it is a hotdog or not.",
        "instruction": "Given an `image classification` task, retrieve tools that can analyze images from the user's photo album to determine specific classifications, such as identifying whether an object in the image is a hotdog or not, and provide the classification results based on the visual content of the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "pipeline('image-classification', model='julien-c/hotdog-not-hotdog')",
                    "api_arguments": "image",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": 0.825
                    },
                    "description": "A model that classifies images as hotdog or not hotdog.",
                    "id": "huggingface_api_194",
                    "name": "julien-c/hotdog-not-hotdog"
                },
                "id": "gorilla_huggingface_tool_190",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_122",
        "query": "Our team is designing a butterfly-themed stationery set for children. We need to generate images of cute butterflies to use in the design.",
        "instruction": "Given an `image generation` task, retrieve tools that perform unconditional image creation by employing models capable of generating themed images, such as cute butterflies, to meet the design requirements for stationery sets.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')",
                    "api_arguments": {
                        "model_id": "clp/sd-class-butterflies-32"
                    },
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "This model is a diffusion model for unconditional image generation of cute butterflies.",
                    "id": "huggingface_api_304",
                    "name": "sd-class-butterflies-32"
                },
                "id": "gorilla_huggingface_tool_300",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_123",
        "query": "We have a collection of low-resolution images of movie characters, and we need to upscale those images to get a more detailed high-resolution image.",
        "instruction": "Given an `image upscaling` task, retrieve tools that enhance images by processing low-resolution inputs to produce high-resolution outputs using methods such as diffusion-based models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Upscaling",
                    "api_call": "StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)",
                    "api_arguments": {
                        "prompt": "text prompt",
                        "image": "low resolution latents",
                        "num_inference_steps": 20,
                        "guidance_scale": 0,
                        "generator": "torch generator"
                    },
                    "python_environment_requirements": [
                        "git+https://github.com/huggingface/diffusers.git",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\nimport torch\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\npipeline.to(cuda)\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\nupscaler.to(cuda)\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\ngenerator = torch.manual_seed(33)\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\nupscaled_image.save(astronaut_1024.png)",
                    "performance": {
                        "dataset": "LAION-2B",
                        "accuracy": "Not specified"
                    },
                    "description": "Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.",
                    "id": "huggingface_api_59",
                    "name": "stabilityai/sd-x2-latent-upscaler"
                },
                "id": "gorilla_huggingface_tool_59",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_124",
        "query": "Help us create an AI solution to automatically label images taken by a security camera.",
        "instruction": "Given an `image labeling` task, retrieve tools that utilize computer vision techniques to automatically classify images by processing visual inputs and assigning appropriate labels based on the content detected within the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "zuppif/regnet-y-040"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor, RegNetForImageClassification",
                        "torch": "torch",
                        "datasets": "load_dataset"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.",
                    "id": "huggingface_api_199",
                    "name": "facebook/regnet-y-008"
                },
                "id": "gorilla_huggingface_tool_195",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_125",
        "query": "We are researching multi-view 3D scanning, and we would like to improve our depth estimation with a pretrained model.",
        "instruction": "Given a `depth estimation enhancement` task, retrieve tools that utilize pretrained models for multi-view 3D scanning improvements by employing computer vision techniques to refine depth accuracy using relevant datasets and frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')",
                    "api_arguments": "pretrained_model_name",
                    "python_environment_requirements": "transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3421,
                            "Mae": 0.27,
                            "Rmse": 0.4042,
                            "Abs Rel": 0.3279,
                            "Log Mae": 0.1132,
                            "Log Rmse": 0.1688,
                            "Delta1": 0.5839,
                            "Delta2": 0.8408,
                            "Delta3": 0.9309
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.",
                    "id": "huggingface_api_168",
                    "name": "glpn-nyu-finetuned-diode-221122-082237"
                },
                "id": "gorilla_huggingface_tool_164",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_126",
        "query": "We need to generate a short video showing Spider-Man water skiing in redshift style based on a textual prompt.",
        "instruction": "Given a `text-to-video` generation task, retrieve tools that process textual prompts to create short videos by leveraging models designed for multimodal output, specifically in styles such as redshift.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video Generation",
                    "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                    "api_arguments": {
                        "prompt": "string",
                        "video_length": "int",
                        "height": "int",
                        "width": "int",
                        "num_inference_steps": "int",
                        "guidance_scale": "float"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "tuneavideo"
                    ],
                    "example_code": "from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\nfrom tuneavideo.models.unet import UNet3DConditionModel\nfrom tuneavideo.util import save_videos_grid\nimport torch\npretrained_model_path = nitrosocke/redshift-diffusion\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\npipe.enable_xformers_memory_efficient_attention()\nprompt = (redshift style) spider man is skiing\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\nsave_videos_grid(video, f./{prompt}.gif)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.",
                    "id": "huggingface_api_98",
                    "name": "redshift-man-skiing"
                },
                "id": "gorilla_huggingface_tool_97",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_127",
        "query": "As a city planner, I need to measure the depth of spaces in a series of images taken from streets.",
        "instruction": "Given a `depth measurement` task, retrieve tools that handle computer vision capabilities, processing image inputs to estimate and measure the depth of spaces accurately and efficiently based on the characteristics of the images and relevant requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "Transformers 4.24.0",
                        "Pytorch 1.12.1+cu116",
                        "Datasets 2.8.0",
                        "Tokenizers 0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.4222,
                            "Mae": 0.411,
                            "Rmse": 0.6292,
                            "Abs Rel": 0.3778,
                            "Log Mae": 0.1636,
                            "Log Rmse": 0.224,
                            "Delta1": 0.432,
                            "Delta2": 0.6806,
                            "Delta3": 0.8068
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_170",
                    "name": "glpn-nyu-finetuned-diode-221221-102136"
                },
                "id": "gorilla_huggingface_tool_166",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_128",
        "query": "We need to estimate the depth of a scene in an image using a pretrained model. Can you please suggest a way?",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize pretrained models to analyze image inputs and provide depth predictions. Ensure the tools are compatible with image processing and equipped with the necessary model architectures for accurate depth estimation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": "DIODE",
                        "accuracy": null
                    },
                    "description": "A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.",
                    "id": "huggingface_api_155",
                    "name": "glpn-nyu-finetuned-diode-221215-095508"
                },
                "id": "gorilla_huggingface_tool_151",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_129",
        "query": "Help me setup a tinyroberta model from deepset for Question and Answer. Provide a sample input and output.",
        "instruction": "Given a `model setup and example query` task, retrieve tools that facilitate setting up an NLP model for Question Answering by providing details about the model framework, function, sample inputs and outputs, as well as any necessary environment configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/tinyroberta-squad2",
                        "question": "Why is model conversion important?",
                        "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = deepset/tinyroberta-squad2\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 78.69114798281817,
                            "f1": 81.9198998536977
                        }
                    },
                    "description": "This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.",
                    "id": "huggingface_api_474",
                    "name": "deepset/tinyroberta-squad2"
                },
                "id": "gorilla_huggingface_tool_470",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_130",
        "query": "A podcast producer is looking to improve the quality of their audio files by removing background noise. What can they do?",
        "instruction": "Given an `audio enhancement` task, retrieve tools that specialize in improving the quality of audio files by removing background noise through advanced speech enhancement techniques, processing audio inputs to deliver clearer outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement)",
                    "api_arguments": {
                        "path": "path to the input audio file"
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\ntorchaudio.save(enhanced_wham16k.wav, est_sources[:, :, 0].detach().cpu(), 16000)",
                    "performance": {
                        "dataset": "WHAM!",
                        "accuracy": {
                            "Test-Set SI-SNR": "14.3 dB",
                            "Test-Set PESQ": "2.20"
                        }
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_788",
                    "name": "speechbrain/sepformer-wham16k-enhancement"
                },
                "id": "gorilla_huggingface_tool_783",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_131",
        "query": "Develop a software to classify an image from a URL into a thousand categories.",
        "instruction": "Given an `image classification` task, retrieve tools that process image inputs from a URL to categorize them into multiple classes, leveraging advanced models that support feature map extraction and produce accurate classification results.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification, Feature Map Extraction, Image Embeddings",
                    "api_call": "timm.create_model('convnext_base.fb_in1k')",
                    "api_arguments": {
                        "pretrained": "True",
                        "features_only": "True",
                        "num_classes": "0"
                    },
                    "python_environment_requirements": [
                        "timm"
                    ],
                    "example_code": [
                        "from urllib.request import urlopen",
                        "from PIL import Image",
                        "import timm",
                        "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))",
                        "model = timm.create_model('convnext_base.fb_in1k', pretrained=True)",
                        "model = model.eval()",
                        "data_config = timm.data.resolve_model_data_config(model)",
                        "transforms = timm.data.create_transform(**data_config, is_training=False)",
                        "output = model(transforms(img).unsqueeze(0))"
                    ],
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "83.82%"
                    },
                    "description": "A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.",
                    "id": "huggingface_api_201",
                    "name": "convnext_base.fb_in1k"
                },
                "id": "gorilla_huggingface_tool_197",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_132",
        "query": "We are a security company and we need a video classification model to analyze CCTV footage for suspicious activities.",
        "instruction": "Given a `video classification` task, retrieve tools that analyze video inputs to categorize and identify suspicious activities in CCTV footage, aligning with the specific security requirements outlined in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')",
                    "api_arguments": "video_path",
                    "python_environment_requirements": "transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": 0.7212
                    },
                    "description": "This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.",
                    "id": "huggingface_api_339",
                    "name": "lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2"
                },
                "id": "gorilla_huggingface_tool_335",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_133",
        "query": "Write a story about a spaceship journey to a distant planet in search of a new home for humanity.",
        "instruction": "Given a `creative writing` task, retrieve tools that facilitate text generation by utilizing advanced language models to craft narrative stories based on thematic prompts and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B')",
                    "api_arguments": {
                        "pretrained_model": "EleutherAI/gpt-j-6B"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForCausalLM"
                    },
                    "example_code": {
                        "loading_model": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(EleutherAI/gpt-j-6B)\nmodel = AutoModelForCausalLM.from_pretrained(EleutherAI/gpt-j-6B)"
                    },
                    "performance": {
                        "dataset": "the_pile",
                        "accuracy": {
                            "LAMBADA_PPL": 3.99,
                            "LAMBADA_Acc": "69.7%",
                            "Winogrande": "65.3%",
                            "Hellaswag": "66.1%",
                            "PIQA": "76.5%"
                        }
                    },
                    "description": "GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI.",
                    "id": "huggingface_api_611",
                    "name": "EleutherAI/gpt-j-6B"
                },
                "id": "gorilla_huggingface_tool_606",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_134",
        "query": "Let's build a Chatbot that can solve daily life problems of a research Assistant.",
        "instruction": "Given a `conversational AI building` task, retrieve tools that can develop a chatbot capable of understanding and solving daily life problems by utilizing pre-trained natural language processing models optimized for multi-turn dialogue generation. Ensure the tools support conversation modeling and response generation aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')",
                    "api_arguments": [
                        "bot_input_ids",
                        "max_length",
                        "pad_token_id"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)",
                    "performance": {
                        "dataset": "Reddit discussion thread",
                        "accuracy": "Comparable to human response quality under a single-turn conversation Turing test"
                    },
                    "description": "DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.",
                    "id": "huggingface_api_577",
                    "name": "microsoft/DialoGPT-large"
                },
                "id": "gorilla_huggingface_tool_573",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_135",
        "query": "I have written a small story in English about the adventures of a superhero who saves the day from evil villains. I would like to translate it into French so that my friends in France can read it too.",
        "instruction": "Given a `translation` task, retrieve tools that handle text-to-text conversion to translate written content from one language to another, specifically from English to French, by processing textual input and delivering the translated output.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Text-to-Text Generation",
                    "api_call": "pipeline('translation')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "optimum.onnxruntime"
                    ],
                    "example_code": "from transformers import AutoTokenizer, pipeline\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\nresults = translator(My name is Eustache and I have a pet raccoon)\nprint(results)",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "N/A"
                    },
                    "description": "T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.",
                    "id": "huggingface_api_918",
                    "name": "optimum/t5-small"
                },
                "id": "gorilla_huggingface_tool_534",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_136",
        "query": "Design a promotional poster for a new line of summer clothing featuring people wearing the clothes with a beach background.",
        "instruction": "Given a `promotional design` task, retrieve tools capable of generating high-quality, realistic images by processing text prompts to visualize specific scenes, such as summer clothing with a beach background, ensuring the outcome aligns with the intended thematic and visual appeal of the promotional material.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)",
                    "api_arguments": {
                        "prompt": "string",
                        "negative_prompt": "string"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\n\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\n\nresult = model(prompt, negative_prompt=negative_prompt)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.",
                    "id": "huggingface_api_37",
                    "name": "Realistic_Vision_V1.4"
                },
                "id": "gorilla_huggingface_tool_37",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_137",
        "query": "Create a script for an AI bot that automatically adds punctuation to users' messages in a chat app.",
        "instruction": "Given a `text enhancement` task, retrieve tools that improve written communication by processing user messages to add appropriate punctuation automatically, ensuring the output aligns with grammatical standards and improves readability.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('token-classification')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "multilingual",
                        "accuracy": 0.98
                    },
                    "description": "A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.",
                    "id": "huggingface_api_934",
                    "name": "kredor/punctuate-all"
                },
                "id": "gorilla_huggingface_tool_415",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_138",
        "query": "We have a surveillance camera in our backyard. We would like to analyze the captured videos to recognize the activities taking place in the backyard.",
        "instruction": "Given a `video analysis` task, retrieve tools that perform activity recognition by processing video inputs to identify and classify actions occurring within the footage according to the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')",
                    "api_arguments": {
                        "learning_rate": 5e-05,
                        "train_batch_size": 8,
                        "eval_batch_size": 8,
                        "seed": 42,
                        "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                        "lr_scheduler_type": "linear",
                        "lr_scheduler_warmup_ratio": 0.1,
                        "training_steps": 148
                    },
                    "python_environment_requirements": {
                        "Transformers": "4.24.0",
                        "Pytorch": "1.12.1+cu113",
                        "Datasets": "2.6.1",
                        "Tokenizers": "0.13.2"
                    },
                    "example_code": "from transformers import AutoModelForVideoClassification, AutoTokenizer\nmodel = AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\ntokenizer = AutoTokenizer.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": 0.8645
                    },
                    "description": "This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645.",
                    "id": "huggingface_api_343",
                    "name": "sayakpaul/videomae-base-finetuned-ucf101-subset"
                },
                "id": "gorilla_huggingface_tool_339",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_139",
        "query": "Analyze the type of plants in the image provided and provide the name of the probable plant.",
        "instruction": "Given an `image classification` task, retrieve tools that analyze the contents of an image to identify and provide the names of probable plant species by utilizing zero-shot image classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                    "api_arguments": {
                        "image_path": "path to the image file",
                        "labels": "list of possible class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "70.8 - 71.7%"
                    },
                    "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_357",
                    "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')"
                },
                "id": "gorilla_huggingface_tool_353",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_140",
        "query": "My boss wants me to extract captions from images of people in different settings.",
        "instruction": "Given an `image captioning` task, retrieve tools that can process image inputs to automatically generate descriptive captions by utilizing advanced multimodal image-to-text capabilities, specifically designed for extracting textual information from visual data.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image-to-Text",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')",
                    "api_arguments": {
                        "loc": "ydshieh/vit-gpt2-coco-en"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "requests",
                        "PIL",
                        "transformers"
                    ],
                    "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\nloc = ydshieh/vit-gpt2-coco-en\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\ntokenizer = AutoTokenizer.from_pretrained(loc)\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\nmodel.eval()\ndef predict(image):\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\n with torch.no_grad():\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nwith Image.open(requests.get(url, stream=True).raw) as image:\n preds = predict(image)\nprint(preds)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.",
                    "id": "huggingface_api_73",
                    "name": "ydshieh/vit-gpt2-coco-en"
                },
                "id": "gorilla_huggingface_tool_73",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_141",
        "query": "Create an object detector that can detect blood cells in an image, such as platelets, red blood cells, and white blood cells.",
        "instruction": "Given an `object detection` task for identifying blood cells in images, retrieve tools that leverage computer vision models specifically designed to detect various blood cell types, such as platelets, red blood cells, and white blood cells. These tools should process image inputs using advanced architectures to provide accurate and reliable detection results aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Blood Cell Detection",
                    "api_call": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": "ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.893
                    },
                    "description": "This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_232",
                    "name": "keremberke/yolov8n-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_228",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_142",
        "query": "Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.",
        "instruction": "Given an `article recommendation` task, retrieve tools that utilize natural language processing to measure article similarity by analyzing the content of user interactions, thus enabling the recommendation of articles akin to previously liked content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.",
                    "id": "huggingface_api_2",
                    "name": "princeton-nlp/unsup-simcse-roberta-base"
                },
                "id": "gorilla_huggingface_tool_2",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_143",
        "query": "I'm an author and want to create a short video based on a brief passage from my book. Can you generate a video based on this text?",
        "instruction": "Given a `text-to-video` task, retrieve tools that transform text inputs into video outputs, aligning the generated video content with the narrative provided in the text.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video",
                    "api_call": "pipeline('text-to-video', model='camenduru/text2-video-zero')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.",
                    "id": "huggingface_api_93",
                    "name": "camenduru/text2-video-zero"
                },
                "id": "gorilla_huggingface_tool_93",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_144",
        "query": "I have a recorded meeting between multiple participants. Identify when two or more people are speaking simultaneously.",
        "instruction": "Given an `overlapped speech detection` task, retrieve tools that specialize in analyzing audio inputs to identify and detect moments where two or more participants are speaking simultaneously, using appropriate audio processing methods.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "pyannote.audio",
                    "functionality": "overlapped-speech-detection",
                    "api_call": "pipeline.from_pretrained('pyannote/overlapped-speech-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # two or more speakers are active between speech.start and speech.end\n  ...",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": null
                    },
                    "description": "Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.",
                    "id": "huggingface_api_750",
                    "name": "pyannote/overlapped-speech-detection"
                },
                "id": "gorilla_huggingface_tool_745",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_145",
        "query": "Give me a solution to find out the license plate in the given car images.",
        "instruction": "Given a `license plate detection` task, retrieve tools that utilize computer vision object detection to identify and extract license plate information from provided car images, ensuring the output aligns with the query's requirements for accurate recognition.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "License Plate Detection",
                    "api_call": "yolov5.load('keremberke/yolov5m-license-plate')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": [
                        "import yolov5",
                        "model = yolov5.load('keremberke/yolov5m-license-plate')",
                        "model.conf = 0.25",
                        "model.iou = 0.45",
                        "model.agnostic = False",
                        "model.multi_label = False",
                        "model.max_det = 1000",
                        "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model(img, size=640)",
                        "results = model(img, augment=True)",
                        "predictions = results.pred[0]",
                        "boxes = predictions[:, :4]",
                        "scores = predictions[:, 4]",
                        "categories = predictions[:, 5]",
                        "results.show()",
                        "results.save(save_dir='results/')"
                    ],
                    "performance": {
                        "dataset": "keremberke/license-plate-object-detection",
                        "accuracy": 0.988
                    },
                    "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.",
                    "id": "huggingface_api_214",
                    "name": "keremberke/yolov5m-license-plate"
                },
                "id": "gorilla_huggingface_tool_210",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_146",
        "query": "A famous writer is working on a novel. He needs your help to predict the punctuation marks needed in his written draft.",
        "instruction": "Given a `punctuation prediction` task, retrieve tools that are designed to assist in natural language processing by analyzing text inputs and accurately determining the necessary punctuation marks to enhance the coherence and readability of the written content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('token-classification')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "multilingual",
                        "accuracy": 0.98
                    },
                    "description": "A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.",
                    "id": "huggingface_api_934",
                    "name": "kredor/punctuate-all"
                },
                "id": "gorilla_huggingface_tool_415",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_147",
        "query": "We are a medical research team working on a COVID-19 project. We need assistance in question answering related to the pandemic and related research papers.",
        "instruction": "Given a `question answering` task related to COVID-19, retrieve tools that specialize in processing natural language queries and providing detailed answers from a database of pandemic-related research documents. The tools should use pre-trained models optimized for extractive question answering to ensure accurate and contextually relevant responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2-covid'), tokenizer=RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2-covid'))",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2-covid",
                        "tokenizer": "deepset/roberta-base-squad2-covid"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "QA_input": {
                            "question": "Why is model conversion important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                        },
                        "res": "nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "XVAL_EM": 0.17890995260663506,
                            "XVAL_f1": 0.49925444207319924,
                            "XVAL_top_3_recall": 0.8021327014218009
                        }
                    },
                    "description": "This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.",
                    "id": "huggingface_api_472",
                    "name": "deepset/roberta-base-squad2-covid"
                },
                "id": "gorilla_huggingface_tool_468",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_148",
        "query": "We have some clients asking for estimating CO2 emissions based on their historic data, which is in a CSV file.",
        "instruction": "Given an `emission estimation` task, retrieve tools that can process tabular data inputs, typically from CSV files, to estimate CO2 emissions using regression models, ensuring compatibility with the task's requirements and data format.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-dragino-7-7",
                        "accuracy": {
                            "Loss": 84.433,
                            "R2": 0.54,
                            "MSE": 7129.004,
                            "MAE": 62.626,
                            "RMSLE": 0.418
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540.",
                    "id": "huggingface_api_879",
                    "name": "autotrain-dragino-7-7-1860763606"
                },
                "id": "gorilla_huggingface_tool_872",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_149",
        "query": "I am building an AI to create in work report, this model should be able to complete the sentence for the report.",
        "instruction": "Given a `sentence completion` task, retrieve tools that employ masked language modeling techniques to predict and fill in missing words or phrases in a text, thereby aiding in the creation of coherent and contextually appropriate report content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='xlm-roberta-base')",
                    "api_arguments": {
                        "model": "xlm-roberta-base"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='xlm-roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "CommonCrawl",
                        "accuracy": "N/A"
                    },
                    "description": "XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task.",
                    "id": "huggingface_api_657",
                    "name": "xlm-roberta-base"
                },
                "id": "gorilla_huggingface_tool_652",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_150",
        "query": "I want to identify when people are speaking in an audio file.",
        "instruction": "Given an `audio voice activity detection` task, retrieve tools that analyze audio files to identify periods of active speech by processing audio inputs and providing timestamps or segments where speech activity is detected.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Automatic Speech Recognition",
                    "api_call": "Pipeline.from_pretrained('pyannote/voice-activity-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # active speech between speech.start and speech.end",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": "Not specified"
                    },
                    "description": "A pretrained voice activity detection pipeline that detects active speech in audio files.",
                    "id": "huggingface_api_745",
                    "name": "pyannote/voice-activity-detection"
                },
                "id": "gorilla_huggingface_tool_740",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_151",
        "query": "I have a Catalan language text and I need to publish it as a blog article in Spanish, so I want to translate my Catalan text into Spanish.",
        "instruction": "Given a `language translation` task, retrieve tools that facilitate translating text from Catalan to Spanish by processing the source language input and producing a coherent target language output aligned with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ca-es') , MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ca-es')",
                    "api_arguments": [
                        "source_languages",
                        "target_languages",
                        "dataset",
                        "model",
                        "pre-processing",
                        "download_original_weights",
                        "test_set_translations",
                        "test_set_scores"
                    ],
                    "python_environment_requirements": [
                        "PyTorch",
                        "TensorFlow",
                        "Transformers"
                    ],
                    "example_code": "from transformers import MarianMTModel, MarianTokenizer",
                    "performance": {
                        "dataset": "Tatoeba.ca.es",
                        "accuracy": {
                            "BLEU": 74.9,
                            "chr-F": 0.863
                        }
                    },
                    "description": "A Hugging Face model for translation between Catalan (ca) and Spanish (es) languages, based on the OPUS dataset and using the transformer-align architecture. The model has been pre-processed with normalization and SentencePiece.",
                    "id": "huggingface_api_541",
                    "name": "opus-mt-ca-es"
                },
                "id": "gorilla_huggingface_tool_537",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_152",
        "query": "A Russia-based online lesson website needs an English subtitle for the recorded lessons of the students.",
        "instruction": "Given an `audio-to-text subtitle generation` task, retrieve tools that facilitate the creation of English subtitles for Russian audio content by employing speech recognition to transcribe recordings, ensuring the outputs match the linguistic and contextual requirements of the lessons.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Recognition",
                    "api_call": "SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')",
                    "api_arguments": [
                        "audio_paths"
                    ],
                    "python_environment_requirements": [
                        "huggingsound",
                        "torch",
                        "librosa",
                        "datasets",
                        "transformers"
                    ],
                    "example_code": "from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\ntranscriptions = model.transcribe(audio_paths)",
                    "performance": {
                        "dataset": "mozilla-foundation/common_voice_6_0",
                        "accuracy": {
                            "Test WER": 13.3,
                            "Test CER": 2.88,
                            "Test WER (+LM)": 9.57,
                            "Test CER (+LM)": 2.24
                        }
                    },
                    "description": "Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.",
                    "id": "huggingface_api_758",
                    "name": "jonatasgrosman/wav2vec2-large-xlsr-53-russian"
                },
                "id": "gorilla_huggingface_tool_753",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_153",
        "query": "The client is a real estate company working on virtual tours. We need to help them estimate depth in images of houses.",
        "instruction": "Given a `depth estimation` task, retrieve tools that specialize in computer vision applications by processing image inputs to accurately estimate depth information in virtual tours or real estate imagery.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3497,
                            "Mae": 0.2847,
                            "Rmse": 0.3977,
                            "Abs Rel": 0.3477,
                            "Log Mae": 0.1203,
                            "Log Rmse": 0.1726,
                            "Delta1": 0.5217,
                            "Delta2": 0.8246,
                            "Delta3": 0.9436
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.",
                    "id": "huggingface_api_169",
                    "name": "glpn-kitti-finetuned-diode-221214-123047"
                },
                "id": "gorilla_huggingface_tool_165",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_154",
        "query": "Our company is exploring the Chinese market and we need to communicate effectively with our clients. Help me create a generic Chinese response.",
        "instruction": "Given a `language generation` task, retrieve tools that facilitate effective communication by generating coherent responses in a specified language, focusing on natural language processing models tailored for the target linguistic needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('bert-base-chinese')",
                    "api_arguments": {
                        "pretrained_model_name": "bert-base-chinese"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForMaskedLM"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)",
                    "performance": {
                        "dataset": "[More Information Needed]",
                        "accuracy": "[More Information Needed]"
                    },
                    "description": "This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.",
                    "id": "huggingface_api_667",
                    "name": "bert-base-chinese"
                },
                "id": "gorilla_huggingface_tool_662",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_155",
        "query": "We want to classify images supplied by our users into categories such as cats, dogs, birds, and more.",
        "instruction": "Given an `image classification` task, retrieve tools that are capable of categorizing images into predefined classes, such as animals, by processing visual inputs and leveraging zero-shot learning techniques to generalize classifications without needing task-specific training data.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('openai/clip-vit-base-patch16')",
                    "api_arguments": [
                        "text",
                        "images",
                        "return_tensors",
                        "padding"
                    ],
                    "python_environment_requirements": [
                        "PIL",
                        "requests",
                        "transformers"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch16)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            "Food101",
                            "CIFAR10",
                            "CIFAR100",
                            "Birdsnap",
                            "SUN397",
                            "Stanford Cars",
                            "FGVC Aircraft",
                            "VOC2007",
                            "DTD",
                            "Oxford-IIIT Pet dataset",
                            "Caltech101",
                            "Flowers102",
                            "MNIST",
                            "SVHN",
                            "IIIT5K",
                            "Hateful Memes",
                            "SST-2",
                            "UCF101",
                            "Kinetics700",
                            "Country211",
                            "CLEVR Counting",
                            "KITTI Distance",
                            "STL-10",
                            "RareAct",
                            "Flickr30",
                            "MSCOCO",
                            "ImageNet",
                            "ImageNet-A",
                            "ImageNet-R",
                            "ImageNet Sketch",
                            "ObjectNet (ImageNet Overlap)",
                            "Youtube-BB",
                            "ImageNet-Vid"
                        ],
                        "accuracy": "varies depending on the dataset"
                    },
                    "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.",
                    "id": "huggingface_api_351",
                    "name": "openai/clip-vit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_347",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_156",
        "query": "We are a company that provides transcription services. We require voice activity detection in our audio recordings.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that specialize in analyzing audio recordings to identify and segment periods of speech activity by using models trained on relevant datasets to enhance transcription services.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Voice Activity Detection",
                    "api_call": "Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "pyannote.audio 2.1.1",
                    "example_code": "from pyannote.audio import Model\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": "N/A"
                    },
                    "description": "Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.",
                    "id": "huggingface_api_832",
                    "name": "popcornell/pyannote-segmentation-chime6-mixer6"
                },
                "id": "gorilla_huggingface_tool_827",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_157",
        "query": "Develop a tool that helps me get answers to questions related to a specific text.",
        "instruction": "Given a `question answering development` task, retrieve tools that facilitate generating answers to questions based on specific textual context by employing natural language processing techniques to interpret and respond accurately to user queries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='deepset/roberta-large-squad2')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "Not provided"
                    },
                    "description": "A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.",
                    "id": "huggingface_api_471",
                    "name": "deepset/roberta-large-squad2"
                },
                "id": "gorilla_huggingface_tool_467",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_158",
        "query": "I have a website text about technology and I want to know if it represents a positive sentiment or a negative one.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that process text inputs to determine the sentiment expressed, whether positive or negative, to align with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')",
                    "api_arguments": "text, candidate_labels",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\nresult = nlp('The movie was great!', ['positive', 'negative'])\nprint(result)",
                    "performance": {
                        "dataset": "MNLI",
                        "accuracy": {
                            "matched_acc": "89.19",
                            "mismatched_acc": "89.01"
                        }
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.",
                    "id": "huggingface_api_511",
                    "name": "valhalla/distilbart-mnli-12-6"
                },
                "id": "gorilla_huggingface_tool_507",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_159",
        "query": "Our organization sells movies. We need to collect reviews from various platforms to understand the popularity of a movie.",
        "instruction": "Given a `movie review collection` task, retrieve tools that can gather reviews from various platforms to analyze movie popularity, ensuring compatibility with text classification and sentiment analysis frameworks to interpret public opinion accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Classification",
                    "api_call": "pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers",
                        "pytorch"
                    ],
                    "example_code": "classifier('I love this movie!')",
                    "performance": {
                        "dataset": "imdb",
                        "accuracy": 0.928
                    },
                    "description": "This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.",
                    "id": "huggingface_api_389",
                    "name": "lvwerra/distilbert-imdb"
                },
                "id": "gorilla_huggingface_tool_385",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_160",
        "query": "Organizing a special event and want to identify the place where street pictures were taken for invitations.",
        "instruction": "Given an `image geolocalization` task, retrieve tools that can identify the locations of street pictures by processing image inputs to provide their geographic details and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Geolocalization",
                    "api_call": "CLIPModel.from_pretrained('geolocal/StreetCLIP')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "geolocal/StreetCLIP"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "IM2GPS",
                                "accuracy": {
                                    "25km": 28.3,
                                    "200km": 45.1,
                                    "750km": 74.7,
                                    "2500km": 88.2
                                }
                            },
                            {
                                "name": "IM2GPS3K",
                                "accuracy": {
                                    "25km": 22.4,
                                    "200km": 37.4,
                                    "750km": 61.3,
                                    "2500km": 80.4
                                }
                            }
                        ]
                    },
                    "description": "StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.",
                    "id": "huggingface_api_374",
                    "name": "geolocal/StreetCLIP"
                },
                "id": "gorilla_huggingface_tool_370",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_161",
        "query": "I am an astronaut in space, writing my diary every day. I need a summary of my diary before sharing it with my family.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise summaries, ensuring the output maintains the essence and key points of the original diary entries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "2109.02492"
                    },
                    "description": "DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.",
                    "id": "huggingface_api_642",
                    "name": "DialogLED-base-16384"
                },
                "id": "gorilla_huggingface_tool_637",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_162",
        "query": "Help me to build a visual-question-answering model that takes a question and an image as input and returns an answer based on the image.",
        "instruction": "Given a `visual-question-answering model building` task, retrieve tools that facilitate creating models which process image and text inputs to provide answers based on the visual content. These tools should support integration with libraries such as Hugging Face Transformers to enable effective multimodal processing.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')",
                    "api_arguments": {
                        "model": "Bingsu/temp_vilt_vqa",
                        "tokenizer": "Bingsu/temp_vilt_vqa"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A visual question answering model for answering questions related to images using the Hugging Face Transformers library.",
                    "id": "huggingface_api_112",
                    "name": "temp_vilt_vqa"
                },
                "id": "gorilla_huggingface_tool_111",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_163",
        "query": "We are developing a virtual assistant. One of its major functionalities is to convert written text to speech.",
        "instruction": "Given a `text-to-speech` task, retrieve tools that convert written text inputs into spoken audio outputs by using functionalities such as model ensemble and text-to-speech synthesis, ensuring the virtual assistant can perform speech functions as required.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-200_speaker-cv4,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": null
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.",
                    "id": "huggingface_api_735",
                    "name": "fastspeech2-en-male1"
                },
                "id": "gorilla_huggingface_tool_730",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_164",
        "query": "I am an English teacher. What kind of creative writing exercise could I give to my students that they could write a story using the words \"moon\", \"rabbit\", \"forest\", and \"magic\"?",
        "instruction": "Given a `creative writing exercise` task, retrieve tools that facilitate generative text tasks, such as forming coherent and imaginative stories, by processing input words or themes to produce creative narrative outputs aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Generative Commonsense Reasoning",
                    "api_call": "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')",
                    "api_arguments": [
                        "words",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\ndef gen_sentence(words, max_length=32):\n input_text = words\n features = tokenizer([input_text], return_tensors='pt')\noutput = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'], max_length=max_length)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nwords = tree plant ground hole dig\ngen_sentence(words)",
                    "performance": {
                        "dataset": "common_gen",
                        "accuracy": {
                            "ROUGE-2": 17.1,
                            "ROUGE-L": 39.47
                        }
                    },
                    "description": "Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.",
                    "id": "huggingface_api_631",
                    "name": "mrm8488/t5-base-finetuned-common_gen"
                },
                "id": "gorilla_huggingface_tool_626",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_165",
        "query": "We want to build an audiobook service. We need to convert text to speech for the user to listen.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform text inputs into synthesized speech outputs, facilitating the creation of audiobooks or other audio content aligned with the user's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": ""
                    },
                    "description": "A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.",
                    "id": "huggingface_api_738",
                    "name": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan"
                },
                "id": "gorilla_huggingface_tool_733",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_166",
        "query": "Can you teach me to build a program that will analyze video and describe what's happening in natural language?",
        "instruction": "Given a `video analysis and description` task, retrieve tools capable of processing video inputs to analyze and generate descriptive natural language text that explains the content and actions occurring in the video, aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "XClipModel.from_pretrained('microsoft/xclip-base-patch32')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "Kinetics 400",
                        "accuracy": {
                            "top-1": 80.4,
                            "top-5": 95.0
                        }
                    },
                    "description": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.",
                    "id": "huggingface_api_312",
                    "name": "microsoft/xclip-base-patch32"
                },
                "id": "gorilla_huggingface_tool_308",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_167",
        "query": "We are building an automatic video generation platform based on user-provided text. We need a reliable model to convert text instructions into appropriate videos.",
        "instruction": "Given a `text-to-video conversion` task, retrieve tools that transform text inputs into corresponding video outputs by utilizing models capable of generating multimedia content based on the provided text instructions and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video",
                    "api_call": "pipeline('text-to-video', model='ImRma/Brucelee')",
                    "api_arguments": [
                        "your_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Hugging Face model for converting Persian and English text into video.",
                    "id": "huggingface_api_92",
                    "name": "ImRma/Brucelee"
                },
                "id": "gorilla_huggingface_tool_92",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_168",
        "query": "There is an upcoming event called \"Space Party\" and we need a representative image for the event. Can you assist us in creating an image containing a party in space with astronauts and aliens having fun together?",
        "instruction": "Given an `image generation` task, retrieve tools that utilize text prompts to create visually appealing images based on the query's specifications, such as theme and elements to be included in the image.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')",
                    "api_arguments": {
                        "prompt": "a photo of an astronaut riding a horse on mars"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.",
                    "id": "huggingface_api_36",
                    "name": "stabilityai/stable-diffusion-2-1"
                },
                "id": "gorilla_huggingface_tool_36",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_169",
        "query": "We are building a product which can identify birds in the images. Design the model which can help us segment the birds in an image.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process image inputs to identify and segment objects such as birds by leveraging transformer models designed for instance segmentation tasks, aligning with the requirements of computer vision projects.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mask2former-swin-tiny-coco-instance"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\npredicted_instance_map = result['segmentation']",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.",
                    "id": "huggingface_api_250",
                    "name": "facebook/mask2former-swin-tiny-coco-instance"
                },
                "id": "gorilla_huggingface_tool_246",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_170",
        "query": "A company has collected data about fish measurements and weights. We need to estimate a fish's weight based on its measurements.",
        "instruction": "Given a `weight estimation` task, retrieve tools that perform regression analysis by processing measurement data to predict or estimate a fish's weight, leveraging models like GradientBoostingRegressor to achieve the desired prediction accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "GradientBoostingRegressor",
                    "api_call": "skops.io.load('path_to_folder/example.pkl')",
                    "api_arguments": {
                        "model_path": "path_to_folder/example.pkl"
                    },
                    "python_environment_requirements": {
                        "skops.hub_utils": "download",
                        "skops.io": "load"
                    },
                    "example_code": "from skops.hub_utils import download\nfrom skops.io import load\ndownload('brendenc/Fish-Weight', 'path_to_folder')\nmodel = load('path_to_folder/example.pkl')",
                    "performance": {
                        "dataset": "Fish dataset",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.",
                    "id": "huggingface_api_886",
                    "name": "Fish-Weight"
                },
                "id": "gorilla_huggingface_tool_879",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_171",
        "query": "We are launching a new blog and need a paragraph with tips on how to take care of houseplants.",
        "instruction": "Given a `content creation` task, retrieve tools that generate coherent and informative text by processing inputs related to the specified topic, such as tips for taking care of houseplants, to produce a paragraph aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='bigscience/bloom-7b1')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-generation', model='bigscience/bloom-7b1')\nresult = model('Once upon a time')\nprint(result)",
                    "performance": {
                        "dataset": "Training Data",
                        "accuracy": {
                            "Training Loss": 2.3,
                            "Validation Loss": 2.9,
                            "Perplexity": 16
                        }
                    },
                    "description": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text.",
                    "id": "huggingface_api_606",
                    "name": "bigscience/bloom-7b1"
                },
                "id": "gorilla_huggingface_tool_601",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_172",
        "query": "I need a method to compare the similarity between two sentences to be used within a meme generator, so we can produce a meme with a similar caption.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that evaluate and compare the semantic similarity between two sentences by processing textual inputs and generating embeddings or similarity scores to assist in creating related content, such as memes.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_173",
        "query": "Generate an image of a beautiful fantasy landscape based on the description provided: a peaceful scene in a lush green forest with a crystal-clear river flowing through it, under a blue sky with fluffy white clouds.",
        "instruction": "Given an `image generation` task, retrieve tools that transform descriptive text prompts into visual representations, specifically creating detailed and aesthetically appealing images of fantasy landscapes using advanced text-to-image models and techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')",
                    "api_arguments": {
                        "model_id": "darkstorm2150/Protogen_v5.8_Official_Release",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "diffusers"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\nprompt = (\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\n)\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.",
                    "id": "huggingface_api_57",
                    "name": "darkstorm2150/Protogen_x5.8_Official_Release"
                },
                "id": "gorilla_huggingface_tool_57",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_174",
        "query": "I would like to give a brief overview of our team meeting to my supervisor, so I need a summary of the conversation.",
        "instruction": "Given a `text summarization` task, retrieve tools that can process conversational inputs to generate concise summaries, effectively capturing the key points of the meeting as outlined in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Summarization",
                    "api_call": "pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')",
                    "api_arguments": {
                        "model": "philschmid/distilbart-cnn-12-6-samsum"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\nconversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation? \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "samsum",
                        "accuracy": {
                            "ROUGE-1": 41.09,
                            "ROUGE-2": 20.746,
                            "ROUGE-L": 31.595,
                            "ROUGE-LSUM": 38.339
                        }
                    },
                    "description": "This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.",
                    "id": "huggingface_api_552",
                    "name": "distilbart-cnn-12-6-samsum"
                },
                "id": "gorilla_huggingface_tool_548",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_175",
        "query": "As a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.",
        "instruction": "Given a `speech sentiment analysis` task, retrieve tools that perform audio feature extraction to analyze and interpret the sentiment present in a group of people within an audio input, utilizing advanced speech representation learning techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "api_arguments": "pretrained model name",
                    "python_environment_requirements": "transformers",
                    "example_code": "hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "performance": {
                        "dataset": "Libri-Light",
                        "accuracy": "matches or improves upon the state-of-the-art wav2vec 2.0 performance"
                    },
                    "description": "Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.",
                    "id": "huggingface_api_12",
                    "name": "hubert-large-ll60k"
                },
                "id": "gorilla_huggingface_tool_12",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_176",
        "query": "The customer is writing a book about the solar system and just needs the answer to a question: How long does it take for Mars to orbit the sun?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing techniques to extract answers from a provided context, specifically targeting factual information about the solar system.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_177",
        "query": "Assist us in creating a question answering system to provide quick answers for customer inquiries.",
        "instruction": "Given a `question answering system development` task, retrieve tools that leverage natural language processing to create systems capable of providing quick and accurate answers to customer inquiries by utilizing models fine-tuned on relevant datasets.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_178",
        "query": "I need assistance in predicting carbon emissions of a city based on historical data. Use the dataset provided to predict future carbon emissions.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that perform regression analysis using historical data to forecast future carbon emissions, ensuring compatibility with tabular data inputs and leveraging trained models for accurate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-mikrotik-7-7",
                        "accuracy": {
                            "Loss": 48.213,
                            "R2": 0.654,
                            "MSE": 2324.518,
                            "MAE": 32.634,
                            "RMSLE": 0.586
                        }
                    },
                    "description": "A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.",
                    "id": "huggingface_api_874",
                    "name": "pcoloc/autotrain-mikrotik-7-7-1860563588"
                },
                "id": "gorilla_huggingface_tool_867",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_179",
        "query": "I work at an art school and our professor wants to create an AI chatbot that can study an image of a painting and answer questions about it.",
        "instruction": "Given a `visual question answering` task, retrieve tools that leverage multimodal capabilities to analyze images and generate informative textual responses based on the visual content and accompanying questions.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_180",
        "query": "We want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.",
        "instruction": "Given a `sports video classification` task, retrieve tools that classify video content based on the type of sports depicted, utilizing advanced video analysis techniques to process video inputs and accurately categorize them according to sports-related criteria.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')",
                    "api_arguments": [
                        "video"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": {
                            "top-1": 84.7,
                            "top-5": 96.5
                        }
                    },
                    "description": "VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_327",
                    "name": "MCG-NJU/videomae-large-finetuned-kinetics"
                },
                "id": "gorilla_huggingface_tool_323",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_181",
        "query": "I am working in a bank, I want to estimate the mortgage for a given housing using the housing's features.",
        "instruction": "Given a `mortgage estimation` task, retrieve tools that utilize housing features and predictive models to estimate the mortgage value by processing tabular data inputs and leveraging regression algorithms to provide accurate financial assessments.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": {
                        "data": "pandas.DataFrame"
                    },
                    "python_environment_requirements": {
                        "joblib": "latest",
                        "pandas": "latest"
                    },
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 134406.507,
                            "R2": 0.861,
                            "MSE": 18065109105.27,
                            "MAE": 103271.843,
                            "RMSLE": 0.139
                        }
                    },
                    "description": "A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.",
                    "id": "huggingface_api_871",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761511"
                },
                "id": "gorilla_huggingface_tool_864",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_182",
        "query": "A client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.",
        "instruction": "Given an `object detection in images` task, retrieve tools designed for computer vision that can analyze images to list objects present by taking in image data and providing precise detections and attributes relevant to a real estate context.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')",
                    "api_arguments": [
                        "texts",
                        "images"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "PIL",
                        "torch",
                        "transformers"
                    ],
                    "example_code": "processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not provided"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.",
                    "id": "huggingface_api_221",
                    "name": "google/owlvit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_217",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_183",
        "query": "Write a script to translate the following French sentence into English: \"Je tâ€™aime.\"",
        "instruction": "Given a `translation` task, retrieve tools that utilize natural language processing to translate text from one language to another by processing language-specific inputs and returning accurate translations aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')",
                    "api_arguments": {
                        "checkpoint": "bigscience/bloomz-560m",
                        "inputs": "Translate to English: Je tâ€™aime."
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "accelerate",
                        "bitsandbytes"
                    ],
                    "example_code": {
                        "CPU": "from transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = bigscience/bloomz-560m\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\ninputs = tokenizer.encode(Translate to English: Je tâ€™aime., return_tensors=pt)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))"
                    },
                    "performance": {
                        "dataset": "bigscience/xP3",
                        "accuracy": {
                            "Winogrande XL (xl) validation set": 52.41,
                            "XWinograd (en) test set": 51.01,
                            "XWinograd (fr) test set": 51.81,
                            "XWinograd (jp) test set": 52.03,
                            "XWinograd (pt) test set": 53.99,
                            "XWinograd (ru) test set": 53.97,
                            "XWinograd (zh) test set": 54.76,
                            "ANLI (r1) validation set": 33.4,
                            "ANLI (r2) validation set": 33.4,
                            "ANLI (r3) validation set": 33.5
                        }
                    },
                    "description": "BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.",
                    "id": "huggingface_api_627",
                    "name": "bigscience/bloomz-560m"
                },
                "id": "gorilla_huggingface_tool_622",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_184",
        "query": "Our client is an AI gaming company and we need to develop a bot for the game Valorant. The bot should detect objects like dropped spike, enemy, planted spike, and teammate within the game.",
        "instruction": "Given a `game bot development` task, retrieve tools that perform object detection within a gaming environment by recognizing specific in-game elements such as dropped spikes, enemies, planted spikes, and teammates, aligning with the role of detecting these features in games like Valorant.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-valorant-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "valorant-object-detection",
                        "accuracy": 0.965
                    },
                    "description": "A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.",
                    "id": "huggingface_api_215",
                    "name": "keremberke/yolov8m-valorant-detection"
                },
                "id": "gorilla_huggingface_tool_211",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_185",
        "query": "I need to create a new piece of art to add to my digital gallery that resembles WikiArt images.",
        "instruction": "Given an `art creation` task, retrieve tools that facilitate digital art generation by leveraging image generation capabilities to produce artwork resembling specific art styles or collections, such as WikiArt.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "https://huggingface.co/datasets/huggan/wikiart",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.",
                    "id": "huggingface_api_297",
                    "name": "johnowhitaker/sd-class-wikiart-from-bedrooms"
                },
                "id": "gorilla_huggingface_tool_293",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_186",
        "query": "You have been provided with the dataset of plants, and your task is to identify the species of plants among Iris Setosa, Iris Versicolor, and Iris Virginica.",
        "instruction": "Given a `plant species classification` task, retrieve tools that can identify plant species using a pre-trained machine learning model by processing tabular data inputs to provide predictions of species such as Iris Setosa, Iris Versicolor, and Iris Virginica.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Transformers",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "scikit-learn/iris",
                        "accuracy": 0.9
                    },
                    "description": "A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.",
                    "id": "huggingface_api_859",
                    "name": "abhishek/autotrain-iris-knn"
                },
                "id": "gorilla_huggingface_tool_852",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_187",
        "query": "We were given an audio file of a company presentation, and we need it transcribed verbatim.",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in processing audio inputs for verbatim transcriptions, ensuring accurate and detailed conversion of spoken content to text, aligned with the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription and Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')",
                    "api_arguments": [
                        "sample",
                        "sampling_rate",
                        "language",
                        "task",
                        "skip_special_tokens"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": "from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "LibriSpeech (clean)",
                                "accuracy": 2.9
                            },
                            {
                                "name": "LibriSpeech (other)",
                                "accuracy": 5.9
                            },
                            {
                                "name": "Common Voice 11.0",
                                "accuracy": 53.87
                            }
                        ]
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.",
                    "id": "huggingface_api_767",
                    "name": "openai/whisper-medium"
                },
                "id": "gorilla_huggingface_tool_762",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_188",
        "query": "Our company deals with insurance claims. We need a smart assistant who can retrieve information from documents, especially invoices, such as total amount, date of invoice, and name of the service provider.",
        "instruction": "Given a `document information retrieval` task, retrieve tools that can process documents, especially invoices, by extracting and returning key information such as total amount, date, and service provider name from the content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "vision-encoder-decoder",
                    "api_call": "pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')",
                    "api_arguments": "image_path, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "doc_vqa(image_path='path/to/image.jpg', question='What is the title?')",
                    "performance": {
                        "dataset": "DocVQA",
                        "accuracy": "Not provided"
                    },
                    "description": "Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.",
                    "id": "huggingface_api_124",
                    "name": "jinhybr/OCR-DocVQA-Donut"
                },
                "id": "gorilla_huggingface_tool_123",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_189",
        "query": "We are designing an application for professional athletes. They need a tool to categorize their exercises based on videos.",
        "instruction": "Given a `video classification` task, retrieve tools that categorize exercises by interpreting video data, leveraging models capable of understanding and classifying actions within video content to meet the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')",
                    "api_arguments": [
                        "images"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": null
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_318",
                    "name": "facebook/timesformer-base-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_314",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_190",
        "query": "I want to know the depth information of an image for a robot navigation project.",
        "instruction": "Given a `depth estimation` task, retrieve tools that analyze image inputs to provide depth information, focusing on functionality suitable for projects such as robot navigation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Depth Estimation",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')",
                    "api_arguments": "None",
                    "python_environment_requirements": "Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2",
                    "example_code": "None",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.548,
                            "Rmse": "nan"
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_161",
                    "name": "glpn-nyu-finetuned-diode-221116-062619"
                },
                "id": "gorilla_huggingface_tool_157",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_191",
        "query": "The company needs to separate voice from background noise in a recorded podcast episode.",
        "instruction": "Given an `audio processing` task to separate voice from background noise, retrieve tools that enhance audio clarity by isolating speech from noise in recorded audio files, ensuring improved sound quality and intelligibility.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_192",
        "query": "We are building a fiction-sharing platform for multiple languages. Extract named entities from the given text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform multilingual entity extraction from text by utilizing models capable of processing and identifying entities across multiple languages, aligning with the query's needs for a fiction-sharing platform.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)",
                        "tokenizer": "AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = My name is Wolfgang and I live in Berlin\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": "Babelscape/wikineural-multilingual-ner",
                        "accuracy": "span-based F1-score up to 6 points over previous state-of-the-art systems for data creation"
                    },
                    "description": "A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.",
                    "id": "huggingface_api_435",
                    "name": "Babelscape/wikineural-multilingual-ner"
                },
                "id": "gorilla_huggingface_tool_431",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_193",
        "query": "A user wants to improve the quality of a small sized picture from her vacation. We need to upscale the picture to 2x its size without losing quality.",
        "instruction": "Given an `image upscaling` task, retrieve tools that enhance images by increasing their resolution while maintaining quality by processing small-sized image inputs to produce higher resolution outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Super-Resolution",
                    "api_call": "Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')",
                    "api_arguments": "image, model, feature_extractor",
                    "python_environment_requirements": "transformers",
                    "example_code": "Refer to the documentation.",
                    "performance": {
                        "dataset": "arxiv: 2209.11345",
                        "accuracy": "Not provided"
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.",
                    "id": "huggingface_api_273",
                    "name": "caidas/swin2SR-classical-sr-x2-64"
                },
                "id": "gorilla_huggingface_tool_269",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_194",
        "query": "I want to have a personal assistant app that can answer questions from a given text.",
        "instruction": "Given a `question answering` task, retrieve tools that process text inputs to find and extract relevant answers from a provided context, enabling the querying of information with high accuracy and speed.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\ncontext = r\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\nprint(\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\n...)",
                    "performance": {
                        "dataset": "SQuAD v1.1",
                        "accuracy": {
                            "Exact Match": 79.6,
                            "F1": 86.996
                        }
                    },
                    "description": "DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.",
                    "id": "huggingface_api_468",
                    "name": "distilbert-base-cased-distilled-squad"
                },
                "id": "gorilla_huggingface_tool_464",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_195",
        "query": "We need to classify actions of athletes in sports videos. Can you help us to analyze and classify these videos?",
        "instruction": "Given a `video classification` task, retrieve tools that are capable of analyzing video content to classify actions within sports videos, leveraging models that process video inputs as sequences of frames or patches to extract features for classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')",
                    "api_arguments": "video",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss",
                    "performance": {
                        "dataset": "Something-Something-v2",
                        "accuracy": ""
                    },
                    "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.",
                    "id": "huggingface_api_337",
                    "name": "videomae-base-ssv2"
                },
                "id": "gorilla_huggingface_tool_333",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_196",
        "query": "Create a tool that receives a table and a question in natural language, and returns an answer to the question based on the inputted table.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that process natural language queries and tabular data inputs to provide accurate responses based on the information within the tables.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')",
                    "api_arguments": [
                        "model_name",
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\noutputs = model(**inputs)\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())",
                    "performance": {
                        "dataset": "SQA",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.",
                    "id": "huggingface_api_455",
                    "name": "lysandre/tapas-temporary-repo"
                },
                "id": "gorilla_huggingface_tool_451",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_197",
        "query": "The security department needs assistance to detect suspicious objects and people using a zero-shot text-conditioned object detection system.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize zero-shot text-conditioned models to detect and identify suspicious objects and individuals by processing text and image inputs in varied environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')",
                    "api_arguments": [
                        "texts",
                        "images"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "PIL",
                        "torch",
                        "transformers"
                    ],
                    "example_code": "processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not provided"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.",
                    "id": "huggingface_api_221",
                    "name": "google/owlvit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_217",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_198",
        "query": "I have a picture of a room demonstrating a mixture of objects. The model needs to seperate the objects and label them accordingly.",
        "instruction": "Given an `image segmentation and labeling` task, retrieve tools that utilize computer vision algorithms to process image inputs, separate objects within the image, and assign appropriate labels to each object based on their characteristics.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')",
                    "api_arguments": {
                        "from_pretrained": "facebook/maskformer-swin-large-ade"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\ninputs = processor(images=image, return_tensors='pt')\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ADE20k",
                        "accuracy": "Not provided"
                    },
                    "description": "MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.",
                    "id": "huggingface_api_243",
                    "name": "facebook/maskformer-swin-large-ade"
                },
                "id": "gorilla_huggingface_tool_239",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_199",
        "query": "I have this app for sharing cooking recipes. Users upload photos and ask questions about the showcased recipe. I need to automatically answer their questions based on the recipe image provided.",
        "instruction": "Given a `visual question answering` task, retrieve tools that utilize multimodal models to interpret and respond to questions based on visual input, such as images, by integrating both visual and textual data to generate accurate answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A VisualBERT model for Visual Question Answering.",
                    "id": "huggingface_api_484",
                    "name": "uclanlp/visualbert-vqa"
                },
                "id": "gorilla_huggingface_tool_480",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_200",
        "query": "Our team is working on a recommendation system for a news article app. We should understand the semantic similarity of various texts.",
        "instruction": "Given a `semantic similarity analysis` task, retrieve tools that utilize feature extraction techniques to evaluate and compare the semantic content of various texts, supporting tasks that involve understanding and recommending based on textual similarities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)",
                    "performance": {
                        "dataset": "STS tasks",
                        "accuracy": "Spearman's correlation (See associated paper Appendix B)"
                    },
                    "description": "A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.",
                    "id": "huggingface_api_13",
                    "name": "sup-simcse-roberta-large"
                },
                "id": "gorilla_huggingface_tool_13",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_201",
        "query": "We are developing a voice-controlled drone. Please identify the spoken command in the audio clip provided.",
        "instruction": "Given a `spoken command recognition` task, retrieve tools that classify and interpret audio inputs to accurately identify spoken commands for integration with voice-controlled devices.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='mazkooleg/0-9up-unispeech-sat-base-ft')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.27.1, pytorch==1.11.0, datasets==2.10.1, tokenizers==0.12.1",
                    "example_code": "",
                    "performance": {
                        "dataset": "mazkooleg/0-9up_google_speech_commands_augmented_raw",
                        "accuracy": 0.9979
                    },
                    "description": "This model is a fine-tuned version of microsoft/unispeech-sat-base on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0123, Accuracy: 0.9979.",
                    "id": "huggingface_api_823",
                    "name": "mazkooleg/0-9up-unispeech-sat-base-ft"
                },
                "id": "gorilla_huggingface_tool_818",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_202",
        "query": "I want to build a personal assistant to check the logical relationship between two sentences, especially when I have a lot of texts in English.",
        "instruction": "Given a `logical relationship evaluation` task, retrieve tools that utilize natural language processing techniques to assess and determine the relationship between two sentences by processing textual inputs and delivering classification outcomes such as contradiction, entailment, or neutral.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-MiniLM2-L6-H768')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-MiniLM2-L6-H768')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": "SNLI and MultiNLI",
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder for evaluation results"
                    },
                    "description": "This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_507",
                    "name": "cross-encoder/nli-MiniLM2-L6-H768"
                },
                "id": "gorilla_huggingface_tool_503",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_203",
        "query": "I have jobs descriptions in French for sales manager, please highlight names of organizations or cities within the text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that specialize in identifying and highlighting entities such as organizations and cities within text. These tools should effectively process textual content in different languages and provide precise results based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')",
                    "api_arguments": {
                        "model": "model",
                        "tokenizer": "tokenizer",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\n\nfrom transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot Â« computer  est retir le 9 janvier 2015.)",
                    "performance": {
                        "dataset": "wikiner-fr",
                        "accuracy": {
                            "overall_f1": 0.8914,
                            "PER_f1": 0.9483,
                            "ORG_f1": 0.8181,
                            "LOC_f1": 0.8955,
                            "MISC_f1": 0.8146
                        }
                    },
                    "description": "camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.",
                    "id": "huggingface_api_405",
                    "name": "Jean-Baptiste/camembert-ner"
                },
                "id": "gorilla_huggingface_tool_401",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_204",
        "query": "We recently received low resolution images of newly released products and need to upscale them for better quality.",
        "instruction": "Given an `image enhancement` task, retrieve tools that focus on image processing operations, particularly upscaling images to improve resolution and quality by applying state-of-the-art transformer models designed for image super-resolution.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').",
                    "api_arguments": "feature_extractor, model",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.",
                    "id": "huggingface_api_277",
                    "name": "swin2SR-lightweight-x2-64"
                },
                "id": "gorilla_huggingface_tool_273",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_205",
        "query": "We have generated user reviews for movies. We need to check user opinions about the movie 'Inception' based on their reviews.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that process user reviews to determine opinions about specific movies, using natural language processing to classify sentiment as positive or negative based on the content of the reviews.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')",
                    "api_arguments": "text, candidate_labels",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\nresult = nlp('The movie was great!', ['positive', 'negative'])\nprint(result)",
                    "performance": {
                        "dataset": "MNLI",
                        "accuracy": {
                            "matched_acc": "89.19",
                            "mismatched_acc": "89.01"
                        }
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.",
                    "id": "huggingface_api_511",
                    "name": "valhalla/distilbart-mnli-12-6"
                },
                "id": "gorilla_huggingface_tool_507",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_206",
        "query": "My company wants to develop an application that will analyze images in relation to food and answer questions about them. We want it to handle questions like \"what is in the dish\" and \"how many calories does it have\".",
        "instruction": "Given an `image analysis and question-answering` task, retrieve tools that process visual inputs and answer related questions by integrating image data with food-specific queries to provide information like dish composition and calorie counts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')",
                    "api_arguments": {
                        "image": "path_to_image",
                        "question": "question_text"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Visual Question Answering model fine-tuned on the Polish language.",
                    "id": "huggingface_api_108",
                    "name": "azwierzc/vilt-b32-finetuned-vqa-pl"
                },
                "id": "gorilla_huggingface_tool_107",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_207",
        "query": "In surveillance operations, transcripts are used to turn the audio feed into chunks such that after processing transcripts there are no speakers talking over each other in the output.",
        "instruction": "Given a `speaker diarization` task, retrieve tools that process audio feeds to identify and separate individual speakers, ensuring no overlap in the transcription outputs by utilizing segmentation and diarization techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker diarization",
                    "api_call": "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": {
                        "load_pipeline": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "apply_pipeline": "diarization = pipeline(audio.wav)",
                        "save_output": "with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)"
                    },
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.",
                    "id": "huggingface_api_846",
                    "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_840",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_208",
        "query": "I want a language model that can fill in short blanks for example sentences, quizzes, or trivia questions.",
        "instruction": "Given a `fill-in-the-blank` task, retrieve tools that process textual inputs to generate or suggest suitable words or phrases to complete blank spaces in example sentences, quizzes, or trivia questions aligned with the query's content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaModel.from_pretrained('microsoft/deberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\nfill_mask('The capital of France is [MASK].')",
                    "performance": {
                        "dataset": {
                            "SQuAD 1.1": "93.1/87.2",
                            "SQuAD 2.0": "86.2/83.1",
                            "MNLI-m": "88.8"
                        }
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.",
                    "id": "huggingface_api_668",
                    "name": "microsoft/deberta-base"
                },
                "id": "gorilla_huggingface_tool_663",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_209",
        "query": "Develop a text-to-speech model for our mobile app to read news articles for our users.",
        "instruction": "Given a `text-to-speech development` task, retrieve tools that transform text inputs into spoken output for integration into mobile applications, focusing on their ability to handle diverse language datasets and deliver synthesized speech suited for user engagement with written content.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/fastspeech2-en-200_speaker-cv4')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-200_speaker-cv4,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": null
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.",
                    "id": "huggingface_api_735",
                    "name": "fastspeech2-en-male1"
                },
                "id": "gorilla_huggingface_tool_730",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_210",
        "query": "In this Star Wars movie scene, I want to create a depth estimation for the stormtroopers.",
        "instruction": "Given a `depth estimation` task, retrieve tools that perform depth estimation in images by applying computer vision techniques to estimate and provide depth-related insights from the visual scene, aligning with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Depth Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "PIL",
                        "numpy",
                        "torch"
                    ],
                    "example_code": {
                        "install_packages": "pip install diffusers transformers accelerate",
                        "code": [
                            "from transformers import pipeline",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "from PIL import Image",
                            "import numpy as np",
                            "import torch",
                            "from diffusers.utils import load_image",
                            "depth_estimator = pipeline('depth-estimation')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)",
                            "image = depth_estimator(image)['depth']",
                            "image = np.array(image)",
                            "image = image[:, :, None]",
                            "image = np.concatenate([image, image, image], axis=2)",
                            "image = Image.fromarray(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]",
                            "image.save('./images/stormtrooper_depth_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "3M depth-image, caption pairs",
                        "accuracy": "500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_265",
                    "name": "lllyasviel/sd-controlnet-depth"
                },
                "id": "gorilla_huggingface_tool_261",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_211",
        "query": "I am designing a quiz app that caters to blind users. The app should interpret the images and answer questions based on the image contents.",
        "instruction": "Given a `visual question answering` task, retrieve tools that can interpret and analyze images to answer questions based on their content, ensuring accessibility for users with visual impairments.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A VisualBERT model for Visual Question Answering.",
                    "id": "huggingface_api_484",
                    "name": "uclanlp/visualbert-vqa"
                },
                "id": "gorilla_huggingface_tool_480",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_212",
        "query": "We have serveral articles translated in different languages, and we want to group the articles discussing the specific topic.",
        "instruction": "Given a `topic-based article grouping` task, retrieve tools that utilize sentence embeddings to identify and cluster related content across multiple languages, enabling efficient organization of articles by specific topics.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "N/A"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_701",
                    "name": "sentence-transformers/distiluse-base-multilingual-cased-v1"
                },
                "id": "gorilla_huggingface_tool_696",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_213",
        "query": "Help me find similarity scores for different restaurant reviews.",
        "instruction": "Given a `sentence similarity evaluation` task, retrieve tools that analyze text inputs by computing similarity scores for reviews, leveraging models designed to capture semantic information and align with the query's content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')",
                    "api_arguments": "text",
                    "python_environment_requirements": "sentence-transformers library",
                    "example_code": "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')\ntext = Replace me by any text you'd like.\ntext_embbedding = model.encode(text)",
                    "performance": {
                        "dataset": "1,097,953,922",
                        "accuracy": "N/A"
                    },
                    "description": "The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.",
                    "id": "huggingface_api_713",
                    "name": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6"
                },
                "id": "gorilla_huggingface_tool_708",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_214",
        "query": "We are worried about price inflation in our country. Can you answer our questions on price inflation using the BERT large cased whole word masking finetuned model on SQuAD?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing models, specifically those fine-tuned for accurate comprehension and response generation about the topic of price inflation, using a specified model and dataset such as BERT large cased whole word masking fine-tuned on SQuAD.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-cased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_cased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\nprint(result)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BookCorpus",
                                "accuracy": "N/A"
                            },
                            {
                                "name": "English Wikipedia",
                                "accuracy": "N/A"
                            }
                        ]
                    },
                    "description": "BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.",
                    "id": "huggingface_api_486",
                    "name": "bert-large-cased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_482",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_215",
        "query": "Your company has launched a new chatbot service. We need to create conversations with the customers to provide information about our products and answer their queries.",
        "instruction": "Given a `customer interaction` task, retrieve tools that facilitate creating conversational dialogues with customers by employing text-generation models designed to simulate natural and informative exchanges based on input prompts and dialogue contexts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text-generation",
                    "api_call": "pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')",
                    "api_arguments": "input_prompt",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:",
                    "performance": {
                        "dataset": "56MB of dialogue data",
                        "accuracy": "Not provided"
                    },
                    "description": "Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.",
                    "id": "huggingface_api_584",
                    "name": "pygmalion-1.3b"
                },
                "id": "gorilla_huggingface_tool_579",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_216",
        "query": "Our company's voice assistant needs to be able to detect voice activity in a conversation.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that can identify active speech segments in audio inputs, allowing the system to distinguish between speech and non-speech elements in conversational audio data.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Voice Activity Detection",
                    "api_call": "Inference('julien-c/voice-activity-detection')",
                    "api_arguments": {
                        "audio": "TheBigBangTheory.wav"
                    },
                    "python_environment_requirements": "pyannote.audio",
                    "example_code": "from pyannote.audio.core.inference import Inference\nmodel = Inference('julien-c/voice-activity-detection', device='cuda')\nmodel({\n audio: TheBigBangTheory.wav\n})",
                    "performance": {
                        "dataset": "dihard",
                        "accuracy": "Not provided"
                    },
                    "description": "Example pyannote-audio Voice Activity Detection model using PyanNet. Imported from https://github.com/pyannote/pyannote-audio-hub and trained by @hbredin.",
                    "id": "huggingface_api_835",
                    "name": "julien-c/voice-activity-detection"
                },
                "id": "gorilla_huggingface_tool_830",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_217",
        "query": "I am running a chain of wine stores and I want to categorize wines and recommend them based on their quality.",
        "instruction": "Given a `wine categorization and recommendation` task, retrieve tools that utilize tabular data classification techniques to assess wine quality and generate recommendations based on quality analysis to meet the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))",
                    "api_arguments": "X",
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])",
                    "performance": {
                        "dataset": "winequality-red.csv",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_847",
                    "name": "osanseviero/wine-quality"
                },
                "id": "gorilla_huggingface_tool_841",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_218",
        "query": "A movie studio needs to estimate the human pose of an actor from an image for an upcoming film project.",
        "instruction": "Given a `human pose estimation` task, retrieve tools that analyze image inputs to estimate and provide details of human poses, applicable for projects such as film production requiring accurate pose predictions from visual data.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Human Pose Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                    "api_arguments": {
                        "text": "chef in the kitchen",
                        "image": "image",
                        "num_inference_steps": 20
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": "from PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers.utils import load_image\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\nimage = openpose(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\nimage.save('images/chef_pose_out.png')",
                    "performance": {
                        "dataset": "200k pose-image, caption pairs",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_262",
                    "name": "lllyasviel/sd-controlnet-openpose"
                },
                "id": "gorilla_huggingface_tool_258",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_219",
        "query": "We are making a mobile app related to fitness. We need to estimate the human pose from an image of a user performing an exercise.",
        "instruction": "Given a `human pose estimation` task, retrieve tools that process images to estimate and output the human pose, leveraging neural network models to handle visual input specific to fitness-related tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Human Pose Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                    "api_arguments": {
                        "text": "chef in the kitchen",
                        "image": "image",
                        "num_inference_steps": 20
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": "from PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers.utils import load_image\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\nimage = openpose(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\nimage.save('images/chef_pose_out.png')",
                    "performance": {
                        "dataset": "200k pose-image, caption pairs",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_262",
                    "name": "lllyasviel/sd-controlnet-openpose"
                },
                "id": "gorilla_huggingface_tool_258",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_220",
        "query": "My coworker sent me a voice message in Spanish. Translate this audio message to English, so that I can understand it.",
        "instruction": "Given a `speech translation` task, retrieve tools that process spoken language inputs, specifically audio in Spanish, to accurately produce textual translations in English aligned with the query's requirements and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "textless_sm_sl_es()",
                    "api_arguments": null,
                    "python_environment_requirements": "fairseq",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "A Fairseq model for audio-to-audio speech-to-speech translation.",
                    "id": "huggingface_api_800",
                    "name": "textless_sm_sl_es"
                },
                "id": "gorilla_huggingface_tool_795",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_221",
        "query": "I need a solution to detect whether a piece of news is talking about technology, sports, or politics.",
        "instruction": "Given a `news topic classification` task, retrieve tools capable of analyzing text inputs to determine the relevant category, such as technology, sports, or politics, using natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "CrossEncoder('cross-encoder/nli-roberta-base')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\nsent = Apple just announced the newest iPhone X\ncandidate_labels = [technology, sports, politics]\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": [
                            "SNLI",
                            "MultiNLI"
                        ],
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder"
                    },
                    "description": "Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_503",
                    "name": "cross-encoder/nli-roberta-base"
                },
                "id": "gorilla_huggingface_tool_499",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_222",
        "query": "I'm a nutritionist and want to help my clients by answering questions about their meals. They will send me an image of their food and ask me a question about it, like \"Is this vegan?\" or \"How many calories do you think it contains?\"",
        "instruction": "Given a `visual question answering task`, retrieve tools that handle image inputs and answer questions related to the content of the images. These tools should be capable of processing visual data and providing relevant answers to queries, such as dietary information or content analysis, based on the depicted items.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')",
                    "api_arguments": {
                        "model": "Bingsu/temp_vilt_vqa",
                        "tokenizer": "Bingsu/temp_vilt_vqa"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A visual question answering model for answering questions related to images using the Hugging Face Transformers library.",
                    "id": "huggingface_api_112",
                    "name": "temp_vilt_vqa"
                },
                "id": "gorilla_huggingface_tool_111",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_223",
        "query": "I am a drone maker that is building a navigation module for my drones. I need to sharpen the images captured from the drone in real-time.",
        "instruction": "Given an `image enhancement` task, retrieve tools that process real-time images captured from drones, enhancing clarity by applying image sharpening or super-resolution techniques to meet the task's visual improvement requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').",
                    "api_arguments": "feature_extractor, model",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.",
                    "id": "huggingface_api_277",
                    "name": "swin2SR-lightweight-x2-64"
                },
                "id": "gorilla_huggingface_tool_273",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_224",
        "query": "As a clerk in a school, you want to extract information from some student enrollment forms. These forms contain students' details such as Name, age, and address.",
        "instruction": "Given a `document data extraction` task, retrieve tools that process student enrollment forms to accurately extract and list relevant information such as names, ages, and addresses from the forms based on the inputs provided.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')",
                    "api_arguments": {
                        "image": "path/to/image/file"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_125",
                    "name": "tiny-random-LayoutLMv3ForQuestionAnswering"
                },
                "id": "gorilla_huggingface_tool_124",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_225",
        "query": "We have a robotic arm in our warehouse that needs to be trained to optimize loading and unloading tasks. The robotic arm is based on the CartPole environment.",
        "instruction": "Given a `robotic arm training` task, retrieve tools that utilize reinforcement learning frameworks, specifically targeting the optimization of tasks in a CartPole environment, by leveraging pre-trained models and appropriate algorithms to enhance task efficiency and performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/",
                    "performance": {
                        "dataset": "seals/CartPole-v0",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_891",
                    "name": "ppo-seals-CartPole-v0"
                },
                "id": "gorilla_huggingface_tool_884",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_226",
        "query": "We are making an AI copywriter for marketing content. Help me to provide content for a product relating to eco-friendly kitchenware.",
        "instruction": "Given a `marketing content generation` task, retrieve tools that specialize in producing creative and compelling text by processing thematic inputs related to products, such as eco-friendly kitchenware, to generate effective marketing copy.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='facebook/opt-125m')",
                    "api_arguments": {
                        "do_sample": "True"
                    },
                    "python_environment_requirements": "from transformers import pipeline, set_seed",
                    "example_code": "generator(Hello, I'm am conscious and)",
                    "performance": {
                        "dataset": "Various",
                        "accuracy": "Roughly matches GPT-3 performance"
                    },
                    "description": "OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.",
                    "id": "huggingface_api_617",
                    "name": "facebook/opt-125m"
                },
                "id": "gorilla_huggingface_tool_612",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_227",
        "query": "To improve customer satisfaction, we want to automate the creation of written explanations that sound more natural and less robotic. Can you suggest a way to generate text that sounds conscious and alive?",
        "instruction": "Given a `text generation for natural language` task, retrieve tools that utilize advanced language models to produce written content that mimics natural, human-like language by processing given prompts and generating coherent, lively text outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b')",
                    "api_arguments": [
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b', torch_dtype=torch.float16).cuda()\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b', use_fast=False)\nprompt = Hello, I'm am conscious and\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\ngenerated_ids = model.generate(input_ids)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": {
                            "BookCorpus": "unknown",
                            "CC-Stories": "unknown",
                            "The Pile": "unknown",
                            "Pushshift.io Reddit": "unknown",
                            "CCNewsV2": "unknown"
                        },
                        "accuracy": "unknown"
                    },
                    "description": "OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example.",
                    "id": "huggingface_api_624",
                    "name": "facebook/opt-6.7b"
                },
                "id": "gorilla_huggingface_tool_619",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_228",
        "query": "I have a text question about an image, and I would like to receive an appropriate answer.",
        "instruction": "Given an `image-based question answering` task, retrieve tools that handle multimodal inputs by processing both text and image data to deliver accurate answers based on the query's visual content and associated question.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')",
                    "api_arguments": {
                        "model": "JosephusCheung/GuanacoVQAOnConsumerHardware",
                        "tokenizer": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "vqa(image_path, question)",
                    "performance": {
                        "dataset": "JosephusCheung/GuanacoVQADataset",
                        "accuracy": "unknown"
                    },
                    "description": "A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.",
                    "id": "huggingface_api_113",
                    "name": "JosephusCheung/GuanacoVQAOnConsumerHardware"
                },
                "id": "gorilla_huggingface_tool_112",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_229",
        "query": "We are building a robot for hopping in a controlled environment. Train it to perform a hop using Decision Transformers.",
        "instruction": "Given a `robot training` task, retrieve tools that utilize Reinforcement Learning frameworks, specifically Decision Transformers, to train a robot for performing actions such as hopping by processing environment-specific inputs and configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')",
                    "api_arguments": {
                        "mean": [
                            1.311279,
                            -0.08469521,
                            -0.5382719,
                            -0.07201576,
                            0.04932366,
                            2.1066856,
                            -0.15017354,
                            0.00878345,
                            -0.2848186,
                            -0.18540096,
                            -0.28461286
                        ],
                        "std": [
                            0.17790751,
                            0.05444621,
                            0.21297139,
                            0.14530419,
                            0.6124444,
                            0.85174465,
                            1.4515252,
                            0.6751696,
                            1.536239,
                            1.6160746,
                            5.6072536
                        ]
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": "See our Blog Post, Colab notebook or Example Script for usage.",
                    "performance": {
                        "dataset": "Gym Hopper environment",
                        "accuracy": "Not provided"
                    },
                    "description": "Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.",
                    "id": "huggingface_api_896",
                    "name": "decision-transformer-gym-hopper-medium"
                },
                "id": "gorilla_huggingface_tool_889",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_230",
        "query": "We need to classify the type of an image for an inventory.",
        "instruction": "Given an `image classification` task, retrieve tools that process visual inputs to identify and categorize images based on predefined classes, utilizing computer vision and machine learning techniques to align with inventory needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "zuppif/regnet-y-040"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor, RegNetForImageClassification",
                        "torch": "torch",
                        "datasets": "load_dataset"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.",
                    "id": "huggingface_api_199",
                    "name": "facebook/regnet-y-008"
                },
                "id": "gorilla_huggingface_tool_195",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_231",
        "query": "Our business is expanding to international markets. Analyze the sentiment of the following customer review to better understand their satisfaction with our product: \"Â¡Esto es maravilloso! Me encanta.\"",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze customer reviews by processing text inputs to determine the sentiment expressed, using multilingual capabilities to provide insights across different languages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "result = sentiment_pipeline('I love this product!')",
                    "performance": {
                        "dataset": [
                            {
                                "language": "English",
                                "accuracy": {
                                    "exact": "67%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Dutch",
                                "accuracy": {
                                    "exact": "57%",
                                    "off-by-1": "93%"
                                }
                            },
                            {
                                "language": "German",
                                "accuracy": {
                                    "exact": "61%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "French",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "Italian",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Spanish",
                                "accuracy": {
                                    "exact": "58%",
                                    "off-by-1": "95%"
                                }
                            }
                        ]
                    },
                    "description": "This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
                    "id": "huggingface_api_914",
                    "name": "bert-base-multilingual-uncased-sentiment"
                },
                "id": "gorilla_huggingface_tool_379",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_232",
        "query": "### Instruction: I teach at a school, in a natural language processing subject and I want to improve the readability and grammaticality of the provided sentence by suggesting the best replacement for the masked part.",
        "instruction": "Given a `sentence enhancement` task, retrieve tools focused on natural language processing that can improve sentence readability and grammaticality by suggesting contextually appropriate replacements for masked words or phrases based on high accuracy language models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')",
                    "api_arguments": "Mask token: [MASK]",
                    "python_environment_requirements": "PyTorch, TensorFlow",
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQuAD 1.1",
                                "accuracy": "95.8/90.8"
                            },
                            {
                                "name": "SQuAD 2.0",
                                "accuracy": "91.4/88.9"
                            },
                            {
                                "name": "MNLI-m/mm",
                                "accuracy": "91.7/91.6"
                            },
                            {
                                "name": "SST-2",
                                "accuracy": "97.5"
                            },
                            {
                                "name": "QNLI",
                                "accuracy": "95.8"
                            },
                            {
                                "name": "CoLA",
                                "accuracy": "71.1"
                            },
                            {
                                "name": "RTE",
                                "accuracy": "93.9"
                            },
                            {
                                "name": "MRPC",
                                "accuracy": "92.0/94.2"
                            },
                            {
                                "name": "QQP",
                                "accuracy": "92.3/89.8"
                            },
                            {
                                "name": "STS-B",
                                "accuracy": "92.9/92.9"
                            }
                        ]
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.",
                    "id": "huggingface_api_683",
                    "name": "microsoft/deberta-v2-xlarge"
                },
                "id": "gorilla_huggingface_tool_678",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_233",
        "query": "Our client is involved in a sports organization, and they require a solution to classify sports videos efficiently.",
        "instruction": "Given a `video classification` task, retrieve tools that efficiently classify sports videos by processing video inputs to determine their categorical labels using advanced models such as TimeSformer.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')",
                    "api_arguments": {
                        "images": "video",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "numpy",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 448, 448))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n  outputs = model(**inputs)\n  logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": "Not provided"
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.",
                    "id": "huggingface_api_320",
                    "name": "facebook/timesformer-hr-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_316",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_234",
        "query": "We need a solution for creating Polish subtitles for YouTube videos in Spanish. The AI should provide the translation.",
        "instruction": "Given a `subtitle translation` task, retrieve tools that facilitate language translation by processing video language inputs and generating translated subtitles in the target language, aligning with the task's requirement for multilingual support.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Text2Text Generation",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mbart-large-50-many-to-many-mmt"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.0"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi model = MBartForConditionalGeneration.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer = MBart50TokenizerFast.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer.src_lang = hi_IN\nencoded_hi = tokenizer(article_hi, return_tensors=pt)\ngenerated_tokens = model.generate(\n **encoded_hi,\n forced_bos_token_id=tokenizer.lang_code_to_id[fr_XX]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "Multilingual Translation",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_661",
                    "name": "facebook/mbart-large-50-many-to-many-mmt"
                },
                "id": "gorilla_huggingface_tool_656",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_235",
        "query": "As a community manager, I would like to monitor my forum's comment section for toxic or harmful content. I want to find a solution that can flag these types of comments automatically, so I can address them promptly.",
        "instruction": "Given a `content moderation` task, retrieve tools that can automatically analyze text inputs for harmful or toxic language, enabling community managers to swiftly identify and address inappropriate content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_236",
        "query": "Create a program that predicts carbon emissions for new data using the given model.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that utilize pre-trained regression models to predict emissions by processing new data inputs and applying the specified model configurations to generate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "model.joblib",
                        "config.json",
                        "data.csv"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-dragino-7-7-max_300m",
                        "accuracy": {
                            "Loss": 50.918,
                            "R2": 0.304,
                            "MSE": 2592.667,
                            "MAE": 39.693,
                            "RMSLE": 0.429
                        }
                    },
                    "description": "A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain.",
                    "id": "huggingface_api_881",
                    "name": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640"
                },
                "id": "gorilla_huggingface_tool_874",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_237",
        "query": "Our client wants to create marketing slogans. Help them by completing this slogan \"Customer satisfaction is our top <mask>.\"",
        "instruction": "Given a `slogan completion` task, retrieve tools that perform masked language modeling by processing incomplete text inputs to generate suitable words or phrases to fill in the blanks according to the context and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-large')",
                    "api_arguments": {
                        "model": "roberta-large"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-large')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "GLUE",
                        "accuracy": {
                            "MNLI": 90.2,
                            "QQP": 92.2,
                            "QNLI": 94.7,
                            "SST-2": 96.4,
                            "CoLA": 68.0,
                            "STS-B": 96.4,
                            "MRPC": 90.9,
                            "RTE": 86.6
                        }
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.",
                    "id": "huggingface_api_671",
                    "name": "roberta-large"
                },
                "id": "gorilla_huggingface_tool_666",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_238",
        "query": "We are a company managing hotel bookings. We need to answer our customer's questions regarding rental rates from our pricing document.",
        "instruction": "Given a `document question answering` task, retrieve tools that utilize document analysis models to answer questions by processing textual context and structured data from documents.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('question-answering', model='pardeepSF/layoutlm-vqa')",
                    "api_arguments": {
                        "question": "string",
                        "context": "string"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A model for document question answering using the LayoutLM architecture.",
                    "id": "huggingface_api_121",
                    "name": "layoutlm-vqa"
                },
                "id": "gorilla_huggingface_tool_120",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_239",
        "query": "Our team has built a Japanese language learning app for students. We want to make sure that the user-submitted image is an anime art created by humans and not AI-generated.",
        "instruction": "Given an `anime art classification` task, retrieve tools capable of classifying images by evaluating whether an anime artwork is human-created or AI-generated, utilizing image inputs to verify authenticity.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('image-classification', model='saltacc/anime-ai-detect')",
                    "api_arguments": [
                        "image"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "aibooru and imageboard sites",
                        "accuracy": "96%"
                    },
                    "description": "A BEiT classifier to see if anime art was made by an AI or a human.",
                    "id": "huggingface_api_196",
                    "name": "saltacc/anime-ai-detect"
                },
                "id": "gorilla_huggingface_tool_192",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_240",
        "query": "Take a look at this document image and tell me the answer to my question: \"What is the total amount due?\".",
        "instruction": "Given an `image-based document question answering` task, retrieve tools that utilize multimodal capabilities to interpret and process images containing text and layout information in order to extract specific answers to questions, such as the total amount due from a document image.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": ">=4.11.0"
                    },
                    "example_code": {},
                    "performance": {
                        "dataset": {},
                        "accuracy": {}
                    },
                    "description": "A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.",
                    "id": "huggingface_api_134",
                    "name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023"
                },
                "id": "gorilla_huggingface_tool_132",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_241",
        "query": "Help me generate a realistic bedroom interior image that can be used as reference for a 3D model being created for a virtual reality game.",
        "instruction": "Given an `image generation` task, retrieve tools that leverage diffusion probabilistic models to create high-quality and realistic images suited for use as references in 3D modeling and virtual reality applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Denoising Diffusion Probabilistic Models (DDPM)",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')",
                    "api_arguments": "None",
                    "python_environment_requirements": "diffusers",
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-bedroom-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception score": 9.46,
                            "FID score": 3.17
                        }
                    },
                    "description": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.",
                    "id": "huggingface_api_295",
                    "name": "google/ddpm-bedroom-256"
                },
                "id": "gorilla_huggingface_tool_291",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_242",
        "query": "Our company needs a versatile NLP model to build a social media manager to generate summaries of lengthy articles for sharing on social media.",
        "instruction": "Given a `NLP summarization` task, retrieve tools that utilize advanced natural language processing techniques to generate concise summaries from lengthy articles, suitable for social media sharing, by processing text inputs and leveraging model capabilities designed for multi-purpose NLP tasks such as summarization, translation, or sentiment analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis, Regression",
                    "api_call": "T5Model.from_pretrained('t5-large')",
                    "api_arguments": {
                        "input_ids": "tokenizer(..., return_tensors='pt').input_ids",
                        "decoder_input_ids": "tokenizer(..., return_tensors='pt').input_ids"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import T5Tokenizer, T5Model"
                    },
                    "example_code": "tokenizer = T5Tokenizer.from_pretrained('t5-large')\nmodel = T5Model.from_pretrained('t5-large')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.",
                    "id": "huggingface_api_526",
                    "name": "t5-large"
                },
                "id": "gorilla_huggingface_tool_522",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_243",
        "query": "Create interesting variations of the given phrase 'How can I improve my time management skills?'.",
        "instruction": "Given a `phrase variation` task, retrieve tools that specialize in paraphrasing by processing text inputs to generate diverse and coherent variations of the original phrase.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Paraphrasing",
                    "api_call": "Parrot(model_tag='prithivida/parrot_paraphraser_on_T5')",
                    "api_arguments": [
                        "input_phrase",
                        "diversity_ranker",
                        "do_diverse",
                        "max_return_phrases",
                        "max_length",
                        "adequacy_threshold",
                        "fluency_threshold"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from parrot import Parrot\nimport torch\nimport warnings\nwarnings.filterwarnings(ignore)\n\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\nphrases = [Can you recommed some upscale restaurants in Newyork?,\n What are the famous places we should not miss in Russia?\n]\nfor phrase in phrases:\n print(-*100)\n print(Input_phrase: , phrase)\n print(-*100)\n para_phrases = parrot.augment(input_phrase=phrase)\n for para_phrase in para_phrases:\n  print(para_phrase)",
                    "performance": {
                        "dataset": "Not mentioned",
                        "accuracy": "Not mentioned"
                    },
                    "description": "Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.",
                    "id": "huggingface_api_630",
                    "name": "prithivida/parrot_paraphraser_on_T5"
                },
                "id": "gorilla_huggingface_tool_625",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_244",
        "query": "We're trying to help out a friend who's developing an application for composing text. He is trying to create a bot that comes up with creative ideas for your paragraph.",
        "instruction": "Given a `creative text generation` task, retrieve tools that assist in generating innovative and engaging text by processing input prompts to inspire creative writing or ideas, aligned with the application's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation')",
                    "api_arguments": [
                        "model"
                    ],
                    "python_environment_requirements": [
                        "from transformers import pipeline, set_seed"
                    ],
                    "example_code": "set_seed(42)\ngenerator(Hello, Iâ€™m a language model, max_length=20, num_return_sequences=5)",
                    "performance": {
                        "dataset": "WikiText-103",
                        "accuracy": "21.100"
                    },
                    "description": "DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.",
                    "id": "huggingface_api_916",
                    "name": "distilgpt2"
                },
                "id": "gorilla_huggingface_tool_596",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_245",
        "query": "We are building an AI chatbot that reads out user messages using synthesized human-like speech. For this purpose, we need to convert text messages to audio.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform text messages into synthesized human-like audio by processing text inputs and generating speech outputs suitable for AI chatbot applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": ""
                    },
                    "description": "A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.",
                    "id": "huggingface_api_738",
                    "name": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan"
                },
                "id": "gorilla_huggingface_tool_733",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_246",
        "query": "A forum moderator wants a tool to assess user-generated comments for toxic content. How does this model help?",
        "instruction": "Given a `content moderation` task, retrieve tools that can process text inputs to detect and classify toxic content, aiding moderation efforts by providing reliable assessments of user-generated comments.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_247",
        "query": "I have a table containing information about various animals and their important characteristics. I need the system to answer a query to provide information about the tallest animal in the table.",
        "instruction": "Given a `table question answering` task, retrieve tools that process table data and queries to provide accurate information by extracting relevant details from structured datasets according to the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')",
                    "api_arguments": [
                        "model_name",
                        "table",
                        "queries"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.5148
                    },
                    "description": "TAPAS mini model fine-tuned on Sequential Question Answering (SQA)",
                    "id": "huggingface_api_457",
                    "name": "google/tapas-mini-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_453",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_248",
        "query": "My company has a large data table of employees, containing their names, titles, departments, and hire dates. We need a tool that can find all employees with the title of \"Software Engineer\" hired in 2020.",
        "instruction": "Given a `data table query` task, retrieve tools that utilize table reasoning capabilities to filter and identify specific records within a dataset, such as employees with particular titles and hire dates, using structured query execution based on the query's criteria.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-sql-execution')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "pandas"
                    ],
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-sql-execution)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-sql-execution)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "synthetic corpus",
                        "accuracy": "not specified"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.",
                    "id": "huggingface_api_448",
                    "name": "microsoft/tapex-large-sql-execution"
                },
                "id": "gorilla_huggingface_tool_444",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_249",
        "query": "Our e-commerce platform needs automatic classification of product images without retraining. Develop a classifier for the e-commerce platform.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize zero-shot learning to classify product images based on their content without requiring retraining, leveraging pre-trained models to ensure adaptability across diverse categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')",
                    "api_arguments": {
                        "model_name": "OFA-Sys/chinese-clip-vit-large-patch14"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]",
                    "performance": {
                        "dataset": "MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC",
                        "accuracy": "Varies depending on the dataset"
                    },
                    "description": "Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.",
                    "id": "huggingface_api_375",
                    "name": "chinese-clip-vit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_371",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_250",
        "query": "I want to build a tool to recognize urban landscapes and identify different objects in the image.",
        "instruction": "Given an `image recognition and segmentation` task, retrieve tools that perform semantic segmentation by processing image inputs to identify and categorize different objects within urban landscapes, providing detailed insights into the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "CityScapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_249",
                    "name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_245",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_251",
        "query": "A medical research team requests an automated procedure for detecting blood cells in microscopic images of blood samples. Develop a solution to address this need.",
        "instruction": "Given an `image analysis` task for detecting blood cells in microscopic images, retrieve tools that specialize in computer vision and object detection to analyze and identify cellular structures within image inputs, supporting the research team's automated detection requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": [
                        "from ultralyticsplus import YOLO, render_result",
                        "model = YOLO('keremberke/yolov8m-blood-cell-detection')",
                        "model.overrides['conf'] = 0.25",
                        "model.overrides['iou'] = 0.45",
                        "model.overrides['agnostic_nms'] = False",
                        "model.overrides['max_det'] = 1000",
                        "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model.predict(image)",
                        "print(results[0].boxes)",
                        "render = render_result(model=model, image=image, result=results[0])",
                        "render.show()"
                    ],
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.927
                    },
                    "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_224",
                    "name": "keremberke/yolov8m-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_220",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_252",
        "query": "We are building an IoT device to monitor apartment corridors for security. Please detect objects in the image and notify if anything abnormal is detected.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize computer vision to analyze images for object identification and alert upon detecting anomalies by processing image inputs to pinpoint objects and their attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Transformers",
                    "functionality": "Object Detection",
                    "api_call": "yolov5.load('fcakyon/yolov5s-v7.0')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": "import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')",
                    "performance": {
                        "dataset": "detection-datasets/coco",
                        "accuracy": null
                    },
                    "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.",
                    "id": "huggingface_api_226",
                    "name": "fcakyon/yolov5s-v7.0"
                },
                "id": "gorilla_huggingface_tool_222",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_253",
        "query": "We have customer reviews of various software products. We want to extract company names in those reviews.",
        "instruction": "Given a `entity extraction` task, retrieve tools that utilize natural language processing to identify and extract specific entities, such as company names, from textual data based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification, AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-company_all",
                        "accuracy": 0.9979930566588805
                    },
                    "description": "A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.",
                    "id": "huggingface_api_414",
                    "name": "903429548"
                },
                "id": "gorilla_huggingface_tool_410",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_254",
        "query": "Gather information about annual income and age demographics of employees to predict retirement patterns. Make sure to identify top employees for potential promotions.",
        "instruction": "Given a `data synthesis and analysis` task, retrieve tools that collect and analyze employee demographic data, such as annual income and age, to predict retirement patterns and assess performance for identifying top candidates for promotions, thereby providing actionable insights based on structured data inputs like tables.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')",
                    "api_arguments": [
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "https://huggingface.co/google/tapas-large-finetuned-sqa",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.7289
                    },
                    "description": "TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_450",
                    "name": "google/tapas-large-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_446",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_255",
        "query": "We are a medical company providing health FAQs. We need to answer customers' questions accurately.",
        "instruction": "Given a `question answering` task, retrieve tools that are capable of providing precise and accurate answers to health-related FAQs by processing contextual information and questions. These tools should leverage natural language processing models fine-tuned on medical datasets to ensure the quality and relevance of the responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})",
                    "performance": {
                        "dataset": "SQuAD2.0 Dev",
                        "accuracy": {
                            "exact": 84.33420365535248,
                            "f1": 87.49354241889522
                        }
                    },
                    "description": "BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.",
                    "id": "huggingface_api_463",
                    "name": "sultan/BioM-ELECTRA-Large-SQuAD2"
                },
                "id": "gorilla_huggingface_tool_459",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_256",
        "query": "We have a new platform that offers various services related to digital art, and we want it to be able to produce creative story ideas based on a short description. Can you help me providing suggestions?",
        "instruction": "Given a `creative story generation` task, retrieve tools that leverage text generation models to produce creative and coherent story ideas by processing short descriptive inputs and aligning with the query's thematic requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "PyTorch Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModel.from_pretrained('decapoda-research/llama-7b-hf')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\n\ngen = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n\nresult = gen('Once upon a time')\nprint(result[0]['generated_text'])",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BoolQ",
                                "accuracy": 76.5
                            },
                            {
                                "name": "PIQA",
                                "accuracy": 79.8
                            },
                            {
                                "name": "SIQA",
                                "accuracy": 48.9
                            },
                            {
                                "name": "HellaSwag",
                                "accuracy": 76.1
                            },
                            {
                                "name": "WinoGrande",
                                "accuracy": 70.1
                            },
                            {
                                "name": "ARC-e",
                                "accuracy": 76.7
                            },
                            {
                                "name": "ARC-c",
                                "accuracy": 47.6
                            },
                            {
                                "name": "OBQAC",
                                "accuracy": 57.2
                            },
                            {
                                "name": "COPA",
                                "accuracy": 93
                            }
                        ]
                    },
                    "description": "LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English.",
                    "id": "huggingface_api_608",
                    "name": "decapoda-research/llama-7b-hf"
                },
                "id": "gorilla_huggingface_tool_603",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_257",
        "query": "As a translation company, we are translating messages between co-workers in a multinational company. Translate the message from Hindi to French.",
        "instruction": "Given a `translation task`, retrieve tools that facilitate multilingual machine translation by processing text inputs in one language and accurately converting them to another specified language based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Text2Text Generation",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mbart-large-50-many-to-many-mmt"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.0"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi model = MBartForConditionalGeneration.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer = MBart50TokenizerFast.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer.src_lang = hi_IN\nencoded_hi = tokenizer(article_hi, return_tensors=pt)\ngenerated_tokens = model.generate(\n **encoded_hi,\n forced_bos_token_id=tokenizer.lang_code_to_id[fr_XX]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "Multilingual Translation",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_661",
                    "name": "facebook/mbart-large-50-many-to-many-mmt"
                },
                "id": "gorilla_huggingface_tool_656",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_258",
        "query": "Our app offers assistance to people with hearing problems by enhancing the clarity of speech. We need a feature to clean and enhance the audio.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that process audio inputs to improve the clarity and quality of speech, such as noise reduction and speech enhancement algorithms, in order to assist individuals with hearing impairments.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_259",
        "query": "To enhance our FAQ bot, we need to extract answers from a given knowledge base text.",
        "instruction": "Given a `knowledge extraction` task, retrieve tools that perform question answering by leveraging a given text to extract accurate and relevant answers, aligning with the FAQ enhancement objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_260",
        "query": "I am interested in building a wall of generated pictures for my gallery. My specifications include a size of 256x256 pixels.",
        "instruction": "Given an `image generation` task, retrieve tools that perform unconditional image synthesis, processing parameters such as image size to produce high-quality generated pictures according to the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-church-256')",
                    "api_arguments": [
                        "model_id"
                    ],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-church-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception_score": 9.46,
                            "FID_score": 3.17
                        }
                    },
                    "description": "Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.",
                    "id": "huggingface_api_292",
                    "name": "google/ddpm-church-256"
                },
                "id": "gorilla_huggingface_tool_288",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_261",
        "query": "Create a method to determine, as an entertainment recommendation system, which category a text message about a daily activity belongs to.",
        "instruction": "Given a `text classification` task, retrieve tools that utilize zero-shot learning models to categorize text messages related to daily activities into specific entertainment categories by analyzing the content and predicting the most relevant category without predefined labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "NLI-based Zero Shot Text Classification",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')",
                    "api_arguments": {
                        "sequence_to_classify": "one day I will see the world",
                        "candidate_labels": "['travel', 'cooking', 'dancing']"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
                    },
                    "example_code": {
                        "with_pipeline": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\nsequence_to_classify = 'one day I will see the world'\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)",
                        "with_manual_pytorch": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\npremise = sequence\nhypothesis = f'This example is {label}.'\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]"
                    },
                    "performance": {
                        "dataset": "multi_nli",
                        "accuracy": "Not specified"
                    },
                    "description": "This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.",
                    "id": "huggingface_api_496",
                    "name": "facebook/bart-large-mnli"
                },
                "id": "gorilla_huggingface_tool_492",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_262",
        "query": "Analyze an image of an urban scene to identify and separate regions with different semantics, such as streets, pedestrians, buildings, and vehicles.",
        "instruction": "Given an `image analysis` task, retrieve tools that perform semantic segmentation by processing image inputs to identify and delineate regions with distinct semantics such as streets, pedestrians, buildings, and vehicles, aligning with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "PIL": "latest",
                        "requests": "latest"
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "Cityscapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_235",
                    "name": "nvidia/segformer-b2-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_231",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_263",
        "query": "An environmental organization would like to use our Carbon Emissions prediction model to estimate CO2 emissions of different configurations of vehicles.",
        "instruction": "Given a `carbon emissions estimation` task, retrieve tools that leverage prediction models to analyze input configurations and provide estimates of CO2 emissions based on the provided vehicle data.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-600-dragino",
                        "accuracy": {
                            "Loss": 93.595,
                            "R2": 0.502,
                            "MSE": 8760.052,
                            "MAE": 77.527,
                            "RMSLE": 0.445
                        }
                    },
                    "description": "This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.",
                    "id": "huggingface_api_872",
                    "name": "pcoloc/autotrain-600-dragino-1839063122"
                },
                "id": "gorilla_huggingface_tool_865",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_264",
        "query": "We are a company producing wine. Based on the chemical properties of our wine data, we need to analyze the quality of our products and determine whether they are good or bad.",
        "instruction": "Given a `wine quality analysis` task, retrieve tools that evaluate the quality of wine based on its chemical properties by employing classification models to process tabular data inputs and determine quality levels.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))",
                    "api_arguments": [
                        "X"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])\nmodel.score(X, Y)",
                    "performance": {
                        "dataset": "julien-c/wine-quality",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_842",
                    "name": "julien-c/wine-quality"
                },
                "id": "gorilla_huggingface_tool_837",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_265",
        "query": "My company develops drones for agriculture purposes, and we need a model to segment aerial images accurately.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process aerial images for accurate segmentation, focused on distinguishing features relevant to agricultural applications by implementing advanced computer vision models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')",
                    "api_arguments": {
                        "from_pretrained": "facebook/maskformer-swin-base-ade"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "PIL": "latest",
                        "requests": "latest"
                    },
                    "example_code": "from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\ninputs = feature_extractor(images=image, return_tensors=pt)\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\noutputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ADE20k",
                        "accuracy": "Not provided"
                    },
                    "description": "MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.",
                    "id": "huggingface_api_251",
                    "name": "facebook/maskformer-swin-base-ade"
                },
                "id": "gorilla_huggingface_tool_247",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_266",
        "query": "We are building a content moderation system. Our clients upload the content, it can be generated by human or AI. We want to have a filtering API to advise on the original text if it is generated by GPT-2.",
        "instruction": "Given a `content moderation` task, retrieve tools that utilize text classification models to analyze and determine whether input text has been generated by GPT-2, thereby enabling effective content filtering and moderation based on the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Detect GPT-2 generated text",
                    "api_call": "pipeline('text-classification', model='roberta-base-openai-detector')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\nprint(pipe(Hello world! Is this content AI-generated?))",
                    "performance": {
                        "dataset": "WebText",
                        "accuracy": "95%"
                    },
                    "description": "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.",
                    "id": "huggingface_api_382",
                    "name": "roberta-base-openai-detector"
                },
                "id": "gorilla_huggingface_tool_378",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_267",
        "query": "I want to create a solution that can answer questions related to an image of my pet dogs.",
        "instruction": "Given an `image-based question answering` task, retrieve tools that address queries related to images by processing visual inputs and generating relevant textual answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_268",
        "query": "We need to create a content filter for images submitted by users in our online community. We want to detect and filter out adult content and offensive images.",
        "instruction": "Given an `image content filtering` task, retrieve tools that perform image classification to detect and filter out specific categories, such as adult content and offensive images, by processing image data and classifying it based on predefined classes to meet the query's content moderation requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('zero-shot-classification')",
                    "api_arguments": {
                        "image": "path/to/image",
                        "class_names": [
                            "class1",
                            "class2",
                            "class3"
                        ]
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline; classifier = pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K'); classifier(image='path/to/image', class_names=['class1', 'class2', 'class3'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": 66.6
                    },
                    "description": "A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others.",
                    "id": "huggingface_api_917",
                    "name": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"
                },
                "id": "gorilla_huggingface_tool_341",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_269",
        "query": "We have a list of documents written in multiple Romance languages, including texts in French, Spanish, and Italian. We want to make these texts accessible to our English-speaking audience by translating them.",
        "instruction": "Given a `translation` task, retrieve tools that translate texts from multiple Romance languages, such as French, Spanish, and Italian, to English, utilizing models capable of handling diverse source languages to meet the query's multilingual processing requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')",
                    "api_arguments": [
                        "source languages",
                        "target languages"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": 62.2,
                            "chr-F": 0.75
                        }
                    },
                    "description": "A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.",
                    "id": "huggingface_api_529",
                    "name": "opus-mt-ROMANCE-en"
                },
                "id": "gorilla_huggingface_tool_525",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_270",
        "query": "A game studio is now creating a story for their new action game, they need a hint for creating the setting of the game.",
        "instruction": "Given a `creative writing assistance` task, retrieve tools that support the development of narrative elements by generating text based on creative prompts or contextual hints to help writers build engaging stories.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='bigscience/bloom-7b1')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-generation', model='bigscience/bloom-7b1')\nresult = model('Once upon a time')\nprint(result)",
                    "performance": {
                        "dataset": "Training Data",
                        "accuracy": {
                            "Training Loss": 2.3,
                            "Validation Loss": 2.9,
                            "Perplexity": 16
                        }
                    },
                    "description": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text.",
                    "id": "huggingface_api_606",
                    "name": "bigscience/bloom-7b1"
                },
                "id": "gorilla_huggingface_tool_601",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_271",
        "query": "A game developer needs to predict the HP of a new Pokemon character based on several attributes. Provide the code to make that prediction.",
        "instruction": "Given a `prediction task` for game development, retrieve tools that implement tabular regression models to forecast the HP of a Pokemon based on input attributes, utilizing machine learning techniques to provide the most accurate outcomes.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Predicting Pokemon HP",
                    "api_call": "pipeline('regression', model='julien-c/pokemon-predict-hp')",
                    "api_arguments": "input_data",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "julien-c/kaggle-rounakbanik-pokemon",
                        "accuracy": {
                            "mean_absolute_error": 15.909,
                            "model_loss": 647.605
                        }
                    },
                    "description": "A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.",
                    "id": "huggingface_api_889",
                    "name": "julien-c/pokemon-predict-hp"
                },
                "id": "gorilla_huggingface_tool_882",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_272",
        "query": "Provide a way to translate spoken English to spoken Hokkien for an audio file.",
        "instruction": "Given a `speech-to-speech translation` task, retrieve tools that process audio inputs to translate spoken language from English to Hokkien, focusing on transforming speech directly into the target language with tools designed to handle both input and output in audio format.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')",
                    "api_arguments": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text",
                        "cache_dir": "cache_dir"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "hub_utils",
                        "torchaudio",
                        "IPython.display",
                        "huggingface_hub"
                    ],
                    "example_code": [
                        "import json",
                        "import os",
                        "from pathlib import Path",
                        "import IPython.display as ipd",
                        "from fairseq import hub_utils",
                        "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub",
                        "from fairseq.models.speech_to_text.hub_interface import S2THubInterface",
                        "from fairseq.models.text_to_speech import CodeHiFiGANVocoder",
                        "from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface",
                        "from huggingface_hub import snapshot_download",
                        "import torchaudio",
                        "cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)",
                        "models, cfg, task = load_model_ensemble_and_task_from_hf_hub(",
                        " facebook/xm_transformer_unity_en-hk,",
                        " arg_overrides={config_yaml: config.yaml, task: speech_to_text},",
                        " cache_dir=cache_dir,",
                        ")",
                        "model = models[0].cpu()",
                        "cfg[task].cpu = True",
                        "generator = task.build_generator([model], cfg)",
                        "audio, _ = torchaudio.load(/path/to/an/audio/file)",
                        "sample = S2THubInterface.get_model_input(task, audio)",
                        "unit = S2THubInterface.get_prediction(task, model, generator, sample)",
                        "library_name = fairseq",
                        "cache_dir = (",
                        " cache_dir or (Path.home() / .cache / library_name).as_posix()",
                        ")",
                        "cache_dir = snapshot_download(",
                        " ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name",
                        ")",
                        "x = hub_utils.from_pretrained(",
                        " cache_dir,",
                        " model.pt,",
                        " .,",
                        " archive_map=CodeHiFiGANVocoder.hub_models(),",
                        " config_yaml=config.json,",
                        " fp16=False,",
                        " is_vocoder=True,",
                        ")",
                        "with open(f{x['args']['data']}/config.json) as f:",
                        " vocoder_cfg = json.load(f)",
                        "assert (",
                        " len(x[args][model_path]) == 1",
                        "), Too many vocoder models in the input",
                        "vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)",
                        "tts_model = VocoderHubInterface(vocoder_cfg, vocoder)",
                        "tts_sample = tts_model.get_model_input(unit)",
                        "wav, sr = tts_model.get_prediction(tts_sample)",
                        "ipd.Audio(wav, rate=sr)"
                    ],
                    "performance": {
                        "dataset": "MuST-C",
                        "accuracy": null
                    },
                    "description": "Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.",
                    "id": "huggingface_api_787",
                    "name": "xm_transformer_unity_en-hk"
                },
                "id": "gorilla_huggingface_tool_782",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_273",
        "query": "We have an AI-powered document management system that needs to answer questions based on the content of a given document.",
        "instruction": "Given a `document-based question answering` task, retrieve tools that facilitate AI-powered systems in extracting answers from documents by processing text inputs and leveraging machine learning models to interpret and correctly respond to queries based on document content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": "4.12.2",
                        "pytorch": "1.8.0+cu101",
                        "datasets": "1.14.0",
                        "tokenizers": "0.10.3"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 2.087
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_131",
                    "name": "layoutlmv2-base-uncased-finetuned-infovqa"
                },
                "id": "gorilla_huggingface_tool_130",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_274",
        "query": "As a business assistant of an international company, find the most relevant sentence among a list of sentences that answers a specific question.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that utilize natural language processing techniques to evaluate and determine the most relevant sentence among a list based on the semantic similarity to the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')",
                    "api_arguments": [
                        "query",
                        "docs"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer, util\nquery = How many people live in London?\ndocs = [Around 9 Million people live in London, London is known for its financial district]\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\ndoc_score_pairs = list(zip(docs, scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\nfor doc, score in doc_score_pairs:\n print(score, doc)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "WikiAnswers",
                                "accuracy": 77427422
                            },
                            {
                                "name": "PAQ",
                                "accuracy": 64371441
                            },
                            {
                                "name": "Stack Exchange",
                                "accuracy": 25316456
                            }
                        ]
                    },
                    "description": "This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.",
                    "id": "huggingface_api_693",
                    "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1"
                },
                "id": "gorilla_huggingface_tool_688",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_275",
        "query": "I am the director of a movie theater chain, and we are thinking of implementing a chat product to get movie goers to classify movie reviews as positive or negative through AI.",
        "instruction": "Given a `sentiment analysis implementation` task, retrieve tools that facilitate AI-driven categorization of movie reviews as positive or negative by leveraging machine learning models capable of binary classification on text data.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face",
                    "functionality": "Binary Classification",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "desertdev/autotrain-data-imdb-sentiment-analysis",
                        "accuracy": 0.565
                    },
                    "description": "A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.",
                    "id": "huggingface_api_858",
                    "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085"
                },
                "id": "gorilla_huggingface_tool_851",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_276",
        "query": "We are building a customer service analytics software. The software should recognize the voice of the customers.",
        "instruction": "Given a `speaker recognition` task, retrieve tools that enable the identification and verification of customer voices by processing audio data to extract speaker embeddings, which can be utilized in customer service analytics software.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker Verification",
                    "api_call": "EncoderClassifier.from_hparams(source='speechbrain/spkrec-xvect-voxceleb')",
                    "api_arguments": [
                        "source",
                        "savedir"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "import torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)",
                    "performance": {
                        "dataset": "Voxceleb1-test set (Cleaned)",
                        "accuracy": "EER(%) 3.2"
                    },
                    "description": "This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.",
                    "id": "huggingface_api_815",
                    "name": "speechbrain/spkrec-xvect-voxceleb"
                },
                "id": "gorilla_huggingface_tool_810",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_277",
        "query": "Develop a fashion app that segments and identifies clothing items in an uploaded image.",
        "instruction": "Given a `fashion image segmentation` task, retrieve tools that can process image inputs to identify and segment clothing items, providing visual output aligned with the task requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')",
                    "api_arguments": [
                        "image"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests",
                        "matplotlib",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = extractor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits.cpu()\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\npred_seg = upsampled_logits.argmax(dim=1)[0]\nplt.imshow(pred_seg)",
                    "performance": {
                        "dataset": "mattmdjaga/human_parsing_dataset",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on ATR dataset for clothes segmentation.",
                    "id": "huggingface_api_239",
                    "name": "mattmdjaga/segformer_b2_clothes"
                },
                "id": "gorilla_huggingface_tool_235",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_278",
        "query": "Our task is to complete a given sentence with a missing word. The sentence is from an electronic health record.",
        "instruction": "Given a `text completion` task in the clinical domain, retrieve tools that facilitate Natural Language Processing by leveraging fill-mask techniques to accurately complete sentences based on contextual inputs from electronic health records.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel",
                        "from_pretrained"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "performance": {
                        "dataset": "MIMIC III",
                        "accuracy": "Not provided"
                    },
                    "description": "Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).",
                    "id": "huggingface_api_674",
                    "name": "emilyalsentzer/Bio_ClinicalBERT"
                },
                "id": "gorilla_huggingface_tool_669",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_279",
        "query": "Bob is designing an app for his city. He needs to quickly identify if the provided image has a bike or a car. Design a model that recognizes a bike or car and provides classification output for the given image.",
        "instruction": "Given an `image classification` task, retrieve tools that use visual inputs to determine the presence of specific objects, such as bikes or cars, by analyzing and classifying image data to produce accurate recognition outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')",
                    "api_arguments": "image, class_names",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "79.1 - 79.4"
                    },
                    "description": "A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_364",
                    "name": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind"
                },
                "id": "gorilla_huggingface_tool_360",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_280",
        "query": "We want our System to generate possible user queries for a document provided as a text input.",
        "instruction": "Given a `query generation` task, retrieve tools that leverage text-to-text transformation capabilities by processing textual inputs in order to generate user queries aligned with the document's content and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')",
                    "api_arguments": "text, max_length",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "MS MARCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A T5 model trained on the MS MARCO dataset for generating queries from documents.",
                    "id": "huggingface_api_648",
                    "name": "castorini/doc2query-t5-base-msmarco"
                },
                "id": "gorilla_huggingface_tool_643",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_281",
        "query": "Create a virtual assistant that can provide answers to trivia questions about history.",
        "instruction": "Given a `virtual assistant creation` task, retrieve tools that enable the construction of a virtual assistant capable of question answering in the domain of history trivia by employing natural language processing models to process queries and produce accurate answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_282",
        "query": "We need to analyze user reviews for our app to determine whether they are positive, negative, or neutral.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that assess the emotional tone of text data by processing user reviews to classify them as positive, negative, or neutral, consistent with the queryâ€™s requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_283",
        "query": "I need an AI feature capable of extracting names of people, organizations, and locations from various news articles in different languages.",
        "instruction": "Given a `named entity extraction` task, retrieve tools that can identify and extract names of people, organizations, and locations from text inputs, specifically from multilingual news articles, by utilizing token classification models designed for Named Entity Recognition in various languages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')",
                    "api_arguments": {
                        "model": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
                        "tokenizer": "Davlan/distilbert-base-multilingual-cased-ner-hrl"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "ANERcorp",
                                "language": "Arabic"
                            },
                            {
                                "name": "conll 2003",
                                "language": "German"
                            },
                            {
                                "name": "conll 2003",
                                "language": "English"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Spanish"
                            },
                            {
                                "name": "Europeana Newspapers",
                                "language": "French"
                            },
                            {
                                "name": "Italian I-CAB",
                                "language": "Italian"
                            },
                            {
                                "name": "Latvian NER",
                                "language": "Latvian"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Dutch"
                            },
                            {
                                "name": "Paramopama + Second Harem",
                                "language": "Portuguese"
                            },
                            {
                                "name": "MSRA",
                                "language": "Chinese"
                            }
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).",
                    "id": "huggingface_api_408",
                    "name": "distilbert-base-multilingual-cased-ner-hrl"
                },
                "id": "gorilla_huggingface_tool_404",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_284",
        "query": "We have a dataset with customer reviews of our financial service app, and we'd like to analyze their sentiment.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that can assess customer feedback by processing text data to determine the sentiment expressed in reviews, especially focusing on financial service contexts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "financial-sentiment-analysis",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')",
                    "performance": {
                        "dataset": "Financial PhraseBank",
                        "accuracy": "Not provided"
                    },
                    "description": "FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.",
                    "id": "huggingface_api_378",
                    "name": "ProsusAI/finbert"
                },
                "id": "gorilla_huggingface_tool_374",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_285",
        "query": "Our research team is focused on the analysis and separation of complex audio recordings. We need a model to be able to separate speaker voices from mixed sound.",
        "instruction": "Given a `speaker separation` task, retrieve tools that specialize in processing complex audio recordings to efficiently separate and isolate individual speaker voices, leveraging advanced audio processing models and techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "ConvTasNet_Libri3Mix_sepclean_8k()",
                    "api_arguments": {
                        "n_src": 3,
                        "sample_rate": 8000,
                        "segment": 3,
                        "task": "sep_clean",
                        "train_dir": "data/wav8k/min/train-360",
                        "valid_dir": "data/wav8k/min/dev",
                        "kernel_size": 16,
                        "n_filters": 512,
                        "stride": 8,
                        "bn_chan": 128,
                        "hid_chan": 512,
                        "mask_act": "relu",
                        "n_blocks": 8,
                        "n_repeats": 3,
                        "skip_chan": 128,
                        "lr": 0.001,
                        "optimizer": "adam",
                        "weight_decay": 0.0,
                        "batch_size": 24,
                        "early_stop": true,
                        "epochs": 200,
                        "half_lr": true,
                        "num_workers": 4
                    },
                    "python_environment_requirements": "Asteroid",
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri3Mix",
                        "accuracy": {
                            "si_sdr": 8.581797049575108,
                            "si_sdr_imp": 11.977037288467368,
                            "sdr": 9.305885208641385,
                            "sdr_imp": 12.3943409734845,
                            "sir": 16.42030534048559,
                            "sir_imp": 19.508759460400984,
                            "sar": 10.641943911079238,
                            "sar_imp": -56.4345187842095,
                            "stoi": 0.8365148408724333,
                            "stoi_imp": 0.24401766199806396
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset.",
                    "id": "huggingface_api_796",
                    "name": "ConvTasNet_Libri3Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_791",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_286",
        "query": "The company needs a tool to analyze customers' reviews about their products. We need to find out which ones are positive, neutral, or negative.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that utilize customer review data to determine the sentiment polarity as positive, neutral, or negative by processing textual inputs and returning sentiment classifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_287",
        "query": "A teacher wants to create a quiz for her students. We are now working on the questions and answers for the quiz that be arranged in a table format.",
        "instruction": "Given a `quiz creation` task, retrieve tools that facilitate the development of quiz content by supporting the compilation of questions and answers into a structured table format, tailored to meet educational requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "PyTorch Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "pipeline('table-question-answering', model='Meena/table-question-answering-tapas')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQA (Sequential Question Answering by Microsoft)",
                                "accuracy": null
                            },
                            {
                                "name": "WTQ (Wiki Table Questions by Stanford University)",
                                "accuracy": null
                            },
                            {
                                "name": "WikiSQL (by Salesforce)",
                                "accuracy": null
                            }
                        ]
                    },
                    "description": "TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.",
                    "id": "huggingface_api_452",
                    "name": "table-question-answering-tapas"
                },
                "id": "gorilla_huggingface_tool_448",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_288",
        "query": "I am learning English literature. I plan to build a system that has a huge database of English sentences and keeps the important ones.",
        "instruction": "Given a `sentence database` construction task, retrieve tools that assist in building and managing large collections of English sentences, with functionalities focusing on storing, indexing, and retrieving significant sentences as per specified criteria or importance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/all-distilroberta-v1')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "s2orc",
                                "accuracy": "Not provided"
                            },
                            {
                                "name": "MS Marco",
                                "accuracy": "Not provided"
                            },
                            {
                                "name": "yahoo_answers_topics",
                                "accuracy": "Not provided"
                            }
                        ]
                    },
                    "description": "This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_689",
                    "name": "sentence-transformers/all-distilroberta-v1"
                },
                "id": "gorilla_huggingface_tool_684",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_289",
        "query": "We are tasked to analyze text for a Russian newspaper to help understand general sentiment and trends in the text.",
        "instruction": "Given a `text analysis` task for sentiment and trend detection in a Russian newspaper, retrieve tools that leverage language models capable of processing Russian text to extract meaningful features, embeddings, and insights aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')",
                    "api_arguments": [
                        "sentences",
                        "padding",
                        "truncation",
                        "max_length",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n\n# Sentences we want sentence embeddings for sentences = ['?']\n\n# Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])",
                    "performance": {
                        "dataset": "Russian SuperGLUE",
                        "accuracy": "Not provided"
                    },
                    "description": "BERT large model multitask (cased) for Sentence Embeddings in Russian language.",
                    "id": "huggingface_api_24",
                    "name": "sberbank-ai/sbert_large_mt_nlu_ru"
                },
                "id": "gorilla_huggingface_tool_24",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_290",
        "query": "Create a smart speaker that can recognize voice commands such as \"Turn on the lights,\" \"Play music,\" or \"Set a timer.\"",
        "instruction": "Given a `smart speaker development` task, retrieve tools that enable the creation of devices capable of recognizing and processing voice commands by utilizing audio input for command classification and execution.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')",
                    "api_arguments": "audio file",
                    "python_environment_requirements": "transformers library",
                    "example_code": "result = audio_classifier('path/to/audio/file.wav')",
                    "performance": {
                        "dataset": "Speech Commands v2",
                        "accuracy": "98.120"
                    },
                    "description": "Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.",
                    "id": "huggingface_api_831",
                    "name": "ast-finetuned-speech-commands-v2"
                },
                "id": "gorilla_huggingface_tool_826",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_291",
        "query": "Write a script for an online forum moderator that will help them to detect gibberish text in a post.",
        "instruction": "Given a `gibberish text detection` task, retrieve tools that utilize natural language processing models for text classification, specifically designed to analyze textual inputs and identify gibberish content within them.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457')",
                    "api_arguments": {
                        "inputs": "I love AutoNLP"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForSequenceClassification",
                        "AutoTokenizer": "from_pretrained"
                    },
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "madhurjindal/autonlp-data-Gibberish-Detector",
                        "accuracy": 0.9735624586913417
                    },
                    "description": "A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.",
                    "id": "huggingface_api_398",
                    "name": "madhurjindal/autonlp-Gibberish-Detector-492513457"
                },
                "id": "gorilla_huggingface_tool_394",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_292",
        "query": "Our company specializes in providing information on similar topics. We want to find similar sentences in a text document.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that apply natural language processing techniques, particularly sentence transformers, to analyze and find similarities between sentences in a text document.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "1,170,060,424 training pairs",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_691",
                    "name": "sentence-transformers/all-MiniLM-L12-v2"
                },
                "id": "gorilla_huggingface_tool_686",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_293",
        "query": "A healthcare professional wants to get quick answers to COVID-19 related questions from the latest research articles.",
        "instruction": "Given a `question answering` task related to COVID-19, retrieve tools that utilize natural language processing models to process text inputs and provide accurate responses based on the latest research articles.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2-covid'), tokenizer=RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2-covid'))",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2-covid",
                        "tokenizer": "deepset/roberta-base-squad2-covid"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "QA_input": {
                            "question": "Why is model conversion important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                        },
                        "res": "nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "XVAL_EM": 0.17890995260663506,
                            "XVAL_f1": 0.49925444207319924,
                            "XVAL_top_3_recall": 0.8021327014218009
                        }
                    },
                    "description": "This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.",
                    "id": "huggingface_api_472",
                    "name": "deepset/roberta-base-squad2-covid"
                },
                "id": "gorilla_huggingface_tool_468",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_294",
        "query": "A language service wants to incorporate a speech-to-speech translation feature that assists users in translating Hokkien to English on an audio file.",
        "instruction": "Given a `speech-to-speech translation` task, retrieve tools that facilitate the translation of audio inputs from one language to another, specifically focusing on converting Hokkien speech to English through automated processing and model generation techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "Speech-to-speech translation",
                    "api_call": "S2THubInterface()",
                    "api_arguments": {
                        "task": "speech_to_text",
                        "model": "facebook/xm_transformer_s2ut_hk-en",
                        "generator": "task.build_generator([model], cfg)",
                        "sample": "S2THubInterface.get_model_input(task, audio)"
                    },
                    "python_environment_requirements": {
                        "fairseq": "latest",
                        "torchaudio": "latest",
                        "huggingface_hub": "latest"
                    },
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/xm_transformer_s2ut_hk-en,\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\n cache_dir=cache_dir,\n)\nmodel = models[0].cpu()\ncfg[task].cpu = True\ngenerator = task.build_generator([model], cfg)\naudio, _ = torchaudio.load(/path/to/an/audio/file)\nsample = S2THubInterface.get_model_input(task, audio)\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": "TED, drama, TAT domain",
                        "accuracy": "Not provided"
                    },
                    "description": "Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.",
                    "id": "huggingface_api_798",
                    "name": "xm_transformer_s2ut_hk-en"
                },
                "id": "gorilla_huggingface_tool_793",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_295",
        "query": "The marketing department wants to use AI-generated images for their next social media campaign. They want a high-resolution image of a vintage sports car racing through a desert landscape during sunset.",
        "instruction": "Given an `AI-generated image creation` task, retrieve tools that produce high-resolution images from text-based descriptions, specifically focusing on generating visual art such as a vintage sports car in a desert landscape during sunset, by processing detailed prompts to achieve desired visual attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('prompthero/openjourney')",
                    "api_arguments": {
                        "prompt": "string"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "torch"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = prompthero/openjourney\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\nimage = pipe(prompt).images[0]\nimage.save(./retro_cars.png)",
                    "performance": {
                        "dataset": "Midjourney images",
                        "accuracy": "Not specified"
                    },
                    "description": "Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.",
                    "id": "huggingface_api_31",
                    "name": "prompthero/openjourney"
                },
                "id": "gorilla_huggingface_tool_31",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_296",
        "query": "I wish to automatically classify a given text's emotion. What Transformers model should I use?",
        "instruction": "Given a `emotion classification` task, retrieve tools that utilize Transformers models to analyze text inputs and determine the emotional content by performing text classification as specified by the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Text Classification",
                    "api_call": "pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')",
                    "api_arguments": "text",
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\nresult = nlp('I am so happy today!')",
                    "performance": {
                        "dataset": "go_emotions"
                    },
                    "description": "This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.",
                    "id": "huggingface_api_395",
                    "name": "joeddav/distilbert-base-uncased-go-emotions-student"
                },
                "id": "gorilla_huggingface_tool_391",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_297",
        "query": "I am a novel writer. I plan to write some paragraphs, but I cannot find the exact word placeholder or missing word. Can you help me make a tool to complete the given text below?",
        "instruction": "Given a `text completion` task, retrieve tools that utilize masked language models to fill in missing words or placeholders in text, thereby assisting in developing coherent and complete text passages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MNLI",
                                "accuracy": 87.6
                            },
                            {
                                "name": "QQP",
                                "accuracy": 91.9
                            },
                            {
                                "name": "QNLI",
                                "accuracy": 92.8
                            },
                            {
                                "name": "SST-2",
                                "accuracy": 94.8
                            },
                            {
                                "name": "CoLA",
                                "accuracy": 63.6
                            },
                            {
                                "name": "STS-B",
                                "accuracy": 91.2
                            },
                            {
                                "name": "MRPC",
                                "accuracy": 90.2
                            },
                            {
                                "name": "RTE",
                                "accuracy": 78.7
                            }
                        ]
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. This model is case-sensitive and can be fine-tuned on a downstream task.",
                    "id": "huggingface_api_659",
                    "name": "roberta-base"
                },
                "id": "gorilla_huggingface_tool_654",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_298",
        "query": "We need to analyze satellite images to categorize the types of land use. For this purpose, I need to segment the images and identify different objects.",
        "instruction": "Given an `image segmentation` task, retrieve tools that analyze satellite images for land use categorization by processing visual inputs to identify and segment different objects in the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')",
                    "api_arguments": {
                        "images": "image",
                        "task_inputs": [
                            "semantic",
                            "instance",
                            "panoptic"
                        ],
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\n\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\nsemantic_outputs = model(**semantic_inputs)\n\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ydshieh/coco_dataset_script",
                        "accuracy": "Not provided"
                    },
                    "description": "OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.",
                    "id": "huggingface_api_242",
                    "name": "shi-labs/oneformer_coco_swin_large"
                },
                "id": "gorilla_huggingface_tool_238",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_299",
        "query": "A food delivery app wants to help users understand ingredients in the food item by analyzing the images. We need a solution to process the food images and give textual information about the items.",
        "instruction": "Given an `image-to-text analysis` task, retrieve tools that process image inputs to provide detailed textual descriptions or information about the food items depicted, aligning with the query's requirements for ingredient identification.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_300",
        "query": "I need to extract biomedical entities from a given set of case reports to make it easier for researchers to analyze the data.",
        "instruction": "Given a `biomedical entity extraction` task, retrieve tools that specialize in Named Entity Recognition (NER) to process text inputs and identify relevant biomedical entities from case reports, supporting the query's data analysis objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)",
                        "tokenizer": "AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
                    },
                    "example_code": "pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)",
                    "performance": {
                        "dataset": "Maccrobat",
                        "accuracy": "Not provided"
                    },
                    "description": "An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.",
                    "id": "huggingface_api_404",
                    "name": "d4data/biomedical-ner-all"
                },
                "id": "gorilla_huggingface_tool_400",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_301",
        "query": "The company wants to create a chatbot to help answer customer questions regarding the chatbot's consciousness. We need to be able to generate sensible responses.",
        "instruction": "Given a `chatbot development` task, retrieve tools capable of generating coherent and contextually appropriate text outputs to address inquiries about chatbot consciousness by leveraging advanced natural language processing models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('facebook/opt-66b')",
                    "api_arguments": [
                        "input_ids",
                        "do_sample",
                        "num_return_sequences",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\nprompt = Hello, I am conscious and\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\nset_seed(32)\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "GPT-3",
                        "accuracy": "roughly matched"
                    },
                    "description": "OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.",
                    "id": "huggingface_api_625",
                    "name": "facebook/opt-66b"
                },
                "id": "gorilla_huggingface_tool_620",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_302",
        "query": "Our company is working on an app that allows music producers to detect beats in a sample. We want to use the Hubert-large-ll60k model for this.",
        "instruction": "Given a `music feature extraction` task, retrieve tools that are designed for detecting musical elements like beats in audio samples, utilizing advanced models such as Hubert-large-ll60k for accurate audio analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "api_arguments": "pretrained model name",
                    "python_environment_requirements": "transformers",
                    "example_code": "hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')",
                    "performance": {
                        "dataset": "Libri-Light",
                        "accuracy": "matches or improves upon the state-of-the-art wav2vec 2.0 performance"
                    },
                    "description": "Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.",
                    "id": "huggingface_api_12",
                    "name": "hubert-large-ll60k"
                },
                "id": "gorilla_huggingface_tool_12",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_303",
        "query": "We need a product description for an image-based online store platform that will help customers understand the specifics of the product.",
        "instruction": "Given a `product description generation` task, retrieve tools that process image inputs to produce descriptive and informative text outputs, helping customers understand product specifics in an online store environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')",
                    "api_arguments": "image, text",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.",
                    "id": "huggingface_api_77",
                    "name": "git-large-coco"
                },
                "id": "gorilla_huggingface_tool_77",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_304",
        "query": "We are building a social media site which creates automatic captions for users when they post a picture",
        "instruction": "Given an `image captioning` task, retrieve tools that generate textual captions for images by processing visual inputs from users' photos and automatically producing descriptive text suitable for social media posts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Captioning",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')",
                    "api_arguments": {
                        "model": "nlpconnect/vit-gpt2-image-captioning"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "PIL"
                    ],
                    "example_code": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']",
                    "performance": {
                        "dataset": "Not provided",
                        "accuracy": "Not provided"
                    },
                    "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.",
                    "id": "huggingface_api_58",
                    "name": "nlpconnect/vit-gpt2-image-captioning"
                },
                "id": "gorilla_huggingface_tool_58",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_305",
        "query": "In our new app, we are building a feature that recommends books in different languages. To do this, first, we need to translate the book title and details from English to French. Help us to decide the best translation model to use here.",
        "instruction": "Given a `translation` task, retrieve tools that specialize in converting text from English to French by using models optimized for language translation and capable of handling diverse linguistic features to maintain accuracy and context in the output.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Translation",
                    "api_call": "translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.en.fr": 33.8,
                                "newsdiscusstest2015-enfr.en.fr": 40.0,
                                "newssyscomb2009.en.fr": 29.8,
                                "news-test2008.en.fr": 27.5,
                                "newstest2009.en.fr": 29.4,
                                "newstest2010.en.fr": 32.7,
                                "newstest2011.en.fr": 34.3,
                                "newstest2012.en.fr": 31.8,
                                "newstest2013.en.fr": 33.2,
                                "Tatoeba.en.fr": 50.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.",
                    "id": "huggingface_api_522",
                    "name": "Helsinki-NLP/opus-mt-en-fr"
                },
                "id": "gorilla_huggingface_tool_518",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_306",
        "query": "A travel app we work on can translate language of a guide in real-time. We are targeting Spanish-speaking tourists.",
        "instruction": "Given a `real-time language translation` task, retrieve tools that facilitate speech-to-speech translation by processing audio inputs to deliver translated audio outputs, particularly for Spanish-speaking audiences in a travel context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "textless_sm_sl_es()",
                    "api_arguments": null,
                    "python_environment_requirements": "fairseq",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "A Fairseq model for audio-to-audio speech-to-speech translation.",
                    "id": "huggingface_api_800",
                    "name": "textless_sm_sl_es"
                },
                "id": "gorilla_huggingface_tool_795",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_307",
        "query": "We want to make sure clarify some questions about the legal implications of a new partnership contract for a real estate development project.",
        "instruction": "Given a `legal question answering` task, retrieve tools that specialize in processing natural language questions about legal topics by extracting relevant information and providing detailed answers based on legal documents or contracts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')",
                    "api_arguments": {
                        "tokenizer": "AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": {
                        "import": "from transformers import AutoTokenizer, AutoModelForQuestionAnswering",
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)",
                        "model": "model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)"
                    },
                    "performance": {
                        "dataset": "cuad",
                        "accuracy": "46.6%"
                    },
                    "description": "This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.",
                    "id": "huggingface_api_469",
                    "name": "Rakib/roberta-base-on-cuad"
                },
                "id": "gorilla_huggingface_tool_465",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_308",
        "query": "We are a city planning department and want to evaluate the city layout. Analyze the image we provide to segment and understand the various urban elements.",
        "instruction": "Given an `urban layout analysis` task, retrieve tools that perform image segmentation to process and analyze urban elements by utilizing techniques like semantic segmentation to identify and categorize different components within a cityscape image.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "CityScapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_249",
                    "name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_245",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_309",
        "query": "Iâ€™m putting together a dating site where users can submit questions they'd like the matching algorithm to ask. I want to suggest questions like the ones they have already submitted. Can you provide me with a model to do that?",
        "instruction": "Given a `question similarity` task, retrieve tools that utilize sentence embedding techniques to suggest questions similar to already submitted ones by processing text inputs and mapping them to a vector space for clustering or semantic similarity assessment.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "snli, multi_nli, ms_marco",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_707",
                    "name": "sentence-transformers/paraphrase-MiniLM-L3-v2"
                },
                "id": "gorilla_huggingface_tool_702",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_310",
        "query": "We need to integrate the personal assistant we're building with the capability of having conversations with people, sometimes answering general knowledge questions.",
        "instruction": "Given a `conversational AI integration` task, retrieve tools that enable interactive and dynamic text-based interactions by leveraging natural language processing capabilities to facilitate conversations and provide answers to general inquiries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('conversational', model='mywateriswet/ShuanBot')",
                    "api_arguments": "message",
                    "python_environment_requirements": "transformers",
                    "example_code": "response = chatbot('What is your name?')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.",
                    "id": "huggingface_api_585",
                    "name": "mywateriswet/ShuanBot"
                },
                "id": "gorilla_huggingface_tool_580",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_311",
        "query": "A podcast company reached out to us wanting to change their speaker's voice in a recorded podcast.",
        "instruction": "Given a `voice conversion` task, retrieve tools that enable transformation of speaker's voice in audio recordings by processing speech inputs and applying conversion models to match the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')",
                    "api_arguments": {
                        "audio": "example_speech",
                        "sampling_rate": "sampling_rate",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "numpy",
                        "torch",
                        "soundfile"
                    ],
                    "example_code": "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\ndataset = dataset.sort('id')\nsampling_rate = dataset.features['audio'].sampling_rate\nexample_speech = dataset[0]['audio']['array']\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nimport numpy as np\nimport torch\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\nimport soundfile as sf\nsf.write('speech.wav', speech.numpy(), samplerate=16000)",
                    "performance": {
                        "dataset": "CMU ARCTIC",
                        "accuracy": "Not specified"
                    },
                    "description": "SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.",
                    "id": "huggingface_api_784",
                    "name": "microsoft/speecht5_vc"
                },
                "id": "gorilla_huggingface_tool_779",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_312",
        "query": "We are a voice assistant service, and we need to verify the speaker identity for enhanced security when users access the system.",
        "instruction": "Given a `speaker verification` task, retrieve tools that facilitate speaker identity verification by processing audio inputs to ensure enhanced security through accurate speaker classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='superb/wav2vec2-base-superb-sid')",
                    "api_arguments": {
                        "model": "superb/wav2vec2-base-superb-sid"
                    },
                    "python_environment_requirements": {
                        "datasets": "load_dataset",
                        "transformers": "pipeline"
                    },
                    "example_code": "from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-sid)\nlabels = classifier(dataset[0][file], top_k=5)",
                    "performance": {
                        "dataset": "VoxCeleb1",
                        "accuracy": 0.7518
                    },
                    "description": "This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Identification task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_829",
                    "name": "superb/wav2vec2-base-superb-sid"
                },
                "id": "gorilla_huggingface_tool_824",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_313",
        "query": "Provide the code to enhance a single audio track, possibly containing dialogue, music and background noise, extracted from a video game.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that enhance audio tracks by processing audio inputs and improving sound quality, specifically for dialogue, music, and background noise scenarios.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')",
                    "api_arguments": "pretrained_model_name_or_path",
                    "python_environment_requirements": [
                        "transformers",
                        "asteroid"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri1Mix",
                        "accuracy": {
                            "si_sdr": 13.329767398333798,
                            "si_sdr_imp": 9.879986092474098,
                            "sdr": 13.87279932997016,
                            "sdr_imp": 10.370136530757103,
                            "sir": "Infinity",
                            "sir_imp": "NaN",
                            "sar": 13.87279932997016,
                            "sar_imp": 10.370136530757103,
                            "stoi": 0.9140907015623948,
                            "stoi_imp": 0.11817087802185405
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.",
                    "id": "huggingface_api_776",
                    "name": "DCCRNet_Libri1Mix_enhsingle_16k"
                },
                "id": "gorilla_huggingface_tool_771",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_314",
        "query": "We are developing an AI chatbot for a Russian company. I want to extract features from the clients' text messages in Russian language.",
        "instruction": "Given a `feature extraction` task, retrieve tools that specialize in processing Russian text to extract and analyze linguistic features, specifically catering to language understanding and natural language processing requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Russian part of Wikipedia and news data",
                        "accuracy": ""
                    },
                    "description": "RuBERT (Russian, cased, 12â€‘layer, 768â€‘hidden, 12â€‘heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTâ€‘base as an initialization for RuBERT[1].",
                    "id": "huggingface_api_15",
                    "name": "DeepPavlov/rubert-base-cased"
                },
                "id": "gorilla_huggingface_tool_15",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_315",
        "query": "Write a welcome email to a new employee joining the company.",
        "instruction": "Given a `text generation` task, retrieve tools that facilitate natural language processing to create personalized and professional email content by processing textual inputs and generating coherent outputs suitable for welcoming new employees.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "nlp('Once upon a time...')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random mt5 model for text generation",
                    "id": "huggingface_api_14",
                    "name": "lewtun/tiny-random-mt5"
                },
                "id": "gorilla_huggingface_tool_14",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_316",
        "query": "As a news agency, we need a summarized version of a recent article about YouTube's new policy on vaccine misinformation.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise and coherent summaries that capture the main points of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_317",
        "query": "Someone asked the question \"What is the capital of Sweden?\" in the context \"Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\"",
        "instruction": "Given a `question answering` task, retrieve tools that employ natural language processing techniques to provide accurate answers by analyzing the context and query to deliver high-quality results.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-uncased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_uncased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": {
                        "torch": "1.9.0",
                        "transformers": "4.9.2"
                    },
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\nprint(result)",
                    "performance": {
                        "dataset": "SQuAD",
                        "accuracy": {
                            "f1": 93.15,
                            "exact_match": 86.91
                        }
                    },
                    "description": "BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.",
                    "id": "huggingface_api_466",
                    "name": "bert-large-uncased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_462",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_318",
        "query": "Help me to translate a Spanish text to English. The Spanish text is: \"Lo siento, pero no puedo ir a la reuniÃ³n debido a una emergencia personal. AvisarÃ© al equipo y nos pondremos en contacto para reprogramar la reuniÃ³n.\"",
        "instruction": "Given a `language translation` task, retrieve tools that perform text translation by processing Spanish text inputs and providing accurate English translations aligned with the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, Â¿cÃ³mo estÃ¡s?')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "newssyscomb2009-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 30.6,
                                    "chr-F": 0.57
                                }
                            },
                            {
                                "name": "news-test2008-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 27.9,
                                    "chr-F": 0.553
                                }
                            },
                            {
                                "name": "newstest2009-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 30.4,
                                    "chr-F": 0.572
                                }
                            },
                            {
                                "name": "newstest2010-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 36.1,
                                    "chr-F": 0.614
                                }
                            },
                            {
                                "name": "newstest2011-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 34.2,
                                    "chr-F": 0.599
                                }
                            },
                            {
                                "name": "newstest2012-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 37.9,
                                    "chr-F": 0.624
                                }
                            },
                            {
                                "name": "newstest2013-spaeng.spa.eng",
                                "accuracy": {
                                    "BLEU": 35.3,
                                    "chr-F": 0.609
                                }
                            },
                            {
                                "name": "Tatoeba-test.spa.eng",
                                "accuracy": {
                                    "BLEU": 59.6,
                                    "chr-F": 0.739
                                }
                            }
                        ]
                    },
                    "description": "Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.",
                    "id": "huggingface_api_530",
                    "name": "Helsinki-NLP/opus-mt-es-en"
                },
                "id": "gorilla_huggingface_tool_526",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_319",
        "query": "We have a picture of a landscape, and we'd like to add a building and a river in this picture.",
        "instruction": "Given an `image editing` task, retrieve tools that utilize image-to-image transformations by processing existing images and applying additional features or elements specified in the query to achieve the desired modifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Diffusion-based text-to-image generation model",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')",
                    "api_arguments": [
                        "checkpoint",
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\nprompt = make it on fire\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "Stable Diffusion v1-5",
                        "accuracy": "Not provided"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.",
                    "id": "huggingface_api_274",
                    "name": "lllyasviel/control_v11e_sd15_ip2p"
                },
                "id": "gorilla_huggingface_tool_270",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_320",
        "query": "The company needs to summarize articles for its news application. Provide guidelines to use PEGASUS for this purpose.",
        "instruction": "Given a `text summarization` task, retrieve tools that are designed to condense and simplify articles or text content using natural language processing models, specifically tailored for abstractive summarization tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-large')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline('summarization', model='google/pegasus-large')\nsummary = summarizer('your_text_here')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": "47.60/24.83/39.64"
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": "44.16/21.56/41.30"
                            },
                            {
                                "name": "newsroom",
                                "accuracy": "45.98/34.20/42.18"
                            },
                            {
                                "name": "multi_news",
                                "accuracy": "47.65/18.75/24.95"
                            },
                            {
                                "name": "gigaword",
                                "accuracy": "39.65/20.47/36.76"
                            },
                            {
                                "name": "wikihow",
                                "accuracy": "46.39/22.12/38.41"
                            },
                            {
                                "name": "reddit_tifu",
                                "accuracy": "27.99/9.81/22.94"
                            },
                            {
                                "name": "big_patent",
                                "accuracy": "52.29/33.08/41.66"
                            },
                            {
                                "name": "arxiv",
                                "accuracy": "44.21/16.95/25.67"
                            },
                            {
                                "name": "pubmed",
                                "accuracy": "45.97/20.15/28.25"
                            },
                            {
                                "name": "aeslc",
                                "accuracy": "37.68/21.25/36.51"
                            },
                            {
                                "name": "billsum",
                                "accuracy": "59.67/41.58/47.59"
                            }
                        ]
                    },
                    "description": "google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks.",
                    "id": "huggingface_api_558",
                    "name": "google/pegasus-large"
                },
                "id": "gorilla_huggingface_tool_554",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_321",
        "query": "A marketing firm has asked us to build an application to classify social media images into various categories.",
        "instruction": "Given an `image classification` task, retrieve tools that process social media images to categorize them into various categories by utilizing computer vision models optimized for classifying images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')",
                    "api_arguments": [
                        "learning_rate",
                        "train_batch_size",
                        "eval_batch_size",
                        "seed",
                        "gradient_accumulation_steps",
                        "total_train_batch_size",
                        "optimizer",
                        "lr_scheduler_type",
                        "lr_scheduler_warmup_ratio",
                        "num_epochs"
                    ],
                    "python_environment_requirements": [
                        "Transformers 4.28.1",
                        "Pytorch 2.0.0+cu118",
                        "Datasets 2.11.0",
                        "Tokenizers 0.13.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "imagefolder",
                        "accuracy": 0.9726
                    },
                    "description": "This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.",
                    "id": "huggingface_api_197",
                    "name": "swin-tiny-patch4-window7-224-bottom_cleaned_data"
                },
                "id": "gorilla_huggingface_tool_193",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_322",
        "query": "I work for a financial company that stores all of its data in tables. We need a way to extract key information efficiently by asking natural language questions.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that leverage natural language processing to extract key information from tabular data by processing both table and query inputs, enabling efficient querying and returning relevant answers based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "PyTorch Transformers",
                    "functionality": "Table-based QA",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')",
                    "api_arguments": {
                        "table": "pd.DataFrame.from_dict(data)",
                        "query": "str"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "pandas"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": "Not provided"
                    },
                    "description": "OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).",
                    "id": "huggingface_api_458",
                    "name": "neulab/omnitab-large-1024shot"
                },
                "id": "gorilla_huggingface_tool_454",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_323",
        "query": "I would like to create an application where users may listen to translations of English sentences. I need a Text-to-Speech model to support this functionality.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that enable transforming text inputs into audio speech, supporting functionalities such as translating sentences to aid in the creation of applications where users can listen to spoken translations.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "pipeline('text-to-speech', model='mio/Artoria')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('s')",
                    "performance": {
                        "dataset": "fate",
                        "accuracy": "Not provided"
                    },
                    "description": "This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.",
                    "id": "huggingface_api_726",
                    "name": "mio/Artoria"
                },
                "id": "gorilla_huggingface_tool_721",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_324",
        "query": "We want to communicate product information to online customers. Translate the information from English to French.",
        "instruction": "Given a `language translation` task, retrieve tools that process text inputs to translate content from one language to another, ensuring accurate and context-aware translations aligned with the query's language pair and communication requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('t5-3b')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.",
                    "id": "huggingface_api_535",
                    "name": "t5-3b"
                },
                "id": "gorilla_huggingface_tool_531",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_325",
        "query": "You want to create a bot that can play the Pong No Frameskip-v4 game with exceptional skill.",
        "instruction": "Given a `game-playing AI` task, retrieve tools or models capable of leveraging deep reinforcement learning frameworks to train or deploy agents that exhibit advanced skill levels in designated environments, such as playing the Pong No Frameskip-v4 game.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-PongNoFrameskip-v4')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "RL Zoo",
                        "SB3",
                        "SB3 Contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env PongNoFrameskip-v4 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "PongNoFrameskip-v4",
                        "accuracy": "21.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_898",
                    "name": "ppo-PongNoFrameskip-v4"
                },
                "id": "gorilla_huggingface_tool_891",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_326",
        "query": "I'm talking to a new person online. Can this API help keep me safe by filtering out any inappropriate messages they send me?",
        "instruction": "Given a `message filtering` task, retrieve tools that enhance online safety by processing text inputs to detect and filter out inappropriate messages, thus ensuring a secure communication environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "matched acc",
                                "accuracy": 88.1
                            },
                            {
                                "name": "mismatched acc",
                                "accuracy": 88.19
                            }
                        ]
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is a simple and effective technique with very little performance drop.",
                    "id": "huggingface_api_497",
                    "name": "valhalla/distilbart-mnli-12-3"
                },
                "id": "gorilla_huggingface_tool_493",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_327",
        "query": "I want to automate the process of answering questions about historical facts. When given a question and a surrounding context, it should provide an accurate response.",
        "instruction": "Given an `automated question answering` task, retrieve tools that process questions and context information using natural language processing models to deliver precise and accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2",
                        "tokenizer": "deepset/roberta-base-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "code": "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = deepset/roberta-base-squad2\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 79.87029394424324,
                            "f1": 82.91251169582613
                        }
                    },
                    "description": "This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions.",
                    "id": "huggingface_api_461",
                    "name": "deepset/roberta-base-squad2"
                },
                "id": "gorilla_huggingface_tool_457",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_328",
        "query": "We are building AI glasses that should tell us about things that we are seeing with classifications. We want to use a visual transformer architecture.",
        "instruction": "Given a `visual classification with transformer` task, retrieve tools that enable AI applications to label and categorize visual inputs through sophisticated image classification models, utilizing visual transformer architectures to deliver accurate results aligned with the specified needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor",
                        "PIL": "Image",
                        "requests": "requests"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not specified"
                    },
                    "description": "Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.",
                    "id": "huggingface_api_193",
                    "name": "microsoft/swin-tiny-patch4-window7-224"
                },
                "id": "gorilla_huggingface_tool_189",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_329",
        "query": "Generate a classical image by using Diffusion Model",
        "instruction": "Given an `image generation` task, retrieve tools that utilize diffusion models to create classical images by processing relevant input parameters and returning an image based on the predefined model and dataset.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "https://huggingface.co/datasets/huggan/wikiart",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.",
                    "id": "huggingface_api_297",
                    "name": "johnowhitaker/sd-class-wikiart-from-bedrooms"
                },
                "id": "gorilla_huggingface_tool_293",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_330",
        "query": "Create a program to calculate sentence similarity scores between a list of sentences.",
        "instruction": "Given a `sentence similarity calculation` task, retrieve tools that evaluate the semantic similarity between sentences by generating embeddings for each sentence and calculating their similarity scores using models optimized for natural language processing tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Automated evaluation"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_696",
                    "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                },
                "id": "gorilla_huggingface_tool_691",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_331",
        "query": "We want to analyze a conference call recording to identify the speakers and the segments of the conversation they participated in.",
        "instruction": "Given a `speaker identification` task, retrieve tools that perform speaker diarization by processing audio inputs to segment and identify different speakers, aligning with the query's needs to analyze conversation dynamics in a conference call recording.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker Diarization",
                    "api_call": "Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": [
                        "from pyannote.audio import Pipeline",
                        "pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "diarization = pipeline(audio.wav)",
                        "with open(audio.rttm, w) as rttm:",
                        "  diarization.write_rttm(rttm)"
                    ],
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters.",
                    "id": "huggingface_api_841",
                    "name": "philschmid/pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_836",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_332",
        "query": "You have just met a person that speaks French. As a hotel manager, you need to tell them, \"Welcome to our hotel, we hope you enjoy your stay.\" in French.",
        "instruction": "Given a `multilingual translation` task, retrieve tools designed for natural language processing that can translate text from one language to another by processing text inputs in the source language and outputting accurate translations in the target language.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Multilingual Translation",
                    "api_call": "M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')",
                    "api_arguments": {
                        "encoded_input": "Encoded input text",
                        "target_lang": "Target language code"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": [
                        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer",
                        "hi_text = ",
                        "chinese_text = ",
                        "model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)",
                        "tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)",
                        "tokenizer.src_lang = hi",
                        "encoded_hi = tokenizer(hi_text, return_tensors=pt)",
                        "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))",
                        "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "WMT",
                        "accuracy": "Not provided"
                    },
                    "description": "M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.",
                    "id": "huggingface_api_633",
                    "name": "facebook/m2m100_418M"
                },
                "id": "gorilla_huggingface_tool_628",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_333",
        "query": "Create software that extracts answers from input documents when given a set of questions.",
        "instruction": "Given a `document question answering` task, retrieve tools that can extract answers from input documents by processing both the textual content of the documents and a set of questions to deliver accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.12.2",
                        "torch==1.8.0+cu101",
                        "datasets==1.14.0",
                        "tokenizers==0.10.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 1.194
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_115",
                    "name": "layoutlmv2-base-uncased-finetuned-docvqa"
                },
                "id": "gorilla_huggingface_tool_114",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_334",
        "query": "I want to write a story about a brave knight and a dragon but I'm unable to come up with a good start. Help me with that.",
        "instruction": "Given a `story generation` task, retrieve tools that facilitate text generation by using models capable of expanding a narrative or providing creative story prompts based on an initial idea or theme.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')",
                    "api_arguments": {
                        "model": "sshleifer/tiny-gpt2"
                    },
                    "python_environment_requirements": {
                        "huggingface_transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\nresult = nlp('Once upon a time')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.",
                    "id": "huggingface_api_622",
                    "name": "sshleifer/tiny-gpt2"
                },
                "id": "gorilla_huggingface_tool_617",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_335",
        "query": "We have an online journal application that requires users to upload documents in which we need to automatically answer the questions related to the uploaded document.",
        "instruction": "Given a `document question answering` task, retrieve tools that process document inputs to automatically generate answers to questions based on the content of the uploaded document.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Document Question Answering model based on LayoutXLM.",
                    "id": "huggingface_api_120",
                    "name": "CZ_DVQA_layoutxlm-base"
                },
                "id": "gorilla_huggingface_tool_119",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_336",
        "query": "Our company collects data on the salesperson performance in different regions for each month. We want to use the most suitable API from our subscribed_huggingface.co to analyze that in specific table format and then based on provided question, answer accordingly.",
        "instruction": "Given a `table analysis and question answering` task, retrieve tools that utilize natural language processing capabilities to analyze table data and answer questions based on the provided input using table comprehension techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')",
                    "api_arguments": {
                        "model_name": "google/tapas-base-finetuned-wtq"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.12.0"
                    },
                    "example_code": "from transformers import TapasTokenizer, TapasForQuestionAnswering\n\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.4638
                    },
                    "description": "TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.",
                    "id": "huggingface_api_430",
                    "name": "google/tapas-base-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_426",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_337",
        "query": "I am working as the head of customer service for a Spanish speaking market. I want to know the sentiment of my customers on their last call with our support agents.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that evaluate the sentiment of audio inputs by classifying Spanish speech to determine the underlying sentiment expressed during customer interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Classification",
                    "api_call": "Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')",
                    "api_arguments": {
                        "model_name": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.17.0",
                        "pytorch": "1.10.0+cu111",
                        "datasets": "2.0.0",
                        "tokenizers": "0.11.6"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "MESD",
                        "accuracy": 0.9308
                    },
                    "description": "This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.",
                    "id": "huggingface_api_825",
                    "name": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD"
                },
                "id": "gorilla_huggingface_tool_820",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_338",
        "query": "A team is working on a video game where the player needs to land the spaceship on the lunar surface without crashing. They want to implement an AI module that can play the game and test it.",
        "instruction": "Given a `AI module implementation for gaming` task, retrieve tools that facilitate the integration and testing of AI models, specifically those capable of controlling game mechanics and improving gameplay strategies using reinforcement learning frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "LunarLander-v2",
                    "api_call": "DQN.load('araffin/dqn-LunarLander-v2')",
                    "api_arguments": {
                        "checkpoint": "araffin/dqn-LunarLander-v2",
                        "kwargs": {
                            "target_update_interval": 30
                        }
                    },
                    "python_environment_requirements": [
                        "huggingface_sb3",
                        "stable_baselines3"
                    ],
                    "example_code": {
                        "load_model": "from huggingface_sb3 import load_from_hub\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\n\nkwargs = dict(target_update_interval=30)\n\nmodel = DQN.load(checkpoint, **kwargs)\nenv = make_vec_env(LunarLander-v2, n_envs=1)",
                        "evaluate": "mean_reward, std_reward = evaluate_policy(\n model,\n env,\n n_eval_episodes=20,\n deterministic=True,\n)\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})"
                    },
                    "performance": {
                        "dataset": "LunarLander-v2",
                        "accuracy": "280.22 +/- 13.03"
                    },
                    "description": "This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.",
                    "id": "huggingface_api_902",
                    "name": "araffin/dqn-LunarLander-v2"
                },
                "id": "gorilla_huggingface_tool_895",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_339",
        "query": "An IoT device collects images from different locations. Create a model to detect objects in these images to analyze the surroundings.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize computer vision models to analyze images by detecting and identifying objects, processing visual inputs to provide the necessary contextual insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')",
                    "api_arguments": {
                        "image": "Image.open(requests.get(url, stream=True).raw)",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import DetrFeatureExtractor, DetrForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\nbboxes = outputs.pred_boxes",
                    "performance": {
                        "dataset": "COCO 2017 validation",
                        "accuracy": "AP 44.9"
                    },
                    "description": "DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.",
                    "id": "huggingface_api_211",
                    "name": "facebook/detr-resnet-101-dc5"
                },
                "id": "gorilla_huggingface_tool_207",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_340",
        "query": "I want to classify the images of houseplants to find out their type, like whether it's a cactus, fern, or succulent.",
        "instruction": "Given an `image classification` task, retrieve tools that accurately classify images by processing visual inputs to identify the type of houseplant, such as cactus, fern, or succulent, and return the predicted category.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/mobilenet_v1_0.75_192"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.",
                    "id": "huggingface_api_186",
                    "name": "google/mobilenet_v1_0.75_192"
                },
                "id": "gorilla_huggingface_tool_182",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_341",
        "query": "Our customer wants to analyze the sentiment of their customers' feedback. The feedback is in Spanish.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that perform language processing by analyzing textual feedback in Spanish to determine and classify the sentiment as positive, negative, or neutral, based on the query's language and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_342",
        "query": "As a restaurant owner, I want to know if the total revenue for last week met our target revenue.",
        "instruction": "Given a `revenue evaluation` task, retrieve tools that analyze and compare financial data by processing numerical and table inputs to determine whether specific financial goals or targets have been met.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq'), TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')",
                    "api_arguments": "model_name_or_path, table, query",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import TapasForQuestionAnswering, TapasTokenizer\n\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq')\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')\n\ninputs = tokenizer(table=table, queries=query, return_tensors='pt')\noutputs = model(**inputs)\n\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.3762
                    },
                    "description": "TAPAS small model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_434",
                    "name": "google/tapas-small-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_430",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_343",
        "query": "In our online ecommerce platform, we want to build an AI app to automatically recognize the type of products. It should be able to identify common items like clothing, electronics, furniture, and more.",
        "instruction": "Given a `product recognition` task, retrieve tools that utilize image classification to identify various product types by processing visual inputs and discerning categories such as clothing, electronics, and furniture, aligning with the query's requirements for an AI application in ecommerce.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/convnext-large-224"
                    },
                    "python_environment_requirements": {
                        "transformers": "Hugging Face Transformers",
                        "torch": "PyTorch",
                        "datasets": "Hugging Face Datasets"
                    },
                    "example_code": {
                        "import": [
                            "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification",
                            "import torch",
                            "from datasets import load_dataset"
                        ],
                        "load_dataset": "dataset = load_dataset('huggingface/cats-image')",
                        "image": "image = dataset['test']['image'][0]",
                        "feature_extractor": "feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')",
                        "model": "model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')",
                        "inputs": "inputs = feature_extractor(image, return_tensors='pt')",
                        "logits": "with torch.no_grad():\n  logits = model(**inputs).logits",
                        "predicted_label": "predicted_label = logits.argmax(-1).item()",
                        "print": "print(model.config.id2label[predicted_label])"
                    },
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not specified"
                    },
                    "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.",
                    "id": "huggingface_api_174",
                    "name": "facebook/convnext-large-224"
                },
                "id": "gorilla_huggingface_tool_170",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_344",
        "query": "We want to code a loading spinner to display when our system is undergoing maintenance. Can you generate the code for us?",
        "instruction": "Given a `program synthesis` task, retrieve tools that generate executable code by interpreting natural language prompts to provide code solutions that adhere to the queryâ€™s specifications and requirements, particularly for tasks like creating a loading spinner.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Program Synthesis",
                    "api_call": "AutoTokenizer.from_pretrained('Salesforce/codegen-350M-multi')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\ntext = def hello_world():\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "HumanEval and MTPB",
                        "accuracy": "Refer to the paper for accuracy details"
                    },
                    "description": "CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.",
                    "id": "huggingface_api_619",
                    "name": "Salesforce/codegen-350M-multi"
                },
                "id": "gorilla_huggingface_tool_614",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_345",
        "query": "Determine if one Russian sentence logically contradicts the information provided by another Russian sentence.",
        "instruction": "Given a `logical contradiction detection` task, retrieve tools that analyze pairs of Russian sentences to determine whether they logically contradict, entail, or are neutral, utilizing Natural Language Inference capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')",
                    "api_arguments": [
                        "text1",
                        "text2"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nif torch.cuda.is_available():\n model.cuda()\ntext1 = '.'\ntext2 = '.'\nwith torch.inference_mode():\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\nprint({v: proba[k] for k, v in model.config.id2label.items()})",
                    "performance": {
                        "dataset": [
                            "JOCI",
                            "MNLI",
                            "MPE",
                            "SICK",
                            "SNLI",
                            "ANLI",
                            "NLI-style FEVER",
                            "IMPPRES"
                        ],
                        "accuracy": {
                            "ROC AUC": {
                                "entailment": 0.91,
                                "contradiction": 0.71,
                                "neutral": 0.79
                            }
                        }
                    },
                    "description": "This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.",
                    "id": "huggingface_api_520",
                    "name": "cointegrated/rubert-base-cased-nli-threeway"
                },
                "id": "gorilla_huggingface_tool_516",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_346",
        "query": "We have received an invoice document, and would like to extract the total amount from it.",
        "instruction": "Given a `document analysis` task, retrieve tools that are specialized in extracting specific information, such as total amounts, from documents like invoices by processing textual and visual content to provide accurate data extraction.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')",
                    "api_arguments": "question, context",
                    "python_environment_requirements": "transformers",
                    "example_code": "nlp(question='What is the total amount?', context='your_invoice_text')",
                    "performance": {
                        "dataset": "proprietary dataset of invoices, SQuAD2.0, and DocVQA",
                        "accuracy": "Not provided"
                    },
                    "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.",
                    "id": "huggingface_api_122",
                    "name": "layoutlm-invoices"
                },
                "id": "gorilla_huggingface_tool_121",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_347",
        "query": "I am a movie director and I need to detect the genre of a movie based on its actions.",
        "instruction": "Given a `movie genre detection` task, retrieve tools that utilize video action recognition capabilities by processing video inputs to classify the genre based on observed actions and sequences within the footage.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Action Recognition",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "decord",
                        "huggingface_hub"
                    ],
                    "example_code": "from decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n converted_len = int(clip_len * frame_sample_rate)\n end_idx = np.random.randint(converted_len, seg_len)\n start_idx = end_idx - converted_len\n indices = np.linspace(start_idx, end_idx, num=clip_len)\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n return indices\nfile_path = hf_hub_download(\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\ninputs = feature_extractor(list(video), return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "UCF101",
                        "accuracy": 0.758209764957428
                    },
                    "description": "VideoMAE Base model fine tuned on UCF101 for Video Action Recognition",
                    "id": "huggingface_api_342",
                    "name": "videomae-base-finetuned-ucf101"
                },
                "id": "gorilla_huggingface_tool_338",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_348",
        "query": "A company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.",
        "instruction": "Given a `survey analysis` task, retrieve tools that can count specific responses for survey questions by processing tabular data inputs to provide insights about the response distribution for each question.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')",
                    "api_arguments": {
                        "table": "table_data",
                        "query": "query"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline"
                    },
                    "example_code": "from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model & tokenizer\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\n\n# Get predictions\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\nprint(result)",
                    "performance": {
                        "dataset": "wikisql",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.",
                    "id": "huggingface_api_462",
                    "name": "navteca/tapas-large-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_458",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_349",
        "query": "Our company is making a chatbot that needs to extract information from a paragraph. Get the named entities in the paragraph.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and extract named entities, providing insights into categories such as person, organization, location, and other related classifications according to the query requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes-fast')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\ntagger = SequenceTagger.load(flair/ner-english-ontonotes-fast)\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\ntagger.predict(sentence)\nprint(sentence)\nfor entity in sentence.get_spans('ner'):\n  print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "F1-Score: 89.3"
                    },
                    "description": "This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_427",
                    "name": "flair/ner-english-ontonotes-fast"
                },
                "id": "gorilla_huggingface_tool_423",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_350",
        "query": "We would like to build a semantic text search system that can find similar documents in a repository based on a given description.",
        "instruction": "Given a `semantic text search` task, retrieve tools that facilitate finding similar documents by utilizing sentence embeddings and semantic similarity measures to compare and rank document descriptions based on query relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_695",
                    "name": "sentence-transformers/bert-base-nli-mean-tokens"
                },
                "id": "gorilla_huggingface_tool_690",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_351",
        "query": "We are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.",
        "instruction": "Given a `text summarization` task, retrieve tools that are capable of processing long text inputs to generate concise and coherent summaries, effectively condensing the content while preserving the main ideas.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')",
                    "api_arguments": "",
                    "python_environment_requirements": "huggingface/transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "cnn_dailymail",
                                "accuracy": {
                                    "Rouge 2": "22.12",
                                    "Rouge-L": "36.99"
                                }
                            }
                        ]
                    },
                    "description": "DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.",
                    "id": "huggingface_api_545",
                    "name": "sshleifer/distilbart-cnn-12-6"
                },
                "id": "gorilla_huggingface_tool_541",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_352",
        "query": "The school wants a tool to teach foreign students Chinese grammar. They want you to develop a part-of-speech tagging system to detect the words' grammatical roles.",
        "instruction": "Given a `part-of-speech tagging` task, retrieve tools that specialize in natural language processing using token classification. These tools should process text input to identify and label the grammatical roles of words, specifically focusing on Chinese language grammar to assist in educational settings.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Part-of-speech tagging",
                    "api_call": "AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')",
                    "api_arguments": {
                        "tokenizer": "BertTokenizerFast.from_pretrained('bert-base-chinese')"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).",
                    "id": "huggingface_api_423",
                    "name": "ckiplab/bert-base-chinese-pos"
                },
                "id": "gorilla_huggingface_tool_419",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_353",
        "query": "A stock investor is looking to analyze the sentiment of a stock forum, such as StockTwits, to gain insights into the market sentiment for a specific stock.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze textual data related to stock market discussions by processing text inputs to infer market sentiment using pre-trained models specifically tailored for stock-related content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Inferencing for stock-related comments",
                    "api_call": "RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')",
                    "api_arguments": {
                        "model": "RobertaForSequenceClassification",
                        "tokenizer": "RobertaTokenizer"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import RobertaForSequenceClassification, RobertaTokenizer\nfrom transformers import pipeline\nimport pandas as pd\nimport emoji\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\nsentences = list(sentences)\nresults = nlp(sentences)\nprint(results)",
                    "performance": {
                        "dataset": "stocktwits",
                        "accuracy": 0.9343
                    },
                    "description": "This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.",
                    "id": "huggingface_api_384",
                    "name": "zhayunduo/roberta-base-stocktwits-finetuned"
                },
                "id": "gorilla_huggingface_tool_380",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_354",
        "query": "We need to build an AI-powered tool to assist visually impaired users in understanding their surroundings by answering questions about images.",
        "instruction": "Given an `AI visual assistance` task, retrieve tools that process image and text inputs to provide answers to questions related to visual content, enabling visually impaired users to understand their surroundings by generating informative textual descriptions based on the AI model's analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')",
                    "api_arguments": "image, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "TextVQA",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_106",
                    "name": "git-large-textvqa"
                },
                "id": "gorilla_huggingface_tool_105",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_355",
        "query": "I would like to create an application that identifies animals in Chinese language image captions. Specifically, we want to know if a picture includes a cat or a dog.",
        "instruction": "Given an `image caption analysis` task, retrieve tools that perform image classification by processing image inputs and utilizing Chinese language models to identify specific animals such as cats or dogs in captions.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "OFA-Sys/chinese-clip-vit-base-patch16"
                    },
                    "python_environment_requirements": {
                        "transformers": "ChineseCLIPProcessor, ChineseCLIPModel"
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [, , , ]\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": {
                            "MUGE Text-to-Image Retrieval": {
                                "accuracy": {
                                    "Zero-shot R@1": 63.0,
                                    "Zero-shot R@5": 84.1,
                                    "Zero-shot R@10": 89.2,
                                    "Finetune R@1": 68.9,
                                    "Finetune R@5": 88.7,
                                    "Finetune R@10": 93.1
                                }
                            },
                            "Flickr30K-CN Retrieval": {
                                "accuracy": {
                                    "Zero-shot Text-to-Image R@1": 71.2,
                                    "Zero-shot Text-to-Image R@5": 91.4,
                                    "Zero-shot Text-to-Image R@10": 95.5,
                                    "Finetune Text-to-Image R@1": 83.8,
                                    "Finetune Text-to-Image R@5": 96.9,
                                    "Finetune Text-to-Image R@10": 98.6,
                                    "Zero-shot Image-to-Text R@1": 81.6,
                                    "Zero-shot Image-to-Text R@5": 97.5,
                                    "Zero-shot Image-to-Text R@10": 98.8,
                                    "Finetune Image-to-Text R@1": 95.3,
                                    "Finetune Image-to-Text R@5": 99.7,
                                    "Finetune Image-to-Text R@10": 100.0
                                }
                            },
                            "COCO-CN Retrieval": {
                                "accuracy": {
                                    "Zero-shot Text-to-Image R@1": 69.2,
                                    "Zero-shot Text-to-Image R@5": 89.9,
                                    "Zero-shot Text-to-Image R@10": 96.1,
                                    "Finetune Text-to-Image R@1": 81.5,
                                    "Finetune Text-to-Image R@5": 96.9,
                                    "Finetune Text-to-Image R@10": 99.1,
                                    "Zero-shot Image-to-Text R@1": 63.0,
                                    "Zero-shot Image-to-Text R@5": 86.6,
                                    "Zero-shot Image-to-Text R@10": 92.9,
                                    "Finetune Image-to-Text R@1": 83.5,
                                    "Finetune Image-to-Text R@5": 97.3,
                                    "Finetune Image-to-Text R@10": 99.2
                                }
                            },
                            "Zero-shot Image Classification": {
                                "accuracy": {
                                    "CIFAR10": 96.0,
                                    "CIFAR100": 79.7,
                                    "DTD": 51.2,
                                    "EuroSAT": 52.0,
                                    "FER": 55.1,
                                    "FGVC": 26.2,
                                    "KITTI": 49.9,
                                    "MNIST": 79.4,
                                    "PC": 63.5,
                                    "VOC": 84.9
                                }
                            }
                        }
                    },
                    "description": "Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.",
                    "id": "huggingface_api_367",
                    "name": "OFA-Sys/chinese-clip-vit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_363",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_356",
        "query": "### Instruction: We are a retailer in Spain. We need to understand the sentiment of our Spanish-speaking customers when they provide feedback.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that utilize text classification techniques to accurately assess and interpret the sentiment expressed in Spanish-language customer feedback.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline(sentiment-analysis, model='cardiffnlp/twitter-xlm-roberta-base-sentiment')",
                    "api_arguments": [
                        "model_path"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\nsentiment_task(T'estimo!)",
                    "performance": {
                        "dataset": "Twitter",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).",
                    "id": "huggingface_api_373",
                    "name": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
                },
                "id": "gorilla_huggingface_tool_369",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_357",
        "query": "We are a product company selling personalized gadgets, and we want to build a recommender system that shows generated, high-resolution images of human faces on our website.",
        "instruction": "Given an `image generation` task, retrieve tools capable of producing high-resolution images, specifically focusing on the generation of human faces, by utilizing advanced models for creating quality visuals aligned with the queryâ€™s requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')",
                    "api_arguments": {
                        "model_id": "google/ncsnpp-celebahq-256"
                    },
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DiffusionPipeline\nmodel_id = google/ncsnpp-celebahq-256\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\nimage = sde_ve()[sample]\nimage[0].save(sde_ve_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR-10",
                        "accuracy": {
                            "Inception_score": 9.89,
                            "FID": 2.2,
                            "likelihood": 2.99
                        }
                    },
                    "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "id": "huggingface_api_293",
                    "name": "google/ncsnpp-celebahq-256"
                },
                "id": "gorilla_huggingface_tool_289",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_358",
        "query": "I work for a Japanese company, and my manager needs me to take a look at a request from a client. I can understand fluent Japanese, but I need a little help filling in missing words from the text.",
        "instruction": "Given a `text completion` task, retrieve tools that assist in completing or filling in missing words in Japanese text by leveraging pre-trained language models to process the input text and provide contextually appropriate suggestions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "fill_mask('[MASK]')",
                    "performance": {
                        "dataset": "wikipedia",
                        "accuracy": "N/A"
                    },
                    "description": "This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.",
                    "id": "huggingface_api_684",
                    "name": "cl-tohoku/bert-base-japanese"
                },
                "id": "gorilla_huggingface_tool_679",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_359",
        "query": "One of our clients is facing noise issues on their audio recordings. Can you help them to remove the noise from the audio?",
        "instruction": "Given an `audio enhancement` task, retrieve tools that handle audio inputs by reducing or removing noise from recordings to enhance the sound quality, aligning with the query's needs and specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "speech-enhancement",
                    "api_call": "SpectralMaskEnhancement.from_hparams(source='speechbrain/metricgan-plus-voicebank', savedir='pretrained_models/metricgan-plus-voicebank')",
                    "api_arguments": {
                        "source": "speechbrain/metricgan-plus-voicebank",
                        "savedir": "pretrained_models/metricgan-plus-voicebank"
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "import torch\nimport torchaudio\nfrom speechbrain.pretrained import SpectralMaskEnhancement\nenhance_model = SpectralMaskEnhancement.from_hparams(\n source='speechbrain/metricgan-plus-voicebank',\n savedir='pretrained_models/metricgan-plus-voicebank',\n)\nnoisy = enhance_model.load_audio(\n 'speechbrain/metricgan-plus-voicebank/example.wav'\n).unsqueeze(0)\nenhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\ntorchaudio.save('enhanced.wav', enhanced.cpu(), 16000)",
                    "performance": {
                        "dataset": "Voicebank",
                        "accuracy": {
                            "Test PESQ": "3.15",
                            "Test STOI": "93.0"
                        }
                    },
                    "description": "MetricGAN-trained model for Enhancement",
                    "id": "huggingface_api_778",
                    "name": "speechbrain/metricgan-plus-voicebank"
                },
                "id": "gorilla_huggingface_tool_773",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_360",
        "query": "In our language app, we want to include text to speech functionality for Hokkien, a dialect of Chinese, using the TAT-TTS dataset.",
        "instruction": "Given a `text-to-speech` task, retrieve tools that enable the conversion of text into spoken language for dialects like Hokkien by utilizing specified datasets such as TAT-TTS to ensure accurate pronunciation and delivery according to the app's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TT')",
                    "api_arguments": {
                        "unit": "Text input for the TTS model"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "huggingface_hub",
                        "torchaudio"
                    ],
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": "TAT-TTS",
                        "accuracy": "Not provided"
                    },
                    "description": "Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.",
                    "id": "huggingface_api_740",
                    "name": "unit_hifigan_HK_layer12.km2500_frame_TAT-TTS"
                },
                "id": "gorilla_huggingface_tool_735",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_361",
        "query": "Can you help me find the best model to fill in the gap in my legal document? I want a smaller model with higher efficiency but maintains a high level of accuracy.",
        "instruction": "Given a `gap-filling in legal documents` task, retrieve tools that specialize in Natural Language Processing, particularly fill-mask functionalities, to process legal text inputs and offer smaller, efficient models that maintain high accuracy while filling in missing information.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModel"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')",
                    "performance": {
                        "dataset": "Legal Corpora",
                        "accuracy": "Comparable to larger models"
                    },
                    "description": "LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.",
                    "id": "huggingface_api_685",
                    "name": "nlpaueb/legal-bert-small-uncased"
                },
                "id": "gorilla_huggingface_tool_680",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_362",
        "query": "We are developing an interactive story app that would display a picture of each character as mentioned in the story given to our GPT-3 model.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that convert narrative descriptions into visual representations by processing text prompts to produce images that match characters or scenes from the story provided to the model.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0')",
                    "api_arguments": [
                        "prompt",
                        "negative_prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "torch"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-anime-1.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.",
                    "id": "huggingface_api_51",
                    "name": "dreamlike-art/dreamlike-anime-1.0"
                },
                "id": "gorilla_huggingface_tool_51",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_363",
        "query": "We want to implement a chatbot on our website to provide quick responses to customer inquiries.",
        "instruction": "Given a `chatbot implementation` task, retrieve tools that facilitate the deployment of conversational AI on websites by generating dialogue responses based on user inquiries, ensuring quick and relevant interactions aligned with the functionality requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face",
                    "functionality": "Dialogue Response Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')",
                    "api_arguments": [
                        "bot_input_ids",
                        "max_length",
                        "pad_token_id"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\nfor step in range(5):\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))",
                    "performance": {
                        "dataset": "Reddit discussion thread",
                        "accuracy": "Comparable to human response quality under a single-turn conversation Turing test"
                    },
                    "description": "DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.",
                    "id": "huggingface_api_578",
                    "name": "microsoft/DialoGPT-small"
                },
                "id": "gorilla_huggingface_tool_574",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_364",
        "query": "We are working on creating an audiobook. Convert this text: \"The sun was shining brightly, and the birds were singing sweetly\" into speech.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that process textual inputs to generate synthesized speech outputs, transforming written content into an audio format suitable for audiobooks and similar applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "SpeechBrain",
                    "functionality": "Text-to-Speech",
                    "api_call": "Tacotron2.from_hparams(source='speechbrain/tts-tacotron2-ljspeech')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "speechbrain"
                    ],
                    "example_code": [
                        "import torchaudio",
                        "from speechbrain.pretrained import Tacotron2",
                        "from speechbrain.pretrained import HIFIGAN",
                        "tacotron2 = Tacotron2.from_hparams(source=speechbrain/tts-tacotron2-ljspeech, savedir=tmpdir_tts)",
                        "hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)",
                        "mel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)",
                        "waveforms = hifi_gan.decode_batch(mel_output)",
                        "torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)"
                    ],
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": "Not specified"
                    },
                    "description": "This repository provides all the necessary tools for Text-to-Speech (TTS) with SpeechBrain using a Tacotron2 pretrained on LJSpeech. The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.",
                    "id": "huggingface_api_723",
                    "name": "speechbrain/tts-tacotron2-ljspeech"
                },
                "id": "gorilla_huggingface_tool_718",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_365",
        "query": "We are a team of architects and civil engineers looking to estimate the depth of elements in architectural designs from 2D images of the structures. We want to implement a depth estimation model that will transform these images into depictions of depth.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to process 2D images and transform them into depth representations, meeting the query's architectural and engineering requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Depth Estimation",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3736,
                            "Mae": 0.3079,
                            "Rmse": 0.4321,
                            "Abs Rel": 0.3666,
                            "Log Mae": 0.1288,
                            "Log Rmse": 0.1794,
                            "Delta1": 0.4929,
                            "Delta2": 0.7934,
                            "Delta3": 0.9234
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_162",
                    "name": "glpn-nyu-finetuned-diode-221116-104421"
                },
                "id": "gorilla_huggingface_tool_158",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_366",
        "query": "The company develops a digital assistant that can answer questions about software products. Implement a feature to provide answers to user questions.",
        "instruction": "Given a `question answering` task, retrieve tools that are capable of processing text inputs to provide relevant and accurate answers to user questions regarding software products, leveraging models fine-tuned on datasets like SQuAD2.0 for optimized answering capability.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_367",
        "query": "We need to analyze customer reviews and find out how well our new product is doing in the market.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze text inputs, particularly customer reviews, to evaluate the sentiment and determine the product's market performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "result = sentiment_pipeline('I love this product!')",
                    "performance": {
                        "dataset": [
                            {
                                "language": "English",
                                "accuracy": {
                                    "exact": "67%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Dutch",
                                "accuracy": {
                                    "exact": "57%",
                                    "off-by-1": "93%"
                                }
                            },
                            {
                                "language": "German",
                                "accuracy": {
                                    "exact": "61%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "French",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "Italian",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Spanish",
                                "accuracy": {
                                    "exact": "58%",
                                    "off-by-1": "95%"
                                }
                            }
                        ]
                    },
                    "description": "This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
                    "id": "huggingface_api_914",
                    "name": "bert-base-multilingual-uncased-sentiment"
                },
                "id": "gorilla_huggingface_tool_379",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_368",
        "query": "A photographer wants to create artistic interpretations of some of her pictures. Let's help her to get variated styles of her image.",
        "instruction": "Given an `artistic image transformation` task, retrieve tools that process input images to create artistic variations by applying different styles or representations, aligned with the task's creative requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image-to-Image",
                    "api_call": "pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')",
                    "api_arguments": {
                        "input_image": "path/to/image/file"
                    },
                    "python_environment_requirements": {
                        "huggingface_hub": ">=0.0.17",
                        "transformers": ">=4.13.0",
                        "torch": ">=1.10.0"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "poloclub/diffusiondb",
                        "accuracy": "Not provided"
                    },
                    "description": "SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.",
                    "id": "huggingface_api_282",
                    "name": "GreeneryScenery/SheepsControlV5"
                },
                "id": "gorilla_huggingface_tool_278",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_369",
        "query": "We are a company that wants to expand globally. We need to translate our website content from English to Italian.",
        "instruction": "Given a `content translation` task, retrieve tools that facilitate language conversion by processing English text inputs into Italian, ensuring the translated output maintains accuracy and context as per the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')",
                    "api_arguments": {
                        "source_language": "en",
                        "target_language": "it"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; translator = pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it'); translator('Hello, world!')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "newssyscomb2009.en.it": {
                                "BLEU": 30.9,
                                "chr-F": 0.606
                            },
                            "newstest2009.en.it": {
                                "BLEU": 31.9,
                                "chr-F": 0.604
                            },
                            "Tatoeba.en.it": {
                                "BLEU": 48.2,
                                "chr-F": 0.695
                            }
                        }
                    },
                    "description": "A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library.",
                    "id": "huggingface_api_540",
                    "name": "Helsinki-NLP/opus-mt-en-it"
                },
                "id": "gorilla_huggingface_tool_536",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_370",
        "query": "Design an efficient search engine that retrieves most relevant articles based on a pool of long text documents.",
        "instruction": "Given a `search engine design` task, retrieve tools that enable the development of search engines capable of processing long text documents to retrieve the most relevant articles based on search queries, leveraging models trained for generating or matching queries to appropriate content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')",
                    "api_arguments": "text, max_length",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "MS MARCO",
                        "accuracy": "Not specified"
                    },
                    "description": "A T5 model trained on the MS MARCO dataset for generating queries from documents.",
                    "id": "huggingface_api_648",
                    "name": "castorini/doc2query-t5-base-msmarco"
                },
                "id": "gorilla_huggingface_tool_643",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_371",
        "query": "A Dutch friend asked for help in completing a sentence with a missing word. Can you fill in the blank?",
        "instruction": "Given a `sentence completion` task, retrieve tools that perform fill-in-the-blank operations by processing linguistic inputs using language models specifically trained in Dutch, accurately filling the missing words in the context provided.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('GroNLP/bert-base-dutch-cased')",
                    "api_arguments": [
                        "pretrained_model_name_or_path"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel, TFAutoModel\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "CoNLL-2002",
                                "accuracy": "90.24"
                            },
                            {
                                "name": "SoNaR-1",
                                "accuracy": "84.93"
                            },
                            {
                                "name": "spaCy UD LassySmall",
                                "accuracy": "86.10"
                            }
                        ]
                    },
                    "description": "BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.",
                    "id": "huggingface_api_680",
                    "name": "GroNLP/bert-base-dutch-cased"
                },
                "id": "gorilla_huggingface_tool_675",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_372",
        "query": "Our user would like to develop an audiobook using a Text-to-Speech API. Convert the text of a book into an audio file.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform textual content into audio outputs by utilizing models designed for synthesizing speech in various languages from text inputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')",
                    "api_arguments": null,
                    "python_environment_requirements": "huggingface_hub, fairseq",
                    "example_code": null,
                    "performance": {
                        "dataset": "covost2",
                        "accuracy": null
                    },
                    "description": "A text-to-speech model trained on multiple datasets including mtedx, covost2, europarl_st, and voxpopuli. Supports English, Spanish, French, and Italian languages.",
                    "id": "huggingface_api_728",
                    "name": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10"
                },
                "id": "gorilla_huggingface_tool_723",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_373",
        "query": "Develop an application to transcribe audio files with punctuation marks for a podcast platform.",
        "instruction": "Given a `punctuated transcription` task, retrieve tools that transcribe audio files into text while incorporating punctuation marks, ensuring the output is suitable for applications like podcast platforms where clear prosody is essential.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "https://github.com/neonbjb/ocotillo",
                    "performance": {
                        "dataset": "librispeech validation set",
                        "accuracy": "4.45%"
                    },
                    "description": "This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.",
                    "id": "huggingface_api_748",
                    "name": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli"
                },
                "id": "gorilla_huggingface_tool_743",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_374",
        "query": "A real estate agency needs an application that can transform the floor plan images into simple straight line drawings, simplifying the visualization of the properties.",
        "instruction": "Given an `image transformation` task, retrieve tools that specialize in processing image inputs to convert detailed floor plan images into simplified straight line drawings, facilitating easier property visualization.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "ControlNet - M-LSD Straight Line Version",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": {
                        "import": [
                            "from PIL import Image",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "import torch",
                            "from controlnet_aux import MLSDdetector",
                            "from diffusers.utils import load_image"
                        ],
                        "setup": [
                            "mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)",
                            "image = mlsd(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
                        ],
                        "execution": [
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(room, image, num_inference_steps=20).images[0]",
                            "image.save('images/room_mlsd_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "600k edge-image, caption pairs generated from Places2",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_268",
                    "name": "lllyasviel/sd-controlnet-mlsd"
                },
                "id": "gorilla_huggingface_tool_264",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_375",
        "query": "Our client has a medical report and we are trying to assist him in finding relevant information.",
        "instruction": "Given a `medical report analysis` task, retrieve tools that assist in extracting relevant information from clinical documents by utilizing natural language processing techniques to enhance decision-making based on medical data.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "api_arguments": [
                        "AutoTokenizer",
                        "AutoModel",
                        "from_pretrained"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')",
                    "performance": {
                        "dataset": "MIMIC III",
                        "accuracy": "Not provided"
                    },
                    "description": "Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).",
                    "id": "huggingface_api_674",
                    "name": "emilyalsentzer/Bio_ClinicalBERT"
                },
                "id": "gorilla_huggingface_tool_669",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_376",
        "query": "We want to build a Twitter Bot that creates an image based on users' textual requests. Generate an image with an astronaut playing guitar in space using a model.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that transform textual prompts into visual outputs by leveraging models designed to create images based on descriptive text inputs, ensuring alignment with the query's creative requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')",
                    "api_arguments": {
                        "prompt": "photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens"
                    },
                    "python_environment_requirements": {
                        "torch": "torch.float16",
                        "diffusers": "StableDiffusionPipeline"
                    },
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\nimage = pipe(prompt).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "Stable Diffusion 1.5",
                        "accuracy": "Not specified"
                    },
                    "description": "Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.",
                    "id": "huggingface_api_39",
                    "name": "dreamlike-art/dreamlike-photoreal-2.0"
                },
                "id": "gorilla_huggingface_tool_39",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_377",
        "query": "A journalist is looking for historical Olympic host cities and wants to find the year when Beijing hosted the games.",
        "instruction": "Given a `historical data query` task, retrieve tools that perform table-based reasoning by processing a query for specific details from historical data tables, such as identifying years associated with specific events or locations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "pandas"
                        ]
                    },
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "arxiv:2107.07653",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.",
                    "id": "huggingface_api_443",
                    "name": "microsoft/tapex-base"
                },
                "id": "gorilla_huggingface_tool_439",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_378",
        "query": "A media company needs to summarize a news article in order to make it easy for their audience to understand the main points quickly.",
        "instruction": "Given a `text summarization` task, retrieve tools that process text inputs to generate concise summaries, ensuring that the main points are clearly articulated in a manner that is easily digestible by the target audience.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_379",
        "query": "A company developing an application for transcribing customer service calls requires a model that can understand spoken language.",
        "instruction": "Given an `audio transcription` task, retrieve tools that excel in understanding and transcribing spoken language by accurately processing audio inputs to produce textual outputs, suitable for applications like customer service call transcription, and ensure compatibility with relevant machine learning frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription and Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')",
                    "api_arguments": [
                        "sample",
                        "sampling_rate",
                        "language",
                        "task",
                        "skip_special_tokens"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": "from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "LibriSpeech (clean)",
                                "accuracy": 2.9
                            },
                            {
                                "name": "LibriSpeech (other)",
                                "accuracy": 5.9
                            },
                            {
                                "name": "Common Voice 11.0",
                                "accuracy": 53.87
                            }
                        ]
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.",
                    "id": "huggingface_api_767",
                    "name": "openai/whisper-medium"
                },
                "id": "gorilla_huggingface_tool_762",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_380",
        "query": "I want to create a visual representation based on a short description: \"A magical forest with unicorns and a rainbow.\".",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that produce visual representations based on textual descriptions by processing input prompts to generate coherent and visually appealing images aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Diffusion-based text-to-image generation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge')",
                    "api_arguments": {
                        "checkpoint": "lllyasviel/control_v11p_sd15_softedge",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux==0.3.0"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import PidiNetDetector, HEDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\ncontrol_image = processor(image, safe=True)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "ControlNet",
                        "accuracy": "Not provided"
                    },
                    "description": "Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.",
                    "id": "huggingface_api_276",
                    "name": "lllyasviel/control_v11p_sd15_softedge"
                },
                "id": "gorilla_huggingface_tool_272",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_381",
        "query": "We are working on a language learning app for Chinese. We need to give audio examples for each lesson.",
        "instruction": "Given an `audio example generation` task for a language learning app, retrieve tools that utilize Text-to-Speech capabilities to convert text lessons into audio examples by processing text inputs and producing accurate and clear speech outputs in the specified language.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "espnet_model_zoo"
                    ],
                    "example_code": "import soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\ntext = æ˜¥æ±Ÿæ½®æ°´è¿žæµ·å¹³ï¼Œæµ·ä¸Šæ˜Žæœˆå…±æ½®ç”Ÿ\nspeech = text2speech(text)[wav]\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)",
                    "performance": {
                        "dataset": "csmsc",
                        "accuracy": "Not specified"
                    },
                    "description": "A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.",
                    "id": "huggingface_api_737",
                    "name": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best"
                },
                "id": "gorilla_huggingface_tool_732",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_382",
        "query": "Create Japanese audio from the following text: \"ã“ã‚“ã«ã¡ã¯ã€ç§ãŸã¡ã¯ã‚ãªãŸã®åŠ©ã‘ãŒå¿…è¦ã§ã™ã€‚\"",
        "instruction": "Given a `text-to-speech` task, retrieve tools that convert text inputs into audio outputs in Japanese by leveraging models capable of generating natural-sounding speech aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.",
                    "id": "huggingface_api_741",
                    "name": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804"
                },
                "id": "gorilla_huggingface_tool_736",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_383",
        "query": "We need to identify the relationship between two sentences whether they are contradictory, entail each other, or neutral.",
        "instruction": "Given a `sentence relationship identification` task, retrieve tools that can evaluate and classify relationships between sentence pairs as contradictory, entailment, or neutral by processing textual inputs and returning relevant classification scores.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "CrossEncoder('cross-encoder/nli-deberta-v3-small')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.65",
                            "MNLI-mismatched": "87.55"
                        },
                        "accuracy": {
                            "SNLI-test": "91.65",
                            "MNLI-mismatched": "87.55"
                        }
                    },
                    "description": "Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_501",
                    "name": "cross-encoder/nli-deberta-v3-small"
                },
                "id": "gorilla_huggingface_tool_497",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_384",
        "query": "We have a large dataset of customer orders in the form of a table. Help us answer questions about this data.",
        "instruction": "Given a `table question answering` task, retrieve tools that analyze tabular data to answer questions based on the dataset by processing table inputs through advanced machine learning models tailored for question answering tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "pipeline('table-question-answering', model='google/tapas-small-finetuned-sqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.6155
                    },
                    "description": "TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_441",
                    "name": "google/tapas-small-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_437",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_385",
        "query": "A customer wants to convert an input Korean text into a summary. Provide a solution for it.",
        "instruction": "Given a `text summarization` task, retrieve tools that process Korean text inputs to generate concise summaries aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')",
                    "api_arguments": {
                        "tokenizer": "BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import BertTokenizerFast, EncoderDecoderModel\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)",
                    "performance": {
                        "dataset": "70GB Korean text dataset",
                        "accuracy": "42000 lower-cased subwords"
                    },
                    "description": "Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.",
                    "id": "huggingface_api_638",
                    "name": "kykim/bertshared-kor-base"
                },
                "id": "gorilla_huggingface_tool_633",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_386",
        "query": "We need an AI-generated insect image for our biology article about African habitats.",
        "instruction": "Given an `image generation` task, retrieve tools that are capable of generating AI images based on specified themes, such as creating visual representations of insects suited for a biology article, by leveraging pre-trained models and image synthesis techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')",
                    "api_arguments": {
                        "pretrained_model": "schdoel/sd-class-AFHQ-32"
                    },
                    "python_environment_requirements": {
                        "package": "diffusers",
                        "import": "from diffusers import DDPMPipeline"
                    },
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "AFHQ",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model for unconditional image generation of cute ðŸ¦‹.",
                    "id": "huggingface_api_315",
                    "name": "sd-class-pandas-32"
                },
                "id": "gorilla_huggingface_tool_311",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_387",
        "query": "We need to extract useful features from Russian text for downstream tasks.",
        "instruction": "Given a `feature extraction from text` task, retrieve tools that are designed to process text inputs, specifically in Russian, to extract useful features for downstream tasks, by utilizing models capable of analyzing language-specific nuances and generating structured outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Russian part of Wikipedia and news data",
                        "accuracy": ""
                    },
                    "description": "RuBERT (Russian, cased, 12â€‘layer, 768â€‘hidden, 12â€‘heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTâ€‘base as an initialization for RuBERT[1].",
                    "id": "huggingface_api_15",
                    "name": "DeepPavlov/rubert-base-cased"
                },
                "id": "gorilla_huggingface_tool_15",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_388",
        "query": "A Romanian-speaking person wants to communicate with an English-speaking friend over the phone using our platform. Please provide a method to translate their speech in real-time.",
        "instruction": "Given a `real-time speech translation` task, retrieve tools that support audio-to-audio translation by processing Romanian speech inputs and delivering English speech outputs, facilitating seamless communication between languages during phone conversations.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "pipeline('audio-to-audio', model='facebook/textless_sm_ro_en')",
                    "api_arguments": "audio file or recording",
                    "python_environment_requirements": "fairseq, huggingface_hub",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "A speech-to-speech translation model for Romanian to English developed by Facebook AI",
                    "id": "huggingface_api_801",
                    "name": "facebook/textless_sm_ro_en"
                },
                "id": "gorilla_huggingface_tool_796",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_389",
        "query": "We have the financial documents of a company and we want to extract information about the cash flow. Modify the model so it can answer the questions related to the cash flow.",
        "instruction": "Given a `financial document analysis` task, retrieve tools that specialize in processing company financial documents to extract and provide information related to specific topics such as cash flow by using question-answering models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Document Question Answering model based on LayoutXLM.",
                    "id": "huggingface_api_120",
                    "name": "CZ_DVQA_layoutxlm-base"
                },
                "id": "gorilla_huggingface_tool_119",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_390",
        "query": "We're creating a promotional image for a wildlife-themed event. We need to display two tigers in a natural setting.",
        "instruction": "Given an `image generation` task, retrieve tools that process text prompts to create or modify images in accordance with the specified theme and requirements of the promotional content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Generate and modify images based on text prompts",
                    "api_call": "StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')",
                    "api_arguments": {
                        "prompt": "Text prompt to generate image",
                        "image": "Initial image (optional)",
                        "negative_prompt": "Negative text prompt to avoid certain features",
                        "strength": "Strength of the prompt effect on the generated image"
                    },
                    "python_environment_requirements": [
                        "pip install -U git+https://github.com/huggingface/transformers.git",
                        "pip install diffusers transformers accelerate scipy safetensors"
                    ],
                    "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom diffusers import StableDiffusionDepth2ImgPipeline\n\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n stabilityai/stable-diffusion-2-depth,\n torch_dtype=torch.float16,\n).to(cuda)\n\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\ninit_image = Image.open(requests.get(url, stream=True).raw)\nprompt = two tigers\nn_propmt = bad, deformed, ugly, bad anotomy\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.",
                    "id": "huggingface_api_44",
                    "name": "stabilityai/stable-diffusion-2-depth"
                },
                "id": "gorilla_huggingface_tool_44",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_391",
        "query": "Create a machine learning-based image recognition tool that can identify whether an animal in an image is a cat or a dog.",
        "instruction": "Given an `image recognition` task, retrieve tools that facilitate machine learning-based image classification by processing image inputs and determining the presence of specific objects, such as distinguishing between cats and dogs, leveraging zero-shot classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')",
                    "api_arguments": {
                        "image_path": "Path to the image file",
                        "class_names": "List of comma-separated class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "75.9-76.9%"
                    },
                    "description": "A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.",
                    "id": "huggingface_api_366",
                    "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft"
                },
                "id": "gorilla_huggingface_tool_362",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_392",
        "query": "Create a system that translates and synthesizes speech from one language to another using the given model.",
        "instruction": "Given a `speech translation and synthesis` task, retrieve tools that facilitate translating and synthesizing speech between languages by leveraging models that process audio inputs to perform speech-to-speech translation, ensuring compatibility with specified platforms and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "Speech-to-speech translation",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_hk-en')",
                    "api_arguments": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text",
                        "cache_dir": "cache_dir"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "torchaudio",
                        "huggingface_hub"
                    ],
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/xm_transformer_unity_hk-en,\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\n cache_dir=cache_dir,\n)\nmodel = models[0].cpu()\ncfg[task].cpu = True\ngenerator = task.build_generator([model], cfg)\naudio, _ = torchaudio.load(/path/to/an/audio/file)\nsample = S2THubInterface.get_model_input(task, audio)\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": [
                            "TED",
                            "drama",
                            "TAT"
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "A speech-to-speech translation model with two-pass decoder (UnitY) trained on Hokkien-English data from TED, drama, and TAT domains. It uses Facebook's Unit HiFiGAN for speech synthesis.",
                    "id": "huggingface_api_789",
                    "name": "xm_transformer_unity_hk-en"
                },
                "id": "gorilla_huggingface_tool_784",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_393",
        "query": "We are developing a virtual assistant and need to integrate a speech enhancement feature in it.",
        "instruction": "Given a `speech enhancement feature integration` task, retrieve tools that enhance audio quality by processing audio inputs to reduce noise and reverberation, utilizing pre-trained models or specific frameworks designed for optimizing speech clarity in virtual assistant applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source='speechbrain/sepformer-wham-enhancement')",
                    "api_arguments": [
                        "path"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source='speechbrain/sepformer-wham-enhancement', savedir='pretrained_models/sepformer-wham-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\ntorchaudio.save('enhanced_wham.wav', est_sources[:, :, 0].detach().cpu(), 8000)",
                    "performance": {
                        "dataset": "WHAM!",
                        "accuracy": "14.35 dB SI-SNR"
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_803",
                    "name": "sepformer-wham-enhancement"
                },
                "id": "gorilla_huggingface_tool_798",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_394",
        "query": "We have written a summary of a new book's plot. Now, we want to ensure if the summary contains conflicting information.",
        "instruction": "Given an `information consistency verification` task, retrieve tools that assess text consistency through natural language inference processes, identifying contradictions or confirmatory statements between text segments based on the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-MiniLM2-L6-H768')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-MiniLM2-L6-H768')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": "SNLI and MultiNLI",
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder for evaluation results"
                    },
                    "description": "This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_507",
                    "name": "cross-encoder/nli-MiniLM2-L6-H768"
                },
                "id": "gorilla_huggingface_tool_503",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_395",
        "query": "We need to develop a model to classify sports clips by identifying the type of sports being played in the video.",
        "instruction": "Given a `video classification` task, retrieve tools that process video inputs to classify and identify the type of sports being played within the clips, utilizing models pre-trained for recognizing specific actions or categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k400')",
                    "api_arguments": "video, return_tensors",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n  outputs = model(**inputs)\n  logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": "Not provided"
                    },
                    "description": "TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.",
                    "id": "huggingface_api_316",
                    "name": "facebook/timesformer-base-finetuned-k400"
                },
                "id": "gorilla_huggingface_tool_312",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_396",
        "query": "Now I need to create a summary of my chat with my friend last night.",
        "instruction": "Given a `summarization` task, retrieve tools that can process conversational text inputs to generate concise summaries, leveraging models trained on dialogue datasets to provide coherent abstracts of interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='lidiya/bart-large-xsum-samsum')",
                    "api_arguments": "conversation",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\nconversation = '''Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ðŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
                        "accuracy": {
                            "rouge1": 53.306,
                            "rouge2": 28.355,
                            "rougeL": 44.095
                        }
                    },
                    "description": "This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.",
                    "id": "huggingface_api_555",
                    "name": "lidiya/bart-large-xsum-samsum"
                },
                "id": "gorilla_huggingface_tool_551",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_397",
        "query": "I have a table with data about different types of bards and their magical abilities. Let me see if I can find the best bard given the data in the table.",
        "instruction": "Given a `table question answering` task, retrieve tools that utilize table data to answer queries by analyzing and processing the information contained within the table to identify the best option based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')",
                    "api_arguments": "tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\n\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.2854
                    },
                    "description": "TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.",
                    "id": "huggingface_api_453",
                    "name": "google/tapas-mini-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_449",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_398",
        "query": "Our company is working on an autonomous robot and needs a solution to estimate the depth of objects in its environment.",
        "instruction": "Given a `depth estimation` task, retrieve tools that assist in determining object depth in a robotic environment by leveraging computer vision models to process visual data inputs and produce depth-related outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')",
                    "api_arguments": "pretrained_model_name",
                    "python_environment_requirements": "transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3421,
                            "Mae": 0.27,
                            "Rmse": 0.4042,
                            "Abs Rel": 0.3279,
                            "Log Mae": 0.1132,
                            "Log Rmse": 0.1688,
                            "Delta1": 0.5839,
                            "Delta2": 0.8408,
                            "Delta3": 0.9309
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.",
                    "id": "huggingface_api_168",
                    "name": "glpn-nyu-finetuned-diode-221122-082237"
                },
                "id": "gorilla_huggingface_tool_164",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_399",
        "query": "We are trying to create a solution for an HR department to predict whether a candidate would be a potential employee based on a list of background information.",
        "instruction": "Given a `candidate prediction` task, retrieve tools that perform binary classification by processing tabular data of background information to predict a candidate's potential as an employee, ensuring accuracy and alignment with the query's HR-focused requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Binary Classification",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas",
                        "json"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "rajistics/autotrain-data-Adult",
                        "accuracy": 0.8628221244500315
                    },
                    "description": "This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features.",
                    "id": "huggingface_api_853",
                    "name": "abhishek/autotrain-adult-census-xgboost"
                },
                "id": "gorilla_huggingface_tool_838",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_400",
        "query": "I am running a wine store, and I am looking for a machine learning model that can help me classify the quality of wine based on some given features.",
        "instruction": "Given a `wine quality classification` task, retrieve tools that leverage machine learning models to classify wine quality based on input features, ensuring alignment with the query's requirements for classification accuracy and model implementation.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'sklearn_model.joblib')))",
                    "api_arguments": "X",
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])",
                    "performance": {
                        "dataset": "winequality-red.csv",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_847",
                    "name": "osanseviero/wine-quality"
                },
                "id": "gorilla_huggingface_tool_841",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_401",
        "query": "We are building a platform for developers and want to provide automatic code documentation generation for Python functions.",
        "instruction": "Given a `code documentation generation` task, retrieve tools that automatically generate documentation for Python functions by analyzing code inputs to produce descriptive summaries that align with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "Code Documentation Generation",
                    "api_call": "AutoModelWithLMHead.from_pretrained('SEBIS/code_trans_t5_base_code_documentation_generation_python')",
                    "api_arguments": [
                        "tokenized_code"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\npipeline = SummarizationPipeline(\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\n device=0\n)\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\npipeline([tokenized_code])",
                    "performance": {
                        "dataset": "CodeSearchNet Corpus python dataset",
                        "accuracy": "20.26 BLEU score"
                    },
                    "description": "This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.",
                    "id": "huggingface_api_553",
                    "name": "code_trans_t5_base_code_documentation_generation_python"
                },
                "id": "gorilla_huggingface_tool_549",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_402",
        "query": "We want to enhance our search function by improving the ranking of search results.",
        "instruction": "Given a `search enhancement` task, retrieve tools that focus on improving the ranking of search results by utilizing models designed for information retrieval tasks, specifically those that leverage sequence classification to rerank query results effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')",
                    "api_arguments": {
                        "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "torch": "latest"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)",
                    "performance": {
                        "dataset": "MS Marco Passage Reranking",
                        "accuracy": "MRR@10: 39.01%"
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.",
                    "id": "huggingface_api_386",
                    "name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
                },
                "id": "gorilla_huggingface_tool_382",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_403",
        "query": "We are building a memory game where a description is displayed for a few seconds and later on, questions on what is shown comes up for the user to remember.",
        "instruction": "Given a `memory game design` task, retrieve tools that facilitate the creation of interactive memory games by processing game elements like descriptions and questions to enhance user engagement and learning.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nquestion_answerer = pipeline(question-answering, model='distilbert-base-uncased-distilled-squad')\ncontext = r\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\nprint(\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\n...)",
                    "performance": {
                        "dataset": "SQuAD v1.1",
                        "accuracy": "86.9 F1 score"
                    },
                    "description": "DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.",
                    "id": "huggingface_api_464",
                    "name": "distilbert-base-uncased-distilled-squad"
                },
                "id": "gorilla_huggingface_tool_460",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_404",
        "query": "We want to utilize the machine learning model for predicting molecular properties in the drug discovery domain.",
        "instruction": "Given a `molecular property prediction` task, retrieve tools designed for machine learning applications in the drug discovery domain by leveraging models that perform graph classification and representation to predict molecular properties accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Graph Machine Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('graphormer-base-pcqm4mv1')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "See the Graph Classification with Transformers tutorial",
                    "performance": {
                        "dataset": "PCQM4M-LSC",
                        "accuracy": "1st place on the KDD CUP 2021 (quantum prediction track)"
                    },
                    "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.",
                    "id": "huggingface_api_142",
                    "name": "graphormer-base-pcqm4mv1"
                },
                "id": "gorilla_huggingface_tool_138",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_405",
        "query": "The company's legal team is working on a case. They need a highly accurate tool to extract answers from a large set of legal documents. Develop a tool for this purpose.",
        "instruction": "Given a `legal document analysis` task, retrieve tools that specialize in natural language processing, specifically focusing on extracting precise answers from text by processing large sets of legal documents to meet the query's accuracy requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_406",
        "query": "A group of students are doing a project on European capitals. They need to know the capital city of Germany.",
        "instruction": "Given a `geographical information retrieval` task, retrieve tools capable of answering factual questions by processing queries related to geographical knowledge, such as identifying the capital city of a country.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='deepset/roberta-large-squad2')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "Not provided"
                    },
                    "description": "A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.",
                    "id": "huggingface_api_471",
                    "name": "deepset/roberta-large-squad2"
                },
                "id": "gorilla_huggingface_tool_467",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_407",
        "query": "I am a climate change agency, looking to have my research summaries translated into Chinese for international audiences.",
        "instruction": "Given a `translation task`, retrieve tools that specialize in Natural Language Processing, capable of translating text by processing input text and generating outputs in the desired language, aligning with the query's intent for international audience communication.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": [
                        "Translation",
                        "Summarization",
                        "Question Answering",
                        "Text Classification",
                        "Text Regression"
                    ],
                    "api_call": "T5Model.from_pretrained('t5-small')",
                    "api_arguments": {
                        "input_ids": "input tokenized text",
                        "decoder_input_ids": "input tokenized text for decoder"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5Model.from_pretrained('t5-small')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14 for full results"
                    },
                    "description": "T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks.",
                    "id": "huggingface_api_521",
                    "name": "t5-small"
                },
                "id": "gorilla_huggingface_tool_517",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_408",
        "query": "Our designer hired me to create an application to automatically translate colors from a design. Usually, our designer likes to work with English, to get the color in Italian.",
        "instruction": "Given a `color translation` task, retrieve tools that specialize in multilingual processing to translate textual color descriptions from one language to another, specifically converting English color names into Italian while ensuring accuracy and contextual relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text-to-Text Transfer Transformer",
                    "api_call": "MT5ForConditionalGeneration.from_pretrained('google/mt5-base')",
                    "api_arguments": [
                        "model_name",
                        "input_text",
                        "generated_text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\ntokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\ninputs = tokenizer.encode('translate English to German: The house is wonderful.', return_tensors='pt')\noutputs = model.generate(inputs, max_length=40, num_return_sequences=1)\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)",
                    "performance": {
                        "dataset": "mc4",
                        "accuracy": "Not provided"
                    },
                    "description": "mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks.",
                    "id": "huggingface_api_653",
                    "name": "google/mt5-base"
                },
                "id": "gorilla_huggingface_tool_648",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_409",
        "query": "Our company is developing a game, and they want to include an AI agent to play the game. We need to evaluate its performance on the CartPole-v1 environment for consistency.",
        "instruction": "Given a `performance evaluation` task, retrieve tools that facilitate assessing an AI agent's performance in a specific gaming environment, such as CartPole-v1, by leveraging reinforcement learning frameworks to ensure consistency and accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/dqn-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "logs"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python train.py --algo dqn --env CartPole-v1 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_903",
                    "name": "dqn-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_896",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_410",
        "query": "Our company's goal is to predict carbon emissions based on the given features of the compound.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that can utilize tabular regression models to process compound features and predict carbon emissions accurately, emphasizing the model's ability to handle structured data inputs and deliver quantified emission outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "kochetkovIT/autotrain-data-ironhack",
                        "accuracy": {
                            "Loss": 2.603,
                            "R2": 0.013,
                            "MSE": 6.776,
                            "MAE": 1.666,
                            "RMSLE": 0.502
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.",
                    "id": "huggingface_api_870",
                    "name": "kochetkovIT/autotrain-ironhack-49741119788"
                },
                "id": "gorilla_huggingface_tool_863",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_411",
        "query": "We have an international audience for our website and require our website content to be translated into multiple languages for better understanding.",
        "instruction": "Given a `website translation` task, retrieve tools that perform multilingual translation by processing text inputs to produce translations in multiple languages for enhanced understanding across an international audience.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "PyTorch Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')",
                    "api_arguments": [
                        "model",
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')",
                    "performance": {
                        "dataset": "Flores-200",
                        "accuracy": "BLEU, spBLEU, chrF++"
                    },
                    "description": "NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.",
                    "id": "huggingface_api_531",
                    "name": "facebook/nllb-200-distilled-600M"
                },
                "id": "gorilla_huggingface_tool_527",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_412",
        "query": "We are an animal rescue organization, and we are looking for a way to automatically identify if images uploaded to our site contain cats, dogs, or birds.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize machine learning models to analyze images and identify specific categories such as cats, dogs, or birds, based on the content of uploaded images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')",
                    "api_arguments": "image_path, class_names",
                    "python_environment_requirements": "transformers",
                    "example_code": "results = model(image_path, class_names='cat, dog, bird')",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "76.9"
                    },
                    "description": "A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).",
                    "id": "huggingface_api_354",
                    "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"
                },
                "id": "gorilla_huggingface_tool_350",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_413",
        "query": "Use this API to get a suggestion on how to respond to a customer's complaint about the late delivery of their package.",
        "instruction": "Given a `customer response generation` task, retrieve tools capable of generating conversational responses to customer complaints by leveraging external knowledge and contextual dialogue processing to craft appropriate and helpful replies.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')",
                    "api_arguments": [
                        "instruction",
                        "knowledge",
                        "dialog"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output",
                    "performance": {
                        "dataset": "Reddit discussion thread, instruction and knowledge grounded dialogs",
                        "accuracy": "N/A"
                    },
                    "description": "GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.",
                    "id": "huggingface_api_593",
                    "name": "microsoft/GODEL-v1_1-base-seq2seq"
                },
                "id": "gorilla_huggingface_tool_588",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_414",
        "query": "They are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.",
        "instruction": "Given a `language translation` task, retrieve tools that can generate translations of text inputs from one language to another by processing the input text and providing the output in the desired language.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Language model",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')",
                    "api_arguments": [
                        "input_text",
                        "input_ids",
                        "outputs"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\ninput_text = translate English to German: How old are you?\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MMLU",
                                "accuracy": "75.2%"
                            }
                        ]
                    },
                    "description": "FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.",
                    "id": "huggingface_api_634",
                    "name": "google/flan-t5-large"
                },
                "id": "gorilla_huggingface_tool_629",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_415",
        "query": "An editor wants to summarize his article in French.",
        "instruction": "Given a `text summarization` task, retrieve tools capable of performing abstractive summarization by processing text inputs to produce concise output, specifically in the French language, while maintaining the essence of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Abstractive Text Summarization",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('plguillou/t5-base-fr-sum-cnndm')",
                    "api_arguments": {
                        "input_text": "summarize: ARTICLE"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import T5Tokenizer, T5ForConditionalGeneration"
                    },
                    "example_code": "tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)",
                    "performance": {
                        "dataset": "cnn_dailymail",
                        "ROUGE-1": 44.5252,
                        "ROUGE-2": 22.652,
                        "ROUGE-L": 29.8866
                    },
                    "description": "This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.",
                    "id": "huggingface_api_563",
                    "name": "plguillou/t5-base-fr-sum-cnndm"
                },
                "id": "gorilla_huggingface_tool_559",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_416",
        "query": "Detect if there are any harmful messages in a chat room.",
        "instruction": "Given a `text analysis` task, retrieve tools that utilize natural language processing to detect and classify potentially harmful or toxic messages within text data by processing the content and returning relevant classifications or insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_417",
        "query": "We are a consulting firm, and we want to easily identify company names from texts.",
        "instruction": "Given a `company name extraction` task, retrieve tools that perform entity extraction by processing text inputs to identify and extract company names from the content accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification, AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-company_all",
                        "accuracy": 0.9979930566588805
                    },
                    "description": "A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.",
                    "id": "huggingface_api_414",
                    "name": "903429548"
                },
                "id": "gorilla_huggingface_tool_410",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_418",
        "query": "Extract the named entities from a given text snippet.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and classify entities within the text, based on categories such as names, dates, and locations, following the specifications of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\ntagger.predict(sentence)\nprint(sentence)\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "89.27"
                    },
                    "description": "This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_417",
                    "name": "flair/ner-english-ontonotes"
                },
                "id": "gorilla_huggingface_tool_413",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_419",
        "query": "I am a real-estate agent working on a project where I need to convert images of room plans to a better visual representation.",
        "instruction": "Given an `image enhancement` task, retrieve tools that process visual inputs to transform and enhance image representations, particularly focusing on room plans to produce improved visual outputs with specific enhancement techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image-to-Image",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "opencv": "pip install opencv-contrib-python",
                        "diffusers": "pip install diffusers transformers accelerate"
                    },
                    "example_code": "import cv2\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nimport numpy as np\nfrom diffusers.utils import load_image\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\nimage = np.array(image)\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(bird, image, num_inference_steps=20).images[0]\nimage.save('images/bird_canny_out.png')",
                    "performance": {
                        "dataset": "3M edge-image, caption pairs",
                        "accuracy": "600 GPU-hours with Nvidia A100 80G"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_261",
                    "name": "lllyasviel/sd-controlnet-canny"
                },
                "id": "gorilla_huggingface_tool_257",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_420",
        "query": "We are building a road maintenance reporting application. We need to use images to identify and segment road potholes.",
        "instruction": "Given a `road maintenance image processing` task, retrieve tools that perform image segmentation on potholes by processing image inputs to identify and segment the specific features relevant to road maintenance applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "YOLO('keremberke/yolov8m-pothole-segmentation')",
                    "api_arguments": {
                        "image": "URL or local image path"
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.23",
                        "ultralytics==8.0.21"
                    ],
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nprint(results[0].masks)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "pothole-segmentation",
                        "accuracy": {
                            "mAP@0.5(box)": 0.858,
                            "mAP@0.5(mask)": 0.895
                        }
                    },
                    "description": "A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.",
                    "id": "huggingface_api_254",
                    "name": "keremberke/yolov8m-pothole-segmentation"
                },
                "id": "gorilla_huggingface_tool_250",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_421",
        "query": "I have a dataset with CO2 emissions in a CSV file, and I want to classify which sources have high or low emissions.",
        "instruction": "Given a `data classification` task, retrieve tools that use tabular data processing to classify dataset entries, such as determining high or low CO2 emissions, by utilizing machine learning models and frameworks like Hugging Face Transformers.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Tabular Classification",
                    "api_call": "AutoModel.from_pretrained('datadmg/autotrain-test-news-44534112235')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "datadmg/autotrain-data-test-news",
                        "accuracy": 0.333
                    },
                    "description": "This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.",
                    "id": "huggingface_api_857",
                    "name": "datadmg/autotrain-test-news-44534112235"
                },
                "id": "gorilla_huggingface_tool_850",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_422",
        "query": "We're creating a chatbot that will detect the user's emotion. I want to start by implementing the basic functionality of emotion detection from the user's responses.",
        "instruction": "Given an `emotion detection` task, retrieve tools that leverage natural language processing to analyze text inputs and identify emotions or sentiments based on user responses, facilitating the development of chatbots with emotion recognition capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "classifier(I love this!)",
                    "performance": {
                        "dataset": [
                            "Crowdflower (2016)",
                            "Emotion Dataset, Elvis et al. (2018)",
                            "GoEmotions, Demszky et al. (2020)",
                            "ISEAR, Vikash (2018)",
                            "MELD, Poria et al. (2019)",
                            "SemEval-2018, EI-reg, Mohammad et al. (2018)",
                            "Emotion Lines (Friends)"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.",
                    "id": "huggingface_api_400",
                    "name": "michellejieli/emotion_text_classifier"
                },
                "id": "gorilla_huggingface_tool_396",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_423",
        "query": "We need a classifier that can sort movie synopses in German into three categories: crime, tragedy, and theft.",
        "instruction": "Given a `text classification` task, retrieve tools that perform language processing to sort textual data, specifically movie synopses, into specified categories using zero-shot classification techniques. This involves analyzing language content and aligning it with predefined labels even without direct training on the specific task.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "api_arguments": {
                        "sequence": "string",
                        "candidate_labels": "list of strings",
                        "hypothesis_template": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\ncandidate_labels = [Verbrechen,TragÃ¶die,Stehlen]\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "performance": {
                        "dataset": {
                            "XNLI DEV (german)": {
                                "accuracy": 85.5
                            },
                            "XNLI TEST (german)": {
                                "accuracy": 83.6
                            }
                        }
                    },
                    "description": "This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.",
                    "id": "huggingface_api_514",
                    "name": "AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')"
                },
                "id": "gorilla_huggingface_tool_510",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_424",
        "query": "The development team is trying to create a function in Python to print \"Hello, World!\" but they're not sure how to proceed. Generate this function for them.",
        "instruction": "Given a `function generation` task, retrieve tools that assist in the creation of simple programming functions, particularly in Python, by generating code snippets aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('bigcode/santacoder', trust_remote_code=True)",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = bigcode/santacoder\ndevice = cuda # for GPU usage or cpu for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\ninputs = tokenizer.encode(def print_hello_world():, return_tensors=pt).to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))",
                    "performance": {
                        "dataset": "bigcode/the-stack",
                        "accuracy": {
                            "pass@1 on MultiPL HumanEval (Python)": 0.18,
                            "pass@10 on MultiPL HumanEval (Python)": 0.29,
                            "pass@100 on MultiPL HumanEval (Python)": 0.49,
                            "pass@1 on MultiPL MBPP (Python)": 0.35,
                            "pass@10 on MultiPL MBPP (Python)": 0.58,
                            "pass@100 on MultiPL MBPP (Python)": 0.77,
                            "pass@1 on MultiPL HumanEval (JavaScript)": 0.16,
                            "pass@10 on MultiPL HumanEval (JavaScript)": 0.27,
                            "pass@100 on MultiPL HumanEval (JavaScript)": 0.47,
                            "pass@1 on MultiPL MBPP (Javascript)": 0.28,
                            "pass@10 on MultiPL MBPP (Javascript)": 0.51,
                            "pass@100 on MultiPL MBPP (Javascript)": 0.7,
                            "pass@1 on MultiPL HumanEval (Java)": 0.15,
                            "pass@10 on MultiPL HumanEval (Java)": 0.26,
                            "pass@100 on MultiPL HumanEval (Java)": 0.41,
                            "pass@1 on MultiPL MBPP (Java)": 0.28,
                            "pass@10 on MultiPL MBPP (Java)": 0.44,
                            "pass@100 on MultiPL MBPP (Java)": 0.59,
                            "single_line on HumanEval FIM (Python)": 0.44,
                            "single_line on MultiPL HumanEval FIM (Java)": 0.62,
                            "single_line on MultiPL HumanEval FIM (JavaScript)": 0.6,
                            "BLEU on CodeXGLUE code-to-text (Python)": 18.13
                        }
                    },
                    "description": "The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). The main model uses Multi Query Attention, was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective. In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations.",
                    "id": "huggingface_api_610",
                    "name": "bigcode/santacoder"
                },
                "id": "gorilla_huggingface_tool_605",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_425",
        "query": "Create a program to generate a description for an image provided as input.",
        "instruction": "Given an `image-to-text description generation` task, retrieve tools that process image inputs to generate descriptive textual outputs, detailing the contents of the image as required by the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('text-generation', model='microsoft/git-large-r-textcaps')",
                    "api_arguments": "image",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "TextCaps",
                        "accuracy": ""
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_85",
                    "name": "git-large-r-textcaps"
                },
                "id": "gorilla_huggingface_tool_85",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_426",
        "query": "Guide me to create a quiz project where I will summarize an article into a paragraph and from the summary I will develop a question with some multiple options. I need to check the correct answer for that question.",
        "instruction": "Given a `quiz and question generation` task, retrieve tools that support summarizing text content and developing questions with multiple choices by processing articles to enable effective content review and assess understanding through generated questions and verified answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-cased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_cased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\nprint(result)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BookCorpus",
                                "accuracy": "N/A"
                            },
                            {
                                "name": "English Wikipedia",
                                "accuracy": "N/A"
                            }
                        ]
                    },
                    "description": "BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.",
                    "id": "huggingface_api_486",
                    "name": "bert-large-cased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_482",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_427",
        "query": "I am building a voice assistant for my mobile app. Give me an example of how I can implement this Text-to-Speech model.",
        "instruction": "Given a `text-to-speech implementation` task, retrieve tools that provide examples and guidance to implement a Text-to-Speech model for voice assistant applications by offering necessary resources and instructions to effectively utilize specific TTS frameworks and models.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus",
                    "api_arguments": {
                        "model_name": "mio/amadeus"
                    },
                    "python_environment_requirements": {
                        "espnet": "d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f",
                        "transformers": "latest"
                    },
                    "example_code": "cd espnet\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\npip install -e .\ncd egs2/amadeus/tts1\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus",
                    "performance": {
                        "dataset": "amadeus",
                        "accuracy": "Not provided"
                    },
                    "description": "This model was trained by mio using amadeus recipe in espnet.",
                    "id": "huggingface_api_718",
                    "name": "mio/amadeus"
                },
                "id": "gorilla_huggingface_tool_713",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_428",
        "query": "I run a call center and I need a system that can identify the person on the other end of the line by analyzing their voice.",
        "instruction": "Given a `voice identification` task, retrieve tools that analyze audio inputs to identify and verify speakers by utilizing models specifically trained for speaker recognition tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForAudioXVector.from_pretrained('anton-l/wav2vec2-base-superb-sv')",
                    "api_arguments": "anton-l/wav2vec2-base-superb-sv",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import AutoProcessor, AutoModelForAudioXVector\nprocessor = AutoProcessor.from_pretrained(anton-l/wav2vec2-base-superb-sv)\nmodel = AutoModelForAudioXVector.from_pretrained(anton-l/wav2vec2-base-superb-sv)",
                    "performance": {
                        "dataset": "superb",
                        "accuracy": "More information needed"
                    },
                    "description": "This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_827",
                    "name": "wav2vec2-base-superb-sv"
                },
                "id": "gorilla_huggingface_tool_822",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_429",
        "query": "We have a large text dataset and want to extract some important features from it for our data analysis.",
        "instruction": "Given a `text feature extraction` task, retrieve tools that can process large text datasets to identify and extract important features, facilitating data analysis by leveraging natural language processing capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('YituTech/conv-bert-base')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.",
                    "id": "huggingface_api_0",
                    "name": "YituTech/conv-bert-base"
                },
                "id": "gorilla_huggingface_tool_0",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_430",
        "query": "Develop a system that can detect voices in a podcast to find out if guests are speaking or not.",
        "instruction": "Given a `voice detection and segmentation` task, retrieve tools that specialize in analyzing audio inputs to identify and segment different speakers and voice activities to determine when guests are speaking in a podcast.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker segmentation, Voice activity detection, Overlapped speech detection, Resegmentation, Raw scores",
                    "api_call": "Model.from_pretrained('pyannote/segmentation')",
                    "api_arguments": {
                        "use_auth_token": "ACCESS_TOKEN_GOES_HERE"
                    },
                    "python_environment_requirements": "pyannote.audio 2.1.1",
                    "example_code": {
                        "voice_activity_detection": "from pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n onset: 0.5, offset: 0.5,\n min_duration_on: 0.0,\n min_duration_off: 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(audio.wav)",
                        "overlapped_speech_detection": "from pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(audio.wav)",
                        "resegmentation": "from pyannote.audio.pipelines import Resegmentation\npipeline = Resegmentation(segmentation=model, diarization=baseline)\npipeline.instantiate(HYPER_PARAMETERS)\nresegmented_baseline = pipeline({audio: audio.wav, baseline: baseline})"
                    },
                    "performance": {
                        "dataset": {
                            "AMI Mix-Headset": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.684,
                                    "offset": 0.577,
                                    "min_duration_on": 0.181,
                                    "min_duration_off": 0.037
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.448,
                                    "offset": 0.362,
                                    "min_duration_on": 0.116,
                                    "min_duration_off": 0.187
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.542,
                                    "offset": 0.527,
                                    "min_duration_on": 0.044,
                                    "min_duration_off": 0.705
                                }
                            },
                            "DIHARD3": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.767,
                                    "offset": 0.377,
                                    "min_duration_on": 0.136,
                                    "min_duration_off": 0.067
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.43,
                                    "offset": 0.32,
                                    "min_duration_on": 0.091,
                                    "min_duration_off": 0.144
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.592,
                                    "offset": 0.489,
                                    "min_duration_on": 0.163,
                                    "min_duration_off": 0.182
                                }
                            },
                            "VoxConverse": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.767,
                                    "offset": 0.713,
                                    "min_duration_on": 0.182,
                                    "min_duration_off": 0.501
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.587,
                                    "offset": 0.426,
                                    "min_duration_on": 0.337,
                                    "min_duration_off": 0.112
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.537,
                                    "offset": 0.724,
                                    "min_duration_on": 0.41,
                                    "min_duration_off": 0.563
                                }
                            }
                        }
                    },
                    "description": "A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework.",
                    "id": "huggingface_api_844",
                    "name": "pyannote/segmentation"
                },
                "id": "gorilla_huggingface_tool_832",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_431",
        "query": "Our startup team is now building an app for diagnosing plant diseases based on images. We need to get the diagnosis for different types of plant issues.",
        "instruction": "Given an `image-based disease diagnosis` task, retrieve tools that employ zero-shot image classification to process images of plants and provide diagnostic insights into plant diseases without requiring additional training.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "clip.load('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')",
                    "api_arguments": "image, class_names",
                    "python_environment_requirements": "huggingface_hub, openai, transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.",
                    "id": "huggingface_api_363",
                    "name": "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k"
                },
                "id": "gorilla_huggingface_tool_359",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_432",
        "query": "A soccer simulation company wants to use a reinforcement learning agent that can play SoccerTwos effectively.",
        "instruction": "Given a `reinforcement learning agent deployment` task, retrieve tools that facilitate training and deploying reinforcement learning agents using Unity ML-Agents, specifically for playing games like SoccerTwos, by utilizing configuration files and managing the execution environment to ensure smooth functioning.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Unity ML-Agents",
                    "functionality": "Train and play SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "ml-agents"
                    ],
                    "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_900",
                    "name": "Raiden-1001/poca-Soccerv7.1"
                },
                "id": "gorilla_huggingface_tool_893",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_433",
        "query": "Create a program to determine the depth map from an input image of a street filled with people.",
        "instruction": "Given an `image processing` and `depth estimation` task, retrieve tools that utilize neural network structures to process input images and generate depth maps by estimating the relative distance of objects within the scene.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Depth Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "PIL",
                        "numpy",
                        "torch"
                    ],
                    "example_code": {
                        "install_packages": "pip install diffusers transformers accelerate",
                        "code": [
                            "from transformers import pipeline",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "from PIL import Image",
                            "import numpy as np",
                            "import torch",
                            "from diffusers.utils import load_image",
                            "depth_estimator = pipeline('depth-estimation')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)",
                            "image = depth_estimator(image)['depth']",
                            "image = np.array(image)",
                            "image = image[:, :, None]",
                            "image = np.concatenate([image, image, image], axis=2)",
                            "image = Image.fromarray(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]",
                            "image.save('./images/stormtrooper_depth_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "3M depth-image, caption pairs",
                        "accuracy": "500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_265",
                    "name": "lllyasviel/sd-controlnet-depth"
                },
                "id": "gorilla_huggingface_tool_261",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_434",
        "query": "We need to summarize a scientific article. The input must include all the important points discussed in the article, and the result should be a concise abstraction of the content.",
        "instruction": "Given a `summarization` task, retrieve tools that process extensive textual inputs to generate concise and coherent abstract summaries, ensuring the inclusion of critical points from the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-large')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline('summarization', model='google/pegasus-large')\nsummary = summarizer('your_text_here')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": "47.60/24.83/39.64"
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": "44.16/21.56/41.30"
                            },
                            {
                                "name": "newsroom",
                                "accuracy": "45.98/34.20/42.18"
                            },
                            {
                                "name": "multi_news",
                                "accuracy": "47.65/18.75/24.95"
                            },
                            {
                                "name": "gigaword",
                                "accuracy": "39.65/20.47/36.76"
                            },
                            {
                                "name": "wikihow",
                                "accuracy": "46.39/22.12/38.41"
                            },
                            {
                                "name": "reddit_tifu",
                                "accuracy": "27.99/9.81/22.94"
                            },
                            {
                                "name": "big_patent",
                                "accuracy": "52.29/33.08/41.66"
                            },
                            {
                                "name": "arxiv",
                                "accuracy": "44.21/16.95/25.67"
                            },
                            {
                                "name": "pubmed",
                                "accuracy": "45.97/20.15/28.25"
                            },
                            {
                                "name": "aeslc",
                                "accuracy": "37.68/21.25/36.51"
                            },
                            {
                                "name": "billsum",
                                "accuracy": "59.67/41.58/47.59"
                            }
                        ]
                    },
                    "description": "google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks.",
                    "id": "huggingface_api_558",
                    "name": "google/pegasus-large"
                },
                "id": "gorilla_huggingface_tool_554",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_435",
        "query": "We need a picture that has a nostalgic look in high quality for the cover of our upcoming magazine.",
        "instruction": "Given an `image generation` task, retrieve tools that create high-quality visual outputs with specific aesthetic attributes, such as a nostalgic look, to align with the query's requirements for use in a magazine cover.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Diffusers",
                    "api_call": "DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class",
                    "id": "huggingface_api_311",
                    "name": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs"
                },
                "id": "gorilla_huggingface_tool_307",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_436",
        "query": "I wish to edit my images by detecting straight lines and controlling the diffusion models in the image's diffusion process.",
        "instruction": "Given an `image editing` task focused on straight line detection and diffusion model control, retrieve tools that can process and modify images by detecting geometric features like straight lines and integrating these into the diffusion process to achieve the desired edits.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "ControlNet - M-LSD Straight Line Version",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": {
                        "import": [
                            "from PIL import Image",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "import torch",
                            "from controlnet_aux import MLSDdetector",
                            "from diffusers.utils import load_image"
                        ],
                        "setup": [
                            "mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)",
                            "image = mlsd(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
                        ],
                        "execution": [
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(room, image, num_inference_steps=20).images[0]",
                            "image.save('images/room_mlsd_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "600k edge-image, caption pairs generated from Places2",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_268",
                    "name": "lllyasviel/sd-controlnet-mlsd"
                },
                "id": "gorilla_huggingface_tool_264",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_437",
        "query": "Design an explanation on how to use Pix2Struct to analyze and generate text based on visuals such as graphs and charts.",
        "instruction": "Given a `model explanation` task, retrieve tools that provide detailed insights and methods for utilizing Pix2Struct in tasks involving visual data analysis, such as parsing charts and graphs, by outlining its functionalities, application procedures, and potential use cases.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')",
                    "api_arguments": [
                        "t5x_checkpoint_path",
                        "pytorch_dump_path",
                        "use-large"
                    ],
                    "python_environment_requirements": "transformers",
                    "example_code": "python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE",
                    "performance": {
                        "dataset": "ChartQA",
                        "accuracy": "Not provided"
                    },
                    "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.",
                    "id": "huggingface_api_78",
                    "name": "google/pix2struct-chartqa-base"
                },
                "id": "gorilla_huggingface_tool_78",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_438",
        "query": "Our delivery drones need to detect and avoid obstacles while flying. Develop a solution for them to detect objects in their path.",
        "instruction": "Given an `object detection` task, retrieve tools that implement computer vision techniques to analyze visual inputs and accurately identify and locate objects in an environment, aiding in the development of obstacle detection solutions for drone navigation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')",
                    "api_arguments": {
                        "model_name": "hustvl/yolos-small"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": {
                        "import": [
                            "from transformers import YolosFeatureExtractor, YolosForObjectDetection",
                            "from PIL import Image",
                            "import requests"
                        ],
                        "url": "http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image": "Image.open(requests.get(url, stream=True).raw)",
                        "feature_extractor": "YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')",
                        "model": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')",
                        "inputs": "feature_extractor(images=image, return_tensors='pt')",
                        "outputs": "model(**inputs)",
                        "logits": "outputs.logits",
                        "bboxes": "outputs.pred_boxes"
                    },
                    "performance": {
                        "dataset": "COCO 2017 validation",
                        "accuracy": "36.1 AP"
                    },
                    "description": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).",
                    "id": "huggingface_api_210",
                    "name": "hustvl/yolos-small"
                },
                "id": "gorilla_huggingface_tool_206",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_439",
        "query": "I want to estimate the price of a house based on its features using this API. Please provide the code.",
        "instruction": "Given a `house price estimation` task using features, retrieve tools that perform regression analysis by processing the input features and estimating property prices based on a trained model, emphasizing requirements for integration with APIs and consideration of necessary frameworks or libraries.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 100581.032,
                            "R2": 0.922,
                            "MSE": 10116543945.03,
                            "MAE": 81586.656,
                            "RMSLE": 0.101
                        }
                    },
                    "description": "A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.",
                    "id": "huggingface_api_868",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761513"
                },
                "id": "gorilla_huggingface_tool_861",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_440",
        "query": "We need to analyze some pictures from nature and classify them to protect some species of animals.",
        "instruction": "Given an `image classification` task, retrieve tools that analyze and classify images by processing visual data inputs to identify different species, aiding in wildlife protection efforts.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/mobilenet_v1_0.75_192"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.",
                    "id": "huggingface_api_186",
                    "name": "google/mobilenet_v1_0.75_192"
                },
                "id": "gorilla_huggingface_tool_182",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_441",
        "query": "Create a program to separate music and vocals from an audio file using a pretrained model.",
        "instruction": "Given an `audio source separation` task, retrieve tools that utilize pretrained models to process audio inputs, enabling the separation of music and vocals, and delivering distinct audio outputs aligned with the task's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')",
                    "api_arguments": "audio_file",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "WHAM!",
                        "si_sdr": 19.316743490695334,
                        "si_sdr_imp": 19.317895273889842,
                        "sdr": 19.68085347190952,
                        "sdr_imp": 19.5298092932871,
                        "sir": 30.362213998701232,
                        "sir_imp": 30.21116982007881,
                        "sar": 20.15553251343315,
                        "sar_imp": -129.02091762351188,
                        "stoi": 0.97772664309074,
                        "stoi_imp": 0.23968091518217424
                    },
                    "description": "This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.",
                    "id": "huggingface_api_780",
                    "name": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean"
                },
                "id": "gorilla_huggingface_tool_775",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_442",
        "query": "Our energy company is trying to identify anomalies in the energy consumption data. Could you perform anomaly detection on the time series data?",
        "instruction": "Given an `anomaly detection` task, retrieve tools designed to identify irregular patterns in time series data by leveraging advanced techniques such as reconstruction convolutional autoencoder models to analyze and detect deviations in ordered, timestamped metrics.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Keras",
                    "functionality": "anomaly-detection",
                    "api_call": "TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')",
                    "api_arguments": {
                        "optimizer": {
                            "name": "Adam",
                            "learning_rate": 0.001,
                            "decay": 0.0,
                            "beta_1": 0.9,
                            "beta_2": 0.999,
                            "epsilon": 1e-07,
                            "amsgrad": false
                        },
                        "training_precision": "float32"
                    },
                    "python_environment_requirements": [
                        "tensorflow",
                        "keras"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Numenta Anomaly Benchmark(NAB)",
                        "accuracy": {
                            "Train Loss": 0.006,
                            "Validation Loss": 0.008
                        }
                    },
                    "description": "This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.",
                    "id": "huggingface_api_869",
                    "name": "keras-io/timeseries-anomaly-detection"
                },
                "id": "gorilla_huggingface_tool_862",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_443",
        "query": "We have a set of pictures for pets (dogs and cats). We need to offer an AI-based solution to classify the pictures given the pet name.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize AI and computer vision models to classify pet images by processing image inputs and identifying the specified pet names from a list of potential class labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                    "api_arguments": {
                        "image_path": "path to the image file",
                        "labels": "list of possible class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "70.8 - 71.7%"
                    },
                    "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_357",
                    "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')"
                },
                "id": "gorilla_huggingface_tool_353",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_444",
        "query": "We are creating an AI assistant for banking clients. The customers should be able to talk to the bot to do various tasks. We need to have a meaningful dialogue with them.",
        "instruction": "Given a `conversational AI development` task, retrieve tools that enable natural language processing capabilities for generating meaningful dialogues, focusing on processing text-based inputs aligned with banking client interactions and returning contextually appropriate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/GODEL-v1_1-base-seq2seq')",
                    "api_arguments": [
                        "instruction",
                        "knowledge",
                        "dialog"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\ndef generate(instruction, knowledge, dialog):\n if knowledge != '':\n knowledge = '[KNOWLEDGE] ' + knowledge\n dialog = ' EOS '.join(dialog)\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n return output",
                    "performance": {
                        "dataset": "Reddit discussion thread, instruction and knowledge grounded dialogs",
                        "accuracy": "N/A"
                    },
                    "description": "GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.",
                    "id": "huggingface_api_593",
                    "name": "microsoft/GODEL-v1_1-base-seq2seq"
                },
                "id": "gorilla_huggingface_tool_588",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_445",
        "query": "Create a character-like artwork image based on the phrase 'anime-style girl with a guitar'.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that create character-like artwork based on textual descriptions by leveraging multimodal models capable of translating text prompts into detailed and stylistically appropriate images.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')",
                    "api_arguments": {
                        "model_id": "andite/anything-v4.0",
                        "torch_dtype": "torch.float16",
                        "device": "cuda",
                        "prompt": "hatsune_miku"
                    },
                    "python_environment_requirements": {
                        "diffusers": "StableDiffusionPipeline",
                        "torch": "torch"
                    },
                    "example_code": {
                        "from diffusers import StableDiffusionPipeline": "",
                        "import torch": "",
                        "model_id = andite/anything-v4.0": "",
                        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)": "",
                        "pipe = pipe.to(cuda)": "",
                        "prompt = hatsune_miku": "",
                        "image = pipe(prompt).images[0]": "",
                        "image.save(./hatsune_miku.png)": ""
                    },
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.",
                    "id": "huggingface_api_41",
                    "name": "andite/anything-v4.0"
                },
                "id": "gorilla_huggingface_tool_41",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_446",
        "query": "Develop a video content recommendation engine that can understand and generate multiple categories, such as sports, comedy, and news, based on the videos.",
        "instruction": "Given a `video recommendation` task, retrieve tools that can analyze video content to categorize and recommend based topics such as sports, comedy, and news by leveraging video classification capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')",
                    "api_arguments": "video",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Something-Something-v2",
                        "accuracy": {
                            "top-1": 70.6,
                            "top-5": 92.6
                        }
                    },
                    "description": "VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_325",
                    "name": "MCG-NJU/videomae-base-finetuned-ssv2"
                },
                "id": "gorilla_huggingface_tool_321",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_447",
        "query": "My lecture has been recorded, and I would like to transcribe the audio to create a transcript of my presentation.",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in automatic speech recognition by processing recorded audio inputs to generate an accurate and coherent textual transcript of the presentation.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Automatic Speech Recognition and Speech Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                    "api_arguments": {
                        "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')",
                        "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                        "model.config.forced_decoder_ids = None",
                        "ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')",
                        "sample = ds[0]['audio']",
                        "input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech test-clean",
                        "accuracy": 3.0003583080317573
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_757",
                    "name": "openai/whisper-large-v2"
                },
                "id": "gorilla_huggingface_tool_752",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_448",
        "query": "Design a chatbot for a school website, which can answer queries related to admissions, classes, teachers, and extracurriculars.",
        "instruction": "Given a `chatbot design` task, retrieve tools that specialize in conversational interfaces, which process textual inputs related to specific topics and provide informative and empathetic responses to user queries on a school website context.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face",
                    "functionality": "Conversational",
                    "api_call": "BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')",
                    "api_arguments": [
                        "message"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "Input a message to start chatting with facebook/blenderbot_small-90M.",
                    "performance": {
                        "dataset": "blended_skill_talk",
                        "accuracy": "Not provided"
                    },
                    "description": "Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.",
                    "id": "huggingface_api_573",
                    "name": "facebook/blenderbot_small-90M"
                },
                "id": "gorilla_huggingface_tool_569",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_449",
        "query": "I am designing an e-commerce website, and I want a program to generate images based on written descriptions to be used as mock product images.",
        "instruction": "Given an `image generation` task, retrieve tools that can create visual representations by processing textual descriptions to generate images suitable for use as product mockups in an e-commerce context.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))",
                    "api_arguments": {
                        "model": "CompVis/stable-diffusion-v1-4",
                        "vae": "AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)"
                    },
                    "python_environment_requirements": {
                        "diffusers": "diffusers library"
                    },
                    "example_code": "from diffusers.models import AutoencoderKL\nfrom diffusers import StableDiffusionPipeline\nmodel = CompVis/stable-diffusion-v1-4\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)",
                    "performance": {
                        "dataset": {
                            "COCO 2017 (256x256, val, 5000 images)": {
                                "accuracy": {
                                    "rFID": 4.42,
                                    "PSNR": "23.8 +/- 3.9",
                                    "SSIM": "0.69 +/- 0.13",
                                    "PSIM": "0.96 +/- 0.27"
                                }
                            },
                            "LAION-Aesthetics 5+ (256x256, subset, 10000 images)": {
                                "accuracy": {
                                    "rFID": 1.77,
                                    "PSNR": "26.7 +/- 4.8",
                                    "SSIM": "0.82 +/- 0.12",
                                    "PSIM": "0.67 +/- 0.34"
                                }
                            }
                        }
                    },
                    "description": "This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.",
                    "id": "huggingface_api_43",
                    "name": "stabilityai/sd-vae-ft-ema"
                },
                "id": "gorilla_huggingface_tool_43",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_450",
        "query": "We want to develop a chatbot that can engage with multilingual users. Please help us create a model to encode sentences and understand user input in various languages.",
        "instruction": "Given a `multilingual chatbot development` task, retrieve tools that encode and understand user input in multiple languages by leveraging models capable of extracting language-agnostic sentence embeddings, enabling the chatbot to effectively handle diverse linguistic interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BertModel.from_pretrained('setu4993/LaBSE')",
                    "api_arguments": [
                        "english_sentences",
                        "italian_sentences",
                        "japanese_sentences"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\nmodel = model.eval()\nenglish_sentences = [\n 'dog',\n 'Puppies are nice.',\n 'I enjoy taking long walks along the beach with my dog.',\n]\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\nwith torch.no_grad():\n english_outputs = model(**english_inputs)\nenglish_embeddings = english_outputs.pooler_output",
                    "performance": {
                        "dataset": "CommonCrawl and Wikipedia",
                        "accuracy": "Not Specified"
                    },
                    "description": "Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.",
                    "id": "huggingface_api_25",
                    "name": "setu4993/LaBSE"
                },
                "id": "gorilla_huggingface_tool_25",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_451",
        "query": "I'm working on a phonebot, and I need the bot to be able to read a sensitive warning message to the users.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that convert text inputs into audio outputs capable of reading messages aloud, emphasizing clarity and accuracy aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "Fairseq",
                    "functionality": "Text-to-Speech",
                    "api_call": "TTSHubInterface.get_prediction('facebook/fastspeech2-en-ljspeech')",
                    "api_arguments": {
                        "task": "task",
                        "model": "model",
                        "generator": "generator",
                        "sample": "sample"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "IPython"
                    ],
                    "example_code": "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/fastspeech2-en-ljspeech,\n arg_overrides={vocoder: hifigan, fp16: False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = Hello, this is a test run.\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": "N/A"
                    },
                    "description": "FastSpeech 2 text-to-speech model from fairseq S^2. English single-speaker female voice trained on LJSpeech.",
                    "id": "huggingface_api_720",
                    "name": "fastspeech2-en-ljspeech"
                },
                "id": "gorilla_huggingface_tool_715",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_452",
        "query": "We are an online platform that offers various Chinese language courses for students. Now we need to autoplay a fill-in-the-blank video game for our users.",
        "instruction": "Given a `video game automation` task, retrieve tools that facilitate the integration and execution of interactive multimedia elements in educational platforms, specifically those capable of autogenerating and managing fill-in-the-blank style content for user engagement.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('bert-base-chinese')",
                    "api_arguments": {
                        "pretrained_model_name": "bert-base-chinese"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForMaskedLM"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)",
                    "performance": {
                        "dataset": "[More Information Needed]",
                        "accuracy": "[More Information Needed]"
                    },
                    "description": "This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.",
                    "id": "huggingface_api_667",
                    "name": "bert-base-chinese"
                },
                "id": "gorilla_huggingface_tool_662",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_453",
        "query": "Let's create a smart agent that can learn to play CartPole-v0 using reinforcement learning.",
        "instruction": "Given a `reinforcement learning agent creation` task, retrieve tools that support the development and training of smart agents through reinforcement learning frameworks, specifically focusing on environments like 'CartPole-v0', by utilizing libraries and models designed for deep reinforcement learning tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/",
                    "performance": {
                        "dataset": "seals/CartPole-v0",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_891",
                    "name": "ppo-seals-CartPole-v0"
                },
                "id": "gorilla_huggingface_tool_884",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_454",
        "query": "Our PR team requires a machine learning approach to creative sentence generation for marketing content creation.",
        "instruction": "Given a `creative sentence generation` task, retrieve tools that utilize machine learning approaches to produce imaginative and contextually relevant sentences for marketing content, focusing on natural language processing techniques and accommodating domain-specific requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Generative Commonsense Reasoning",
                    "api_call": "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')",
                    "api_arguments": [
                        "words",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\ndef gen_sentence(words, max_length=32):\n input_text = words\n features = tokenizer([input_text], return_tensors='pt')\noutput = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'], max_length=max_length)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nwords = tree plant ground hole dig\ngen_sentence(words)",
                    "performance": {
                        "dataset": "common_gen",
                        "accuracy": {
                            "ROUGE-2": 17.1,
                            "ROUGE-L": 39.47
                        }
                    },
                    "description": "Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.",
                    "id": "huggingface_api_631",
                    "name": "mrm8488/t5-base-finetuned-common_gen"
                },
                "id": "gorilla_huggingface_tool_626",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_455",
        "query": "We are building a video analysis tool that can automatically detect the main action happening in a given video clip.",
        "instruction": "Given a `video action recognition` task, retrieve tools that focus on analyzing video content to identify and classify main actions occurring within video clips, utilizing pretrained models designed for accurate video classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Action Recognition",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "decord",
                        "huggingface_hub"
                    ],
                    "example_code": "from decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n converted_len = int(clip_len * frame_sample_rate)\n end_idx = np.random.randint(converted_len, seg_len)\n start_idx = end_idx - converted_len\n indices = np.linspace(start_idx, end_idx, num=clip_len)\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n return indices\nfile_path = hf_hub_download(\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\ninputs = feature_extractor(list(video), return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "UCF101",
                        "accuracy": 0.758209764957428
                    },
                    "description": "VideoMAE Base model fine tuned on UCF101 for Video Action Recognition",
                    "id": "huggingface_api_342",
                    "name": "videomae-base-finetuned-ucf101"
                },
                "id": "gorilla_huggingface_tool_338",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_456",
        "query": "Our multinational company is dealing with a French client. Please help us communicate in French by translating an English sentence into French.",
        "instruction": "Given a `translation` task, retrieve tools that enable translating text from English to French, ensuring the output message effectively facilitates communication with the French client by processing the input sentence and providing an accurate translation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment Ã§a va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_457",
        "query": "The marketing team needs different variations of a product image to use in advertising and promotional materials.",
        "instruction": "Given an `image variation generation` task, retrieve tools that utilize image processing techniques to create different variations of a provided product image for advertising and promotional purposes, ensuring they align with the team's specifications and creativity needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Variations",
                    "api_call": "StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')",
                    "api_arguments": {
                        "revision": "v2.0"
                    },
                    "python_environment_requirements": "Diffusers >=0.8.0",
                    "example_code": "from diffusers import StableDiffusionImageVariationPipeline\nfrom PIL import Image\ndevice = cuda:0\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n lambdalabs/sd-image-variations-diffusers,\n revision=v2.0,\n)\nsd_pipe = sd_pipe.to(device)\nim = Image.open(path/to/image.jpg)\ntform = transforms.Compose([\n transforms.ToTensor(),\n transforms.Resize(\n  (224, 224),\n  interpolation=transforms.InterpolationMode.BICUBIC,\n  antialias=False,\n ),\n transforms.Normalize(\n  [0.48145466, 0.4578275, 0.40821073],\n  [0.26862954, 0.26130258, 0.27577711]),\n])\ninp = tform(im).to(device).unsqueeze(0)\nout = sd_pipe(inp, guidance_scale=3)\nout[images][0].save(result.jpg)",
                    "performance": {
                        "dataset": "ChristophSchuhmann/improved_aesthetics_6plus",
                        "accuracy": "N/A"
                    },
                    "description": "This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.",
                    "id": "huggingface_api_260",
                    "name": "lambdalabs/sd-image-variations-diffusers"
                },
                "id": "gorilla_huggingface_tool_256",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_458",
        "query": "People in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.",
        "instruction": "Given a `video categorization` task, retrieve tools that utilize video classification capabilities to accurately recognize and categorize the main themes of videos, processing video inputs to deliver highly reliable classification results.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')",
                    "api_arguments": [
                        "video"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\ninputs = processor(video, return_tensors='pt')\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": {
                            "top-1": 79.4,
                            "top-5": 94.1
                        }
                    },
                    "description": "VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_328",
                    "name": "MCG-NJU/videomae-base-short-finetuned-kinetics"
                },
                "id": "gorilla_huggingface_tool_324",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_459",
        "query": "Help us improve the listener experience from our customers by enhancing the audio of noisy recordings.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that improve the audio quality of noisy recordings by processing audio inputs to perform denoising and dereverberation, resulting in clearer listener experiences.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')",
                    "api_arguments": {
                        "path": "Path to the input audio file."
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir='pretrained_models/sepformer-whamr-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-whamr-enhancement/example_whamr.wav')\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)",
                    "performance": {
                        "dataset": "WHAMR!",
                        "accuracy": "10.59 dB SI-SNR"
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_782",
                    "name": "speechbrain/sepformer-whamr-enhancement"
                },
                "id": "gorilla_huggingface_tool_777",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_460",
        "query": "Develop an automated podcast recommender system to suggest content that matches user's taste based on their favorite podcast speakers.",
        "instruction": "Given a `podcast recommendation` task, retrieve tools that facilitate the development of automated recommender systems by processing user preferences and audio features, such as speaker identification, to suggest personalized content based on a user's favorite podcast speakers.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker Verification",
                    "api_call": "EncoderClassifier.from_hparams(source='speechbrain/spkrec-xvect-voxceleb')",
                    "api_arguments": [
                        "source",
                        "savedir"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "import torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)",
                    "performance": {
                        "dataset": "Voxceleb1-test set (Cleaned)",
                        "accuracy": "EER(%) 3.2"
                    },
                    "description": "This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.",
                    "id": "huggingface_api_815",
                    "name": "speechbrain/spkrec-xvect-voxceleb"
                },
                "id": "gorilla_huggingface_tool_810",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_461",
        "query": "As a specialist in computer vision, we need to use the OwlViT model to identify objects in an image described by specific text phrases like \"a photo of a cat\" and \"a photo of a dog.\"",
        "instruction": "Given an `object detection` task using a specific model, retrieve tools that support zero-shot object detection by processing image inputs and text phrases to accurately identify objects described by the query, utilizing computer vision techniques and pre-trained model frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')",
                    "api_arguments": {
                        "model_name": "google/owlvit-large-patch14"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": [
                        "import requests",
                        "from PIL import Image",
                        "import torch",
                        "from transformers import OwlViTProcessor, OwlViTForObjectDetection",
                        "processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)",
                        "model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)",
                        "url = http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image = Image.open(requests.get(url, stream=True).raw)",
                        "texts = [[a photo of a cat, a photo of a dog]",
                        "inputs = processor(text=texts, images=image, return_tensors=pt)",
                        "outputs = model(**inputs)",
                        "target_sizes = torch.Tensor([image.size[::-1]])",
                        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                        "i = 0",
                        "text = texts[i]",
                        "boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]",
                        "score_threshold = 0.1",
                        "for box, score, label in zip(boxes, scores, labels):",
                        " box = [round(i, 2) for i in box.tolist()]",
                        " if score >= score_threshold:",
                        " print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})"
                    ],
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.",
                    "id": "huggingface_api_218",
                    "name": "google/owlvit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_214",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_462",
        "query": "An interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to process photographic images and accurately determine the depth information necessary for remodeling purposes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "DIODE",
                        "accuracy": ""
                    },
                    "description": "A depth estimation model fine-tuned on the DIODE dataset.",
                    "id": "huggingface_api_153",
                    "name": "glpn-nyu-finetuned-diode-221215-093747"
                },
                "id": "gorilla_huggingface_tool_149",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_463",
        "query": "Provide a short summary of an article about cryptocurrency investment risks.",
        "instruction": "Given a `summary generation` task, retrieve tools that process textual content to produce concise and coherent summaries aligned with the query's subject matter and key details.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='decapoda-research/llama-13b-hf')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "generator('Once upon a time')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BoolQ",
                                "accuracy": "85.3"
                            },
                            {
                                "name": "PIQA",
                                "accuracy": "82.8"
                            },
                            {
                                "name": "SIQA",
                                "accuracy": "52.3"
                            },
                            {
                                "name": "HellaSwag",
                                "accuracy": "84.2"
                            },
                            {
                                "name": "WinoGrande",
                                "accuracy": "77"
                            },
                            {
                                "name": "ARC-e",
                                "accuracy": "81.5"
                            },
                            {
                                "name": "ARC-c",
                                "accuracy": "56"
                            },
                            {
                                "name": "OBQACOPA",
                                "accuracy": "60.2"
                            }
                        ]
                    },
                    "description": "LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.",
                    "id": "huggingface_api_623",
                    "name": "decapoda-research/llama-13b-hf"
                },
                "id": "gorilla_huggingface_tool_618",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_464",
        "query": "The marketing team wants a tool to quickly classify new advertisement videos.",
        "instruction": "Given a `video classification` task, retrieve tools that utilize video inputs to categorize and classify content by employing advanced computer vision models to understand and label advertisement videos.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')",
                    "api_arguments": [
                        "images"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, TimesformerForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(8, 3, 224, 224))\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\ninputs = processor(images=video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-600",
                        "accuracy": null
                    },
                    "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_318",
                    "name": "facebook/timesformer-base-finetuned-k600"
                },
                "id": "gorilla_huggingface_tool_314",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_465",
        "query": "Our customer is a fitness platform. We need to analyze workout videos for offering customized workout plans.",
        "instruction": "Given a `video analysis` task, retrieve tools that perform video classification by processing video inputs to extract relevant features for generating customized workout plans aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "MCG-NJU/videomae-base-short"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers"
                        ]
                    },
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\npixel_values = processor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": "Not provided"
                    },
                    "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.",
                    "id": "huggingface_api_326",
                    "name": "MCG-NJU/videomae-base-short"
                },
                "id": "gorilla_huggingface_tool_322",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_466",
        "query": "I work at GreenTech, a startup that provides eco-friendly solutions, and need to determine if a given set of input data will result in high carbon emissions or not.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools designed for binary classification using tabular data inputs to predict whether the provided data will result in high carbon emissions, incorporating machine learning model functionalities.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Validation Metrics",
                        "accuracy": 0.983
                    },
                    "description": "Binary Classification model for Carbon Emissions prediction",
                    "id": "huggingface_api_864",
                    "name": "jwan2021/autotrain-jwan-autotrain1-1768961489"
                },
                "id": "gorilla_huggingface_tool_857",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_467",
        "query": "We are a sports analysis company that processes data from spreadsheets with game statistics. We need to identify the player who has scored the maximum goals in a given match.",
        "instruction": "Given a `data analysis` task, retrieve tools that process spreadsheet data to identify key statistics such as the player with the maximum goals in a specific match by analyzing game statistics and providing clear outcomes based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')",
                    "api_arguments": [
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "https://huggingface.co/google/tapas-large-finetuned-sqa",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.7289
                    },
                    "description": "TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_450",
                    "name": "google/tapas-large-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_446",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_468",
        "query": "A researcher needs information about how to use the Whisper ASR model to transcribe and analyze the sentiment of an audio file.",
        "instruction": "Given an `audio transcription and sentiment analysis` task, retrieve tools that combine Automatic Speech Recognition (ASR) functionalities with sentiment analysis capabilities by processing audio inputs to accurately transcribe and analyze sentiment from spoken content, in alignment with the query's requirements and utilizing models such as Whisper ASR.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Automatic Speech Recognition and Speech Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                    "api_arguments": {
                        "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')",
                        "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                        "model.config.forced_decoder_ids = None",
                        "ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')",
                        "sample = ds[0]['audio']",
                        "input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech test-clean",
                        "accuracy": 3.0003583080317573
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_757",
                    "name": "openai/whisper-large-v2"
                },
                "id": "gorilla_huggingface_tool_752",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_469",
        "query": "We are opening a platform where users can record their own podcast and host it on our platform, can you help us to convert the audio into text automatically?",
        "instruction": "Given an `audio-to-text conversion` task, retrieve tools designed for automatic speech recognition by processing audio signals and generating accurate textual transcriptions that can be used to host podcasts on a platform.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Transformers",
                    "functionality": "Transcription",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')",
                    "api_arguments": [
                        "input_values"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "torch",
                        "jiwer"
                    ],
                    "example_code": "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom datasets import load_dataset\nimport torch\n\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\nlogits = model(input_values).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)",
                    "performance": {
                        "dataset": "LibriSpeech",
                        "accuracy": {
                            "clean": 3.4,
                            "other": 8.6
                        }
                    },
                    "description": "Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.",
                    "id": "huggingface_api_751",
                    "name": "facebook/wav2vec2-base-960h"
                },
                "id": "gorilla_huggingface_tool_746",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_470",
        "query": "A new city planning company wants to estimate carbon emissions for different types of buildings to provide sustainable living solutions. We need a model to classify the carbon emissions.",
        "instruction": "Given an `emission classification` task, retrieve tools that can classify carbon emissions from building data by processing tabular inputs to provide predictions aligned with the requirements for sustainable city planning solutions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": {
                            "accuracy": 0.827
                        }
                    },
                    "description": "Multi-class Classification Model for Carbon Emissions",
                    "id": "huggingface_api_861",
                    "name": "tejas23/autotrain-amx2-1702259725"
                },
                "id": "gorilla_huggingface_tool_854",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_471",
        "query": "I am a game developer working on a game project involving moving carts. I need to use reinforcement learning to improve the game experience.",
        "instruction": "Given a `reinforcement learning application` task, retrieve tools that utilize reinforcement learning models and frameworks, such as Stable-Baselines3, to enhance game experiences by applying algorithms like PPO to specific environments, such as CartPole-v1, as per the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_901",
                    "name": "sb3/ppo-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_894",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_472",
        "query": "Our hospital needs to analyze digital blood samples in order to detect and count platelets, red blood cells, and white blood cells.",
        "instruction": "Given a `medical image analysis` task, retrieve tools that employ computer vision techniques to analyze digital images for detecting and quantifying specific cellular components such as platelets, red blood cells, and white blood cells.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": [
                        "from ultralyticsplus import YOLO, render_result",
                        "model = YOLO('keremberke/yolov8m-blood-cell-detection')",
                        "model.overrides['conf'] = 0.25",
                        "model.overrides['iou'] = 0.45",
                        "model.overrides['agnostic_nms'] = False",
                        "model.overrides['max_det'] = 1000",
                        "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model.predict(image)",
                        "print(results[0].boxes)",
                        "render = render_result(model=model, image=image, result=results[0])",
                        "render.show()"
                    ],
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.927
                    },
                    "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_224",
                    "name": "keremberke/yolov8m-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_220",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_473",
        "query": "Our client wants to analyze videos for their marketing website. They need a quick solution to categorize video content without worrying about accuracy.",
        "instruction": "Given a `video classification` task, retrieve tools that categorize video content by utilizing models capable of processing video inputs to deliver quick classification results without prioritizing high accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')",
                    "api_arguments": "model",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random VideoMAE model for video classification.",
                    "id": "huggingface_api_340",
                    "name": "tiny-random-VideoMAEForVideoClassification"
                },
                "id": "gorilla_huggingface_tool_336",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_474",
        "query": "I'm working on a project that requires generating high-quality images of faces for a set of characters in a video game.",
        "instruction": "Given an `image generation` task, retrieve tools that perform high-quality image synthesis, particularly focusing on generating facial images by implementing advanced image generation models to produce visual representations of characters.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DiffusionPipeline.from_pretrained('CompVis/ldm-celebahq-256')",
                    "api_arguments": [
                        "model_id"
                    ],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DiffusionPipeline\nmodel_id = CompVis/ldm-celebahq-256\npipeline = DiffusionPipeline.from_pretrained(model_id)\nimage = pipeline(num_inference_steps=200)[sample]\nimage[0].save(ldm_generated_image.png)",
                    "performance": {
                        "dataset": "CelebA-HQ",
                        "accuracy": "N/A"
                    },
                    "description": "Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.",
                    "id": "huggingface_api_291",
                    "name": "CompVis/ldm-celebahq-256"
                },
                "id": "gorilla_huggingface_tool_287",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_475",
        "query": "I am building a social media app that requires people to write fascinating stories rather than boring sentences. Detect named entities in a sentence by using an NER model.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform natural language processing by detecting and classifying named entities such as names, dates, and locations from textual input using advanced models and embeddings.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Flair",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes-large')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\n\n# make example sentence\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": 90.93
                    },
                    "description": "English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.",
                    "id": "huggingface_api_436",
                    "name": "flair/ner-english-ontonotes-large"
                },
                "id": "gorilla_huggingface_tool_432",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_476",
        "query": "I'm working on creating images of various scenes based on their textual descriptions. The models should also consider the actual positions and poses of the objects in the scenes.",
        "instruction": "Given an `image generation from textual descriptions` task, retrieve tools that create images based on descriptive text inputs and consider specific spatial arrangements and object poses, using advanced techniques like text-to-image diffusion models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Diffusers",
                    "functionality": "Text-to-Image Diffusion Models",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')",
                    "api_arguments": {
                        "checkpoint": "lllyasviel/control_v11p_sd15_openpose",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux==0.3.0"
                    ],
                    "example_code": {
                        "import_libraries": [
                            "import torch",
                            "import os",
                            "from huggingface_hub import HfApi",
                            "from pathlib import Path",
                            "from diffusers.utils import load_image",
                            "from PIL import Image",
                            "import numpy as np",
                            "from controlnet_aux import OpenposeDetector",
                            "from diffusers import (",
                            " ControlNetModel,",
                            " StableDiffusionControlNetPipeline,",
                            " UniPCMultistepScheduler,",
                            ")"
                        ],
                        "load_model": [
                            "checkpoint = lllyasviel/control_v11p_sd15_openpose",
                            "controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)"
                        ],
                        "example_usage": [
                            "image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)",
                            "prompt = chef in the kitchen",
                            "processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')",
                            "control_image = processor(image, hand_and_face=True)",
                            "control_image.save(./images/control.png)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(",
                            " runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16",
                            ")",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_model_cpu_offload()",
                            "generator = torch.manual_seed(0)",
                            "image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]",
                            "image.save('images/image_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.",
                    "id": "huggingface_api_272",
                    "name": "lllyasviel/control_v11p_sd15_openpose"
                },
                "id": "gorilla_huggingface_tool_268",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_477",
        "query": "Our company wants to predict housing prices in the US based on given features. Help us use the trained model to predict the prices.",
        "instruction": "Given a `housing price prediction` task, retrieve tools that utilize trained regression models to process tabular data inputs, specifically housing features, and output predicted housing prices aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": {
                        "data": "pandas.DataFrame"
                    },
                    "python_environment_requirements": {
                        "joblib": "latest",
                        "pandas": "latest"
                    },
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 134406.507,
                            "R2": 0.861,
                            "MSE": 18065109105.27,
                            "MAE": 103271.843,
                            "RMSLE": 0.139
                        }
                    },
                    "description": "A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.",
                    "id": "huggingface_api_871",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761511"
                },
                "id": "gorilla_huggingface_tool_864",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_478",
        "query": "I want to build a system that can answer questions from users reading a book.",
        "instruction": "Given a `question answering` system development task, retrieve tools that leverage Natural Language Processing (NLP) frameworks to interpret text inputs, providing accurate responses to questions by utilizing pre-trained models designed for understanding and extracting information from book passages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')",
                    "api_arguments": {
                        "context": "string",
                        "question": "string"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\nprint(result)",
                    "performance": {
                        "dataset": "squad_v2",
                        "exact": 79.8366040596311,
                        "f1": 83.916407079888
                    },
                    "description": "This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.",
                    "id": "huggingface_api_480",
                    "name": "deepset/roberta-base-squad2-distilled"
                },
                "id": "gorilla_huggingface_tool_476",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_479",
        "query": "As a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that utilize natural language processing to interpret inquiries about tables, processing both tabular data and textual questions to provide accurate responses aligned with the researcher's query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForSeq2SeqLM",
                        "pandas": "pd"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": null
                    },
                    "description": "OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.",
                    "id": "huggingface_api_442",
                    "name": "neulab/omnitab-large-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_438",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_480",
        "query": "In a kitchen, as a robot chef, we need to identify different objects such as fruits and dishes.",
        "instruction": "Given an `object detection` task in a robotic kitchen environment, retrieve tools that perform zero-shot object detection by processing visual inputs for identifying and distinguishing objects such as fruits and dishes, using a pre-trained model.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')",
                    "api_arguments": {
                        "model_name": "google/owlvit-large-patch14"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": [
                        "import requests",
                        "from PIL import Image",
                        "import torch",
                        "from transformers import OwlViTProcessor, OwlViTForObjectDetection",
                        "processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)",
                        "model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)",
                        "url = http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image = Image.open(requests.get(url, stream=True).raw)",
                        "texts = [[a photo of a cat, a photo of a dog]",
                        "inputs = processor(text=texts, images=image, return_tensors=pt)",
                        "outputs = model(**inputs)",
                        "target_sizes = torch.Tensor([image.size[::-1]])",
                        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                        "i = 0",
                        "text = texts[i]",
                        "boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]",
                        "score_threshold = 0.1",
                        "for box, score, label in zip(boxes, scores, labels):",
                        " box = [round(i, 2) for i in box.tolist()]",
                        " if score >= score_threshold:",
                        " print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})"
                    ],
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.",
                    "id": "huggingface_api_218",
                    "name": "google/owlvit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_214",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_481",
        "query": "We are working on a project to detect patterns and correlations in a dataset that contains a mix of code segments and comments. Find a suitable model to complete this task.",
        "instruction": "Given a `pattern detection` task, retrieve tools that are designed to analyze datasets composed of mixed data types, such as code segments and comments, to identify patterns and correlations. These tools should employ feature extraction capabilities suitable for processing multimodal data inputs and providing relevant insights or models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Engineering",
                    "api_call": "AutoModel.from_pretrained('microsoft/unixcoder-base')",
                    "api_arguments": {
                        "tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModel"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')",
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.",
                    "id": "huggingface_api_18",
                    "name": "microsoft/unixcoder-base"
                },
                "id": "gorilla_huggingface_tool_18",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_482",
        "query": "Analyze a set of sentences to find the most similar pairs.",
        "instruction": "Given a `sentence similarity analysis` task, retrieve tools that process sets of sentences to extract features and identify the most similar pairs by utilizing models capable of mapping sentences into dense vector spaces for semantic comparisons.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_22",
                    "name": "sentence-transformers/distilbert-base-nli-mean-tokens"
                },
                "id": "gorilla_huggingface_tool_22",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_483",
        "query": "A doctor needs to find medical information in a large document. We are building an AI model to help them extract answers to their questions.",
        "instruction": "Given an `information extraction` task, retrieve tools capable of processing large textual data to extract accurate answers based on specific questions, aligning with the medical context and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')",
                    "api_arguments": [
                        "context",
                        "question"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.7.0",
                        "torch==1.8.0",
                        "datasets==1.4.1",
                        "tokenizers==0.10.2"
                    ],
                    "example_code": "qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can the model be loaded?'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "1.2582"
                    },
                    "description": "This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset.",
                    "id": "huggingface_api_487",
                    "name": "bigwiz83/sapbert-from-pubmedbert-squad2"
                },
                "id": "gorilla_huggingface_tool_483",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_484",
        "query": "Our company is cooperating with a German partner. We have some materials in English, but need to translate them into German.",
        "instruction": "Given a `translation` task, retrieve tools that facilitate language conversion by processing text inputs in a source language to produce accurate translations in a target language, aligned with the query's linguistic requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Multilingual Sequence-to-Sequence",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')",
                    "api_arguments": {
                        "src_lang": "en_XX",
                        "tgt_lang": "ro_RO"
                    },
                    "python_environment_requirements": {
                        "transformers": "MBartForConditionalGeneration, MBart50TokenizerFast"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50', src_lang='en_XX', tgt_lang='ro_RO')\nsrc_text = ' UN Chief Says There Is No Military Solution in Syria'\ntgt_text = 'Åžeful ONU declarÄƒ cÄƒ nu existÄƒ o soluÅ£ie militarÄƒ Ã®n Siria'\nmodel_inputs = tokenizer(src_text, return_tensors='pt')\nwith tokenizer.as_target_tokenizer():\n labels = tokenizer(tgt_text, return_tensors='pt').input_ids\nmodel(**model_inputs, labels=labels)",
                    "performance": {
                        "dataset": "Multilingual Denoising Pretraining",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the 'Multilingual Denoising Pretraining' objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_645",
                    "name": "facebook/mbart-large-50"
                },
                "id": "gorilla_huggingface_tool_640",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_485",
        "query": "I'm working on a biomedical research project and need to extract features from a set of entity names to help me understand their relationships.",
        "instruction": "Given a `biomedical feature extraction` task, retrieve tools that facilitate the extraction of features from biomedical entities by leveraging pre-trained models tailored for entity relationship analysis, ensuring they handle inputs of entity names and yield meaningful embeddings or representations.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')",
                    "api_arguments": "input_ids, attention_mask",
                    "python_environment_requirements": "transformers",
                    "example_code": "inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]",
                    "performance": {
                        "dataset": "UMLS",
                        "accuracy": "N/A"
                    },
                    "description": "SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.",
                    "id": "huggingface_api_3",
                    "name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
                },
                "id": "gorilla_huggingface_tool_3",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_486",
        "query": "I am a journalist. I am writing an article about European start-ups. I need to see all the dates and company names in the text.",
        "instruction": "Given a `information extraction` task, retrieve tools that specialize in Named Entity Recognition (NER) to identify and extract specific entities such as dates and company names from the text, aligning with the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')",
                    "api_arguments": {
                        "model": "model",
                        "tokenizer": "tokenizer",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\n\nfrom transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot Â« computer  est retir le 9 janvier 2015.)",
                    "performance": {
                        "dataset": "wikiner-fr",
                        "accuracy": {
                            "overall_f1": 0.8914,
                            "PER_f1": 0.9483,
                            "ORG_f1": 0.8181,
                            "LOC_f1": 0.8955,
                            "MISC_f1": 0.8146
                        }
                    },
                    "description": "camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.",
                    "id": "huggingface_api_405",
                    "name": "Jean-Baptiste/camembert-ner"
                },
                "id": "gorilla_huggingface_tool_401",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_487",
        "query": "Our marketing team wants a video commercial for our product. They are looking for a video with a couple sitting in a cafe and laughing while using our product.",
        "instruction": "Given a `video synthesis` task, retrieve tools that utilize text descriptions to generate video content by processing the narrative details of the scene to create visual representations that align with the specified concepts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-video-synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps",
                        "num_frames"
                    ],
                    "python_environment_requirements": [
                        "pip install git+https://github.com/huggingface/diffusers transformers accelerate"
                    ],
                    "example_code": "pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": "Webvid",
                        "accuracy": "Not specified"
                    },
                    "description": "A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.",
                    "id": "huggingface_api_95",
                    "name": "damo-vilab/text-to-video-ms-1.7b"
                },
                "id": "gorilla_huggingface_tool_90",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_488",
        "query": "I have received a PDF document and a question. My task is to find the answer part in the document.",
        "instruction": "Given a `document question answering` task, retrieve tools that utilize content extraction and comprehension techniques on PDF documents to accurately find answers related to specific queries in the document.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "pipeline('question-answering')",
                    "api_arguments": [
                        "image_url",
                        "question"
                    ],
                    "python_environment_requirements": [
                        "PIL",
                        "pytesseract",
                        "PyTorch",
                        "transformers"
                    ],
                    "example_code": "nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)",
                    "performance": {
                        "dataset": [
                            "SQuAD2.0",
                            "DocVQA"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.",
                    "id": "huggingface_api_930",
                    "name": "impira/layoutlm-document-qa"
                },
                "id": "gorilla_huggingface_tool_113",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_489",
        "query": "I am an artist who wants to create a new piece of artwork based on the prompt \"A futuristic city under the ocean\".",
        "instruction": "Given an `art generation` task, retrieve tools capable of transforming text prompts into visual artwork by using text-to-image models to create imaginative and high-quality images aligned with the given creative concept.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')",
                    "api_arguments": [
                        "prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "scipy"
                    ],
                    "example_code": "import torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = CompVis/stable-diffusion-v1-4\ndevice = cuda\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.",
                    "id": "huggingface_api_30",
                    "name": "CompVis/stable-diffusion-v1-4"
                },
                "id": "gorilla_huggingface_tool_30",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_490",
        "query": "I have a collection of audio recordings from interviews that I need transcribed into text. How can I achieve this using automatic speech recognition?",
        "instruction": "Given an `audio transcription` task, retrieve tools that specialize in converting audio recordings, such as interviews, into text using automatic speech recognition (ASR) technologies. These tools should be capable of handling different audio formats and providing accurate transcriptions in alignment with the query's needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny.en')",
                    "api_arguments": {
                        "model_name": "openai/whisper-tiny.en"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "torch"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)",
                        "model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)",
                        "ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)",
                        "sample = ds[0][audio]",
                        "input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech (clean)",
                        "accuracy": 8.437
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_754",
                    "name": "openai/whisper-tiny.en"
                },
                "id": "gorilla_huggingface_tool_749",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_491",
        "query": "Tell me a text summary and answer a question from an image.",
        "instruction": "Given a `multimodal image-to-text analysis` task, retrieve tools that combine image processing with text generation to produce a text summary and answer questions based on the visual content, accommodating the diverse requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_492",
        "query": "We are building a robot for elderly care. The robot should be able to understand what's happening and take appropriate actions based on the elderly's current activities.",
        "instruction": "Given a `robotic perception and action` task, retrieve tools that facilitate the development of robots capable of interpreting and responding to dynamic environments, specifically through understanding human activities and executing corresponding actions. Tools should incorporate vision and sensory data to enable embodied AI functionalities.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning Robotics",
                    "framework": "Hugging Face Transformers",
                    "functionality": "EmbodiedAI tasks",
                    "api_call": "model_utils.load_model('model_utils.VC1_BASE_NAME')",
                    "api_arguments": "img",
                    "python_environment_requirements": "from vc_models.models.vit import model_utils",
                    "example_code": "model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\nimg = your_function_here ...\ntransformed_img = model_transforms(img)\nembedding = model(transformed_img)",
                    "performance": {
                        "dataset": "CortexBench",
                        "accuracy": "Mean Success: 68.7%"
                    },
                    "description": "The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.",
                    "id": "huggingface_api_910",
                    "name": "VC1_BASE_NAME"
                },
                "id": "gorilla_huggingface_tool_903",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_493",
        "query": "Determine the electricity consumption of a residential area based on historical data.",
        "instruction": "Given an `electricity consumption prediction` task, retrieve tools that utilize regression models to analyze historical data and predict future consumption, focusing on tools capable of processing tabular data and implementing predictive algorithms effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "skops",
                    "api_call": "RandomForestRegressor()",
                    "api_arguments": {
                        "bootstrap": "True",
                        "ccp_alpha": "0.0",
                        "criterion": "squared_error",
                        "max_depth": "10",
                        "max_features": "1.0",
                        "max_leaf_nodes": "",
                        "max_samples": "",
                        "min_impurity_decrease": "0.0",
                        "min_samples_leaf": "1",
                        "min_samples_split": "2",
                        "min_weight_fraction_leaf": "0.0",
                        "n_estimators": "50",
                        "n_jobs": "",
                        "oob_score": "False",
                        "random_state": "59",
                        "verbose": "0",
                        "warm_start": "False"
                    },
                    "python_environment_requirements": "",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A RandomForestRegressor model for electricity consumption prediction.",
                    "id": "huggingface_api_888",
                    "name": "rajistics/MAPIE-TS-Electricity"
                },
                "id": "gorilla_huggingface_tool_881",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_494",
        "query": "We are a group of teachers looking to create an automated summary based on the student's essays.",
        "instruction": "Given an `automated summarization` task, retrieve tools that process text inputs, like student essays, to generate concise and coherent summaries, aligning with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('facebook/bart-base')",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartModel.from_pretrained('facebook/bart-base')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "Not provided"
                    },
                    "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).",
                    "id": "huggingface_api_4",
                    "name": "facebook/bart-base"
                },
                "id": "gorilla_huggingface_tool_4",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_495",
        "query": "Our company seeks a solution to reduce the carbon emissions due to our operations. We have collected historical data about our emissions. We are looking for a machine learning model to predict the carbon emissions based on this data.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that utilize machine learning models specifically for tabular regression to predict carbon emissions based on historical data inputs, providing insights into potential reductions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Robertooo/autotrain-data-hmaet",
                        "accuracy": {
                            "Loss": 0.067,
                            "R2": 0.486,
                            "MSE": 0.005,
                            "MAE": 0.055,
                            "RMSLE": 0.036
                        }
                    },
                    "description": "A tabular regression model trained with AutoTrain to predict carbon emissions.",
                    "id": "huggingface_api_875",
                    "name": "Robertooo/autotrain-hmaet-2037366891"
                },
                "id": "gorilla_huggingface_tool_868",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_496",
        "query": "The team is creating a home security system. We are currently looking at understanding the depth of objects in the video stream.",
        "instruction": "Given a `depth estimation` task, retrieve tools that perform computer vision-based analysis on video streams to determine the depth of objects within the footage, using appropriate machine learning models and frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3597,
                            "Mae": 0.3054,
                            "Rmse": 0.4481,
                            "Abs Rel": 0.3462,
                            "Log Mae": 0.1256,
                            "Log Rmse": 0.1798,
                            "Delta1": 0.5278,
                            "Delta2": 0.8055,
                            "Delta3": 0.9191
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_157",
                    "name": "glpn-nyu-finetuned-diode-221122-030603"
                },
                "id": "gorilla_huggingface_tool_153",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_497",
        "query": "We have a dataset related to coffee and tea prices. We need to answer a question on who sells hot chocolate and their prices.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that can process tabular data inputs to answer specific queries about the dataset, focusing on entities and their corresponding attributes or prices.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')",
                    "api_arguments": [
                        "model_name",
                        "table",
                        "queries"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.5148
                    },
                    "description": "TAPAS mini model fine-tuned on Sequential Question Answering (SQA)",
                    "id": "huggingface_api_457",
                    "name": "google/tapas-mini-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_453",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_498",
        "query": "Develop an emotion analysis system to understand customer satisfaction over the phone for a telecommunication company in Russia.",
        "instruction": "Given an `emotion analysis` task, retrieve tools that can analyze audio inputs to identify and classify emotions, focusing on customer satisfaction in a specific linguistic and cultural context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')",
                    "api_arguments": {
                        "path": "/path/to/russian_audio_speech.wav",
                        "sampling_rate": 16000
                    },
                    "python_environment_requirements": [
                        "torch",
                        "torchaudio",
                        "transformers",
                        "librosa",
                        "numpy"
                    ],
                    "example_code": "result = predict('/path/to/russian_audio_speech.wav', 16000)\nprint(result)",
                    "performance": {
                        "dataset": "Russian Emotional Speech Dialogs",
                        "accuracy": "72%"
                    },
                    "description": "A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.",
                    "id": "huggingface_api_826",
                    "name": "wav2vec2-xlsr-53-russian-emotion-recognition"
                },
                "id": "gorilla_huggingface_tool_821",
                "relevance": 1
            }
        ],
        "category": "code"
    },
    {
        "id": "gorilla_huggingface_query_499",
        "query": "We need to analyze the user's text for extracting entities and improve our virtual assistant interaction.",
        "instruction": "Given an `entity extraction` task, retrieve tools that analyze text to identify and extract entities, enhancing interactions with virtual assistants by processing user inputs for improved natural language understanding.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification",
                        "tokenizer": "AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-name_all",
                        "accuracy": 0.9989316041363876
                    },
                    "description": "This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.",
                    "id": "huggingface_api_413",
                    "name": "904029577"
                },
                "id": "gorilla_huggingface_tool_409",
                "relevance": 1
            }
        ],
        "category": "code"
    }
]