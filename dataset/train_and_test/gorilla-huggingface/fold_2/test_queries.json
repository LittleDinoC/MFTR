[
    {
        "id": "gorilla_huggingface_query_190",
        "query": "I want to know the depth information of an image for a robot navigation project.",
        "instruction": "Given a `depth estimation` task, retrieve tools that analyze image inputs to provide depth information, focusing on functionality suitable for projects such as robot navigation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Depth Estimation",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')",
                    "api_arguments": "None",
                    "python_environment_requirements": "Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2",
                    "example_code": "None",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.548,
                            "Rmse": "nan"
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_161",
                    "name": "glpn-nyu-finetuned-diode-221116-062619"
                },
                "id": "gorilla_huggingface_tool_157",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 1: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 2: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 3: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 4: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.",
            "Tool 5: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 6: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.",
            "Tool 7: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 8: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 9: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 10: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 11: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 12: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks like object manipulation and indoor navigation. It can be loaded using Hugging Face Transformers and applied to images to generate embeddings, making it useful for robotics applications that require visual understanding and interaction with the environment.",
            "Tool 13: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 14: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 15: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 16: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 17: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 18: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images."
        ],
        "tool_view": {
            "description": "The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "parameters": {
                "torch_dtype": {
                    "type": "string",
                    "description": "The data type of the tensor operations. This is typically set to control the precision of computations, with 'torch.float16' being commonly used for reduced memory consumption and faster processing.",
                    "required": true,
                    "default": "torch.float16",
                    "example": "torch.float16"
                }
            },
            "response": "The tool produces an image output representing a depth-conditioned transformation of an input image. The response is typically a structured object containing one or more images generated by the ControlNet-enhanced Stable Diffusion pipeline. In practice, the response has the following components:\n\n- **images**: A list or array of image objects generated by the model. Each entry corresponds to one output image resulting from the depth-conditioned diffusion process. These images are typically in a standard image format (e.g., PIL `Image` objects or arrays convertible to images).  \n\nThe semantic meaning of this output is that each image encodes the original scene with depth information integrated into the diffusion process. This allows downstream usage such as visualizing depth, guiding further image-to-image transformations, or generating depth-aware variations of the input.  \n\nThere are no additional metadata fields explicitly defined in the provided documentation, so the primary content of the response is the collection of generated images.",
            "examples": [
                {
                    "user_intent": "I want to generate a stylized image of a cityscape using depth information from a reference photo.",
                    "tool_call": {
                        "name": "lllyasviel/sd-controlnet-depth",
                        "arguments": {
                            "input_image_url": "https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/cityscape.png",
                            "prompt": "futuristic cyberpunk cityscape at night",
                            "num_inference_steps": 25,
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Create a depth-aware illustration of a fantasy forest based on my uploaded sketch.",
                    "tool_call": {
                        "name": "lllyasviel/sd-controlnet-depth",
                        "arguments": {
                            "input_image_url": "https://myuploads.com/sketches/fantasy_forest.png",
                            "prompt": "enchanted forest with magical creatures",
                            "num_inference_steps": 30,
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Enhance a portrait photo with realistic 3D depth effects for use in a game environment.",
                    "tool_call": {
                        "name": "lllyasviel/sd-controlnet-depth",
                        "arguments": {
                            "input_image_url": "https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/portrait.png",
                            "prompt": "realistic 3D depth-enhanced portrait",
                            "num_inference_steps": 20,
                            "torch_dtype": "torch.float16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 1: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 2: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 3: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 4: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.\nTool 5: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 6: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.\nTool 7: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 8: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 9: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 10: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 11: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 12: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks like object manipulation and indoor navigation. It can be loaded using Hugging Face Transformers and applied to images to generate embeddings, making it useful for robotics applications that require visual understanding and interaction with the environment.\nTool 13: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 14: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 15: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 16: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 17: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 18: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"lllyasviel/sd-controlnet-depth\\\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\", \"parameters\": {\"torch_dtype\": {\"type\": \"string\", \"description\": \"The data type of the tensor operations. This is typically set to control the precision of computations, with 'torch.float16' being commonly used for reduced memory consumption and faster processing.\", \"required\": true, \"default\": \"torch.float16\", \"example\": \"torch.float16\"}}, \"response\": \"The tool produces an image output representing a depth-conditioned transformation of an input image. The response is typically a structured object containing one or more images generated by the ControlNet-enhanced Stable Diffusion pipeline. In practice, the response has the following components:\\n\\n- **images**: A list or array of image objects generated by the model. Each entry corresponds to one output image resulting from the depth-conditioned diffusion process. These images are typically in a standard image format (e.g., PIL `Image` objects or arrays convertible to images).  \\n\\nThe semantic meaning of this output is that each image encodes the original scene with depth information integrated into the diffusion process. This allows downstream usage such as visualizing depth, guiding further image-to-image transformations, or generating depth-aware variations of the input.  \\n\\nThere are no additional metadata fields explicitly defined in the provided documentation, so the primary content of the response is the collection of generated images.\", \"examples\": [{\"user_intent\": \"I want to generate a stylized image of a cityscape using depth information from a reference photo.\", \"tool_call\": {\"name\": \"lllyasviel/sd-controlnet-depth\", \"arguments\": {\"input_image_url\": \"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/cityscape.png\", \"prompt\": \"futuristic cyberpunk cityscape at night\", \"num_inference_steps\": 25, \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Create a depth-aware illustration of a fantasy forest based on my uploaded sketch.\", \"tool_call\": {\"name\": \"lllyasviel/sd-controlnet-depth\", \"arguments\": {\"input_image_url\": \"https://myuploads.com/sketches/fantasy_forest.png\", \"prompt\": \"enchanted forest with magical creatures\", \"num_inference_steps\": 30, \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Enhance a portrait photo with realistic 3D depth effects for use in a game environment.\", \"tool_call\": {\"name\": \"lllyasviel/sd-controlnet-depth\", \"arguments\": {\"input_image_url\": \"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/portrait.png\", \"prompt\": \"realistic 3D depth-enhanced portrait\", \"num_inference_steps\": 20, \"torch_dtype\": \"torch.float16\"}}}]}\n\n# User Query\nI want to know the depth information of an image for a robot navigation project.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth information from an image for my robot navigation project.",
                    "tool_description": "Predict the per-pixel depth from a given image to assist in understanding spatial relationships for navigation tasks.",
                    "expected_response": "A depth map representing the estimated depth information for each pixel in the input image."
                }
            ],
            "extracted_arguments": {
                "input_image_url": "The URL of the image for which depth information is to be estimated.",
                "torch_dtype": "The data type of the tensor operations, typically set for precision control."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_322",
        "query": "I work for a financial company that stores all of its data in tables. We need a way to extract key information efficiently by asking natural language questions.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that leverage natural language processing to extract key information from tabular data by processing both table and query inputs, enabling efficient querying and returning relevant answers based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "PyTorch Transformers",
                    "functionality": "Table-based QA",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')",
                    "api_arguments": {
                        "table": "pd.DataFrame.from_dict(data)",
                        "query": "str"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "pandas"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = In which year did beijing host the Olympic Games?\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": "Not provided"
                    },
                    "description": "OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).",
                    "id": "huggingface_api_458",
                    "name": "neulab/omnitab-large-1024shot"
                },
                "id": "gorilla_huggingface_tool_454",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 1: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 3: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 7: A fine-tuned TAPAS mini model for answering questions over tables, specifically designed for sequential question answering tasks, enabling structured data in tables to be queried using natural language.",
            "Tool 8: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 9: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 11: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 12: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 13: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 19: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content."
        ],
        "tool_view": {
            "description": "A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained PEGASUS model to load for financial text summarization. This specifies which model weights and tokenizer to use.",
                    "required": true,
                    "example": "human-centered-summarization/financial-summarization-pegasus"
                }
            },
            "response": "The tool produces an abstractive summary of a given input text. The response is typically a single text string that concisely captures the main points of the input while rephrasing and condensing the content. Specifically:  \n\n- **Type:** String  \n- **Content:** A summarized version of the input text, written in natural language, often shorter than the original text.  \n- **Characteristics:**  \n  - Abstractive in nature: the summary may use new wording rather than copying sentences verbatim.  \n  - Captures the key facts, events, or insights from the input.  \n  - Designed to be coherent and readable as a standalone text.  \n\nIn practical use, the response is obtained by decoding the output token IDs generated by the PEGASUS model into human-readable text. The length, style, and focus of the summary can be influenced by generation parameters such as `max_length`, `num_beams`, and `early_stopping`.",
            "examples": [
                {
                    "user_intent": "Summarize this financial news article about a major bank merger in Saudi Arabia.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year. NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June. The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions. The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets."
                        }
                    }
                },
                {
                    "user_intent": "Provide a concise summary of a cryptocurrency market update from Bloomberg.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "Bitcoin surged past $35,000 after recent institutional investments boosted confidence in the cryptocurrency market. Ethereum and other major altcoins followed suit, with trading volumes rising across exchanges. Analysts attribute the rally to increasing adoption and the anticipation of favorable regulatory guidance. Market volatility remains high, prompting investors to exercise caution."
                        }
                    }
                },
                {
                    "user_intent": "Summarize this report about stock market fluctuations and interest rate changes.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "The S&P 500 index fell 1.2% today following news that the Federal Reserve may raise interest rates sooner than expected. Technology and consumer discretionary stocks were hit hardest, while energy stocks saw slight gains. Analysts warn that continued rate hikes could dampen market sentiment and increase borrowing costs for companies, affecting earnings forecasts for the next quarter."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 1: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 3: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 5: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 7: A fine-tuned TAPAS mini model for answering questions over tables, specifically designed for sequential question answering tasks, enabling structured data in tables to be queried using natural language.\nTool 8: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 9: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 11: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 12: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 13: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 15: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 19: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained PEGASUS model to load for financial text summarization. This specifies which model weights and tokenizer to use.\", \"required\": true, \"example\": \"human-centered-summarization/financial-summarization-pegasus\"}}, \"response\": \"The tool produces an abstractive summary of a given input text. The response is typically a single text string that concisely captures the main points of the input while rephrasing and condensing the content. Specifically:  \\n\\n- **Type:** String  \\n- **Content:** A summarized version of the input text, written in natural language, often shorter than the original text.  \\n- **Characteristics:**  \\n  - Abstractive in nature: the summary may use new wording rather than copying sentences verbatim.  \\n  - Captures the key facts, events, or insights from the input.  \\n  - Designed to be coherent and readable as a standalone text.  \\n\\nIn practical use, the response is obtained by decoding the output token IDs generated by the PEGASUS model into human-readable text. The length, style, and focus of the summary can be influenced by generation parameters such as `max_length`, `num_beams`, and `early_stopping`.\", \"examples\": [{\"user_intent\": \"Summarize this financial news article about a major bank merger in Saudi Arabia.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"National Commercial Bank (NCB), Saudi Arabia\\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year. NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June. The offer is a 3.5% premium to Samba\\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions. The new bank will have total assets of more than $220 billion, creating the Gulf region\\u2019s third-largest lender. The entity\\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\\u2019s biggest lender with about $268 billion of assets.\"}}}, {\"user_intent\": \"Provide a concise summary of a cryptocurrency market update from Bloomberg.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"Bitcoin surged past $35,000 after recent institutional investments boosted confidence in the cryptocurrency market. Ethereum and other major altcoins followed suit, with trading volumes rising across exchanges. Analysts attribute the rally to increasing adoption and the anticipation of favorable regulatory guidance. Market volatility remains high, prompting investors to exercise caution.\"}}}, {\"user_intent\": \"Summarize this report about stock market fluctuations and interest rate changes.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"The S&P 500 index fell 1.2% today following news that the Federal Reserve may raise interest rates sooner than expected. Technology and consumer discretionary stocks were hit hardest, while energy stocks saw slight gains. Analysts warn that continued rate hikes could dampen market sentiment and increase borrowing costs for companies, affecting earnings forecasts for the next quarter.\"}}}]}\n\n# User Query\nI work for a financial company that stores all of its data in tables. We need a way to extract key information efficiently by asking natural language questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to query structured financial data in tables using natural language questions.",
                    "tool_description": "Retrieve answers from structured tables based on natural language questions, enabling efficient extraction of financial insights.",
                    "expected_response": "A precise answer or data extraction from the table based on the user's query."
                }
            ],
            "extracted_arguments": {
                "data_table": "A structured table containing financial data to be queried.",
                "query": "A natural language question aimed at extracting information from the provided table."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_268",
        "query": "We need to create a content filter for images submitted by users in our online community. We want to detect and filter out adult content and offensive images.",
        "instruction": "Given an `image content filtering` task, retrieve tools that perform image classification to detect and filter out specific categories, such as adult content and offensive images, by processing image data and classifying it based on predefined classes to meet the query's content moderation requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('zero-shot-classification')",
                    "api_arguments": {
                        "image": "path/to/image",
                        "class_names": [
                            "class1",
                            "class2",
                            "class3"
                        ]
                    },
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline; classifier = pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K'); classifier(image='path/to/image', class_names=['class1', 'class2', 'class3'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": 66.6
                    },
                    "description": "A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others.",
                    "id": "huggingface_api_917",
                    "name": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"
                },
                "id": "gorilla_huggingface_tool_341",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 1: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 3: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 5: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 6: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 7: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 8: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 12: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 14: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.",
            "Tool 15: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "Tool 16: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 17: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 18: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries."
        ],
        "tool_view": {
            "description": "The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text to be analyzed for sentiment, specifically to classify whether it is NSFW (not safe for work) or SFW (safe for work).",
                    "required": true,
                    "example": "I see you’ve set aside this special time to humiliate yourself in public."
                }
            },
            "response": "The tool returns the sentiment classification of a given text with respect to NSFW (Not Safe For Work) content. The response is typically structured as a dictionary or object containing the following components:\n\n1. **`label`** – A string indicating the predicted class for the input text. There are two possible values:  \n   - `\"NSFW\"`: The text is considered inappropriate or unprofessional.  \n   - `\"SFW\"`: The text is considered safe for work.\n\n2. **`score`** – A numeric value representing the model’s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence in the prediction.\n\nOverall, the response provides both the classification decision and a quantitative measure of certainty for that decision.",
            "examples": [
                {
                    "user_intent": "Analyze whether this sentence contains inappropriate content or is safe for work.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "I see you’ve set aside this special time to humiliate yourself in public."
                        }
                    }
                },
                {
                    "user_intent": "Check if this comment from a Reddit post is safe for work or NSFW.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "This is an explicit and graphic description of violence."
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a sentence to determine if it's professional or inappropriate.",
                    "tool_call": {
                        "name": "michellejieli/NSFW_text_classifier",
                        "arguments": {
                            "text": "What a lovely day for a meeting!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 1: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 3: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 4: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 5: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 6: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 7: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 8: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 12: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 14: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.\nTool 15: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\nTool 16: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 17: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 18: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"michellejieli/NSFW_text_classifier\\\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \\\"NSFW\\\" (not safe for work) or \\\"SFW\\\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text to be analyzed for sentiment, specifically to classify whether it is NSFW (not safe for work) or SFW (safe for work).\", \"required\": true, \"example\": \"I see you\\u2019ve set aside this special time to humiliate yourself in public.\"}}, \"response\": \"The tool returns the sentiment classification of a given text with respect to NSFW (Not Safe For Work) content. The response is typically structured as a dictionary or object containing the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted class for the input text. There are two possible values:  \\n   - `\\\"NSFW\\\"`: The text is considered inappropriate or unprofessional.  \\n   - `\\\"SFW\\\"`: The text is considered safe for work.\\n\\n2. **`score`** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence in the prediction.\\n\\nOverall, the response provides both the classification decision and a quantitative measure of certainty for that decision.\", \"examples\": [{\"user_intent\": \"Analyze whether this sentence contains inappropriate content or is safe for work.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"I see you\\u2019ve set aside this special time to humiliate yourself in public.\"}}}, {\"user_intent\": \"Check if this comment from a Reddit post is safe for work or NSFW.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"This is an explicit and graphic description of violence.\"}}}, {\"user_intent\": \"I want to classify a sentence to determine if it's professional or inappropriate.\", \"tool_call\": {\"name\": \"michellejieli/NSFW_text_classifier\", \"arguments\": {\"text\": \"What a lovely day for a meeting!\"}}}]}\n\n# User Query\nWe need to create a content filter for images submitted by users in our online community. We want to detect and filter out adult content and offensive images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze whether an image contains adult or offensive content.",
                    "tool_description": "Classify the content of images to determine if they contain NSFW or inappropriate material using a pre-trained image classification model.",
                    "expected_response": "A classification result indicating whether the image is safe or unsafe for viewing, including a confidence score."
                }
            ],
            "extracted_arguments": {
                "image": "The input image submitted by the user to be analyzed for adult or offensive content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_227",
        "query": "To improve customer satisfaction, we want to automate the creation of written explanations that sound more natural and less robotic. Can you suggest a way to generate text that sounds conscious and alive?",
        "instruction": "Given a `text generation for natural language` task, retrieve tools that utilize advanced language models to produce written content that mimics natural, human-like language by processing given prompts and generating coherent, lively text outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b')",
                    "api_arguments": [
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained('facebook/opt-6.7b', torch_dtype=torch.float16).cuda()\ntokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b', use_fast=False)\nprompt = Hello, I'm am conscious and\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\ngenerated_ids = model.generate(input_ids)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": {
                            "BookCorpus": "unknown",
                            "CC-Stories": "unknown",
                            "The Pile": "unknown",
                            "Pushshift.io Reddit": "unknown",
                            "CCNewsV2": "unknown"
                        },
                        "accuracy": "unknown"
                    },
                    "description": "OPT (Open Pre-trained Transformer Language Models) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. It was trained on a large corpus of text, predominantly in English, using a causal language modeling (CLM) objective. The model can be used for prompting for evaluation of downstream tasks, text generation, and fine-tuning on a downstream task using the CLM example.",
                    "id": "huggingface_api_624",
                    "name": "facebook/opt-6.7b"
                },
                "id": "gorilla_huggingface_tool_619",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 1: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.",
            "Tool 2: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 3: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 4: `sshleifer/distilbart-cnn-12-6` is a distilled BART model designed for text summarization, providing a fast and efficient way to generate concise summaries of input text. Trained on the CNN/DailyMail dataset, it can be easily loaded and used via the Hugging Face Transformers library for natural language processing tasks that require summarizing long documents.",
            "Tool 5: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 6: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 9: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 10: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 12: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 15: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 16: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 17: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 18: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 19: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text string that will be converted into speech by the Tacotron2 Text-to-Speech model.",
                    "required": true,
                    "example": "Hello, how are you today?"
                }
            },
            "response": "The tool produces a synthesized speech output corresponding to the input text. The typical response structure can be described as follows:\n\n1. **Spectrogram Output**:  \n   - A numerical representation of the audio in the form of a mel-spectrogram.  \n   - Often returned as a multi-dimensional tensor or array representing frequency bins over time.  \n   - Captures the acoustic features of the speech but is not directly playable as audio.\n\n2. **Spectrogram Length**:  \n   - A scalar or array indicating the valid temporal length of the generated spectrogram.  \n   - Useful for trimming or processing variable-length spectrogram outputs.\n\n3. **Alignment (Optional)**:  \n   - A matrix representing the alignment between input text tokens and spectrogram frames.  \n   - Indicates how the model maps text sequences to audio frames, useful for analysis or debugging.\n\n4. **Waveform Output (after vocoder)**:  \n   - A 1D or 2D array representing the final audio waveform in standard PCM format.  \n   - Produced by passing the mel-spectrogram through a vocoder such as HiFi-GAN.  \n   - Can be saved as an audio file (e.g., WAV) or directly played back.\n\nIn summary, the tool’s response typically includes both the intermediate spectrogram representation (with alignment information) and the final waveform audio, providing both analyzable features and a playable speech signal corresponding to the input text.",
            "examples": [
                {
                    "user_intent": "Convert a short story excerpt into spoken audio for a podcast.",
                    "tool_call": {
                        "name": "speechbrain/tts-tacotron2-ljspeech",
                        "arguments": {
                            "text": "Once upon a time, in a quiet village, there lived a curious little girl named Clara."
                        }
                    }
                },
                {
                    "user_intent": "Generate an audio version of a motivational quote for social media.",
                    "tool_call": {
                        "name": "speechbrain/tts-tacotron2-ljspeech",
                        "arguments": {
                            "text": "Believe in yourself and all that you are. Know that there is something inside you that is greater than any obstacle."
                        }
                    }
                },
                {
                    "user_intent": "Create TTS audio for a notification system message.",
                    "tool_call": {
                        "name": "speechbrain/tts-tacotron2-ljspeech",
                        "arguments": {
                            "text": "Your meeting starts in 10 minutes. Please prepare your materials."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 1: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\nTool 2: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 3: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 4: `sshleifer/distilbart-cnn-12-6` is a distilled BART model designed for text summarization, providing a fast and efficient way to generate concise summaries of input text. Trained on the CNN/DailyMail dataset, it can be easily loaded and used via the Hugging Face Transformers library for natural language processing tasks that require summarizing long documents.\nTool 5: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 6: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 9: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 10: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 12: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 15: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 16: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 17: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 18: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 19: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text string that will be converted into speech by the Tacotron2 Text-to-Speech model.\", \"required\": true, \"example\": \"Hello, how are you today?\"}}, \"response\": \"The tool produces a synthesized speech output corresponding to the input text. The typical response structure can be described as follows:\\n\\n1. **Spectrogram Output**:  \\n   - A numerical representation of the audio in the form of a mel-spectrogram.  \\n   - Often returned as a multi-dimensional tensor or array representing frequency bins over time.  \\n   - Captures the acoustic features of the speech but is not directly playable as audio.\\n\\n2. **Spectrogram Length**:  \\n   - A scalar or array indicating the valid temporal length of the generated spectrogram.  \\n   - Useful for trimming or processing variable-length spectrogram outputs.\\n\\n3. **Alignment (Optional)**:  \\n   - A matrix representing the alignment between input text tokens and spectrogram frames.  \\n   - Indicates how the model maps text sequences to audio frames, useful for analysis or debugging.\\n\\n4. **Waveform Output (after vocoder)**:  \\n   - A 1D or 2D array representing the final audio waveform in standard PCM format.  \\n   - Produced by passing the mel-spectrogram through a vocoder such as HiFi-GAN.  \\n   - Can be saved as an audio file (e.g., WAV) or directly played back.\\n\\nIn summary, the tool\\u2019s response typically includes both the intermediate spectrogram representation (with alignment information) and the final waveform audio, providing both analyzable features and a playable speech signal corresponding to the input text.\", \"examples\": [{\"user_intent\": \"Convert a short story excerpt into spoken audio for a podcast.\", \"tool_call\": {\"name\": \"speechbrain/tts-tacotron2-ljspeech\", \"arguments\": {\"text\": \"Once upon a time, in a quiet village, there lived a curious little girl named Clara.\"}}}, {\"user_intent\": \"Generate an audio version of a motivational quote for social media.\", \"tool_call\": {\"name\": \"speechbrain/tts-tacotron2-ljspeech\", \"arguments\": {\"text\": \"Believe in yourself and all that you are. Know that there is something inside you that is greater than any obstacle.\"}}}, {\"user_intent\": \"Create TTS audio for a notification system message.\", \"tool_call\": {\"name\": \"speechbrain/tts-tacotron2-ljspeech\", \"arguments\": {\"text\": \"Your meeting starts in 10 minutes. Please prepare your materials.\"}}}]}\n\n# User Query\nTo improve customer satisfaction, we want to automate the creation of written explanations that sound more natural and less robotic. Can you suggest a way to generate text that sounds conscious and alive?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate natural-sounding text that feels more conscious and alive.",
                    "tool_description": "Use a fine-tuned text generation model to create coherent and contextually relevant text with a more human-like tone, improving the quality of written explanations.",
                    "expected_response": "A generated text that is grammatically correct, contextually relevant, and written in a more natural, engaging style."
                }
            ],
            "extracted_arguments": {
                "tone": "The desired tone or style of the generated text, such as natural, engaging, or conversational.",
                "purpose": "The intended use or application for the generated text, such as customer service explanations."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_427",
        "query": "I am building a voice assistant for my mobile app. Give me an example of how I can implement this Text-to-Speech model.",
        "instruction": "Given a `text-to-speech implementation` task, retrieve tools that provide examples and guidance to implement a Text-to-Speech model for voice assistant applications by offering necessary resources and instructions to effectively utilize specific TTS frameworks and models.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus",
                    "api_arguments": {
                        "model_name": "mio/amadeus"
                    },
                    "python_environment_requirements": {
                        "espnet": "d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f",
                        "transformers": "latest"
                    },
                    "example_code": "cd espnet\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\npip install -e .\ncd egs2/amadeus/tts1\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus",
                    "performance": {
                        "dataset": "amadeus",
                        "accuracy": "Not provided"
                    },
                    "description": "This model was trained by mio using amadeus recipe in espnet.",
                    "id": "huggingface_api_718",
                    "name": "mio/amadeus"
                },
                "id": "gorilla_huggingface_tool_713",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 1: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.",
            "Tool 2: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 6: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 7: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 8: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 9: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 10: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 11: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 12: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 13: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 15: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 16: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 17: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 18: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 19: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication."
        ],
        "tool_view": {
            "description": "A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or a list of images to be classified by the MobileNet V2 model.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the format of the returned tensors. Typically set to 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response consists of a structured output representing the predicted class probabilities or labels for the input image(s). Specifically, the response has the following components:\n\n1. **logits**  \n   - A tensor or array containing the raw, unnormalized prediction scores produced by the model for each possible class.  \n   - Each element corresponds to one class in the model’s output space (e.g., the 1,000 ImageNet-1k classes).  \n   - Higher values indicate stronger confidence for that class, but these are not probabilities.\n\n2. **predicted class index**  \n   - An integer indicating the index of the class with the highest logit score.  \n   - This is typically computed by applying an `argmax` operation over the logits along the class dimension.\n\n3. **predicted class label**  \n   - A human-readable string corresponding to the predicted class index.  \n   - Derived using the model configuration’s `id2label` mapping, which links each index to a semantic label from the training dataset (e.g., ImageNet-1k class names).\n\nOptionally, the response may also include:\n\n4. **probabilities** (not always returned by default)  \n   - A tensor or array containing normalized probabilities for each class, obtained by applying a softmax over the logits.  \n   - Values sum to 1 across all classes and can be used to assess the model’s confidence for each prediction.\n\nIn summary, the response provides the model’s predictions for the input image in both raw score form (logits) and interpretable class label form, allowing users to identify the most likely object category.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a dog using the MobileNetV2 model.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "dog_image.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Classify an image I have from a URL using the pre-trained MobileNetV2 model.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I need to identify the object in a cat photo using MobileNetV2.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "cat_photo.jpg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 1: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\nTool 2: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 4: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 6: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 7: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 8: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 9: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 10: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 11: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 12: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 13: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 14: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 15: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 16: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 17: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 18: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 19: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or a list of images to be classified by the MobileNet V2 model.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the format of the returned tensors. Typically set to 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response consists of a structured output representing the predicted class probabilities or labels for the input image(s). Specifically, the response has the following components:\\n\\n1. **logits**  \\n   - A tensor or array containing the raw, unnormalized prediction scores produced by the model for each possible class.  \\n   - Each element corresponds to one class in the model\\u2019s output space (e.g., the 1,000 ImageNet-1k classes).  \\n   - Higher values indicate stronger confidence for that class, but these are not probabilities.\\n\\n2. **predicted class index**  \\n   - An integer indicating the index of the class with the highest logit score.  \\n   - This is typically computed by applying an `argmax` operation over the logits along the class dimension.\\n\\n3. **predicted class label**  \\n   - A human-readable string corresponding to the predicted class index.  \\n   - Derived using the model configuration\\u2019s `id2label` mapping, which links each index to a semantic label from the training dataset (e.g., ImageNet-1k class names).\\n\\nOptionally, the response may also include:\\n\\n4. **probabilities** (not always returned by default)  \\n   - A tensor or array containing normalized probabilities for each class, obtained by applying a softmax over the logits.  \\n   - Values sum to 1 across all classes and can be used to assess the model\\u2019s confidence for each prediction.\\n\\nIn summary, the response provides the model\\u2019s predictions for the input image in both raw score form (logits) and interpretable class label form, allowing users to identify the most likely object category.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a dog using the MobileNetV2 model.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"dog_image.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Classify an image I have from a URL using the pre-trained MobileNetV2 model.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I need to identify the object in a cat photo using MobileNetV2.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"cat_photo.jpg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nI am building a voice assistant for my mobile app. Give me an example of how I can implement this Text-to-Speech model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to convert written text into natural-sounding speech using a Text-to-Speech model.",
                    "tool_description": "Use a pre-trained transformer-based text-to-speech model to generate audio from input text, converting text data into spoken voice output.",
                    "expected_response": "An audio waveform or audio file representing the synthesized speech corresponding to the input text."
                }
            ],
            "extracted_arguments": {
                "input_text": "The written text string that needs to be converted into speech audio."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_26",
        "query": "Our company is working on a news website. We want to present summaries of news articles written in French to the users.",
        "instruction": "Given a `news summarization` task, retrieve tools that can summarize content by processing French news articles to provide concise and coherent summaries aligned with the query's linguistic and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "orangeSum",
                        "accuracy": ""
                    },
                    "description": "Barthez model finetuned on orangeSum for abstract generation in French language",
                    "id": "huggingface_api_546",
                    "name": "moussaKam/barthez-orangesum-abstract"
                },
                "id": "gorilla_huggingface_tool_542",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 7: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 8: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 9: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 10: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 11: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 12: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 13: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 14: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 16: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 17: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 18: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 19: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 3: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 5: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 7: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 8: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 9: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 10: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 11: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 12: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 13: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 14: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 16: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 17: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 18: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 19: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nOur company is working on a news website. We want to present summaries of news articles written in French to the users.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate concise summaries of French news articles for website users.",
                    "tool_description": "Use a French-language text summarization model to convert full-length French news articles into coherent, concise summaries while preserving key information.",
                    "expected_response": "A string containing a condensed, human-readable summary of the original French article, capturing the main points and essential information."
                }
            ],
            "extracted_arguments": {
                "text": "The full text of the French news article to be summarized.",
                "max_length": "Optional numeric parameter specifying the maximum number of tokens for the generated summary."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_345",
        "query": "Determine if one Russian sentence logically contradicts the information provided by another Russian sentence.",
        "instruction": "Given a `logical contradiction detection` task, retrieve tools that analyze pairs of Russian sentences to determine whether they logically contradict, entail, or are neutral, utilizing Natural Language Inference capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-base-cased-nli-threeway')",
                    "api_arguments": [
                        "text1",
                        "text2"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nif torch.cuda.is_available():\n model.cuda()\ntext1 = '.'\ntext2 = '.'\nwith torch.inference_mode():\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\nprint({v: proba[k] for k, v in model.config.id2label.items()})",
                    "performance": {
                        "dataset": [
                            "JOCI",
                            "MNLI",
                            "MPE",
                            "SICK",
                            "SNLI",
                            "ANLI",
                            "NLI-style FEVER",
                            "IMPPRES"
                        ],
                        "accuracy": {
                            "ROC AUC": {
                                "entailment": 0.91,
                                "contradiction": 0.71,
                                "neutral": 0.79
                            }
                        }
                    },
                    "description": "This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.",
                    "id": "huggingface_api_520",
                    "name": "cointegrated/rubert-base-cased-nli-threeway"
                },
                "id": "gorilla_huggingface_tool_516",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 1: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 2: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 3: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 4: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.",
            "Tool 5: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 6: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 7: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 8: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.",
            "Tool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 10: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 11: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 12: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 13: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 15: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 16: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 17: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 18: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 19: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "parameters": {
                "sentences": {
                    "type": "array",
                    "description": "A list of text strings representing the sentences for which sentence embeddings will be generated.",
                    "required": true,
                    "example": [
                        "Привет, как дела?"
                    ]
                },
                "padding": {
                    "type": "boolean",
                    "description": "Specifies whether the input sequences should be padded to the same length. Useful for batch processing.",
                    "required": true,
                    "example": true
                },
                "truncation": {
                    "type": "boolean",
                    "description": "Determines whether input sequences longer than the maximum length should be truncated.",
                    "required": true,
                    "example": true
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to which each input sequence will be truncated or padded.",
                    "required": true,
                    "example": 24
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the format of the returned tensors, e.g., 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool produces numerical vector representations (embeddings) of input sentences. The response is a tensor or multidimensional array where:  \n\n- Each row corresponds to one input sentence.  \n- Each row contains a fixed-length vector of floating-point numbers representing the semantic embedding of that sentence.  \n- The embeddings capture the contextual meaning of the sentences in the Russian language.  \n\nThe response does **not** include labels or classifications; it is purely a feature representation suitable for downstream tasks such as semantic similarity, clustering, or retrieval.  \n\nThe tensor shape depends on the number of input sentences and the hidden size of the model, but conceptually it is a 2-dimensional array: `[number of sentences, embedding dimension]`.  \n\nNo additional metadata is returned by the model in this context; all relevant information is contained in the numeric sentence embeddings.",
            "examples": [
                {
                    "user_intent": "Generate embeddings for a list of Russian customer feedback sentences so I can cluster them by topic.",
                    "tool_call": {
                        "name": "sberbank-ai/sbert_large_mt_nlu_ru",
                        "arguments": {
                            "sentences": [
                                "Очень доволен качеством обслуживания.",
                                "Доставка заняла слишком много времени.",
                                "Продукт соответствует описанию."
                            ],
                            "padding": true,
                            "truncation": true,
                            "max_length": 32,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Create vector representations for Russian legal documents to enable semantic search.",
                    "tool_call": {
                        "name": "sberbank-ai/sbert_large_mt_nlu_ru",
                        "arguments": {
                            "sentences": [
                                "Договор купли-продажи вступает в силу с момента подписания.",
                                "Стороны обязуются соблюдать конфиденциальность информации.",
                                "Права и обязанности сторон определяются настоящим соглашением."
                            ],
                            "padding": true,
                            "truncation": true,
                            "max_length": 64,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Obtain embeddings for short Russian chatbot messages to measure semantic similarity.",
                    "tool_call": {
                        "name": "sberbank-ai/sbert_large_mt_nlu_ru",
                        "arguments": {
                            "sentences": [
                                "Привет, как дела?",
                                "Здравствуйте, как вы сегодня?",
                                "Добрый день, чем могу помочь?"
                            ],
                            "padding": true,
                            "truncation": true,
                            "max_length": 24,
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 1: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 2: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 3: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 4: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.\nTool 5: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 6: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 7: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 8: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.\nTool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 10: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 11: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 12: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 13: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 15: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 16: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 17: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 18: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 19: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\", \"parameters\": {\"sentences\": {\"type\": \"array\", \"description\": \"A list of text strings representing the sentences for which sentence embeddings will be generated.\", \"required\": true, \"example\": [\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?\"]}, \"padding\": {\"type\": \"boolean\", \"description\": \"Specifies whether the input sequences should be padded to the same length. Useful for batch processing.\", \"required\": true, \"example\": true}, \"truncation\": {\"type\": \"boolean\", \"description\": \"Determines whether input sequences longer than the maximum length should be truncated.\", \"required\": true, \"example\": true}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to which each input sequence will be truncated or padded.\", \"required\": true, \"example\": 24}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the format of the returned tensors, e.g., 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool produces numerical vector representations (embeddings) of input sentences. The response is a tensor or multidimensional array where:  \\n\\n- Each row corresponds to one input sentence.  \\n- Each row contains a fixed-length vector of floating-point numbers representing the semantic embedding of that sentence.  \\n- The embeddings capture the contextual meaning of the sentences in the Russian language.  \\n\\nThe response does **not** include labels or classifications; it is purely a feature representation suitable for downstream tasks such as semantic similarity, clustering, or retrieval.  \\n\\nThe tensor shape depends on the number of input sentences and the hidden size of the model, but conceptually it is a 2-dimensional array: `[number of sentences, embedding dimension]`.  \\n\\nNo additional metadata is returned by the model in this context; all relevant information is contained in the numeric sentence embeddings.\", \"examples\": [{\"user_intent\": \"Generate embeddings for a list of Russian customer feedback sentences so I can cluster them by topic.\", \"tool_call\": {\"name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"arguments\": {\"sentences\": [\"\\u041e\\u0447\\u0435\\u043d\\u044c \\u0434\\u043e\\u0432\\u043e\\u043b\\u0435\\u043d \\u043a\\u0430\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e\\u043c \\u043e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u044f.\", \"\\u0414\\u043e\\u0441\\u0442\\u0430\\u0432\\u043a\\u0430 \\u0437\\u0430\\u043d\\u044f\\u043b\\u0430 \\u0441\\u043b\\u0438\\u0448\\u043a\\u043e\\u043c \\u043c\\u043d\\u043e\\u0433\\u043e \\u0432\\u0440\\u0435\\u043c\\u0435\\u043d\\u0438.\", \"\\u041f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442 \\u0441\\u043e\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442 \\u043e\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u044e.\"], \"padding\": true, \"truncation\": true, \"max_length\": 32, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Create vector representations for Russian legal documents to enable semantic search.\", \"tool_call\": {\"name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"arguments\": {\"sentences\": [\"\\u0414\\u043e\\u0433\\u043e\\u0432\\u043e\\u0440 \\u043a\\u0443\\u043f\\u043b\\u0438-\\u043f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438 \\u0432\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442 \\u0432 \\u0441\\u0438\\u043b\\u0443 \\u0441 \\u043c\\u043e\\u043c\\u0435\\u043d\\u0442\\u0430 \\u043f\\u043e\\u0434\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u044f.\", \"\\u0421\\u0442\\u043e\\u0440\\u043e\\u043d\\u044b \\u043e\\u0431\\u044f\\u0437\\u0443\\u044e\\u0442\\u0441\\u044f \\u0441\\u043e\\u0431\\u043b\\u044e\\u0434\\u0430\\u0442\\u044c \\u043a\\u043e\\u043d\\u0444\\u0438\\u0434\\u0435\\u043d\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u0438.\", \"\\u041f\\u0440\\u0430\\u0432\\u0430 \\u0438 \\u043e\\u0431\\u044f\\u0437\\u0430\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438 \\u0441\\u0442\\u043e\\u0440\\u043e\\u043d \\u043e\\u043f\\u0440\\u0435\\u0434\\u0435\\u043b\\u044f\\u044e\\u0442\\u0441\\u044f \\u043d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0438\\u043c \\u0441\\u043e\\u0433\\u043b\\u0430\\u0448\\u0435\\u043d\\u0438\\u0435\\u043c.\"], \"padding\": true, \"truncation\": true, \"max_length\": 64, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Obtain embeddings for short Russian chatbot messages to measure semantic similarity.\", \"tool_call\": {\"name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"arguments\": {\"sentences\": [\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?\", \"\\u0417\\u0434\\u0440\\u0430\\u0432\\u0441\\u0442\\u0432\\u0443\\u0439\\u0442\\u0435, \\u043a\\u0430\\u043a \\u0432\\u044b \\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f?\", \"\\u0414\\u043e\\u0431\\u0440\\u044b\\u0439 \\u0434\\u0435\\u043d\\u044c, \\u0447\\u0435\\u043c \\u043c\\u043e\\u0433\\u0443 \\u043f\\u043e\\u043c\\u043e\\u0447\\u044c?\"], \"padding\": true, \"truncation\": true, \"max_length\": 24, \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nDetermine if one Russian sentence logically contradicts the information provided by another Russian sentence.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Check whether one Russian sentence contradicts another.",
                    "tool_description": "Use a Russian-language natural language inference model to predict the logical relationship between two sentences, classifying them as entailment, contradiction, or neutral.",
                    "expected_response": "A classification label indicating the relationship between the two sentences: 'entailment', 'contradiction', or 'neutral', possibly with associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "sentence_1": "The first Russian sentence whose logical relationship is being evaluated.",
                "sentence_2": "The second Russian sentence to compare against the first for entailment, contradiction, or neutrality."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_426",
        "query": "Guide me to create a quiz project where I will summarize an article into a paragraph and from the summary I will develop a question with some multiple options. I need to check the correct answer for that question.",
        "instruction": "Given a `quiz and question generation` task, retrieve tools that support summarizing text content and developing questions with multiple choices by processing articles to enable effective content review and assess understanding through generated questions and verified answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-cased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_cased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\nprint(result)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BookCorpus",
                                "accuracy": "N/A"
                            },
                            {
                                "name": "English Wikipedia",
                                "accuracy": "N/A"
                            }
                        ]
                    },
                    "description": "BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.",
                    "id": "huggingface_api_486",
                    "name": "bert-large-cased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_482",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 2: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 3: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 4: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 5: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 6: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 9: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 10: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 11: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 12: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 14: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 16: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 17: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 2: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 3: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 4: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 5: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 6: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 9: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 10: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 11: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 12: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 14: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 16: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 17: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nGuide me to create a quiz project where I will summarize an article into a paragraph and from the summary I will develop a question with some multiple options. I need to check the correct answer for that question.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to summarize an article into a paragraph.",
                    "tool_description": "Use a summarization model to generate a concise summary of an article or longer text input.",
                    "expected_response": "A brief summary of the provided article or document."
                },
                {
                    "user_intent": "I want to create a question from the summary of the article.",
                    "tool_description": "Generate a question based on the given summary or context, typically derived from key information within the text.",
                    "expected_response": "A natural language question that can be answered based on the summary provided."
                },
                {
                    "user_intent": "I want to generate multiple options for the question.",
                    "tool_description": "Create a set of multiple-choice options, including one correct and several distractors, based on the generated question.",
                    "expected_response": "A list of possible answer choices, including one correct answer and multiple incorrect answers."
                },
                {
                    "user_intent": "I need to check the correct answer for the generated question.",
                    "tool_description": "Use a question-answering model to identify the correct answer from a context or passage.",
                    "expected_response": "The correct answer for the question based on the context, along with its metadata (e.g., confidence score)."
                }
            ],
            "extracted_arguments": {
                "article": "The full text or document to be summarized.",
                "summary": "A brief version of the article or document that contains the main points.",
                "question": "The natural language question generated from the summary.",
                "options": "The multiple answer choices, including the correct answer and distractors.",
                "context": "The text or information used to verify the correct answer for the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_141",
        "query": "Create an object detector that can detect blood cells in an image, such as platelets, red blood cells, and white blood cells.",
        "instruction": "Given an `object detection` task for identifying blood cells in images, retrieve tools that leverage computer vision models specifically designed to detect various blood cell types, such as platelets, red blood cells, and white blood cells. These tools should process image inputs using advanced architectures to provide accurate and reliable detection results aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Blood Cell Detection",
                    "api_call": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": "ultralyticsplus==0.0.23 ultralytics==8.0.21",
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.893
                    },
                    "description": "This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_232",
                    "name": "keremberke/yolov8n-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_228",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.",
            "Tool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 3: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.",
            "Tool 4: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 7: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 8: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 10: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 11: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 12: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.",
            "Tool 13: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 15: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 16: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 17: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 18: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 19: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Any detections beyond this number will be ignored.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The response from the tool would likely contain the following components, based on the provided documentation:\n\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\n   - **Label or Index**: A unique identifier or label associated with each detection.\n\n2. **Metadata**: Information regarding the detection process, which might include:\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\n   - **Inference Time**: The time taken to process the image and produce results.\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\n\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\n\n4. **Performance Information (optional)**: Information on the model's performance, including:\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\n\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.",
            "examples": [
                {
                    "user_intent": "Detect and count all red blood cells in this microscope image.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/microscope_rbc_sample.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a blood smear image to identify WBCs, RBCs, and platelets.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/blood_smear_full.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Quickly detect blood cells in a low-resolution image with a higher confidence threshold.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 300,
                            "image": "https://example.com/low_res_blood_sample.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.\nTool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 3: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.\nTool 4: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 7: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 8: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 10: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 11: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 12: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.\nTool 13: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 15: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 16: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 17: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 18: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 19: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8-based model to detect blood cells\\u2014including platelets, red blood cells, and white blood cells\\u2014in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Any detections beyond this number will be ignored.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The response from the tool would likely contain the following components, based on the provided documentation:\\n\\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\\n   - **Label or Index**: A unique identifier or label associated with each detection.\\n\\n2. **Metadata**: Information regarding the detection process, which might include:\\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\\n   - **Inference Time**: The time taken to process the image and produce results.\\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\\n\\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\\n\\n4. **Performance Information (optional)**: Information on the model's performance, including:\\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\\n\\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.\", \"examples\": [{\"user_intent\": \"Detect and count all red blood cells in this microscope image.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/microscope_rbc_sample.jpg\"}}}, {\"user_intent\": \"Analyze a blood smear image to identify WBCs, RBCs, and platelets.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/blood_smear_full.jpg\"}}}, {\"user_intent\": \"Quickly detect blood cells in a low-resolution image with a higher confidence threshold.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 300, \"image\": \"https://example.com/low_res_blood_sample.jpg\"}}}]}\n\n# User Query\nCreate an object detector that can detect blood cells in an image, such as platelets, red blood cells, and white blood cells.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect blood cells including platelets, red blood cells, and white blood cells in an image.",
                    "tool_description": "Use a YOLOv8-based object detection model trained specifically for blood cell identification to analyze an image and detect platelets, RBCs, and WBCs with configurable detection parameters.",
                    "expected_response": "A structured list of detected blood cells with object classes, bounding box coordinates, confidence scores, and optionally a visual overlay on the input image."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to be analyzed for blood cell detection.",
                "conf": "The confidence threshold for detecting blood cells; detections below this value will be ignored.",
                "iou": "The Intersection Over Union threshold used for Non-Maximum Suppression to filter overlapping detections.",
                "agnostic_nms": "A boolean flag indicating whether NMS should be class-agnostic or respect class labels.",
                "max_det": "The maximum number of detections allowed per image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_170",
        "query": "A company has collected data about fish measurements and weights. We need to estimate a fish's weight based on its measurements.",
        "instruction": "Given a `weight estimation` task, retrieve tools that perform regression analysis by processing measurement data to predict or estimate a fish's weight, leveraging models like GradientBoostingRegressor to achieve the desired prediction accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "GradientBoostingRegressor",
                    "api_call": "skops.io.load('path_to_folder/example.pkl')",
                    "api_arguments": {
                        "model_path": "path_to_folder/example.pkl"
                    },
                    "python_environment_requirements": {
                        "skops.hub_utils": "download",
                        "skops.io": "load"
                    },
                    "example_code": "from skops.hub_utils import download\nfrom skops.io import load\ndownload('brendenc/Fish-Weight', 'path_to_folder')\nmodel = load('path_to_folder/example.pkl')",
                    "performance": {
                        "dataset": "Fish dataset",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.",
                    "id": "huggingface_api_886",
                    "name": "Fish-Weight"
                },
                "id": "gorilla_huggingface_tool_879",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "Tool 1: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 4: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 5: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 7: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 8: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 9: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 10: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 11: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 13: Provides an unofficial mirror of the OFA model weights for visual question answering, allowing faster downloads outside of China and enabling multimodal question-answering tasks using the Hugging Face Transformers framework.",
            "Tool 14: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 15: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 18: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 19: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "parameters": {
                "model_path": {
                    "type": "string",
                    "description": "The file path to the saved GradientBoostingRegressor model in .pkl format that will be loaded using skops.io.load.",
                    "required": true,
                    "example": "path_to_folder/example.pkl"
                }
            },
            "response": "The tool’s response is a deserialized `GradientBoostingRegressor` model object loaded from a file. Specifically, after calling the load function on the specified model path, the response contains a fully instantiated scikit-learn regressor with the following characteristics:\n\n- **Model Type**: `GradientBoostingRegressor` — a regression model that uses gradient boosting on decision trees to predict continuous numerical values.  \n- **Model Parameters and State**: The loaded object includes all trained parameters of the model, such as the ensemble of decision trees, learning rate, number of estimators, tree depths, and any other hyperparameters that were used during training.  \n- **Fitted Data Attributes**: The model contains the internal state resulting from training on the fish dataset, including learned feature importances, fitted trees, and any statistics necessary to make predictions on new input data.  \n- **Methods**: As a scikit-learn regressor, the object provides standard methods such as:\n  - `predict(X)`: to generate predictions on new feature data.\n  - `score(X, y)`: to evaluate performance on labeled data.\n  - `get_params()` and `set_params()`: to inspect or modify hyperparameters.  \n- **Intended Use**: The object is ready for inference or experimentation and is primarily for educational purposes on tabular regression tasks.  \n\nIn summary, the response is a fully ready-to-use `GradientBoostingRegressor` instance containing all trained information, capable of performing predictions on tabular input features consistent with the training dataset.",
            "examples": [
                {
                    "user_intent": "I want to load the pre-trained Gradient Boosting model for predicting fish weights from the example folder.",
                    "tool_call": {
                        "name": "Fish-Weight",
                        "arguments": {
                            "model_path": "path_to_folder/example.pkl"
                        }
                    }
                },
                {
                    "user_intent": "Load the fish weight prediction model so I can test it on my own dataset of fish measurements.",
                    "tool_call": {
                        "name": "Fish-Weight",
                        "arguments": {
                            "model_path": "path_to_folder/example.pkl"
                        }
                    }
                },
                {
                    "user_intent": "Retrieve the GradientBoostingRegressor model that was trained on the Fish dataset for educational experiments.",
                    "tool_call": {
                        "name": "Fish-Weight",
                        "arguments": {
                            "model_path": "path_to_folder/example.pkl"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\nTool 1: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 4: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 5: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 7: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 8: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 9: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 10: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 11: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 13: Provides an unofficial mirror of the OFA model weights for visual question answering, allowing faster downloads outside of China and enabling multimodal question-answering tasks using the Hugging Face Transformers framework.\nTool 14: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 15: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 18: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 19: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\", \"parameters\": {\"model_path\": {\"type\": \"string\", \"description\": \"The file path to the saved GradientBoostingRegressor model in .pkl format that will be loaded using skops.io.load.\", \"required\": true, \"example\": \"path_to_folder/example.pkl\"}}, \"response\": \"The tool\\u2019s response is a deserialized `GradientBoostingRegressor` model object loaded from a file. Specifically, after calling the load function on the specified model path, the response contains a fully instantiated scikit-learn regressor with the following characteristics:\\n\\n- **Model Type**: `GradientBoostingRegressor` \\u2014 a regression model that uses gradient boosting on decision trees to predict continuous numerical values.  \\n- **Model Parameters and State**: The loaded object includes all trained parameters of the model, such as the ensemble of decision trees, learning rate, number of estimators, tree depths, and any other hyperparameters that were used during training.  \\n- **Fitted Data Attributes**: The model contains the internal state resulting from training on the fish dataset, including learned feature importances, fitted trees, and any statistics necessary to make predictions on new input data.  \\n- **Methods**: As a scikit-learn regressor, the object provides standard methods such as:\\n  - `predict(X)`: to generate predictions on new feature data.\\n  - `score(X, y)`: to evaluate performance on labeled data.\\n  - `get_params()` and `set_params()`: to inspect or modify hyperparameters.  \\n- **Intended Use**: The object is ready for inference or experimentation and is primarily for educational purposes on tabular regression tasks.  \\n\\nIn summary, the response is a fully ready-to-use `GradientBoostingRegressor` instance containing all trained information, capable of performing predictions on tabular input features consistent with the training dataset.\", \"examples\": [{\"user_intent\": \"I want to load the pre-trained Gradient Boosting model for predicting fish weights from the example folder.\", \"tool_call\": {\"name\": \"Fish-Weight\", \"arguments\": {\"model_path\": \"path_to_folder/example.pkl\"}}}, {\"user_intent\": \"Load the fish weight prediction model so I can test it on my own dataset of fish measurements.\", \"tool_call\": {\"name\": \"Fish-Weight\", \"arguments\": {\"model_path\": \"path_to_folder/example.pkl\"}}}, {\"user_intent\": \"Retrieve the GradientBoostingRegressor model that was trained on the Fish dataset for educational experiments.\", \"tool_call\": {\"name\": \"Fish-Weight\", \"arguments\": {\"model_path\": \"path_to_folder/example.pkl\"}}}]}\n\n# User Query\nA company has collected data about fish measurements and weights. We need to estimate a fish's weight based on its measurements.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained model to predict fish weight from measurements.",
                    "tool_description": "Load a pre-trained GradientBoostingRegressor model designed for predicting fish weight from tabular measurement data, intended for educational and experimentation purposes.",
                    "expected_response": "A fully instantiated GradientBoostingRegressor object with trained parameters and methods ready for prediction on new fish measurement data."
                },
                {
                    "user_intent": "Use the loaded model to estimate fish weight from collected measurements.",
                    "tool_description": "Apply the loaded GradientBoostingRegressor model to input tabular features representing fish measurements to predict their corresponding weights.",
                    "expected_response": "Numerical weight predictions for each fish based on its input measurements."
                }
            ],
            "extracted_arguments": {
                "model_path": "The file path to the saved pre-trained GradientBoostingRegressor model in .pkl format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_307",
        "query": "We want to make sure clarify some questions about the legal implications of a new partnership contract for a real estate development project.",
        "instruction": "Given a `legal question answering` task, retrieve tools that specialize in processing natural language questions about legal topics by extracting relevant information and providing detailed answers based on legal documents or contracts.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')",
                    "api_arguments": {
                        "tokenizer": "AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": {
                        "import": "from transformers import AutoTokenizer, AutoModelForQuestionAnswering",
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)",
                        "model": "model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)"
                    },
                    "performance": {
                        "dataset": "cuad",
                        "accuracy": "46.6%"
                    },
                    "description": "This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.",
                    "id": "huggingface_api_469",
                    "name": "Rakib/roberta-base-on-cuad"
                },
                "id": "gorilla_huggingface_tool_465",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 2: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 4: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 5: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 11: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 13: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 15: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 16: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset."
        ],
        "tool_view": {
            "description": "LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pre-trained model to load. This specifies which version of the LEGAL-BERT model will be used for fill-mask tasks.",
                    "required": true,
                    "example": "nlpaueb/legal-bert-small-uncased"
                }
            },
            "response": "The tool performs a **fill-mask task** using a lightweight BERT model trained specifically on legal text. Its response is not explicitly defined in the JSON, but based on standard fill-mask functionality in Transformers, a typical response would have the following structure:\n\n- **Predictions List**: A list or array of candidate tokens or words that could replace the masked token in the input text.\n  - Each entry represents a single candidate replacement.\n  \n- **Each Prediction Entry** typically contains:\n  1. **Token**: The actual token or word that the model predicts as a possible replacement for the mask.\n  2. **Token ID**: The numerical identifier corresponding to the token in the model’s vocabulary.\n  3. **Score / Probability**: A numerical value representing the model’s confidence or likelihood that this token correctly fills the masked position. Higher values indicate higher confidence.\n  4. **Sequence**: (Optional, depending on the implementation) The full input sentence with the mask replaced by the predicted token, showing the context with the prediction applied.\n\n- **Overall Semantics**: The response ranks the candidate tokens by likelihood, allowing users to see the most probable words the model would use to complete the masked position. This output can be used for tasks like legal text completion, suggestion, or analysis in the legal domain.\n\nIn essence, the response provides **a ranked set of possible words to fill the masked token, along with their confidence scores and optional reconstructed sequences**.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing legal term in this contract sentence: 'The tenant shall pay the ___ on time.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The tenant shall pay the [MASK] on time."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank in this legal statement: 'The court hereby ___ the motion for summary judgment.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The court hereby [MASK] the motion for summary judgment."
                        }
                    }
                },
                {
                    "user_intent": "Complete the missing word in this clause from a lease agreement: 'The landlord is responsible for maintaining the ___ of the property.'",
                    "tool_call": {
                        "name": "nlpaueb/legal-bert-small-uncased",
                        "arguments": {
                            "pretrained_model_name_or_path": "nlpaueb/legal-bert-small-uncased",
                            "input_text": "The landlord is responsible for maintaining the [MASK] of the property."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 2: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 4: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 5: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 9: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 11: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 13: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 15: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 16: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained model to load. This specifies which version of the LEGAL-BERT model will be used for fill-mask tasks.\", \"required\": true, \"example\": \"nlpaueb/legal-bert-small-uncased\"}}, \"response\": \"The tool performs a **fill-mask task** using a lightweight BERT model trained specifically on legal text. Its response is not explicitly defined in the JSON, but based on standard fill-mask functionality in Transformers, a typical response would have the following structure:\\n\\n- **Predictions List**: A list or array of candidate tokens or words that could replace the masked token in the input text.\\n  - Each entry represents a single candidate replacement.\\n  \\n- **Each Prediction Entry** typically contains:\\n  1. **Token**: The actual token or word that the model predicts as a possible replacement for the mask.\\n  2. **Token ID**: The numerical identifier corresponding to the token in the model\\u2019s vocabulary.\\n  3. **Score / Probability**: A numerical value representing the model\\u2019s confidence or likelihood that this token correctly fills the masked position. Higher values indicate higher confidence.\\n  4. **Sequence**: (Optional, depending on the implementation) The full input sentence with the mask replaced by the predicted token, showing the context with the prediction applied.\\n\\n- **Overall Semantics**: The response ranks the candidate tokens by likelihood, allowing users to see the most probable words the model would use to complete the masked position. This output can be used for tasks like legal text completion, suggestion, or analysis in the legal domain.\\n\\nIn essence, the response provides **a ranked set of possible words to fill the masked token, along with their confidence scores and optional reconstructed sequences**.\", \"examples\": [{\"user_intent\": \"I want to predict the missing legal term in this contract sentence: 'The tenant shall pay the ___ on time.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The tenant shall pay the [MASK] on time.\"}}}, {\"user_intent\": \"Fill in the blank in this legal statement: 'The court hereby ___ the motion for summary judgment.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The court hereby [MASK] the motion for summary judgment.\"}}}, {\"user_intent\": \"Complete the missing word in this clause from a lease agreement: 'The landlord is responsible for maintaining the ___ of the property.'\", \"tool_call\": {\"name\": \"nlpaueb/legal-bert-small-uncased\", \"arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\", \"input_text\": \"The landlord is responsible for maintaining the [MASK] of the property.\"}}}]}\n\n# User Query\nWe want to make sure clarify some questions about the legal implications of a new partnership contract for a real estate development project.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to clarify the legal implications of a partnership contract for a real estate project.",
                    "tool_description": "Analyze the legal text of a partnership contract to provide insights into potential legal implications for the involved parties in a real estate development project.",
                    "expected_response": "A detailed breakdown of legal terms, clauses, and their potential implications, including obligations, liabilities, and legal risks."
                }
            ],
            "extracted_arguments": {
                "contract_type": "The type of legal document to be analyzed (e.g., partnership contract).",
                "project_type": "The type of project the contract is related to (e.g., real estate development)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_285",
        "query": "Our research team is focused on the analysis and separation of complex audio recordings. We need a model to be able to separate speaker voices from mixed sound.",
        "instruction": "Given a `speaker separation` task, retrieve tools that specialize in processing complex audio recordings to efficiently separate and isolate individual speaker voices, leveraging advanced audio processing models and techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "ConvTasNet_Libri3Mix_sepclean_8k()",
                    "api_arguments": {
                        "n_src": 3,
                        "sample_rate": 8000,
                        "segment": 3,
                        "task": "sep_clean",
                        "train_dir": "data/wav8k/min/train-360",
                        "valid_dir": "data/wav8k/min/dev",
                        "kernel_size": 16,
                        "n_filters": 512,
                        "stride": 8,
                        "bn_chan": 128,
                        "hid_chan": 512,
                        "mask_act": "relu",
                        "n_blocks": 8,
                        "n_repeats": 3,
                        "skip_chan": 128,
                        "lr": 0.001,
                        "optimizer": "adam",
                        "weight_decay": 0.0,
                        "batch_size": 24,
                        "early_stop": true,
                        "epochs": 200,
                        "half_lr": true,
                        "num_workers": 4
                    },
                    "python_environment_requirements": "Asteroid",
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri3Mix",
                        "accuracy": {
                            "si_sdr": 8.581797049575108,
                            "si_sdr_imp": 11.977037288467368,
                            "sdr": 9.305885208641385,
                            "sdr_imp": 12.3943409734845,
                            "sir": 16.42030534048559,
                            "sir_imp": 19.508759460400984,
                            "sar": 10.641943911079238,
                            "sar_imp": -56.4345187842095,
                            "stoi": 0.8365148408724333,
                            "stoi_imp": 0.24401766199806396
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset.",
                    "id": "huggingface_api_796",
                    "name": "ConvTasNet_Libri3Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_791",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 1: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 2: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 3: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 4: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 5: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.",
            "Tool 6: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 7: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 8: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "Tool 9: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 11: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 12: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 13: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).",
            "Tool 14: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 15: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 16: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 17: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 18: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis."
        ],
        "tool_view": {
            "description": "This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "parameters": {
                "audio_file": {
                    "type": "string",
                    "description": "The path or URL to the input audio file that will be processed for source separation.",
                    "required": true,
                    "example": "path/to/audio.wav"
                }
            },
            "response": "The tool performs audio source separation on a given input audio file and returns a structured output containing the separated audio sources. The typical response structure would include the following components:\n\n- **sources**: A collection of audio signals, each representing one separated source extracted from the input audio. Each source is typically returned as a waveform array or tensor, depending on the underlying framework.  \n- **sample_rate** (optional, depending on implementation): The sampling rate of the returned audio signals, indicating how the waveform arrays correspond to time.  \n- **metadata** (optional, depending on implementation): Additional information about the separation process, such as the number of sources detected, model version, or processing details.  \n\nIn essence, the primary content of the response is the individual audio streams that correspond to distinct sources in the original input, allowing downstream tasks such as playback, analysis, or further audio processing.",
            "examples": [
                {
                    "user_intent": "Separate the sources from this mixed audio file.",
                    "tool_call": {
                        "name": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean",
                        "arguments": {
                            "audio_file": "path_to_audio_file.wav"
                        }
                    }
                },
                {
                    "user_intent": "I have an audio file with multiple sources. Can you extract them separately?",
                    "tool_call": {
                        "name": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean",
                        "arguments": {
                            "audio_file": "path_to_mixed_audio.wav"
                        }
                    }
                },
                {
                    "user_intent": "Can you process this audio file to improve source separation?",
                    "tool_call": {
                        "name": "mpariente/DPRNNTasNet-ks2_WHAM_sepclean",
                        "arguments": {
                            "audio_file": "mixed_audio_sample.mp3"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 1: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 2: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 3: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 4: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 5: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.\nTool 6: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 7: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 8: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\nTool 9: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 11: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 12: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 13: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).\nTool 14: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 15: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 16: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 17: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 18: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 19: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\", \"parameters\": {\"audio_file\": {\"type\": \"string\", \"description\": \"The path or URL to the input audio file that will be processed for source separation.\", \"required\": true, \"example\": \"path/to/audio.wav\"}}, \"response\": \"The tool performs audio source separation on a given input audio file and returns a structured output containing the separated audio sources. The typical response structure would include the following components:\\n\\n- **sources**: A collection of audio signals, each representing one separated source extracted from the input audio. Each source is typically returned as a waveform array or tensor, depending on the underlying framework.  \\n- **sample_rate** (optional, depending on implementation): The sampling rate of the returned audio signals, indicating how the waveform arrays correspond to time.  \\n- **metadata** (optional, depending on implementation): Additional information about the separation process, such as the number of sources detected, model version, or processing details.  \\n\\nIn essence, the primary content of the response is the individual audio streams that correspond to distinct sources in the original input, allowing downstream tasks such as playback, analysis, or further audio processing.\", \"examples\": [{\"user_intent\": \"Separate the sources from this mixed audio file.\", \"tool_call\": {\"name\": \"mpariente/DPRNNTasNet-ks2_WHAM_sepclean\", \"arguments\": {\"audio_file\": \"path_to_audio_file.wav\"}}}, {\"user_intent\": \"I have an audio file with multiple sources. Can you extract them separately?\", \"tool_call\": {\"name\": \"mpariente/DPRNNTasNet-ks2_WHAM_sepclean\", \"arguments\": {\"audio_file\": \"path_to_mixed_audio.wav\"}}}, {\"user_intent\": \"Can you process this audio file to improve source separation?\", \"tool_call\": {\"name\": \"mpariente/DPRNNTasNet-ks2_WHAM_sepclean\", \"arguments\": {\"audio_file\": \"mixed_audio_sample.mp3\"}}}]}\n\n# User Query\nOur research team is focused on the analysis and separation of complex audio recordings. We need a model to be able to separate speaker voices from mixed sound.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to separate individual speaker voices from a mixed audio recording.",
                    "tool_description": "Perform audio source separation on a mixed audio file to isolate individual speaker voices, using a pretrained model suitable for clean speech separation.",
                    "expected_response": "A structured output containing the separated audio streams, each representing one speaker, with optional sample rate and metadata about the separation process."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path or URL to the input audio file containing mixed speaker voices."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_167",
        "query": "We are building an automatic video generation platform based on user-provided text. We need a reliable model to convert text instructions into appropriate videos.",
        "instruction": "Given a `text-to-video conversion` task, retrieve tools that transform text inputs into corresponding video outputs by utilizing models capable of generating multimedia content based on the provided text instructions and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video",
                    "api_call": "pipeline('text-to-video', model='ImRma/Brucelee')",
                    "api_arguments": [
                        "your_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Hugging Face model for converting Persian and English text into video.",
                    "id": "huggingface_api_92",
                    "name": "ImRma/Brucelee"
                },
                "id": "gorilla_huggingface_tool_92",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 2: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 3: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 4: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 5: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.",
            "Tool 6: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 7: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 8: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 9: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 10: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 12: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 13: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 14: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 15: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 16: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.",
            "Tool 17: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 18: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios."
        ],
        "tool_view": {
            "description": "Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A textual description of the scene or action to be generated in the video.",
                    "required": true,
                    "example": "a magical princess is playing guitar, modern disney style"
                },
                "video_length": {
                    "type": "int",
                    "description": "The duration of the generated video in seconds or frames, depending on the model implementation.",
                    "required": true,
                    "example": 8
                },
                "height": {
                    "type": "int",
                    "description": "The height of the output video in pixels.",
                    "required": true,
                    "example": 512
                },
                "width": {
                    "type": "int",
                    "description": "The width of the output video in pixels.",
                    "required": true,
                    "example": 512
                },
                "num_inference_steps": {
                    "type": "int",
                    "description": "The number of denoising steps the model uses during video generation; higher values may improve quality but increase computation time.",
                    "required": true,
                    "example": 50
                },
                "guidance_scale": {
                    "type": "float",
                    "description": "A scaling factor controlling the strength of guidance towards the prompt; higher values make the video more closely follow the prompt.",
                    "required": true,
                    "example": 7.5
                }
            },
            "response": "The tool’s response is a structured object representing the generated video(s) produced from the given textual prompt and generation parameters. The key components of the response are as follows:\n\n1. **videos**  \n   - **Type:** Collection (e.g., list or array) of video objects or video data.  \n   - **Meaning:** Each element in this collection corresponds to a video generated from the input prompt. Even if a single video is generated, it is typically returned as a single-element collection.  \n\n2. **Video object / video data**  \n   - **Type:** Video file, video tensor, or video-compatible array format.  \n   - **Meaning:** Represents the actual visual output. It can be in memory as a tensor suitable for further processing in Python (e.g., a 5D tensor with dimensions for batch, frames, channels, height, and width) or as a saved video file path/URI.  \n   - **Role:** Provides the end-user with the generated video content based on the text prompt and pipeline parameters.\n\n3. **Optional metadata (inferred)**  \n   - The response may include additional information associated with the generated video, such as:  \n     - **shape / dimensions:** Number of frames, resolution (height × width), number of channels.  \n     - **generation parameters:** Echoing the input parameters like `prompt`, `video_length`, `num_inference_steps`, and `guidance_scale` for reproducibility.  \n     - **format or codec info:** Type of video encoding (e.g., GIF, MP4) if applicable.  \n\nOverall, the response encapsulates the generated video content in a format suitable for immediate use, visualization, or saving to disk. The main, mandatory component is the `videos` collection, while any additional metadata serves to provide context or facilitate further processing.",
            "examples": [
                {
                    "user_intent": "Generate a video of a magical princess playing guitar in a modern Disney style.",
                    "tool_call": {
                        "name": "mo-di-bear-guitar",
                        "arguments": {
                            "prompt": "a magical princess is playing guitar, modern disney style",
                            "video_length": 8,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 50,
                            "guidance_scale": 7.5
                        }
                    }
                },
                {
                    "user_intent": "Create a short video of a robot dancing, in a futuristic cartoon style.",
                    "tool_call": {
                        "name": "mo-di-bear-guitar",
                        "arguments": {
                            "prompt": "a robot is dancing, futuristic cartoon style",
                            "video_length": 10,
                            "height": 720,
                            "width": 1280,
                            "num_inference_steps": 60,
                            "guidance_scale": 8.0
                        }
                    }
                },
                {
                    "user_intent": "I want to generate a video of a superhero flying through the city, in a vibrant animated style.",
                    "tool_call": {
                        "name": "mo-di-bear-guitar",
                        "arguments": {
                            "prompt": "a superhero is flying through the city, vibrant animated style",
                            "video_length": 15,
                            "height": 640,
                            "width": 640,
                            "num_inference_steps": 70,
                            "guidance_scale": 9.0
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 2: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 3: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 4: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 5: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.\nTool 6: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 7: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 8: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 9: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 10: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 12: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 13: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 14: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 15: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 16: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.\nTool 17: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 18: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Tune-A-Video (mo-di-bear-guitar) is a Hugging Face\\u2013based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney\\u2013inspired animations, from descriptive text inputs.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A textual description of the scene or action to be generated in the video.\", \"required\": true, \"example\": \"a magical princess is playing guitar, modern disney style\"}, \"video_length\": {\"type\": \"int\", \"description\": \"The duration of the generated video in seconds or frames, depending on the model implementation.\", \"required\": true, \"example\": 8}, \"height\": {\"type\": \"int\", \"description\": \"The height of the output video in pixels.\", \"required\": true, \"example\": 512}, \"width\": {\"type\": \"int\", \"description\": \"The width of the output video in pixels.\", \"required\": true, \"example\": 512}, \"num_inference_steps\": {\"type\": \"int\", \"description\": \"The number of denoising steps the model uses during video generation; higher values may improve quality but increase computation time.\", \"required\": true, \"example\": 50}, \"guidance_scale\": {\"type\": \"float\", \"description\": \"A scaling factor controlling the strength of guidance towards the prompt; higher values make the video more closely follow the prompt.\", \"required\": true, \"example\": 7.5}}, \"response\": \"The tool\\u2019s response is a structured object representing the generated video(s) produced from the given textual prompt and generation parameters. The key components of the response are as follows:\\n\\n1. **videos**  \\n   - **Type:** Collection (e.g., list or array) of video objects or video data.  \\n   - **Meaning:** Each element in this collection corresponds to a video generated from the input prompt. Even if a single video is generated, it is typically returned as a single-element collection.  \\n\\n2. **Video object / video data**  \\n   - **Type:** Video file, video tensor, or video-compatible array format.  \\n   - **Meaning:** Represents the actual visual output. It can be in memory as a tensor suitable for further processing in Python (e.g., a 5D tensor with dimensions for batch, frames, channels, height, and width) or as a saved video file path/URI.  \\n   - **Role:** Provides the end-user with the generated video content based on the text prompt and pipeline parameters.\\n\\n3. **Optional metadata (inferred)**  \\n   - The response may include additional information associated with the generated video, such as:  \\n     - **shape / dimensions:** Number of frames, resolution (height \\u00d7 width), number of channels.  \\n     - **generation parameters:** Echoing the input parameters like `prompt`, `video_length`, `num_inference_steps`, and `guidance_scale` for reproducibility.  \\n     - **format or codec info:** Type of video encoding (e.g., GIF, MP4) if applicable.  \\n\\nOverall, the response encapsulates the generated video content in a format suitable for immediate use, visualization, or saving to disk. The main, mandatory component is the `videos` collection, while any additional metadata serves to provide context or facilitate further processing.\", \"examples\": [{\"user_intent\": \"Generate a video of a magical princess playing guitar in a modern Disney style.\", \"tool_call\": {\"name\": \"mo-di-bear-guitar\", \"arguments\": {\"prompt\": \"a magical princess is playing guitar, modern disney style\", \"video_length\": 8, \"height\": 512, \"width\": 512, \"num_inference_steps\": 50, \"guidance_scale\": 7.5}}}, {\"user_intent\": \"Create a short video of a robot dancing, in a futuristic cartoon style.\", \"tool_call\": {\"name\": \"mo-di-bear-guitar\", \"arguments\": {\"prompt\": \"a robot is dancing, futuristic cartoon style\", \"video_length\": 10, \"height\": 720, \"width\": 1280, \"num_inference_steps\": 60, \"guidance_scale\": 8.0}}}, {\"user_intent\": \"I want to generate a video of a superhero flying through the city, in a vibrant animated style.\", \"tool_call\": {\"name\": \"mo-di-bear-guitar\", \"arguments\": {\"prompt\": \"a superhero is flying through the city, vibrant animated style\", \"video_length\": 15, \"height\": 640, \"width\": 640, \"num_inference_steps\": 70, \"guidance_scale\": 9.0}}}]}\n\n# User Query\nWe are building an automatic video generation platform based on user-provided text. We need a reliable model to convert text instructions into appropriate videos.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert user-provided text instructions into videos automatically.",
                    "tool_description": "Generate videos from textual descriptions using a Hugging Face–based text-to-video model, allowing control over video length, resolution, and generation parameters to produce coherent and visually accurate videos.",
                    "expected_response": "A structured object containing the generated video(s) corresponding to the input text prompt, including optional metadata such as dimensions, frame count, and generation parameters."
                }
            ],
            "extracted_arguments": {
                "prompt": "The textual description of the scene or action to generate in the video.",
                "video_length": "The intended duration of the generated video in seconds or frames.",
                "height": "The height of the output video in pixels.",
                "width": "The width of the output video in pixels.",
                "num_inference_steps": "The number of denoising steps used during video generation to control quality.",
                "guidance_scale": "A scaling factor controlling how closely the video follows the text prompt."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_261",
        "query": "Create a method to determine, as an entertainment recommendation system, which category a text message about a daily activity belongs to.",
        "instruction": "Given a `text classification` task, retrieve tools that utilize zero-shot learning models to categorize text messages related to daily activities into specific entertainment categories by analyzing the content and predicting the most relevant category without predefined labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "NLI-based Zero Shot Text Classification",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')",
                    "api_arguments": {
                        "sequence_to_classify": "one day I will see the world",
                        "candidate_labels": "['travel', 'cooking', 'dancing']"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
                    },
                    "example_code": {
                        "with_pipeline": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\nsequence_to_classify = 'one day I will see the world'\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)",
                        "with_manual_pytorch": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\npremise = sequence\nhypothesis = f'This example is {label}.'\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]"
                    },
                    "performance": {
                        "dataset": "multi_nli",
                        "accuracy": "Not specified"
                    },
                    "description": "This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.",
                    "id": "huggingface_api_496",
                    "name": "facebook/bart-large-mnli"
                },
                "id": "gorilla_huggingface_tool_492",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 1: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 2: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.",
            "Tool 3: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 5: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 6: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 7: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 8: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 9: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 10: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 11: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 12: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 14: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 15: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 16: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 17: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 18: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents."
        ],
        "tool_view": {
            "description": "The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.",
                    "required": true,
                    "example": "nateraw/videomae-base-finetuned-ucf101"
                }
            },
            "response": "The tool’s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\n\n1. **logits**:  \n   - A tensor containing raw, unnormalized scores for each possible action class in the model’s label set.  \n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \n   - These scores represent the model’s confidence for each action before applying any normalization such as softmax.\n\n2. **predicted label (optional convenience mapping)**:  \n   - A single integer or string representing the action class with the highest score.  \n   - If an integer, it indexes into the model’s internal `id2label` mapping.  \n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\"Archery\"`).  \n\n3. **id2label mapping (metadata)**:  \n   - A dictionary mapping integer class indices to human-readable labels.  \n   - Used to interpret the predicted label in a readable format.  \n\nIn practice, the typical response workflow is:\n\n- The model returns the `logits` tensor.  \n- The user selects the class with the maximum score to obtain the `predicted_label`.  \n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \n\nNo additional fields are defined by the tool; the response focuses on the model’s prediction scores and derived class label for the input video clip.",
            "examples": [
                {
                    "user_intent": "I want to classify a video of a person performing an action using a pre-trained model.",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "Can I recognize actions in my video using a Hugging Face model for action classification?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 1: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 2: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.\nTool 3: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 5: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 6: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 7: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 8: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 9: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 10: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 11: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 12: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 14: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 15: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 16: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 17: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 18: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"videomae-base-finetuned-ucf101\\\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.\", \"required\": true, \"example\": \"nateraw/videomae-base-finetuned-ucf101\"}}, \"response\": \"The tool\\u2019s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\\n\\n1. **logits**:  \\n   - A tensor containing raw, unnormalized scores for each possible action class in the model\\u2019s label set.  \\n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \\n   - These scores represent the model\\u2019s confidence for each action before applying any normalization such as softmax.\\n\\n2. **predicted label (optional convenience mapping)**:  \\n   - A single integer or string representing the action class with the highest score.  \\n   - If an integer, it indexes into the model\\u2019s internal `id2label` mapping.  \\n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\\\"Archery\\\"`).  \\n\\n3. **id2label mapping (metadata)**:  \\n   - A dictionary mapping integer class indices to human-readable labels.  \\n   - Used to interpret the predicted label in a readable format.  \\n\\nIn practice, the typical response workflow is:\\n\\n- The model returns the `logits` tensor.  \\n- The user selects the class with the maximum score to obtain the `predicted_label`.  \\n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \\n\\nNo additional fields are defined by the tool; the response focuses on the model\\u2019s prediction scores and derived class label for the input video clip.\", \"examples\": [{\"user_intent\": \"I want to classify a video of a person performing an action using a pre-trained model.\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"Can I recognize actions in my video using a Hugging Face model for action classification?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}]}\n\n# User Query\nCreate a method to determine, as an entertainment recommendation system, which category a text message about a daily activity belongs to.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify a text message about a daily activity into a category.",
                    "tool_description": "Generate a classification of the provided text message, categorizing it based on predefined activity types or themes.",
                    "expected_response": "A classification label that represents the category or theme of the activity described in the text message."
                }
            ],
            "extracted_arguments": {
                "text_message": "The content of the text message describing a daily activity that needs to be categorized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_156",
        "query": "We are a company that provides transcription services. We require voice activity detection in our audio recordings.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that specialize in analyzing audio recordings to identify and segment periods of speech activity by using models trained on relevant datasets to enhance transcription services.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Voice Activity Detection",
                    "api_call": "Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "pyannote.audio 2.1.1",
                    "example_code": "from pyannote.audio import Model\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": "N/A"
                    },
                    "description": "Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.",
                    "id": "huggingface_api_832",
                    "name": "popcornell/pyannote-segmentation-chime6-mixer6"
                },
                "id": "gorilla_huggingface_tool_827",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 1: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 2: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 3: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 4: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 7: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 8: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 9: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 11: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).",
            "Tool 12: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 13: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 14: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 15: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 16: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 17: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.",
            "Tool 18: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 19: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "parameters": {
                "audio_file": {
                    "type": "string",
                    "description": "The path to the audio file to be analyzed for voice activity detection.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool produces a response representing the detected voice activity segments within an input audio file. The response is structured as an object that provides a timeline of speech activity, typically containing the following components:\n\n1. **Timeline object**: This is the main structure in the response that encodes the temporal regions of the audio where speech is detected. It organizes detected speech segments in chronological order.  \n\n2. **Speech segments**: Each element in the timeline represents a contiguous period of active speech. Each segment includes:\n   - **start**: The timestamp marking the beginning of the speech segment.\n   - **end**: The timestamp marking the end of the speech segment.\n   - Optionally, **confidence or score**: Some implementations may include a confidence value indicating the likelihood that the segment contains speech.\n\n3. **Methods for accessing segments**: The response typically provides methods to query or iterate over the timeline, such as `get_timeline().support()`, which returns the collection of speech segments.\n\nIn essence, the response captures **when speech occurs in the audio** and allows programmatic access to these intervals for further processing or analysis.",
            "examples": [
                {
                    "user_intent": "I want to detect all segments where someone is speaking in a meeting recording.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "meeting_recording_2025-12-30.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this podcast episode and mark all periods of active speech.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_42.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Process an interview audio file to find where the speaker is talking versus silence.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "interview_session_15.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 1: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 2: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 3: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 4: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 7: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 8: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 9: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 11: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).\nTool 12: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 13: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 14: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 15: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 16: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 17: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.\nTool 18: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 19: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\", \"parameters\": {\"audio_file\": {\"type\": \"string\", \"description\": \"The path to the audio file to be analyzed for voice activity detection.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool produces a response representing the detected voice activity segments within an input audio file. The response is structured as an object that provides a timeline of speech activity, typically containing the following components:\\n\\n1. **Timeline object**: This is the main structure in the response that encodes the temporal regions of the audio where speech is detected. It organizes detected speech segments in chronological order.  \\n\\n2. **Speech segments**: Each element in the timeline represents a contiguous period of active speech. Each segment includes:\\n   - **start**: The timestamp marking the beginning of the speech segment.\\n   - **end**: The timestamp marking the end of the speech segment.\\n   - Optionally, **confidence or score**: Some implementations may include a confidence value indicating the likelihood that the segment contains speech.\\n\\n3. **Methods for accessing segments**: The response typically provides methods to query or iterate over the timeline, such as `get_timeline().support()`, which returns the collection of speech segments.\\n\\nIn essence, the response captures **when speech occurs in the audio** and allows programmatic access to these intervals for further processing or analysis.\", \"examples\": [{\"user_intent\": \"I want to detect all segments where someone is speaking in a meeting recording.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"meeting_recording_2025-12-30.wav\"}}}, {\"user_intent\": \"Analyze this podcast episode and mark all periods of active speech.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_42.mp3\"}}}, {\"user_intent\": \"Process an interview audio file to find where the speaker is talking versus silence.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"interview_session_15.wav\"}}}]}\n\n# User Query\nWe are a company that provides transcription services. We require voice activity detection in our audio recordings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect all segments of speech in our audio recordings for transcription purposes.",
                    "tool_description": "Perform voice activity detection on audio recordings to automatically identify segments where speech is present, enabling downstream transcription and analysis.",
                    "expected_response": "A structured timeline object indicating the start and end times of each detected speech segment within the audio file, optionally including confidence scores."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the audio recording file to be analyzed for voice activity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_77",
        "query": "Our school needs to create fill-in-the-blank quizzes for students. Is it possible to generate a fill-in-the-blank question from the following sentence: \"The cat chased the mouse and then climbed the tree.\"",
        "instruction": "Given a `fill-in-the-blank generation` task, retrieve tools that create fill-in-the-blank questions by processing sentences and providing options for missing words or phrases aligned with the educational context and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaV2ForMaskedLM.from_pretrained('microsoft/deberta-v2-xxlarge')",
                    "api_arguments": {
                        "model_name_or_path": "microsoft/deberta-v2-xxlarge"
                    },
                    "python_environment_requirements": {
                        "pip_install": [
                            "datasets",
                            "deepspeed"
                        ]
                    },
                    "example_code": "python -m torch.distributed.launch --nproc_per_node=${num_gpus} run_glue.py --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME --do_train --do_eval --max_seq_length 256 --per_device_train_batch_size ${batch_size} --learning_rate 3e-6 --num_train_epochs 3 --output_dir $output_dir --overwrite_output_dir --logging_steps 10 --logging_dir $output_dir --deepspeed ds_config.json",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQuAD 1.1",
                                "accuracy": "F1/EM: 96.1/91.4"
                            },
                            {
                                "name": "SQuAD 2.0",
                                "accuracy": "F1/EM: 92.2/89.7"
                            },
                            {
                                "name": "MNLI-m/mm",
                                "accuracy": "Acc: 91.7/91.9"
                            },
                            {
                                "name": "SST-2",
                                "accuracy": "Acc: 97.2"
                            },
                            {
                                "name": "QNLI",
                                "accuracy": "Acc: 96.0"
                            },
                            {
                                "name": "CoLA",
                                "accuracy": "MCC: 72.0"
                            },
                            {
                                "name": "RTE",
                                "accuracy": "Acc: 93.5"
                            },
                            {
                                "name": "MRPC",
                                "accuracy": "Acc/F1: 93.1/94.9"
                            },
                            {
                                "name": "QQP",
                                "accuracy": "Acc/F1: 92.7/90.3"
                            },
                            {
                                "name": "STS-B",
                                "accuracy": "P/S: 93.2/93.1"
                            }
                        ]
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xxlarge model with 48 layers, 1536 hidden size. The total parameters are 1.5B and it is trained with 160GB raw data.",
                    "id": "huggingface_api_686",
                    "name": "microsoft/deberta-v2-xxlarge"
                },
                "id": "gorilla_huggingface_tool_681",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 1: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 2: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 3: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 4: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 5: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 6: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 7: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 8: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.",
            "Tool 9: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 11: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 12: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.",
            "Tool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 14: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 15: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 16: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.",
            "Tool 17: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts."
        ],
        "tool_view": {
            "description": "This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text containing one or more masked tokens that the model will attempt to predict. Masked tokens should be represented using the '[MASK]' placeholder.",
                    "required": true,
                    "example": "Hello I'm a [MASK] model."
                },
                "top_k": {
                    "type": "integer",
                    "description": "The number of top predictions to return for each masked token in the input text.",
                    "required": false,
                    "default": 5,
                    "example": 5
                }
            },
            "response": "The tool produces a list of predictions for the masked token in a given input text. Each element in the response typically includes the following components:  \n\n1. **`sequence`** – The full input text with the masked token replaced by the predicted word. This shows how the model completes the input with its suggestion.  \n2. **`score`** – A numerical value representing the model’s confidence or probability for this particular prediction. Higher scores indicate higher confidence.  \n3. **`token`** – The numeric ID corresponding to the predicted token in the model’s vocabulary. This can be used for downstream processing or decoding.  \n4. **`token_str`** – The textual form of the predicted token, i.e., the actual word or subword that replaces the `[MASK]`.  \n\nThe list is typically sorted in descending order of `score`, so the most likely prediction appears first. The number of predictions returned can vary depending on the model and the settings used during inference.  \n\nThis response allows users to see multiple plausible completions for a masked word, along with confidence scores and token-level information for further analysis or processing.",
            "examples": [
                {
                    "user_intent": "I need help predicting the masked word in a sentence: 'The cat sat on the [MASK] mat.'",
                    "tool_call": {
                        "name": "distilbert-base-uncased",
                        "arguments": {
                            "model": "distilbert-base-uncased",
                            "input": "The cat sat on the [MASK] mat."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for this sentence: 'I love to play [MASK] games in my free time.'",
                    "tool_call": {
                        "name": "distilbert-base-uncased",
                        "arguments": {
                            "model": "distilbert-base-uncased",
                            "input": "I love to play [MASK] games in my free time."
                        }
                    }
                },
                {
                    "user_intent": "Can you predict the masked word in 'He went to the [MASK] to buy some groceries.'?",
                    "tool_call": {
                        "name": "distilbert-base-uncased",
                        "arguments": {
                            "model": "distilbert-base-uncased",
                            "input": "He went to the [MASK] to buy some groceries."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 1: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 2: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 3: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 4: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 5: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 6: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 7: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 8: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.\nTool 9: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 11: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 12: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.\nTool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 14: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 15: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 16: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.\nTool 17: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text containing one or more masked tokens that the model will attempt to predict. Masked tokens should be represented using the '[MASK]' placeholder.\", \"required\": true, \"example\": \"Hello I'm a [MASK] model.\"}, \"top_k\": {\"type\": \"integer\", \"description\": \"The number of top predictions to return for each masked token in the input text.\", \"required\": false, \"default\": 5, \"example\": 5}}, \"response\": \"The tool produces a list of predictions for the masked token in a given input text. Each element in the response typically includes the following components:  \\n\\n1. **`sequence`** \\u2013 The full input text with the masked token replaced by the predicted word. This shows how the model completes the input with its suggestion.  \\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability for this particular prediction. Higher scores indicate higher confidence.  \\n3. **`token`** \\u2013 The numeric ID corresponding to the predicted token in the model\\u2019s vocabulary. This can be used for downstream processing or decoding.  \\n4. **`token_str`** \\u2013 The textual form of the predicted token, i.e., the actual word or subword that replaces the `[MASK]`.  \\n\\nThe list is typically sorted in descending order of `score`, so the most likely prediction appears first. The number of predictions returned can vary depending on the model and the settings used during inference.  \\n\\nThis response allows users to see multiple plausible completions for a masked word, along with confidence scores and token-level information for further analysis or processing.\", \"examples\": [{\"user_intent\": \"I need help predicting the masked word in a sentence: 'The cat sat on the [MASK] mat.'\", \"tool_call\": {\"name\": \"distilbert-base-uncased\", \"arguments\": {\"model\": \"distilbert-base-uncased\", \"input\": \"The cat sat on the [MASK] mat.\"}}}, {\"user_intent\": \"Fill in the blank for this sentence: 'I love to play [MASK] games in my free time.'\", \"tool_call\": {\"name\": \"distilbert-base-uncased\", \"arguments\": {\"model\": \"distilbert-base-uncased\", \"input\": \"I love to play [MASK] games in my free time.\"}}}, {\"user_intent\": \"Can you predict the masked word in 'He went to the [MASK] to buy some groceries.'?\", \"tool_call\": {\"name\": \"distilbert-base-uncased\", \"arguments\": {\"model\": \"distilbert-base-uncased\", \"input\": \"He went to the [MASK] to buy some groceries.\"}}}]}\n\n# User Query\nOur school needs to create fill-in-the-blank quizzes for students. Is it possible to generate a fill-in-the-blank question from the following sentence: \"The cat chased the mouse and then climbed the tree.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a fill-in-the-blank question from a given sentence for a student quiz.",
                    "tool_description": "Use a masked language modeling tool to identify a word or phrase in the sentence that can be replaced with a mask token, creating a fill-in-the-blank style question.",
                    "expected_response": "A modified version of the input sentence with one or more masked tokens suitable for a quiz, along with predictions for possible correct answers."
                }
            ],
            "extracted_arguments": {
                "sentence": "The complete sentence or text passage from which a fill-in-the-blank question will be generated.",
                "top_k": "Optional integer specifying the number of top predictions to generate for the masked token."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_180",
        "query": "We want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.",
        "instruction": "Given a `sports video classification` task, retrieve tools that classify video content based on the type of sports depicted, utilizing advanced video analysis techniques to process video inputs and accurately categorize them according to sports-related criteria.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')",
                    "api_arguments": [
                        "video"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Kinetics-400",
                        "accuracy": {
                            "top-1": 84.7,
                            "top-5": 96.5
                        }
                    },
                    "description": "VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_327",
                    "name": "MCG-NJU/videomae-large-finetuned-kinetics"
                },
                "id": "gorilla_huggingface_tool_323",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 4: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 5: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 7: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 9: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 10: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 11: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 13: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 14: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 15: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 16: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 17: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.",
            "Tool 18: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 19: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "parameters": {
                "sequence": {
                    "type": "string",
                    "description": "The input text sequence that you want to classify using the zero-shot model.",
                    "required": true,
                    "example": "El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo"
                },
                "candidate_labels": {
                    "type": "array",
                    "description": "A list of labels or categories that the model will consider when classifying the input sequence.",
                    "required": true,
                    "example": [
                        "cultura",
                        "sociedad",
                        "economia",
                        "salud",
                        "deportes"
                    ]
                },
                "hypothesis_template": {
                    "type": "string",
                    "description": "A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.",
                    "required": true,
                    "example": "Este ejemplo es {}."
                }
            },
            "response": "The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\n\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\n   \n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\n\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\n\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\n\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.",
            "examples": [
                {
                    "user_intent": "I want to classify this Spanish sentence into categories: 'El gobierno anunció nuevas medidas económicas para combatir la inflación.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El gobierno anunció nuevas medidas económicas para combatir la inflación.",
                            "candidate_labels": [
                                "economia",
                                "sociedad",
                                "salud",
                                "cultura",
                                "deportes"
                            ],
                            "hypothesis_template": "Este texto trata sobre {}."
                        }
                    }
                },
                {
                    "user_intent": "Determine whether this Spanish sentence is about health, culture, or sports: 'El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.",
                            "candidate_labels": [
                                "deportes",
                                "salud",
                                "cultura",
                                "economia",
                                "sociedad"
                            ],
                            "hypothesis_template": "Este texto se refiere a {}."
                        }
                    }
                },
                {
                    "user_intent": "Classify this Spanish review about a museum visit: 'La exposición de arte contemporáneo fue fascinante y educativa.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "La exposición de arte contemporáneo fue fascinante y educativa.",
                            "candidate_labels": [
                                "cultura",
                                "sociedad",
                                "economia",
                                "deportes",
                                "salud"
                            ],
                            "hypothesis_template": "Este texto es sobre {}."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 3: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 4: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 5: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 7: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 9: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 10: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 11: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 13: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 14: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 15: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 16: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 17: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.\nTool 18: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 19: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\", \"parameters\": {\"sequence\": {\"type\": \"string\", \"description\": \"The input text sequence that you want to classify using the zero-shot model.\", \"required\": true, \"example\": \"El autor se perfila, a los 50 a\\u00f1os de su muerte, como uno de los grandes de su siglo\"}, \"candidate_labels\": {\"type\": \"array\", \"description\": \"A list of labels or categories that the model will consider when classifying the input sequence.\", \"required\": true, \"example\": [\"cultura\", \"sociedad\", \"economia\", \"salud\", \"deportes\"]}, \"hypothesis_template\": {\"type\": \"string\", \"description\": \"A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.\", \"required\": true, \"example\": \"Este ejemplo es {}.\"}}, \"response\": \"The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\\n\\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\\n   \\n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\\n\\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\\n\\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\\n\\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.\", \"examples\": [{\"user_intent\": \"I want to classify this Spanish sentence into categories: 'El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.\", \"candidate_labels\": [\"economia\", \"sociedad\", \"salud\", \"cultura\", \"deportes\"], \"hypothesis_template\": \"Este texto trata sobre {}.\"}}}, {\"user_intent\": \"Determine whether this Spanish sentence is about health, culture, or sports: 'El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.\", \"candidate_labels\": [\"deportes\", \"salud\", \"cultura\", \"economia\", \"sociedad\"], \"hypothesis_template\": \"Este texto se refiere a {}.\"}}}, {\"user_intent\": \"Classify this Spanish review about a museum visit: 'La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.\", \"candidate_labels\": [\"cultura\", \"sociedad\", \"economia\", \"deportes\", \"salud\"], \"hypothesis_template\": \"Este texto es sobre {}.\"}}}]}\n\n# User Query\nWe want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify sports videos to identify the type of sport each video depicts.",
                    "tool_description": "Analyze video content using a pre-trained video classification model to assign action or sport labels to each input video clip.",
                    "expected_response": "A list of predicted sport or action labels for each video, along with confidence scores or probabilities."
                }
            ],
            "extracted_arguments": {
                "video_input": "The video file or video clip data that needs to be classified.",
                "label_set": "An optional set of candidate sport or action categories to classify the video into."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_139",
        "query": "Analyze the type of plants in the image provided and provide the name of the probable plant.",
        "instruction": "Given an `image classification` task, retrieve tools that analyze the contents of an image to identify and provide the names of probable plant species by utilizing zero-shot image classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                    "api_arguments": {
                        "image_path": "path to the image file",
                        "labels": "list of possible class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "70.8 - 71.7%"
                    },
                    "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_357",
                    "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')"
                },
                "id": "gorilla_huggingface_tool_353",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 1: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 2: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 3: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 4: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 5: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 6: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 7: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.",
            "Tool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 11: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.",
            "Tool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 14: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 16: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 17: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 18: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 19: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers."
        ],
        "tool_view": {
            "description": "A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "parameters": {
                "tokenizer": {
                    "type": "object",
                    "description": "A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                }
            },
            "response": "The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\n\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \"Question Query\" or a \"Statement Query.\" \n\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model’s certainty about the classification.\n\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\"shahrukhx01/question-vs-statement-classifier\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\n\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\n   - **Class Label**: Identifies whether the input text is classified as a \"Question Query\" or a \"Statement Query.\"\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\n  \n5. **Error or Exception Handling**: If there’s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \n\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.",
            "examples": [
                {
                    "user_intent": "I want to automatically detect if a user input is a question or just a statement.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "What time does the meeting start?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Classify this customer feedback to see if it contains questions or general statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "I really like the new layout of your website.",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                },
                {
                    "user_intent": "Analyze multiple chatbot messages to separate questions from statements.",
                    "tool_call": {
                        "name": "shahrukhx01/question-vs-statement-classifier",
                        "arguments": {
                            "text": "Can you help me reset my password?",
                            "tokenizer": "AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 1: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 2: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 3: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 4: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 5: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 6: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 7: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.\nTool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 9: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 11: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.\nTool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 14: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 16: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 17: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 18: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 19: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\", \"parameters\": {\"tokenizer\": {\"type\": \"object\", \"description\": \"A tokenizer object from Hugging Face Transformers used to preprocess input text for the sequence classification model. It converts raw text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}, \"response\": \"The tool described is a text classification model designed for distinguishing between question queries and statement queries. The response from this tool would likely consist of:\\n\\n1. **Predicted Label**: The primary output of the tool will be the predicted label for the input text. This label will correspond to whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\" \\n\\n2. **Model Confidence**: The tool may also provide a confidence score or probability associated with the prediction. This indicates the model\\u2019s certainty about the classification.\\n\\n3. **Model Metadata**: There could be metadata related to the model, such as the name of the model (\\\"shahrukhx01/question-vs-statement-classifier\\\") and the tokenizer used (which is specified in the API arguments). However, this information is more likely to be used for technical debugging or auditing rather than part of the core response.\\n\\n4. **Model Output Format**: The output could follow a typical classification result format, which would include:\\n   - **Class Label**: Identifies whether the input text is classified as a \\\"Question Query\\\" or a \\\"Statement Query.\\\"\\n   - **Confidence Score**: A numerical value (if available) representing how confident the model is about the classification.\\n  \\n5. **Error or Exception Handling**: If there\\u2019s an issue with the input or the processing, the response might include an error message or exception details explaining what went wrong (e.g., invalid input, processing error, or model failure). \\n\\nIn summary, the response from the tool will primarily include the classification label (whether the input text is a question or a statement) and possibly a confidence score, alongside any necessary error information if applicable.\", \"examples\": [{\"user_intent\": \"I want to automatically detect if a user input is a question or just a statement.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"What time does the meeting start?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Classify this customer feedback to see if it contains questions or general statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"I really like the new layout of your website.\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}, {\"user_intent\": \"Analyze multiple chatbot messages to separate questions from statements.\", \"tool_call\": {\"name\": \"shahrukhx01/question-vs-statement-classifier\", \"arguments\": {\"text\": \"Can you help me reset my password?\", \"tokenizer\": \"AutoTokenizer.from_pretrained('shahrukhx01/question-vs-statement-classifier')\"}}}]}\n\n# User Query\nAnalyze the type of plants in the image provided and provide the name of the probable plant.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Identify the type of plant shown in an image.",
                    "tool_description": "Perform zero-shot or pre-trained image classification on plant images to determine the most probable species or type of plant depicted.",
                    "expected_response": "A label or set of labels indicating the probable plant type, possibly with confidence scores for each classification."
                }
            ],
            "extracted_arguments": {
                "image": "The image file containing the plant to be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_213",
        "query": "Help me find similarity scores for different restaurant reviews.",
        "instruction": "Given a `sentence similarity evaluation` task, retrieve tools that analyze text inputs by computing similarity scores for reviews, leveraging models designed to capture semantic information and align with the query's content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')",
                    "api_arguments": "text",
                    "python_environment_requirements": "sentence-transformers library",
                    "example_code": "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')\ntext = Replace me by any text you'd like.\ntext_embbedding = model.encode(text)",
                    "performance": {
                        "dataset": "1,097,953,922",
                        "accuracy": "N/A"
                    },
                    "description": "The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.",
                    "id": "huggingface_api_713",
                    "name": "flax-sentence-embeddings/all_datasets_v4_MiniLM-L6"
                },
                "id": "gorilla_huggingface_tool_708",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 3: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 5: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 9: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.",
            "Tool 10: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 11: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 12: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 13: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 14: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 15: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 16: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 17: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 18: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.",
            "Tool 19: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition."
        ],
        "tool_view": {
            "description": "A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('bert-base-uncased')"
                },
                "config": {
                    "type": "string",
                    "description": "The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.",
                    "required": true,
                    "example": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                }
            },
            "response": "The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\n\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\n\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \"positive\" or \"negative\"), which is the predicted sentiment of the input text.\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model’s certainty about its classification.\n  \n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\n\nThus, the expected response would primarily contain:\n- **Tokenized representations** of the input text.\n- **Predicted sentiment labels** along with any associated confidence or probability values.\n- Potential **error or diagnostic messages** if the process fails.",
            "examples": [
                {
                    "user_intent": "Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this review: 'Service was slow but the dessert was delightful.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 3: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 5: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 9: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.\nTool 10: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 11: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 12: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 13: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 14: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 15: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 16: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 17: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 18: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.\nTool 19: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\"}, \"config\": {\"type\": \"string\", \"description\": \"The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.\", \"required\": true, \"example\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}, \"response\": \"The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\\n\\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\\n\\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \\\"positive\\\" or \\\"negative\\\"), which is the predicted sentiment of the input text.\\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model\\u2019s certainty about its classification.\\n  \\n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\\n\\nThus, the expected response would primarily contain:\\n- **Tokenized representations** of the input text.\\n- **Predicted sentiment labels** along with any associated confidence or probability values.\\n- Potential **error or diagnostic messages** if the process fails.\", \"examples\": [{\"user_intent\": \"Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"Check the sentiment of this review: 'Service was slow but the dessert was delightful.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}]}\n\n# User Query\nHelp me find similarity scores for different restaurant reviews.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find similarity scores between different restaurant reviews.",
                    "tool_description": "Retrieve semantic similarity scores between pairs of restaurant reviews based on their content using a model designed for sentence or document-level similarity.",
                    "expected_response": "A list of similarity scores representing the degree of similarity between the input reviews."
                }
            ],
            "extracted_arguments": {
                "reviews": "A list of restaurant reviews to compare for similarity, each containing text to be analyzed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_392",
        "query": "Create a system that translates and synthesizes speech from one language to another using the given model.",
        "instruction": "Given a `speech translation and synthesis` task, retrieve tools that facilitate translating and synthesizing speech between languages by leveraging models that process audio inputs to perform speech-to-speech translation, ensuring compatibility with specified platforms and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "Speech-to-speech translation",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_hk-en')",
                    "api_arguments": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text",
                        "cache_dir": "cache_dir"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "torchaudio",
                        "huggingface_hub"
                    ],
                    "example_code": "import json\nimport os\nfrom pathlib import Path\nimport IPython.display as ipd\nfrom fairseq import hub_utils\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\nfrom huggingface_hub import snapshot_download\nimport torchaudio\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n facebook/xm_transformer_unity_hk-en,\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\n cache_dir=cache_dir,\n)\nmodel = models[0].cpu()\ncfg[task].cpu = True\ngenerator = task.build_generator([model], cfg)\naudio, _ = torchaudio.load(/path/to/an/audio/file)\nsample = S2THubInterface.get_model_input(task, audio)\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\nlibrary_name = fairseq\ncache_dir = (\n cache_dir or (Path.home() / .cache / library_name).as_posix()\n)\ncache_dir = snapshot_download(\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\n)\nx = hub_utils.from_pretrained(\n cache_dir,\n model.pt,\n .,\n archive_map=CodeHiFiGANVocoder.hub_models(),\n config_yaml=config.json,\n fp16=False,\n is_vocoder=True,\n)\nwith open(f{x['args']['data']}/config.json) as f:\n vocoder_cfg = json.load(f)\nassert (\n len(x[args][model_path]) == 1\n), Too many vocoder models in the input\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\ntts_sample = tts_model.get_model_input(unit)\nwav, sr = tts_model.get_prediction(tts_sample)\nipd.Audio(wav, rate=sr)",
                    "performance": {
                        "dataset": [
                            "TED",
                            "drama",
                            "TAT"
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "A speech-to-speech translation model with two-pass decoder (UnitY) trained on Hokkien-English data from TED, drama, and TAT domains. It uses Facebook's Unit HiFiGAN for speech synthesis.",
                    "id": "huggingface_api_789",
                    "name": "xm_transformer_unity_hk-en"
                },
                "id": "gorilla_huggingface_tool_784",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 1: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.",
            "Tool 2: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 3: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 4: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 6: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 8: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 10: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 11: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 13: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 14: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 15: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 16: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 17: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 19: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions."
        ],
        "tool_view": {
            "description": "A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "parameters": {
                "audio": {
                    "type": "string",
                    "description": "The input audio file for speech-to-speech translation. Must be 16000Hz mono channel audio.",
                    "required": true,
                    "example": "/path/to/sample_audio.wav"
                }
            },
            "response": "The tool’s response is an audio waveform corresponding to the translated speech output. In principle, the response has the following components and structure:\n\n1. **Waveform data (`wav`)**  \n   - This is a numeric array or tensor representing the raw audio samples of the synthesized speech in the target language.  \n   - The audio is produced at a specified sampling rate (typically 16 kHz, mono channel, as required by the model).  \n   - The waveform encodes the amplitude of the audio signal over time and is ready for playback or further audio processing.\n\n2. **Sampling rate (`sr`)**  \n   - This is a scalar value indicating the number of samples per second in the waveform (e.g., 16000 Hz).  \n   - It allows audio libraries to correctly interpret the waveform for playback or file saving.\n\n3. **Optional metadata (implicit in pipeline)**  \n   - While not explicitly returned, the process may associate metadata such as source language, target language, or model configuration internally.  \n   - The primary response, however, consists of the waveform and its sampling rate.\n\n**Summary:** The response consists of a tuple of `(wav, sr)`, where `wav` is the generated speech waveform in the target language, and `sr` is the corresponding sample rate for audio playback. This output can be directly fed into audio players or saved as an audio file.",
            "examples": [
                {
                    "user_intent": "Translate a Spanish audio clip to English speech.",
                    "tool_call": {
                        "name": "load_model_ensemble_and_task_from_hf_hub",
                        "arguments": {
                            "audio": "/Users/lpw/audio_samples/spanish_clip_16k.wav"
                        }
                    }
                },
                {
                    "user_intent": "Convert an English audio recording into Italian speech using S2UT.",
                    "tool_call": {
                        "name": "load_model_ensemble_and_task_from_hf_hub",
                        "arguments": {
                            "audio": "/home/user/recordings/meeting_en_16k.wav"
                        }
                    }
                },
                {
                    "user_intent": "Take a French podcast segment and generate Spanish speech output.",
                    "tool_call": {
                        "name": "load_model_ensemble_and_task_from_hf_hub",
                        "arguments": {
                            "audio": "/mnt/data/podcast_fr_segment_16k.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 1: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.\nTool 2: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 3: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 4: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 6: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 8: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 10: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 11: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 13: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 14: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 15: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 16: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 17: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 19: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16\\u202fkHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\", \"parameters\": {\"audio\": {\"type\": \"string\", \"description\": \"The input audio file for speech-to-speech translation. Must be 16000Hz mono channel audio.\", \"required\": true, \"example\": \"/path/to/sample_audio.wav\"}}, \"response\": \"The tool\\u2019s response is an audio waveform corresponding to the translated speech output. In principle, the response has the following components and structure:\\n\\n1. **Waveform data (`wav`)**  \\n   - This is a numeric array or tensor representing the raw audio samples of the synthesized speech in the target language.  \\n   - The audio is produced at a specified sampling rate (typically 16\\u202fkHz, mono channel, as required by the model).  \\n   - The waveform encodes the amplitude of the audio signal over time and is ready for playback or further audio processing.\\n\\n2. **Sampling rate (`sr`)**  \\n   - This is a scalar value indicating the number of samples per second in the waveform (e.g., 16000 Hz).  \\n   - It allows audio libraries to correctly interpret the waveform for playback or file saving.\\n\\n3. **Optional metadata (implicit in pipeline)**  \\n   - While not explicitly returned, the process may associate metadata such as source language, target language, or model configuration internally.  \\n   - The primary response, however, consists of the waveform and its sampling rate.\\n\\n**Summary:** The response consists of a tuple of `(wav, sr)`, where `wav` is the generated speech waveform in the target language, and `sr` is the corresponding sample rate for audio playback. This output can be directly fed into audio players or saved as an audio file.\", \"examples\": [{\"user_intent\": \"Translate a Spanish audio clip to English speech.\", \"tool_call\": {\"name\": \"load_model_ensemble_and_task_from_hf_hub\", \"arguments\": {\"audio\": \"/Users/lpw/audio_samples/spanish_clip_16k.wav\"}}}, {\"user_intent\": \"Convert an English audio recording into Italian speech using S2UT.\", \"tool_call\": {\"name\": \"load_model_ensemble_and_task_from_hf_hub\", \"arguments\": {\"audio\": \"/home/user/recordings/meeting_en_16k.wav\"}}}, {\"user_intent\": \"Take a French podcast segment and generate Spanish speech output.\", \"tool_call\": {\"name\": \"load_model_ensemble_and_task_from_hf_hub\", \"arguments\": {\"audio\": \"/mnt/data/podcast_fr_segment_16k.wav\"}}}]}\n\n# User Query\nCreate a system that translates and synthesizes speech from one language to another using the given model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate speech from one language to another using a speech-to-speech translation model.",
                    "tool_description": "Translate spoken audio in one language into speech in another language using a pre-trained model, processing 16 kHz mono audio.",
                    "expected_response": "An audio waveform representing the translated speech output, with an associated sampling rate."
                }
            ],
            "extracted_arguments": {
                "audio": "The input audio file for speech-to-speech translation. Must be 16000Hz mono channel audio."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_394",
        "query": "We have written a summary of a new book's plot. Now, we want to ensure if the summary contains conflicting information.",
        "instruction": "Given an `information consistency verification` task, retrieve tools that assess text consistency through natural language inference processes, identifying contradictions or confirmatory statements between text segments based on the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-MiniLM2-L6-H768')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-MiniLM2-L6-H768')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": "SNLI and MultiNLI",
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder for evaluation results"
                    },
                    "description": "This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_507",
                    "name": "cross-encoder/nli-MiniLM2-L6-H768"
                },
                "id": "gorilla_huggingface_tool_503",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 2: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 3: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 5: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 9: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.",
            "Tool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 11: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 12: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 13: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 14: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 15: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 16: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 17: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 18: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 19: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 2: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 3: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 5: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 9: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\nTool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 11: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 12: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 13: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 14: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 15: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 16: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 17: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 18: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 19: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nWe have written a summary of a new book's plot. Now, we want to ensure if the summary contains conflicting information.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Check a text summary for internal inconsistencies or conflicting information.",
                    "tool_description": "Analyze the input text to detect logical contradictions, discrepancies, or conflicting statements within the content.",
                    "expected_response": "A report highlighting potential conflicts or inconsistencies in the summary, optionally with locations or examples of the conflicts."
                }
            ],
            "extracted_arguments": {
                "summary_text": "The full text of the book summary to be analyzed for internal consistency."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_72",
        "query": "Our company is an environmental consultancy firm. Determine the carbon emissions of different facilities based on the provided data.",
        "instruction": "Given a `carbon emissions estimation` task, retrieve tools that can process environmental data inputs using tabular regression models to predict carbon emissions of different facilities, aiming for accuracy and alignment with the provided data parameters.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "kochetkovIT/autotrain-data-ironhack",
                        "accuracy": {
                            "Loss": 2.603,
                            "R2": 0.013,
                            "MSE": 6.776,
                            "MAE": 1.666,
                            "RMSLE": 0.502
                        }
                    },
                    "description": "A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.",
                    "id": "huggingface_api_870",
                    "name": "kochetkovIT/autotrain-ironhack-49741119788"
                },
                "id": "gorilla_huggingface_tool_863",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 1: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 2: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 3: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 4: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 5: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 6: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 7: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 11: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 12: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 13: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 15: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 16: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 18: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.",
                    "required": true,
                    "example": "{ 'feature1': [value1, value2], 'feature2': [value3, value4] }"
                }
            },
            "response": "The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\n\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\n\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\n\n3. **Model Information**: The model's name, in this case, \"kochetkovIT/autotrain-ironhack-49741119788,\" could also be part of the response for identification or logging purposes.\n\nIn summary, the response would likely include:\n- The **predictions** from the model based on input data.\n- A set of **performance metrics** that evaluate the quality of the model.\n- **Model identification** (name) for reference or tracking.\n\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my new vehicle dataset using the trained model.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "factory_emissions_input.csv"
                        }
                    }
                },
                {
                    "user_intent": "Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "building_energy.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 1: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 2: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 3: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 4: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 5: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 6: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 7: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 11: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 12: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 13: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 15: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 16: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 18: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 19: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.\", \"required\": true, \"example\": \"{ 'feature1': [value1, value2], 'feature2': [value3, value4] }\"}}, \"response\": \"The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\\n\\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\\n\\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\\n\\n3. **Model Information**: The model's name, in this case, \\\"kochetkovIT/autotrain-ironhack-49741119788,\\\" could also be part of the response for identification or logging purposes.\\n\\nIn summary, the response would likely include:\\n- The **predictions** from the model based on input data.\\n- A set of **performance metrics** that evaluate the quality of the model.\\n- **Model identification** (name) for reference or tracking.\\n\\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my new vehicle dataset using the trained model.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"factory_emissions_input.csv\"}}}, {\"user_intent\": \"Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"building_energy.csv\"}}}]}\n\n# User Query\nOur company is an environmental consultancy firm. Determine the carbon emissions of different facilities based on the provided data.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict the carbon emissions for different facilities using the provided data.",
                    "tool_description": "Use a trained regression model to predict carbon emissions based on input data from various facilities, provided as a structured dataset.",
                    "expected_response": "Predictions of carbon emissions for each facility based on the input data, including performance metrics such as R-squared, Mean Squared Error, and others."
                }
            ],
            "extracted_arguments": {
                "data": "The structured data (e.g., CSV or DataFrame) containing the features of different facilities for which carbon emissions are to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_94",
        "query": "We are building a quiz application where the image will be shown, and we have to choose a dressings matching that image. Please help in classifying the image.",
        "instruction": "Given an `image classification` task, retrieve tools that can classify images by matching visual inputs to specified categories or labels to align with the query's requirements for the quiz application.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "AlignModel.from_pretrained('kakaobrain/align-base')",
                    "api_arguments": [
                        "text",
                        "images",
                        "return_tensors"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "torch",
                        "PIL",
                        "transformers"
                    ],
                    "example_code": "import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\nprocessor = AlignProcessor.from_pretrained(kakaobrain/align-base)\nmodel = AlignModel.from_pretrained(kakaobrain/align-base)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [an image of a cat, an image of a dog]\ninputs = processor(text=candidate_labels, images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)",
                    "performance": {
                        "dataset": "COYO-700M",
                        "accuracy": "on-par or outperforms Google ALIGN's reported metrics"
                    },
                    "description": "The ALIGN model is a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder. It learns to align visual and text representations with contrastive learning. This implementation is trained on the open source COYO dataset and can be used for zero-shot image classification and multi-modal embedding retrieval.",
                    "id": "huggingface_api_361",
                    "name": "kakaobrain/align-base"
                },
                "id": "gorilla_huggingface_tool_357",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 1: The \"microsoft/resnet-18\" is a pre-trained ResNet model for image classification, built on the Hugging Face Transformers framework. It is trained on the ImageNet-1k dataset and utilizes residual connections to enable the training of very deep networks. ResNet-18 won the 2015 ILSVRC & COCO competition and is capable of classifying images into categories, making it suitable for computer vision tasks where image recognition is required.",
            "Tool 2: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.",
            "Tool 3: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 4: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.",
            "Tool 5: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 6: A lightweight Vision Transformer (ViT) model for image classification that uses Hugging Face Transformers, suitable for quickly classifying images with a pretrained tiny-vit-random model.",
            "Tool 7: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 8: A ConvNeXt-V2 image classification model pretrained with a fully convolutional masked autoencoder (FCMAE) and fine-tuned on ImageNet-1k, capable of accurately identifying and classifying images with high precision.",
            "Tool 9: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 10: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 11: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 12: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 13: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.",
            "Tool 14: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 15: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 17: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 18: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 19: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications."
        ],
        "tool_view": {
            "description": "This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path to the image that you want to classify using the zero-shot image classification model.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "labels": {
                    "type": "array",
                    "description": "A list of possible class names or labels to consider when performing zero-shot classification on the image.",
                    "required": true,
                    "example": [
                        "cat",
                        "dog",
                        "bird"
                    ]
                }
            },
            "response": "The tool’s response is a structured output representing the results of zero-shot image classification for a given image against a set of candidate labels. The response typically contains the following components:  \n\n1. **Predicted Labels** – A list of one or more class names from the provided label set that the model considers most likely for the input image. These labels indicate the categories the image most closely aligns with according to the model.  \n\n2. **Confidence Scores** – Corresponding numerical scores or probabilities for each predicted label, representing the model’s estimated likelihood or confidence that the image belongs to that class. These are usually normalized values summing to 1 across all candidate labels, though some implementations may provide raw logits instead.  \n\n3. **Optional Ranking Information** – In some implementations, the labels may be returned in descending order of confidence, effectively ranking the candidate classes from most to least likely.  \n\nOverall, the response conveys, for each input image, how likely it is to belong to each candidate class, enabling selection of the most probable category or further downstream processing such as threshold-based classification or retrieval.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a pet to see if it's a cat, dog, or rabbit without training a model.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/pet_photo.jpg",
                            "labels": [
                                "cat",
                                "dog",
                                "rabbit"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Determine the main object in this outdoor scene image among several possibilities.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/park_scene.png",
                            "labels": [
                                "tree",
                                "bench",
                                "car",
                                "person"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Check what kind of fruit is shown in this photo without manually labeling it.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/fruit_bowl.jpeg",
                            "labels": [
                                "apple",
                                "banana",
                                "orange",
                                "grape"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 1: The \"microsoft/resnet-18\" is a pre-trained ResNet model for image classification, built on the Hugging Face Transformers framework. It is trained on the ImageNet-1k dataset and utilizes residual connections to enable the training of very deep networks. ResNet-18 won the 2015 ILSVRC & COCO competition and is capable of classifying images into categories, making it suitable for computer vision tasks where image recognition is required.\nTool 2: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.\nTool 3: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 4: A ConvNeXt image classification model pretrained on ImageNet-1k, designed for classifying images, extracting feature maps, and generating image embeddings, suitable for computer vision tasks requiring high-quality visual representations.\nTool 5: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 6: A lightweight Vision Transformer (ViT) model for image classification that uses Hugging Face Transformers, suitable for quickly classifying images with a pretrained tiny-vit-random model.\nTool 7: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 8: A ConvNeXt-V2 image classification model pretrained with a fully convolutional masked autoencoder (FCMAE) and fine-tuned on ImageNet-1k, capable of accurately identifying and classifying images with high precision.\nTool 9: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 10: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 11: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 12: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 13: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.\nTool 14: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 15: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 17: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 18: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 19: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path to the image that you want to classify using the zero-shot image classification model.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"labels\": {\"type\": \"array\", \"description\": \"A list of possible class names or labels to consider when performing zero-shot classification on the image.\", \"required\": true, \"example\": [\"cat\", \"dog\", \"bird\"]}}, \"response\": \"The tool\\u2019s response is a structured output representing the results of zero-shot image classification for a given image against a set of candidate labels. The response typically contains the following components:  \\n\\n1. **Predicted Labels** \\u2013 A list of one or more class names from the provided label set that the model considers most likely for the input image. These labels indicate the categories the image most closely aligns with according to the model.  \\n\\n2. **Confidence Scores** \\u2013 Corresponding numerical scores or probabilities for each predicted label, representing the model\\u2019s estimated likelihood or confidence that the image belongs to that class. These are usually normalized values summing to 1 across all candidate labels, though some implementations may provide raw logits instead.  \\n\\n3. **Optional Ranking Information** \\u2013 In some implementations, the labels may be returned in descending order of confidence, effectively ranking the candidate classes from most to least likely.  \\n\\nOverall, the response conveys, for each input image, how likely it is to belong to each candidate class, enabling selection of the most probable category or further downstream processing such as threshold-based classification or retrieval.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a pet to see if it's a cat, dog, or rabbit without training a model.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/pet_photo.jpg\", \"labels\": [\"cat\", \"dog\", \"rabbit\"]}}}, {\"user_intent\": \"Determine the main object in this outdoor scene image among several possibilities.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/park_scene.png\", \"labels\": [\"tree\", \"bench\", \"car\", \"person\"]}}}, {\"user_intent\": \"Check what kind of fruit is shown in this photo without manually labeling it.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/fruit_bowl.jpeg\", \"labels\": [\"apple\", \"banana\", \"orange\", \"grape\"]}}}]}\n\n# User Query\nWe are building a quiz application where the image will be shown, and we have to choose a dressings matching that image. Please help in classifying the image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify an image to identify which dressing it depicts for the quiz application.",
                    "tool_description": "Use a zero-shot image classification model to predict the most likely dressing category for a given image by comparing it to a predefined list of dressing labels.",
                    "expected_response": "A structured output listing the predicted dressing labels with corresponding confidence scores, optionally ranked by likelihood."
                }
            ],
            "extracted_arguments": {
                "image_path": "The file path to the image that needs to be classified.",
                "labels": "A list of possible dressing types or categories to match against the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_62",
        "query": "We want to build a product to classify images of pets into different categories.",
        "instruction": "Given an `image classification` task, retrieve tools that efficiently categorize images by processing visual inputs and assigning them to predefined categories, particularly in a zero-shot learning context aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('openai/clip-vit-large-patch14')",
                    "api_arguments": {
                        "text": [
                            "a photo of a cat",
                            "a photo of a dog"
                        ],
                        "images": "image",
                        "return_tensors": "pt",
                        "padding": "True"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "PIL",
                            "requests",
                            "transformers"
                        ]
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            "Food101",
                            "CIFAR10",
                            "CIFAR100",
                            "Birdsnap",
                            "SUN397",
                            "Stanford Cars",
                            "FGVC Aircraft",
                            "VOC2007",
                            "DTD",
                            "Oxford-IIIT Pet dataset",
                            "Caltech101",
                            "Flowers102",
                            "MNIST",
                            "SVHN",
                            "IIIT5K",
                            "Hateful Memes",
                            "SST-2",
                            "UCF101",
                            "Kinetics700",
                            "Country211",
                            "CLEVR Counting",
                            "KITTI Distance",
                            "STL-10",
                            "RareAct",
                            "Flickr30",
                            "MSCOCO",
                            "ImageNet",
                            "ImageNet-A",
                            "ImageNet-R",
                            "ImageNet Sketch",
                            "ObjectNet (ImageNet Overlap)",
                            "Youtube-BB",
                            "ImageNet-Vid"
                        ],
                        "accuracy": "varies depending on the dataset"
                    },
                    "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.",
                    "id": "huggingface_api_347",
                    "name": "openai/clip-vit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_343",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 1: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 2: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 3: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 4: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 5: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 6: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.",
            "Tool 7: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 8: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 9: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 10: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 11: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 12: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 14: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 16: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 18: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.",
            "Tool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks."
        ],
        "tool_view": {
            "description": "Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The image to be classified, provided either as a file or a URL pointing to the image location.",
                    "required": true,
                    "example": "https://example.com/image.jpg"
                },
                "class_names": {
                    "type": "array",
                    "description": "A list of class labels or categories to consider for the zero-shot classification task. The model will classify the image into one of these classes.",
                    "required": true,
                    "example": [
                        "cat",
                        "dog",
                        "bird"
                    ]
                }
            },
            "response": "The tool returns the results of zero-shot image classification for a given image against a set of candidate class names. The typical response is a structured collection where each entry corresponds to one candidate class and contains a confidence score representing how well the image matches that class.  \n\nA reasonable structure of the response would include:\n\n- **`labels`**: A list of the class names from the provided candidate classes, often ordered by predicted relevance or confidence.\n- **`scores`**: A corresponding list of numerical values representing the model’s confidence for each class label, typically as probabilities or normalized scores between 0 and 1.\n- **Optional `sequence` or `ranking` fields**: Some implementations may include a combined list of objects where each object contains a `label` and its associated `score`, representing the ranking of classes from most to least likely.\n\nSemantically, the response provides a ranked assessment of how likely the input image belongs to each specified class, enabling zero-shot classification without the model being explicitly trained on those classes.",
            "examples": [
                {
                    "user_intent": "I want to classify this image of a cat or a dog without training a model.",
                    "tool_call": {
                        "name": "tiny-random-CLIPSegModel",
                        "arguments": {
                            "image": "https://example.com/images/cute_pet.jpg",
                            "class_names": [
                                "cat",
                                "dog"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Check whether this photo contains a car, a bicycle, or a pedestrian.",
                    "tool_call": {
                        "name": "tiny-random-CLIPSegModel",
                        "arguments": {
                            "image": "/home/user/photos/street_scene.png",
                            "class_names": [
                                "car",
                                "bicycle",
                                "pedestrian"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to identify if this satellite image shows forest, water, or urban areas.",
                    "tool_call": {
                        "name": "tiny-random-CLIPSegModel",
                        "arguments": {
                            "image": "https://example.com/satellite/region123.png",
                            "class_names": [
                                "forest",
                                "water",
                                "urban"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 1: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 2: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 3: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 4: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 5: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 6: The \"vicgalle/xlm-roberta-large-xnli-anli\" tool is a pre-trained XLM-RoBERTa model fine-tuned for zero-shot classification. It enables classifying text sequences into predefined categories without needing task-specific training data. This model, based on multiple natural language inference datasets, can be used to classify text in different languages, with high accuracy on datasets like XNLI. It is integrated into the Hugging Face Transformers framework and requires the \"transformers\" library in a Python environment.\nTool 7: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 8: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 9: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 10: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 11: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 12: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 14: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 16: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 18: A CLIP ViT-bigG/14 model designed for zero-shot image classification, allowing users to classify images into arbitrary categories without task-specific training. Built with the LAION-2B dataset, it is useful for research and exploratory studies in computer vision, enabling analysis of model behavior and performance across diverse visual concepts. The model achieves 80.1% zero-shot top-1 accuracy on ImageNet-1k.\nTool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The image to be classified, provided either as a file or a URL pointing to the image location.\", \"required\": true, \"example\": \"https://example.com/image.jpg\"}, \"class_names\": {\"type\": \"array\", \"description\": \"A list of class labels or categories to consider for the zero-shot classification task. The model will classify the image into one of these classes.\", \"required\": true, \"example\": [\"cat\", \"dog\", \"bird\"]}}, \"response\": \"The tool returns the results of zero-shot image classification for a given image against a set of candidate class names. The typical response is a structured collection where each entry corresponds to one candidate class and contains a confidence score representing how well the image matches that class.  \\n\\nA reasonable structure of the response would include:\\n\\n- **`labels`**: A list of the class names from the provided candidate classes, often ordered by predicted relevance or confidence.\\n- **`scores`**: A corresponding list of numerical values representing the model\\u2019s confidence for each class label, typically as probabilities or normalized scores between 0 and 1.\\n- **Optional `sequence` or `ranking` fields**: Some implementations may include a combined list of objects where each object contains a `label` and its associated `score`, representing the ranking of classes from most to least likely.\\n\\nSemantically, the response provides a ranked assessment of how likely the input image belongs to each specified class, enabling zero-shot classification without the model being explicitly trained on those classes.\", \"examples\": [{\"user_intent\": \"I want to classify this image of a cat or a dog without training a model.\", \"tool_call\": {\"name\": \"tiny-random-CLIPSegModel\", \"arguments\": {\"image\": \"https://example.com/images/cute_pet.jpg\", \"class_names\": [\"cat\", \"dog\"]}}}, {\"user_intent\": \"Check whether this photo contains a car, a bicycle, or a pedestrian.\", \"tool_call\": {\"name\": \"tiny-random-CLIPSegModel\", \"arguments\": {\"image\": \"/home/user/photos/street_scene.png\", \"class_names\": [\"car\", \"bicycle\", \"pedestrian\"]}}}, {\"user_intent\": \"I need to identify if this satellite image shows forest, water, or urban areas.\", \"tool_call\": {\"name\": \"tiny-random-CLIPSegModel\", \"arguments\": {\"image\": \"https://example.com/satellite/region123.png\", \"class_names\": [\"forest\", \"water\", \"urban\"]}}}]}\n\n# User Query\nWe want to build a product to classify images of pets into different categories.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify images of pets into different categories without training a model.",
                    "tool_description": "Perform zero-shot image classification on pet images, allowing classification into custom categories without task-specific training.",
                    "expected_response": "A structured collection of candidate pet categories with confidence scores for each, indicating the likelihood that the image belongs to each category."
                }
            ],
            "extracted_arguments": {
                "image": "The image of a pet to be classified, provided as a file or a URL.",
                "class_names": "A list of pet categories or labels to classify the image into."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_17",
        "query": "We need to prepare some sample conversations featuring frequently asked questions for helping customers with our products.",
        "instruction": "Given a `conversational sample generation` task, retrieve tools that utilize natural language processing capabilities to generate dialogue-based content by processing frequent customer questions and producing interactive and coherent conversational responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Conversational",
                    "api_call": "pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')",
                    "api_arguments": [
                        "message"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "conversational_pipeline('Hello, how are you?')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A GPT-4 model for generating conversational responses in a dialogue setting.",
                    "id": "huggingface_api_588",
                    "name": "ingen51/DialoGPT-medium-GPT4"
                },
                "id": "gorilla_huggingface_tool_583",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 2: Parrot is a paraphrase-based text augmentation tool designed to generate varied versions of input utterances, helping accelerate the training of natural language understanding models by expanding and diversifying their training data.",
            "Tool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 4: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 5: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.",
            "Tool 6: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 7: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 8: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 9: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 10: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 12: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.",
            "Tool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 14: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 15: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).",
            "Tool 16: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 17: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 18: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 2: Parrot is a paraphrase-based text augmentation tool designed to generate varied versions of input utterances, helping accelerate the training of natural language understanding models by expanding and diversifying their training data.\nTool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 4: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 5: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\nTool 6: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 7: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 8: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 9: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 10: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 12: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.\nTool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 14: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 15: This tool uses the ConvTasNet model, trained on the sep_clean task of the Libri3Mix dataset, to perform audio source separation. It processes mixed audio signals into individual sources, leveraging Asteroid and Hugging Face Transformers frameworks. The model is designed to work with 3-source mixtures, operating at a sample rate of 8kHz, and is optimized for tasks like speech separation. It offers high performance with metrics such as improved Signal-to-Distortion Ratio (SDR) and Speech Intelligibility (STOI).\nTool 16: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 17: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 18: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nWe need to prepare some sample conversations featuring frequently asked questions for helping customers with our products.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a list of sample conversations based on frequently asked questions for customer support.",
                    "tool_description": "Generate sample conversations featuring frequently asked questions to assist with customer support for products.",
                    "expected_response": "A list of dialogues with common customer questions and responses."
                }
            ],
            "extracted_arguments": {
                "use_case": "The specific application or industry for which the sample conversations will be created.",
                "question_set": "A collection of frequently asked questions related to the products or services."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_89",
        "query": "As an art director, generating ideas from descriptions can be difficult. Produce an image of a serene lake at sunset.",
        "instruction": "Given an `image generation` task from text descriptions, retrieve tools that can create visuals by processing textual prompts to produce realistic or artistic images aligned with the specified theme or description.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')",
                    "api_arguments": [
                        "prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "scipy"
                    ],
                    "example_code": "import torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = CompVis/stable-diffusion-v1-4\ndevice = cuda\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.",
                    "id": "huggingface_api_30",
                    "name": "CompVis/stable-diffusion-v1-4"
                },
                "id": "gorilla_huggingface_tool_30",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.",
            "Tool 1: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 2: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 3: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 4: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 6: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 7: The `microsoft/git-base-coco` model is a generative image-to-text Transformer that converts images into descriptive text. Fine-tuned on the COCO dataset, it uses a Transformer decoder conditioned on both image and text tokens, enabling tasks such as image and video captioning, visual question answering, and text-based image classification. It is designed for applications that require interpreting visual content and generating natural-language descriptions.",
            "Tool 8: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 9: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 10: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.",
            "Tool 11: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 12: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 13: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 14: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 15: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 18: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents."
        ],
        "tool_view": {
            "description": "Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.",
            "parameters": {
                "t5x_checkpoint_path": {
                    "type": "string",
                    "description": "The file path to the T5X checkpoint that will be used for model loading.",
                    "required": true,
                    "example": "/path/to/t5x_checkpoint"
                },
                "pytorch_dump_path": {
                    "type": "string",
                    "description": "The file path where the PyTorch model will be saved after conversion.",
                    "required": true,
                    "example": "/path/to/save/pytorch_model"
                },
                "use-large": {
                    "type": "boolean",
                    "description": "A flag to indicate whether to use a larger version of the model for improved performance.",
                    "required": false,
                    "example": true
                }
            },
            "response": "The tool produces a textual output in response to a given input image, typically in the context of visual question answering or image captioning. The response structure is as follows:  \n\n- **Generated Text**: The primary component of the response is a string containing the model’s prediction. Depending on the task, this can be:  \n  - An answer to a specific question about the content of an input image (for visual question answering).  \n  - A descriptive caption summarizing the content or structure of the image.  \n  - A structured representation, such as simplified HTML, if the model is used for parsing screenshots or diagrams.  \n\n- **Optional Metadata (implicit in some frameworks)**: While not explicitly defined in the JSON, implementations using Hugging Face Transformers often provide additional optional information alongside the generated text, such as:  \n  - **Token-level probabilities or scores**: Indicating the model’s confidence in each generated token.  \n  - **Generation configuration details**: Parameters like decoding strategy, maximum output length, or sampling temperature.  \n\nThe response is entirely textual and reflects the model’s understanding of the input image conditioned on the task prompt or question. The tool does not return structured numeric metrics in its output; accuracy and performance are reported separately in evaluations, not as part of the per-inference response.",
            "examples": [
                {
                    "user_intent": "I need to load the Pix2Struct model for visual question answering from a checkpoint.",
                    "tool_call": {
                        "name": "google/pix2struct-chartqa-base",
                        "arguments": {
                            "t5x_checkpoint_path": "/path/to/t5x/checkpoints",
                            "pytorch_dump_path": "/path/to/save/pytorch/model"
                        }
                    }
                },
                {
                    "user_intent": "How do I convert a T5X checkpoint to a PyTorch model for Pix2Struct?",
                    "tool_call": {
                        "name": "google/pix2struct-chartqa-base",
                        "arguments": {
                            "t5x_checkpoint_path": "/user/models/t5x_model",
                            "pytorch_dump_path": "/user/models/pytorch_model"
                        }
                    }
                },
                {
                    "user_intent": "Load a large version of the Pix2Struct model for better performance.",
                    "tool_call": {
                        "name": "google/pix2struct-chartqa-base",
                        "arguments": {
                            "use-large": true,
                            "t5x_checkpoint_path": "/user/large/t5x/checkpoints",
                            "pytorch_dump_path": "/user/large/model/pytorch"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\nTool 1: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 2: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 3: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 4: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 6: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 7: The `microsoft/git-base-coco` model is a generative image-to-text Transformer that converts images into descriptive text. Fine-tuned on the COCO dataset, it uses a Transformer decoder conditioned on both image and text tokens, enabling tasks such as image and video captioning, visual question answering, and text-based image classification. It is designed for applications that require interpreting visual content and generating natural-language descriptions.\nTool 8: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 9: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 10: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\nTool 11: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 12: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 13: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 14: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 15: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 18: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\", \"parameters\": {\"t5x_checkpoint_path\": {\"type\": \"string\", \"description\": \"The file path to the T5X checkpoint that will be used for model loading.\", \"required\": true, \"example\": \"/path/to/t5x_checkpoint\"}, \"pytorch_dump_path\": {\"type\": \"string\", \"description\": \"The file path where the PyTorch model will be saved after conversion.\", \"required\": true, \"example\": \"/path/to/save/pytorch_model\"}, \"use-large\": {\"type\": \"boolean\", \"description\": \"A flag to indicate whether to use a larger version of the model for improved performance.\", \"required\": false, \"example\": true}}, \"response\": \"The tool produces a textual output in response to a given input image, typically in the context of visual question answering or image captioning. The response structure is as follows:  \\n\\n- **Generated Text**: The primary component of the response is a string containing the model\\u2019s prediction. Depending on the task, this can be:  \\n  - An answer to a specific question about the content of an input image (for visual question answering).  \\n  - A descriptive caption summarizing the content or structure of the image.  \\n  - A structured representation, such as simplified HTML, if the model is used for parsing screenshots or diagrams.  \\n\\n- **Optional Metadata (implicit in some frameworks)**: While not explicitly defined in the JSON, implementations using Hugging Face Transformers often provide additional optional information alongside the generated text, such as:  \\n  - **Token-level probabilities or scores**: Indicating the model\\u2019s confidence in each generated token.  \\n  - **Generation configuration details**: Parameters like decoding strategy, maximum output length, or sampling temperature.  \\n\\nThe response is entirely textual and reflects the model\\u2019s understanding of the input image conditioned on the task prompt or question. The tool does not return structured numeric metrics in its output; accuracy and performance are reported separately in evaluations, not as part of the per-inference response.\", \"examples\": [{\"user_intent\": \"I need to load the Pix2Struct model for visual question answering from a checkpoint.\", \"tool_call\": {\"name\": \"google/pix2struct-chartqa-base\", \"arguments\": {\"t5x_checkpoint_path\": \"/path/to/t5x/checkpoints\", \"pytorch_dump_path\": \"/path/to/save/pytorch/model\"}}}, {\"user_intent\": \"How do I convert a T5X checkpoint to a PyTorch model for Pix2Struct?\", \"tool_call\": {\"name\": \"google/pix2struct-chartqa-base\", \"arguments\": {\"t5x_checkpoint_path\": \"/user/models/t5x_model\", \"pytorch_dump_path\": \"/user/models/pytorch_model\"}}}, {\"user_intent\": \"Load a large version of the Pix2Struct model for better performance.\", \"tool_call\": {\"name\": \"google/pix2struct-chartqa-base\", \"arguments\": {\"use-large\": true, \"t5x_checkpoint_path\": \"/user/large/t5x/checkpoints\", \"pytorch_dump_path\": \"/user/large/model/pytorch\"}}}]}\n\n# User Query\nAs an art director, generating ideas from descriptions can be difficult. Produce an image of a serene lake at sunset.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an image based on a textual description of a serene lake at sunset.",
                    "tool_description": "Generate a high-quality image based on the provided textual prompt using a text-to-image model.",
                    "expected_response": "A photorealistic image depicting a serene lake at sunset."
                }
            ],
            "extracted_arguments": {
                "description": "A natural language prompt describing the desired visual content, specifically a serene lake at sunset."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_245",
        "query": "We are building an AI chatbot that reads out user messages using synthesized human-like speech. For this purpose, we need to convert text messages to audio.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform text messages into synthesized human-like audio by processing text inputs and generating speech outputs suitable for AI chatbot applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": ""
                    },
                    "description": "A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.",
                    "id": "huggingface_api_738",
                    "name": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan"
                },
                "id": "gorilla_huggingface_tool_733",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 4: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.",
            "Tool 5: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 6: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 7: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 8: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 9: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 11: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 12: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 13: A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.",
            "Tool 14: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 15: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.",
            "Tool 16: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 17: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 18: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 19: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis."
        ],
        "tool_view": {
            "description": "Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "parameters": {
                "input": {
                    "type": "string",
                    "description": "The input message that the model will generate a response for. This serves as the starting point for the conversation.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model’s continuation, reply, or conversational output. Specifically:\n\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\n  - `generated_text`: The actual text output produced by the model.\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model’s confidence or token-level details.  \n\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.",
            "examples": [
                {
                    "user_intent": "Start a conversation with the Zixtrauce chatbot and ask about the weather.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What's the weather like today?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a response from BrandonBot4Epochs about AI advancements.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "Tell me about the latest advancements in AI."
                        }
                    }
                },
                {
                    "user_intent": "Chat with the bot about hobbies and interests.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What do you think is the best hobby to start?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 4: A Text-to-Speech tool using the ESPnet framework, trained on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.\nTool 5: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 6: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 7: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 8: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 9: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 11: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 12: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 13: A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset, that converts text input into natural-sounding synthesized speech.\nTool 14: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 15: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.\nTool 16: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 17: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 18: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 19: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\", \"parameters\": {\"input\": {\"type\": \"string\", \"description\": \"The input message that the model will generate a response for. This serves as the starting point for the conversation.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model\\u2019s continuation, reply, or conversational output. Specifically:\\n\\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \\n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\\n  - `generated_text`: The actual text output produced by the model.\\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model\\u2019s confidence or token-level details.  \\n\\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.\", \"examples\": [{\"user_intent\": \"Start a conversation with the Zixtrauce chatbot and ask about the weather.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What's the weather like today?\"}}}, {\"user_intent\": \"Generate a response from BrandonBot4Epochs about AI advancements.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"Tell me about the latest advancements in AI.\"}}}, {\"user_intent\": \"Chat with the bot about hobbies and interests.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What do you think is the best hobby to start?\"}}}]}\n\n# User Query\nWe are building an AI chatbot that reads out user messages using synthesized human-like speech. For this purpose, we need to convert text messages to audio.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate human-like speech from user text messages for the chatbot.",
                    "tool_description": "Convert written text input into natural-sounding spoken audio using a text-to-speech model.",
                    "expected_response": "An audio file or stream containing the synthesized speech corresponding to the input text."
                }
            ],
            "extracted_arguments": {
                "text_input": "The text message from the user that needs to be converted into speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_247",
        "query": "I have a table containing information about various animals and their important characteristics. I need the system to answer a query to provide information about the tallest animal in the table.",
        "instruction": "Given a `table question answering` task, retrieve tools that process table data and queries to provide accurate information by extracting relevant details from structured datasets according to the query's content and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')",
                    "api_arguments": [
                        "model_name",
                        "table",
                        "queries"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.5148
                    },
                    "description": "TAPAS mini model fine-tuned on Sequential Question Answering (SQA)",
                    "id": "huggingface_api_457",
                    "name": "google/tapas-mini-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_453",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 1: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 2: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 3: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 4: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 6: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 8: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 9: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 11: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 12: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 15: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 16: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 17: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions."
        ],
        "tool_view": {
            "description": "A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "parameters": {
                "table": {
                    "type": "object",
                    "description": "A structured table represented as a nested data structure (e.g., a list of dictionaries or a pandas DataFrame) containing the data to be queried.",
                    "required": true,
                    "example": [
                        {
                            "Name": "Alice",
                            "Age": 30,
                            "City": "New York"
                        },
                        {
                            "Name": "Bob",
                            "Age": 25,
                            "City": "Los Angeles"
                        }
                    ]
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question or query to be answered based on the content of the provided table.",
                    "required": true,
                    "example": "What is the age of Bob?"
                }
            },
            "response": "The response returned by this tool would likely contain the answer to the user's query based on the table provided. Given the functionality of the model, the response would contain:\n\n1. **Answer**: This is the direct response to the query posed by the user, derived by the TAPAS model from the given table. It could be a textual answer, a number, or a set of values depending on the nature of the question and the table data.\n\nThe response structure is simple but crucial for extracting useful information from a table:\n\n- **Answer**: The specific answer to the query asked, typically extracted from the context of the table. The answer can be based on cells, rows, columns, or any relevant information within the provided table.\n  \nAdditionally, depending on the system's implementation or specific configuration, there might be other auxiliary components such as:\n\n- **Confidence score**: A numerical value indicating the model’s confidence in its answer, though this is not explicitly described in the provided tool documentation.\n  \nHowever, based on the provided tool documentation, only the **answer** is the key component of the response.",
            "examples": [
                {
                    "user_intent": "I have a table of Olympic medal counts by country. I want to know which country won the most gold medals in 2020.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Country": [
                                    "USA",
                                    "China",
                                    "Japan"
                                ],
                                "Gold": [
                                    39,
                                    38,
                                    27
                                ],
                                "Silver": [
                                    41,
                                    32,
                                    14
                                ],
                                "Bronze": [
                                    33,
                                    18,
                                    17
                                ]
                            },
                            "query": "Which country won the most gold medals?"
                        }
                    }
                },
                {
                    "user_intent": "Given a sales table for Q1, I need to know the total revenue generated by Product B.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Product": [
                                    "A",
                                    "B",
                                    "C"
                                ],
                                "Units Sold": [
                                    120,
                                    200,
                                    150
                                ],
                                "Unit Price": [
                                    10,
                                    15,
                                    20
                                ]
                            },
                            "query": "What is the total revenue for Product B?"
                        }
                    }
                },
                {
                    "user_intent": "I have a table of movies with release years and box office earnings. I want to know which movie made the highest box office in 2022.",
                    "tool_call": {
                        "name": "google/tapas-medium-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Movie": [
                                    "Movie X",
                                    "Movie Y",
                                    "Movie Z"
                                ],
                                "Release Year": [
                                    2021,
                                    2022,
                                    2022
                                ],
                                "Box Office (Millions)": [
                                    100,
                                    250,
                                    180
                                ]
                            },
                            "query": "Which movie earned the highest box office in 2022?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 1: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 2: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 3: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 4: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 5: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 6: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 8: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 9: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 11: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 12: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 15: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 16: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 17: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 19: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table\\u2019s content, making it useful for extracting structured information from tables in a straightforward, automated way.\", \"parameters\": {\"table\": {\"type\": \"object\", \"description\": \"A structured table represented as a nested data structure (e.g., a list of dictionaries or a pandas DataFrame) containing the data to be queried.\", \"required\": true, \"example\": [{\"Name\": \"Alice\", \"Age\": 30, \"City\": \"New York\"}, {\"Name\": \"Bob\", \"Age\": 25, \"City\": \"Los Angeles\"}]}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question or query to be answered based on the content of the provided table.\", \"required\": true, \"example\": \"What is the age of Bob?\"}}, \"response\": \"The response returned by this tool would likely contain the answer to the user's query based on the table provided. Given the functionality of the model, the response would contain:\\n\\n1. **Answer**: This is the direct response to the query posed by the user, derived by the TAPAS model from the given table. It could be a textual answer, a number, or a set of values depending on the nature of the question and the table data.\\n\\nThe response structure is simple but crucial for extracting useful information from a table:\\n\\n- **Answer**: The specific answer to the query asked, typically extracted from the context of the table. The answer can be based on cells, rows, columns, or any relevant information within the provided table.\\n  \\nAdditionally, depending on the system's implementation or specific configuration, there might be other auxiliary components such as:\\n\\n- **Confidence score**: A numerical value indicating the model\\u2019s confidence in its answer, though this is not explicitly described in the provided tool documentation.\\n  \\nHowever, based on the provided tool documentation, only the **answer** is the key component of the response.\", \"examples\": [{\"user_intent\": \"I have a table of Olympic medal counts by country. I want to know which country won the most gold medals in 2020.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Country\": [\"USA\", \"China\", \"Japan\"], \"Gold\": [39, 38, 27], \"Silver\": [41, 32, 14], \"Bronze\": [33, 18, 17]}, \"query\": \"Which country won the most gold medals?\"}}}, {\"user_intent\": \"Given a sales table for Q1, I need to know the total revenue generated by Product B.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Product\": [\"A\", \"B\", \"C\"], \"Units Sold\": [120, 200, 150], \"Unit Price\": [10, 15, 20]}, \"query\": \"What is the total revenue for Product B?\"}}}, {\"user_intent\": \"I have a table of movies with release years and box office earnings. I want to know which movie made the highest box office in 2022.\", \"tool_call\": {\"name\": \"google/tapas-medium-finetuned-wtq\", \"arguments\": {\"table\": {\"Movie\": [\"Movie X\", \"Movie Y\", \"Movie Z\"], \"Release Year\": [2021, 2022, 2022], \"Box Office (Millions)\": [100, 250, 180]}, \"query\": \"Which movie earned the highest box office in 2022?\"}}}]}\n\n# User Query\nI have a table containing information about various animals and their important characteristics. I need the system to answer a query to provide information about the tallest animal in the table.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find out which animal in my table is the tallest.",
                    "tool_description": "Process a structured table of animal characteristics and answer a natural language query regarding the tallest animal.",
                    "expected_response": "An answer identifying the tallest animal in the table, which may include the animal's name and associated height."
                }
            ],
            "extracted_arguments": {
                "table": "A structured table containing data about various animals and their characteristics.",
                "query": "A natural language question asking for the tallest animal in the provided table."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_129",
        "query": "Help me setup a tinyroberta model from deepset for Question and Answer. Provide a sample input and output.",
        "instruction": "Given a `model setup and example query` task, retrieve tools that facilitate setting up an NLP model for Question Answering by providing details about the model framework, function, sample inputs and outputs, as well as any necessary environment configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/tinyroberta-squad2",
                        "question": "Why is model conversion important?",
                        "context": "The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks."
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = deepset/tinyroberta-squad2\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 78.69114798281817,
                            "f1": 81.9198998536977
                        }
                    },
                    "description": "This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.",
                    "id": "huggingface_api_474",
                    "name": "deepset/tinyroberta-squad2"
                },
                "id": "gorilla_huggingface_tool_470",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 5: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 6: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 7: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 8: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 9: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 10: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 11: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 12: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 13: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 15: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 16: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 17: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 18: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 19: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark."
        ],
        "tool_view": {
            "description": "OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "parameters": {
                "table": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the table data on which the question answering will be performed.",
                    "required": true,
                    "example": "pd.DataFrame.from_dict({'year': [1896, 1900], 'city': ['athens', 'paris']})"
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question string that will be answered based on the provided table.",
                    "required": true,
                    "example": "In which year did Beijing host the Olympic Games?"
                }
            },
            "response": "The tool described is used for table-based question answering (QA) in a Natural Language Processing (NLP) context. It leverages the OmniTab model, which is a pre-trained sequence-to-sequence model based on the BART architecture. This model has been trained with both natural and synthetic data, specifically for tasks that involve answering questions about tabular data.\n\nGiven the tool's purpose and the use of the `AutoModelForSeq2SeqLM` method from the Hugging Face Transformers library, the expected response structure would be as follows:\n\n### Expected Response Structure:\n1. **Answer (String)**:  \n   The response will contain a textual answer derived from the given table based on the query. The model generates this answer by processing the table's content and the natural language question. The answer will be a string that directly responds to the query posed.\n\n2. **Confidence or Probability (Optional)**:  \n   While not explicitly mentioned in the tool documentation, it is common for models like this to provide a measure of confidence in the generated answer. This could take the form of a confidence score or a probability, indicating how confident the model is in the given answer. If present, this would typically be a floating-point number ranging from 0 to 1.\n\n3. **Error or Invalid Response (Optional)**:  \n   If the model is unable to generate a meaningful answer (e.g., due to insufficient or ambiguous data), the response might include an error message or an indication that no answer could be generated. This could also take the form of a special token or a string such as \"Unable to answer\" or \"Invalid query.\"\n\n4. **Additional Metadata (Optional)**:  \n   Depending on the specific implementation, the response might contain extra information, such as the processing time, the number of tokens processed, or the number of possible candidate answers considered by the model.\n\n### Summary:\nThe response from this tool will primarily be a textual answer to the question, generated from the information within the provided table. The answer will be the result of the model processing both the table and the query, potentially accompanied by confidence scores or metadata, depending on the model's configuration and output settings.",
            "examples": [
                {
                    "user_intent": "I have a table of Olympic host cities and years, and I want to know when Beijing hosted the Olympics.",
                    "tool_call": {
                        "name": "neulab/omnitab-large-1024shot",
                        "arguments": {
                            "table": "pd.DataFrame.from_dict({'year': [1896, 1900, 1904, 2004, 2008, 2012], 'city': ['athens', 'paris', 'st. louis', 'athens', 'beijing', 'london']})",
                            "query": "In which year did Beijing host the Olympic Games?"
                        }
                    }
                },
                {
                    "user_intent": "Given a sales table for different products, find out which product had the highest revenue in Q1.",
                    "tool_call": {
                        "name": "neulab/omnitab-large-1024shot",
                        "arguments": {
                            "table": "pd.DataFrame.from_dict({'product': ['Laptop', 'Tablet', 'Smartphone'], 'Q1_revenue': [50000, 30000, 70000]})",
                            "query": "Which product had the highest revenue in Q1?"
                        }
                    }
                },
                {
                    "user_intent": "I have a table of students and their test scores. I want to know who scored above 90 in Math.",
                    "tool_call": {
                        "name": "neulab/omnitab-large-1024shot",
                        "arguments": {
                            "table": "pd.DataFrame.from_dict({'student': ['Alice', 'Bob', 'Charlie'], 'Math': [95, 88, 92], 'English': [85, 90, 87]})",
                            "query": "Which students scored above 90 in Math?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 2: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 5: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 6: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 7: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 8: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 9: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 10: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 11: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 12: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 13: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 15: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 16: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 17: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 18: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 19: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\", \"parameters\": {\"table\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the table data on which the question answering will be performed.\", \"required\": true, \"example\": \"pd.DataFrame.from_dict({'year': [1896, 1900], 'city': ['athens', 'paris']})\"}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question string that will be answered based on the provided table.\", \"required\": true, \"example\": \"In which year did Beijing host the Olympic Games?\"}}, \"response\": \"The tool described is used for table-based question answering (QA) in a Natural Language Processing (NLP) context. It leverages the OmniTab model, which is a pre-trained sequence-to-sequence model based on the BART architecture. This model has been trained with both natural and synthetic data, specifically for tasks that involve answering questions about tabular data.\\n\\nGiven the tool's purpose and the use of the `AutoModelForSeq2SeqLM` method from the Hugging Face Transformers library, the expected response structure would be as follows:\\n\\n### Expected Response Structure:\\n1. **Answer (String)**:  \\n   The response will contain a textual answer derived from the given table based on the query. The model generates this answer by processing the table's content and the natural language question. The answer will be a string that directly responds to the query posed.\\n\\n2. **Confidence or Probability (Optional)**:  \\n   While not explicitly mentioned in the tool documentation, it is common for models like this to provide a measure of confidence in the generated answer. This could take the form of a confidence score or a probability, indicating how confident the model is in the given answer. If present, this would typically be a floating-point number ranging from 0 to 1.\\n\\n3. **Error or Invalid Response (Optional)**:  \\n   If the model is unable to generate a meaningful answer (e.g., due to insufficient or ambiguous data), the response might include an error message or an indication that no answer could be generated. This could also take the form of a special token or a string such as \\\"Unable to answer\\\" or \\\"Invalid query.\\\"\\n\\n4. **Additional Metadata (Optional)**:  \\n   Depending on the specific implementation, the response might contain extra information, such as the processing time, the number of tokens processed, or the number of possible candidate answers considered by the model.\\n\\n### Summary:\\nThe response from this tool will primarily be a textual answer to the question, generated from the information within the provided table. The answer will be the result of the model processing both the table and the query, potentially accompanied by confidence scores or metadata, depending on the model's configuration and output settings.\", \"examples\": [{\"user_intent\": \"I have a table of Olympic host cities and years, and I want to know when Beijing hosted the Olympics.\", \"tool_call\": {\"name\": \"neulab/omnitab-large-1024shot\", \"arguments\": {\"table\": \"pd.DataFrame.from_dict({'year': [1896, 1900, 1904, 2004, 2008, 2012], 'city': ['athens', 'paris', 'st. louis', 'athens', 'beijing', 'london']})\", \"query\": \"In which year did Beijing host the Olympic Games?\"}}}, {\"user_intent\": \"Given a sales table for different products, find out which product had the highest revenue in Q1.\", \"tool_call\": {\"name\": \"neulab/omnitab-large-1024shot\", \"arguments\": {\"table\": \"pd.DataFrame.from_dict({'product': ['Laptop', 'Tablet', 'Smartphone'], 'Q1_revenue': [50000, 30000, 70000]})\", \"query\": \"Which product had the highest revenue in Q1?\"}}}, {\"user_intent\": \"I have a table of students and their test scores. I want to know who scored above 90 in Math.\", \"tool_call\": {\"name\": \"neulab/omnitab-large-1024shot\", \"arguments\": {\"table\": \"pd.DataFrame.from_dict({'student': ['Alice', 'Bob', 'Charlie'], 'Math': [95, 88, 92], 'English': [85, 90, 87]})\", \"query\": \"Which students scored above 90 in Math?\"}}}]}\n\n# User Query\nHelp me setup a tinyroberta model from deepset for Question and Answer. Provide a sample input and output.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to set up a tinyroberta model from deepset for question answering.",
                    "tool_description": "Set up the deepset/roberta-base-squad2 model, a fine-tuned RoBERTa model trained on the SQuAD 2.0 dataset, designed for question answering tasks. It processes a given context and question to extract accurate answers.",
                    "expected_response": "A configuration object for setting up the deepset/roberta-base-squad2 model, possibly including example usage and initialization instructions."
                },
                {
                    "user_intent": "Provide a sample input and output for the tinyroberta model in question answering.",
                    "tool_description": "Provide an example input for the deepset/roberta-base-squad2 model, which consists of a context (text) and a question. The model processes the context and question to generate a precise answer.",
                    "expected_response": "An example input with context and question, along with the corresponding output, which will be the answer extracted by the model."
                }
            ],
            "extracted_arguments": {
                "model_name": "The name or identifier of the question answering model to be used (e.g., deepset/roberta-base-squad2).",
                "context": "The context or passage of text from which the model will extract answers.",
                "question": "The specific question that the model will answer based on the provided context."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_420",
        "query": "We are building a road maintenance reporting application. We need to use images to identify and segment road potholes.",
        "instruction": "Given a `road maintenance image processing` task, retrieve tools that perform image segmentation on potholes by processing image inputs to identify and segment the specific features relevant to road maintenance applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "YOLO('keremberke/yolov8m-pothole-segmentation')",
                    "api_arguments": {
                        "image": "URL or local image path"
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.23",
                        "ultralytics==8.0.21"
                    ],
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nprint(results[0].masks)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "pothole-segmentation",
                        "accuracy": {
                            "mAP@0.5(box)": 0.858,
                            "mAP@0.5(mask)": 0.895
                        }
                    },
                    "description": "A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.",
                    "id": "huggingface_api_254",
                    "name": "keremberke/yolov8m-pothole-segmentation"
                },
                "id": "gorilla_huggingface_tool_250",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 1: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.",
            "Tool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 3: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 4: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 5: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 6: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 7: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 8: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 9: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 10: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 11: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 13: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 14: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 15: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 18: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images."
        ],
        "tool_view": {
            "description": "This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\n\n1. **Boxes**  \n   - A collection of bounding boxes corresponding to detected potholes in the image.  \n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - May include additional metadata such as confidence scores for each detection.\n\n2. **Masks**  \n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\n\n3. **Class Labels (optional)**  \n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\n\n4. **Confidence Scores (optional)**  \n   - A numeric confidence value for each detection, reflecting the model’s certainty that the detected object is indeed a pothole.\n\n5. **Rendered Output (optional)**  \n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\n\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.",
            "examples": [
                {
                    "user_intent": "I want to detect potholes in an image using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/pothole_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Can I use a local image to detect potholes with the YOLOv8 model?",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "/path/to/local/image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/street_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 1: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.\nTool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 3: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 4: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 5: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 6: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 7: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 8: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 9: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 10: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 11: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 13: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 14: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 15: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 18: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\\n\\n1. **Boxes**  \\n   - A collection of bounding boxes corresponding to detected potholes in the image.  \\n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - May include additional metadata such as confidence scores for each detection.\\n\\n2. **Masks**  \\n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \\n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\\n\\n3. **Class Labels (optional)**  \\n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\\n\\n4. **Confidence Scores (optional)**  \\n   - A numeric confidence value for each detection, reflecting the model\\u2019s certainty that the detected object is indeed a pothole.\\n\\n5. **Rendered Output (optional)**  \\n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\\n\\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.\", \"examples\": [{\"user_intent\": \"I want to detect potholes in an image using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/pothole_image.jpg\"}}}, {\"user_intent\": \"Can I use a local image to detect potholes with the YOLOv8 model?\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"/path/to/local/image.jpg\"}}}, {\"user_intent\": \"I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/street_image.jpg\"}}}]}\n\n# User Query\nWe are building a road maintenance reporting application. We need to use images to identify and segment road potholes.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect potholes in an image using the YOLOv8 pothole segmentation model.",
                    "tool_description": "Use a YOLOv8-based image segmentation model to identify potholes in input images, outputting bounding boxes and segmentation masks for each detected pothole.",
                    "expected_response": "A structured result containing bounding boxes, segmentation masks, and confidence scores for each detected pothole in the image."
                }
            ],
            "extracted_arguments": {
                "image": "The input image containing road surfaces to analyze for potholes, provided as a URL or local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_137",
        "query": "Create a script for an AI bot that automatically adds punctuation to users' messages in a chat app.",
        "instruction": "Given a `text enhancement` task, retrieve tools that improve written communication by processing user messages to add appropriate punctuation automatically, ensuring the output aligns with grammatical standards and improves readability.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('token-classification')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "multilingual",
                        "accuracy": 0.98
                    },
                    "description": "A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.",
                    "id": "huggingface_api_934",
                    "name": "kredor/punctuate-all"
                },
                "id": "gorilla_huggingface_tool_415",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 1: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 2: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 3: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 5: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 6: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 7: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 9: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 10: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 11: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 16: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 17: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 18: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 19: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input."
        ],
        "tool_view": {
            "description": "A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "parameters": {},
            "response": "The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model’s generated continuation or reply to the provided input message.  \n\nKey components of the response:  \n\n- **Generated Text**: The main content of the response is a natural language string that represents the model’s reply in a conversational style, intended to mimic Elon Musk’s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\"generated_text\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \n\nSemantically, the response conveys the AI’s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk’s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.",
            "examples": [
                {
                    "user_intent": "I want to chat with a version of Elon Musk. Can you start a conversation?",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Hello, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "Let's see if this AI can make a joke like Elon Musk would.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "Tell me a joke, Elon!"
                        }
                    }
                },
                {
                    "user_intent": "I need to test how Elon Musk would respond to a random question. Ask him about space.",
                    "tool_call": {
                        "name": "Pi3141/DialoGPT-medium-elon-3",
                        "arguments": {
                            "input_text": "What do you think about Mars colonization?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 1: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 2: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 3: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 5: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 6: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 7: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 9: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 10: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 11: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 12: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 16: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 17: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 18: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 19: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A conversational AI model that mimics Elon Musk\\u2019s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\", \"parameters\": {}, \"response\": \"The tool produces a text-generation response in a conversational format. Its response structure is typically a string representing the model\\u2019s generated continuation or reply to the provided input message.  \\n\\nKey components of the response:  \\n\\n- **Generated Text**: The main content of the response is a natural language string that represents the model\\u2019s reply in a conversational style, intended to mimic Elon Musk\\u2019s tweeting or speaking style. This text may include informal language, emojis, or humorous/non-factual content.  \\n- **Optional Metadata (framework-dependent)**: Depending on the Hugging Face Transformers pipeline configuration, the response may be returned as a list of dictionaries, where each dictionary contains a key like `\\\"generated_text\\\"` mapping to the generated string. Additional metadata, such as token usage or generation parameters, might also be included if explicitly enabled, though it is not required by default.  \\n\\nSemantically, the response conveys the AI\\u2019s attempt to continue the conversation coherently based on the input prompt, reflecting the stylistic characteristics learned from Elon Musk\\u2019s tweets. It may vary in relevance and factual accuracy, with occasional nonsensical output.\", \"examples\": [{\"user_intent\": \"I want to chat with a version of Elon Musk. Can you start a conversation?\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Hello, Elon!\"}}}, {\"user_intent\": \"Let's see if this AI can make a joke like Elon Musk would.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"Tell me a joke, Elon!\"}}}, {\"user_intent\": \"I need to test how Elon Musk would respond to a random question. Ask him about space.\", \"tool_call\": {\"name\": \"Pi3141/DialoGPT-medium-elon-3\", \"arguments\": {\"input_text\": \"What do you think about Mars colonization?\"}}}]}\n\n# User Query\nCreate a script for an AI bot that automatically adds punctuation to users' messages in a chat app.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want an AI that automatically adds punctuation to chat messages.",
                    "tool_description": "Use a pretrained punctuation restoration model to analyze unpunctuated text and insert appropriate punctuation marks based on linguistic context.",
                    "expected_response": "A text string with punctuation added, improving readability and grammatical correctness."
                },
                {
                    "user_intent": "Integrate the punctuation AI into a chat application so messages are punctuated automatically.",
                    "tool_description": "Develop a script or API wrapper that feeds user messages to the punctuation model and returns the punctuated text for display in the chat app.",
                    "expected_response": "Don't need response."
                }
            ],
            "extracted_arguments": {
                "input_text": "The user's raw chat message without punctuation, as a string.",
                "output_text": "The processed chat message with restored punctuation, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_254",
        "query": "Gather information about annual income and age demographics of employees to predict retirement patterns. Make sure to identify top employees for potential promotions.",
        "instruction": "Given a `data synthesis and analysis` task, retrieve tools that collect and analyze employee demographic data, such as annual income and age, to predict retirement patterns and assess performance for identifying top candidates for promotions, thereby providing actionable insights based on structured data inputs like tables.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')",
                    "api_arguments": [
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "https://huggingface.co/google/tapas-large-finetuned-sqa",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.7289
                    },
                    "description": "TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_450",
                    "name": "google/tapas-large-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_446",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 2: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 5: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 6: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 7: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 8: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 10: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 11: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 12: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 13: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 14: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 15: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 18: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 19: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction."
        ],
        "tool_view": {
            "description": "A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "parameters": {
                "bootstrap": {
                    "type": "boolean",
                    "description": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
                    "required": true,
                    "default": "True",
                    "example": true
                },
                "ccp_alpha": {
                    "type": "number",
                    "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "criterion": {
                    "type": "string",
                    "description": "The function used to measure the quality of a split. 'squared_error' uses mean squared error.",
                    "required": true,
                    "default": "squared_error",
                    "example": "squared_error"
                },
                "max_depth": {
                    "type": "integer",
                    "description": "The maximum depth of the tree. Limits how deep each tree can grow.",
                    "required": true,
                    "default": "10",
                    "example": 10
                },
                "max_features": {
                    "type": "number",
                    "description": "The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.",
                    "required": true,
                    "default": "1.0",
                    "example": 1.0
                },
                "max_leaf_nodes": {
                    "type": "string",
                    "description": "Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.",
                    "required": true,
                    "example": ""
                },
                "max_samples": {
                    "type": "string",
                    "description": "If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.",
                    "required": true,
                    "example": ""
                },
                "min_impurity_decrease": {
                    "type": "number",
                    "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "min_samples_leaf": {
                    "type": "integer",
                    "description": "The minimum number of samples required to be at a leaf node.",
                    "required": true,
                    "default": "1",
                    "example": 1
                },
                "min_samples_split": {
                    "type": "integer",
                    "description": "The minimum number of samples required to split an internal node.",
                    "required": true,
                    "default": "2",
                    "example": 2
                },
                "min_weight_fraction_leaf": {
                    "type": "number",
                    "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "n_estimators": {
                    "type": "integer",
                    "description": "The number of trees in the forest.",
                    "required": true,
                    "default": "50",
                    "example": 50
                },
                "n_jobs": {
                    "type": "string",
                    "description": "The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.",
                    "required": true,
                    "example": ""
                },
                "oob_score": {
                    "type": "boolean",
                    "description": "Whether to use out-of-bag samples to estimate the R^2 on unseen data.",
                    "required": true,
                    "default": "False",
                    "example": false
                },
                "random_state": {
                    "type": "integer",
                    "description": "Controls the randomness of the estimator for reproducibility.",
                    "required": true,
                    "default": "59",
                    "example": 59
                },
                "verbose": {
                    "type": "integer",
                    "description": "Controls the verbosity when fitting and predicting.",
                    "required": true,
                    "default": "0",
                    "example": 0
                },
                "warm_start": {
                    "type": "boolean",
                    "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.",
                    "required": true,
                    "default": "False",
                    "example": false
                }
            },
            "response": "The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\n\n1. **model description**: \n   - **Domain**: \"Tabular Tabular Regression\" - This indicates that the model is designed for regression tasks involving tabular data.\n   - **Framework**: \"Scikit-learn\" - Specifies that the model is built using the Scikit-learn machine learning library.\n   - **Functionality**: \"skops\" - Refers to the model's compatibility with or use of the Skops library.\n   - **Description**: \"A RandomForestRegressor model for electricity consumption prediction\" - A brief description of what the model is used for.\n\n2. **API call**: \n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\n\n3. **Performance**: \n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\n\n4. **Example Code**: \n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\n\n5. **Python Environment Requirements**: \n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\n\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.",
            "examples": [
                {
                    "user_intent": "I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 100,
                            "n_jobs": -1,
                            "oob_score": true,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 20,
                            "max_features": 1.5,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 2: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 4: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 5: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 6: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 7: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 8: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 10: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 11: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 12: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 13: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 14: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 15: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 18: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 19: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\", \"parameters\": {\"bootstrap\": {\"type\": \"boolean\", \"description\": \"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\", \"required\": true, \"default\": \"True\", \"example\": true}, \"ccp_alpha\": {\"type\": \"number\", \"description\": \"Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"criterion\": {\"type\": \"string\", \"description\": \"The function used to measure the quality of a split. 'squared_error' uses mean squared error.\", \"required\": true, \"default\": \"squared_error\", \"example\": \"squared_error\"}, \"max_depth\": {\"type\": \"integer\", \"description\": \"The maximum depth of the tree. Limits how deep each tree can grow.\", \"required\": true, \"default\": \"10\", \"example\": 10}, \"max_features\": {\"type\": \"number\", \"description\": \"The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.\", \"required\": true, \"default\": \"1.0\", \"example\": 1.0}, \"max_leaf_nodes\": {\"type\": \"string\", \"description\": \"Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\", \"required\": true, \"example\": \"\"}, \"max_samples\": {\"type\": \"string\", \"description\": \"If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.\", \"required\": true, \"example\": \"\"}, \"min_impurity_decrease\": {\"type\": \"number\", \"description\": \"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"min_samples_leaf\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to be at a leaf node.\", \"required\": true, \"default\": \"1\", \"example\": 1}, \"min_samples_split\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to split an internal node.\", \"required\": true, \"default\": \"2\", \"example\": 2}, \"min_weight_fraction_leaf\": {\"type\": \"number\", \"description\": \"The minimum weighted fraction of the sum total of weights required to be at a leaf node.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"n_estimators\": {\"type\": \"integer\", \"description\": \"The number of trees in the forest.\", \"required\": true, \"default\": \"50\", \"example\": 50}, \"n_jobs\": {\"type\": \"string\", \"description\": \"The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.\", \"required\": true, \"example\": \"\"}, \"oob_score\": {\"type\": \"boolean\", \"description\": \"Whether to use out-of-bag samples to estimate the R^2 on unseen data.\", \"required\": true, \"default\": \"False\", \"example\": false}, \"random_state\": {\"type\": \"integer\", \"description\": \"Controls the randomness of the estimator for reproducibility.\", \"required\": true, \"default\": \"59\", \"example\": 59}, \"verbose\": {\"type\": \"integer\", \"description\": \"Controls the verbosity when fitting and predicting.\", \"required\": true, \"default\": \"0\", \"example\": 0}, \"warm_start\": {\"type\": \"boolean\", \"description\": \"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.\", \"required\": true, \"default\": \"False\", \"example\": false}}, \"response\": \"The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\\n\\n1. **model description**: \\n   - **Domain**: \\\"Tabular Tabular Regression\\\" - This indicates that the model is designed for regression tasks involving tabular data.\\n   - **Framework**: \\\"Scikit-learn\\\" - Specifies that the model is built using the Scikit-learn machine learning library.\\n   - **Functionality**: \\\"skops\\\" - Refers to the model's compatibility with or use of the Skops library.\\n   - **Description**: \\\"A RandomForestRegressor model for electricity consumption prediction\\\" - A brief description of what the model is used for.\\n\\n2. **API call**: \\n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \\n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\\n\\n3. **Performance**: \\n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\\n\\n4. **Example Code**: \\n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\\n\\n5. **Python Environment Requirements**: \\n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\\n\\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.\", \"examples\": [{\"user_intent\": \"I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": -1, \"oob_score\": true, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 20, \"max_features\": 1.5, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}]}\n\n# User Query\nGather information about annual income and age demographics of employees to predict retirement patterns. Make sure to identify top employees for potential promotions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze employee tabular data to predict retirement patterns based on age and income.",
                    "tool_description": "Use a tabular regression model to predict retirement likelihood or patterns from structured employee features such as age, income, and employment history.",
                    "expected_response": "A dataset with predicted retirement probabilities or patterns for each employee."
                },
                {
                    "user_intent": "Identify top-performing employees for potential promotions based on demographics and predicted retirement patterns.",
                    "tool_description": "Apply a tabular classification or scoring method to rank employees according to performance, retention risk, or eligibility for promotion.",
                    "expected_response": "A ranked list of employees with associated promotion eligibility scores or categories."
                }
            ],
            "extracted_arguments": {
                "employee_age": "Numeric age of each employee in years.",
                "annual_income": "Numeric annual income of each employee.",
                "employee_performance_metrics": "Structured data representing performance indicators for employees, used for ranking or promotion eligibility.",
                "retirement_target": "Indicator or outcome representing predicted retirement likelihood or timing for each employee."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_388",
        "query": "A Romanian-speaking person wants to communicate with an English-speaking friend over the phone using our platform. Please provide a method to translate their speech in real-time.",
        "instruction": "Given a `real-time speech translation` task, retrieve tools that support audio-to-audio translation by processing Romanian speech inputs and delivering English speech outputs, facilitating seamless communication between languages during phone conversations.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "pipeline('audio-to-audio', model='facebook/textless_sm_ro_en')",
                    "api_arguments": "audio file or recording",
                    "python_environment_requirements": "fairseq, huggingface_hub",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "A speech-to-speech translation model for Romanian to English developed by Facebook AI",
                    "id": "huggingface_api_801",
                    "name": "facebook/textless_sm_ro_en"
                },
                "id": "gorilla_huggingface_tool_796",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 1: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 2: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 3: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 4: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 6: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 7: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 8: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 9: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 11: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 13: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 14: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.",
            "Tool 15: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 16: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 17: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 18: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 19: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing."
        ],
        "tool_view": {
            "description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "parameters": {
                "audio.wav": {
                    "type": "string",
                    "description": "Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\n\n1. **Timeline object**:  \n   - Represents the sequence of time intervals in the audio.  \n   - Each interval corresponds to a period where overlapped speech occurs.\n\n2. **Speech segments** (within the timeline):  \n   - Each segment has at least two key attributes:  \n     - **start**: The timestamp indicating when the overlapped speech begins.  \n     - **end**: The timestamp indicating when the overlapped speech ends.  \n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\n\n3. **Support or confidence information** (optional, depending on the output object):  \n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\n\n4. **Methods for iteration and access**:  \n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\n\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.",
            "examples": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recording.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "conference_call.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a meeting recording to identify parts with overlapping speech.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "team_meeting.wav"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any overlapping speech segments in this podcast episode.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_3.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 1: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 2: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 3: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 4: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 5: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 6: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 7: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 8: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 9: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 11: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 13: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 14: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.\nTool 15: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 16: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 17: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 18: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 19: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\", \"parameters\": {\"audio.wav\": {\"type\": \"string\", \"description\": \"Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\\n\\n1. **Timeline object**:  \\n   - Represents the sequence of time intervals in the audio.  \\n   - Each interval corresponds to a period where overlapped speech occurs.\\n\\n2. **Speech segments** (within the timeline):  \\n   - Each segment has at least two key attributes:  \\n     - **start**: The timestamp indicating when the overlapped speech begins.  \\n     - **end**: The timestamp indicating when the overlapped speech ends.  \\n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\\n\\n3. **Support or confidence information** (optional, depending on the output object):  \\n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\\n\\n4. **Methods for iteration and access**:  \\n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\\n\\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.\", \"examples\": [{\"user_intent\": \"Detect when multiple speakers are talking simultaneously in a recording.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"conference_call.wav\"}}}, {\"user_intent\": \"I need to analyze a meeting recording to identify parts with overlapping speech.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"team_meeting.wav\"}}}, {\"user_intent\": \"Check if there are any overlapping speech segments in this podcast episode.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_3.wav\"}}}]}\n\n# User Query\nA Romanian-speaking person wants to communicate with an English-speaking friend over the phone using our platform. Please provide a method to translate their speech in real-time.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate speech from Romanian to English in real-time.",
                    "tool_description": "Convert Romanian spoken language into English speech, enabling real-time communication between speakers of different languages.",
                    "expected_response": "Translated speech output in English, synchronized with the original Romanian speech."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the input audio (Romanian).",
                "target_language": "The language into which the audio is to be translated (English).",
                "audio_input": "The audio input containing Romanian speech to be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_144",
        "query": "I have a recorded meeting between multiple participants. Identify when two or more people are speaking simultaneously.",
        "instruction": "Given an `overlapped speech detection` task, retrieve tools that specialize in analyzing audio inputs to identify and detect moments where two or more participants are speaking simultaneously, using appropriate audio processing methods.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "pyannote.audio",
                    "functionality": "overlapped-speech-detection",
                    "api_call": "pipeline.from_pretrained('pyannote/overlapped-speech-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # two or more speakers are active between speech.start and speech.end\n  ...",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": null
                    },
                    "description": "Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.",
                    "id": "huggingface_api_750",
                    "name": "pyannote/overlapped-speech-detection"
                },
                "id": "gorilla_huggingface_tool_745",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 3: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 4: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 5: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 7: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 8: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 9: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 10: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 11: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 12: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.",
            "Tool 13: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 14: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 15: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 16: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 17: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 18: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 19: This tool uses the MoritzLaurer/DeBERTa-v3-large model to perform zero-shot text classification, allowing users to assign one or more candidate labels to a given text without prior task-specific training. It leverages a model fine-tuned on multiple NLI datasets, providing high accuracy across diverse benchmarks, and is suitable for rapidly classifying text in scenarios where labeled data is unavailable."
        ],
        "tool_view": {
            "description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "parameters": {
                "audio.wav": {
                    "type": "string",
                    "description": "Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\n\n1. **Timeline object**:  \n   - Represents the sequence of time intervals in the audio.  \n   - Each interval corresponds to a period where overlapped speech occurs.\n\n2. **Speech segments** (within the timeline):  \n   - Each segment has at least two key attributes:  \n     - **start**: The timestamp indicating when the overlapped speech begins.  \n     - **end**: The timestamp indicating when the overlapped speech ends.  \n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\n\n3. **Support or confidence information** (optional, depending on the output object):  \n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\n\n4. **Methods for iteration and access**:  \n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\n\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.",
            "examples": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recording.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "conference_call.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a meeting recording to identify parts with overlapping speech.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "team_meeting.wav"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any overlapping speech segments in this podcast episode.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_3.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 3: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 4: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 5: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 7: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 8: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 9: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 10: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 11: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 12: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.\nTool 13: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 14: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 15: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 16: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 17: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 18: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 19: This tool uses the MoritzLaurer/DeBERTa-v3-large model to perform zero-shot text classification, allowing users to assign one or more candidate labels to a given text without prior task-specific training. It leverages a model fine-tuned on multiple NLI datasets, providing high accuracy across diverse benchmarks, and is suitable for rapidly classifying text in scenarios where labeled data is unavailable.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\", \"parameters\": {\"audio.wav\": {\"type\": \"string\", \"description\": \"Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\\n\\n1. **Timeline object**:  \\n   - Represents the sequence of time intervals in the audio.  \\n   - Each interval corresponds to a period where overlapped speech occurs.\\n\\n2. **Speech segments** (within the timeline):  \\n   - Each segment has at least two key attributes:  \\n     - **start**: The timestamp indicating when the overlapped speech begins.  \\n     - **end**: The timestamp indicating when the overlapped speech ends.  \\n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\\n\\n3. **Support or confidence information** (optional, depending on the output object):  \\n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\\n\\n4. **Methods for iteration and access**:  \\n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\\n\\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.\", \"examples\": [{\"user_intent\": \"Detect when multiple speakers are talking simultaneously in a recording.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"conference_call.wav\"}}}, {\"user_intent\": \"I need to analyze a meeting recording to identify parts with overlapping speech.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"team_meeting.wav\"}}}, {\"user_intent\": \"Check if there are any overlapping speech segments in this podcast episode.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_3.wav\"}}}]}\n\n# User Query\nI have a recorded meeting between multiple participants. Identify when two or more people are speaking simultaneously.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recorded meeting.",
                    "tool_description": "Analyze an audio recording to identify time segments where two or more speakers are speaking at the same time, using automated speech analysis.",
                    "expected_response": "A structured timeline of audio intervals with metadata indicating periods of overlapping speech, including start and end timestamps for each segment."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the recorded meeting audio file in .wav format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_371",
        "query": "A Dutch friend asked for help in completing a sentence with a missing word. Can you fill in the blank?",
        "instruction": "Given a `sentence completion` task, retrieve tools that perform fill-in-the-blank operations by processing linguistic inputs using language models specifically trained in Dutch, accurately filling the missing words in the context provided.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "AutoModel.from_pretrained('GroNLP/bert-base-dutch-cased')",
                    "api_arguments": [
                        "pretrained_model_name_or_path"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModel, TFAutoModel\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "CoNLL-2002",
                                "accuracy": "90.24"
                            },
                            {
                                "name": "SoNaR-1",
                                "accuracy": "84.93"
                            },
                            {
                                "name": "spaCy UD LassySmall",
                                "accuracy": "86.10"
                            }
                        ]
                    },
                    "description": "BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.",
                    "id": "huggingface_api_680",
                    "name": "GroNLP/bert-base-dutch-cased"
                },
                "id": "gorilla_huggingface_tool_675",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 1: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 4: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 7: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 9: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 10: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 11: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 12: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.",
            "Tool 13: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.",
            "Tool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 16: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 17: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 18: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 19: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications."
        ],
        "tool_view": {
            "description": "GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pre-trained model to load. This can be a model identifier from the Hugging Face model hub or a local directory containing the model.",
                    "required": true,
                    "example": "GroNLP/bert-base-dutch-cased"
                }
            },
            "response": "The tool performs a **fill-mask task** using a pre-trained Dutch BERT model. Its response is typically a **ranked list of predicted tokens** that can fill a masked position in a given input sentence. The structure and meaning of the components in the response are as follows:\n\n- **sequence**: The full input text with the masked token replaced by the predicted token. This shows how the model expects the sentence to read with the candidate fill-in.  \n- **score**: A numeric value representing the model's confidence or probability assigned to that token as the correct fill for the masked position. Higher scores indicate greater likelihood.  \n- **token**: The numeric identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The textual representation of the predicted token, i.e., the actual word or subword that can replace the mask.\n\nThe response is generally **sorted in descending order of score**, so the first element represents the most likely prediction according to the model. Each element in the response provides both the textual prediction and its associated confidence, allowing downstream applications to select the best candidate or consider multiple options.",
            "examples": [
                {
                    "user_intent": "I want to predict missing words in a Dutch sentence like 'Ik hou van [MASK] chocolade'.",
                    "tool_call": {
                        "name": "GroNLP/bert-base-dutch-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "GroNLP/bert-base-dutch-cased"
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank in the Dutch sentence 'De kat zit op de [MASK]'.",
                    "tool_call": {
                        "name": "GroNLP/bert-base-dutch-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "GroNLP/bert-base-dutch-cased"
                        }
                    }
                },
                {
                    "user_intent": "Use BERTje to guess missing words in Dutch sentences for a language learning app.",
                    "tool_call": {
                        "name": "GroNLP/bert-base-dutch-cased",
                        "arguments": {
                            "pretrained_model_name_or_path": "GroNLP/bert-base-dutch-cased"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 1: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 4: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 7: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 9: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 10: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 11: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 12: The \"microsoft/deberta-v2-xlarge\" is a large-scale DeBERTa model optimized for natural language understanding tasks. It leverages disentangled attention and an enhanced mask decoder to outperform BERT and RoBERTa on a variety of NLU benchmarks. This model is designed to handle tasks such as question answering, sentence classification, and textual entailment, offering high performance across multiple datasets like SQuAD, MNLI, and SST-2. It can be loaded on-demand using the Inference API with PyTorch or TensorFlow, and is particularly suited for fill-mask tasks, where missing words in a sentence are predicted based on context.\nTool 13: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.\nTool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 16: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 17: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 18: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 19: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained model to load. This can be a model identifier from the Hugging Face model hub or a local directory containing the model.\", \"required\": true, \"example\": \"GroNLP/bert-base-dutch-cased\"}}, \"response\": \"The tool performs a **fill-mask task** using a pre-trained Dutch BERT model. Its response is typically a **ranked list of predicted tokens** that can fill a masked position in a given input sentence. The structure and meaning of the components in the response are as follows:\\n\\n- **sequence**: The full input text with the masked token replaced by the predicted token. This shows how the model expects the sentence to read with the candidate fill-in.  \\n- **score**: A numeric value representing the model's confidence or probability assigned to that token as the correct fill for the masked position. Higher scores indicate greater likelihood.  \\n- **token**: The numeric identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The textual representation of the predicted token, i.e., the actual word or subword that can replace the mask.\\n\\nThe response is generally **sorted in descending order of score**, so the first element represents the most likely prediction according to the model. Each element in the response provides both the textual prediction and its associated confidence, allowing downstream applications to select the best candidate or consider multiple options.\", \"examples\": [{\"user_intent\": \"I want to predict missing words in a Dutch sentence like 'Ik hou van [MASK] chocolade'.\", \"tool_call\": {\"name\": \"GroNLP/bert-base-dutch-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"GroNLP/bert-base-dutch-cased\"}}}, {\"user_intent\": \"Fill in the blank in the Dutch sentence 'De kat zit op de [MASK]'.\", \"tool_call\": {\"name\": \"GroNLP/bert-base-dutch-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"GroNLP/bert-base-dutch-cased\"}}}, {\"user_intent\": \"Use BERTje to guess missing words in Dutch sentences for a language learning app.\", \"tool_call\": {\"name\": \"GroNLP/bert-base-dutch-cased\", \"arguments\": {\"pretrained_model_name_or_path\": \"GroNLP/bert-base-dutch-cased\"}}}]}\n\n# User Query\nA Dutch friend asked for help in completing a sentence with a missing word. Can you fill in the blank?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict missing words in a Dutch sentence for my friend.",
                    "tool_description": "Use a pre-trained Dutch BERT model to perform a fill-mask task, predicting the most likely word(s) to complete a given sentence.",
                    "expected_response": "A ranked list of predicted tokens with their textual representation, confidence scores, and token IDs, showing possible words to fill the blank."
                }
            ],
            "extracted_arguments": {
                "pretrained_model_name_or_path": "The identifier or path of the Dutch BERT model to load for masked language prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_241",
        "query": "Help me generate a realistic bedroom interior image that can be used as reference for a 3D model being created for a virtual reality game.",
        "instruction": "Given an `image generation` task, retrieve tools that leverage diffusion probabilistic models to create high-quality and realistic images suited for use as references in 3D modeling and virtual reality applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Denoising Diffusion Probabilistic Models (DDPM)",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')",
                    "api_arguments": "None",
                    "python_environment_requirements": "diffusers",
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-bedroom-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception score": 9.46,
                            "FID score": 3.17
                        }
                    },
                    "description": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.",
                    "id": "huggingface_api_295",
                    "name": "google/ddpm-bedroom-256"
                },
                "id": "gorilla_huggingface_tool_291",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.",
            "Tool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 2: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 4: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 5: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 6: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 7: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 10: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.",
            "Tool 11: This tool uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images, trained on the unconditional CIFAR10 dataset. It supports various noise schedulers like DDPM, DDIM, and PNDM, and can be used through the Hugging Face Transformers framework. The model delivers impressive performance with an Inception score of 9.46 and an FID score of 3.17, making it suitable for generating realistic images in computer vision tasks.",
            "Tool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 13: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 14: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 15: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 16: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 17: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 18: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 19: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis."
        ],
        "tool_view": {
            "description": "This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.",
            "parameters": {
                "model_id": {
                    "type": "string",
                    "description": "The identifier for the pre-trained model to be loaded. This refers to a model available from a model hub (e.g., Hugging Face).",
                    "required": true,
                    "example": "google/ddpm-bedroom-256"
                }
            },
            "response": "The tool produces a high-resolution, unconditional image generated by a Denoising Diffusion Probabilistic Model (DDPM). The response structure is typically as follows:\n\n- **images**: A list or array containing one or more generated image objects. Each object represents a single synthesized image.  \n  - Each image is usually in a standard image format object compatible with Python imaging libraries (e.g., PIL Image), allowing for direct saving or further processing.  \n\nIn principle, the response contains:  \n\n1. **Generated Image Data** – The primary output, representing the visual content synthesized by the DDPM model based on learned data distributions. For this specific tool (`google/ddpm-bedroom-256`), the images depict bedroom scenes at 256×256 resolution.  \n\n2. **Optional Metadata** (if returned by the pipeline) – While not explicitly required by the documentation, some pipelines may include additional fields such as generation seed, image dimensions, or internal model state for reproducibility, but the core response focuses on the `images` array.  \n\nOverall, the response is a structured container of generated images ready for inspection, saving, or further downstream processing.",
            "examples": [
                {
                    "user_intent": "Generate a realistic 256x256 image of a bedroom using a pretrained diffusion model.",
                    "tool_call": {
                        "name": "DDPMPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "google/ddpm-bedroom-256"
                        }
                    }
                },
                {
                    "user_intent": "I want to create a high-quality bedroom image for a design concept without providing any input image.",
                    "tool_call": {
                        "name": "DDPMPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "google/ddpm-bedroom-256"
                        }
                    }
                },
                {
                    "user_intent": "Produce a single bedroom image using a diffusion probabilistic model for testing image generation quality.",
                    "tool_call": {
                        "name": "DDPMPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "google/ddpm-bedroom-256"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\nTool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 2: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 4: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 5: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 6: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 7: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 9: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 10: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.\nTool 11: This tool uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images, trained on the unconditional CIFAR10 dataset. It supports various noise schedulers like DDPM, DDIM, and PNDM, and can be used through the Hugging Face Transformers framework. The model delivers impressive performance with an Inception score of 9.46 and an FID score of 3.17, making it suitable for generating realistic images in computer vision tasks.\nTool 12: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 13: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 14: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 15: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 16: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 17: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 18: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 19: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256\\u00d7256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\", \"parameters\": {\"model_id\": {\"type\": \"string\", \"description\": \"The identifier for the pre-trained model to be loaded. This refers to a model available from a model hub (e.g., Hugging Face).\", \"required\": true, \"example\": \"google/ddpm-bedroom-256\"}}, \"response\": \"The tool produces a high-resolution, unconditional image generated by a Denoising Diffusion Probabilistic Model (DDPM). The response structure is typically as follows:\\n\\n- **images**: A list or array containing one or more generated image objects. Each object represents a single synthesized image.  \\n  - Each image is usually in a standard image format object compatible with Python imaging libraries (e.g., PIL Image), allowing for direct saving or further processing.  \\n\\nIn principle, the response contains:  \\n\\n1. **Generated Image Data** \\u2013 The primary output, representing the visual content synthesized by the DDPM model based on learned data distributions. For this specific tool (`google/ddpm-bedroom-256`), the images depict bedroom scenes at 256\\u00d7256 resolution.  \\n\\n2. **Optional Metadata** (if returned by the pipeline) \\u2013 While not explicitly required by the documentation, some pipelines may include additional fields such as generation seed, image dimensions, or internal model state for reproducibility, but the core response focuses on the `images` array.  \\n\\nOverall, the response is a structured container of generated images ready for inspection, saving, or further downstream processing.\", \"examples\": [{\"user_intent\": \"Generate a realistic 256x256 image of a bedroom using a pretrained diffusion model.\", \"tool_call\": {\"name\": \"DDPMPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"google/ddpm-bedroom-256\"}}}, {\"user_intent\": \"I want to create a high-quality bedroom image for a design concept without providing any input image.\", \"tool_call\": {\"name\": \"DDPMPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"google/ddpm-bedroom-256\"}}}, {\"user_intent\": \"Produce a single bedroom image using a diffusion probabilistic model for testing image generation quality.\", \"tool_call\": {\"name\": \"DDPMPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"google/ddpm-bedroom-256\"}}}]}\n\n# User Query\nHelp me generate a realistic bedroom interior image that can be used as reference for a 3D model being created for a virtual reality game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a realistic 256x256 image of a bedroom interior to serve as reference for a 3D model.",
                    "tool_description": "Use a pretrained Denoising Diffusion Probabilistic Model (DDPM) to generate high-quality, unconditional images of bedroom interiors.",
                    "expected_response": "A high-resolution 256x256 bedroom image suitable for reference in 3D modeling, returned as an image object compatible with Python imaging libraries."
                }
            ],
            "extracted_arguments": {
                "model_id": "The identifier for the pretrained DDPM model to generate bedroom images."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_2",
        "query": "We are a news agency that wants to extract useful features from Korean news articles for a content recommendation service.",
        "instruction": "Given a `feature extraction` task, retrieve tools that process Korean text inputs, such as news articles, to extract useful features for content recommendation services using language models capable of handling Korean language datasets.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "PyTorch Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "api_arguments": {
                        "tokenizer": "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
                    },
                    "python_environment_requirements": {
                        "transformers": "latest",
                        "tokenizers": "latest"
                    },
                    "example_code": "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')",
                    "performance": {
                        "dataset": "NSMC",
                        "accuracy": 0.901
                    },
                    "description": "KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.",
                    "id": "huggingface_api_8",
                    "name": "kobart-base-v2"
                },
                "id": "gorilla_huggingface_tool_8",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 5: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 6: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 7: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 8: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 9: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 10: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.",
            "Tool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 12: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.",
            "Tool 13: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 14: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 15: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 17: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 18: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 19: BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 4: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 5: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 6: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 7: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 8: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 9: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 10: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.\nTool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 12: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.\nTool 13: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 14: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 15: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 16: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 17: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 18: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 19: BART is a transformer-based encoder-decoder model that combines a bidirectional encoder with an autoregressive decoder, designed for generating and understanding text. It can extract features from input text, making it useful for tasks like text summarization, translation, classification, and question answering. This tool uses the Hugging Face Transformers framework to load a pre-trained BART model and obtain contextualized representations of text for downstream NLP applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nWe are a news agency that wants to extract useful features from Korean news articles for a content recommendation service.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract meaningful text features from Korean news articles for recommendation purposes.",
                    "tool_description": "Process Korean-language text to generate contextualized embeddings or feature representations suitable for downstream applications such as content recommendation, similarity assessment, or classification.",
                    "expected_response": "A vector or set of numerical features representing the semantic and contextual content of the input Korean news articles."
                }
            ],
            "extracted_arguments": {
                "text": "The full content of the Korean news article to be processed, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_18",
        "query": "We have a news article and we need to extract all the entities like the names of people, organizations, and locations.",
        "instruction": "Given an `entity extraction` task, retrieve tools that perform Named Entity Recognition (NER) by processing text inputs to identify and extract entities such as names of people, organizations, and locations from the provided content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')",
                    "api_arguments": {
                        "model": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
                        "tokenizer": "Davlan/distilbert-base-multilingual-cased-ner-hrl"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "ANERcorp",
                                "language": "Arabic"
                            },
                            {
                                "name": "conll 2003",
                                "language": "German"
                            },
                            {
                                "name": "conll 2003",
                                "language": "English"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Spanish"
                            },
                            {
                                "name": "Europeana Newspapers",
                                "language": "French"
                            },
                            {
                                "name": "Italian I-CAB",
                                "language": "Italian"
                            },
                            {
                                "name": "Latvian NER",
                                "language": "Latvian"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Dutch"
                            },
                            {
                                "name": "Paramopama + Second Harem",
                                "language": "Portuguese"
                            },
                            {
                                "name": "MSRA",
                                "language": "Chinese"
                            }
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).",
                    "id": "huggingface_api_408",
                    "name": "distilbert-base-multilingual-cased-ner-hrl"
                },
                "id": "gorilla_huggingface_tool_404",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 1: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 5: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 6: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 8: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 9: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 10: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 11: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 12: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 13: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 15: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 16: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 17: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 18: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 19: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently."
        ],
        "tool_view": {
            "description": "This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "parameters": {
                "Sentence": {
                    "type": "object",
                    "description": "A Sentence object from the Flair library, which contains the text for Named Entity Recognition (NER) tagging.",
                    "required": true,
                    "example": "Sentence('George Washington ging nach Washington')"
                }
            },
            "response": "The tool produces a response representing the results of Named Entity Recognition (NER) on a given input sentence. The response is associated with the `Sentence` object provided to the model and consists of one or more **entity spans** detected within the text. Each span contains structured information describing a recognized named entity. The typical structure and meaning of each component are as follows:\n\n1. **Sentence Object**  \n   - Represents the input text that was analyzed.  \n   - Contains the original tokens of the sentence.  \n   - After prediction, it holds a collection of entity spans that the model detected.\n\n2. **Entity Spans**  \n   Each entity span represents a contiguous sequence of tokens in the sentence that the model has labeled as a named entity. Each span typically includes:\n   - **Text**: The substring of the sentence corresponding to the entity.  \n   - **Start and End Positions**: Character indices or token positions marking where the entity occurs in the sentence.  \n   - **NER Label**: The predicted entity type, which for this model can be one of `PER` (person), `LOC` (location), `ORG` (organization), or `MISC` (miscellaneous).  \n   - **Confidence Score**: A numeric value indicating the model’s confidence in the prediction for that entity (optional depending on the implementation).  \n\n3. **Aggregated Information**  \n   - The sentence object can provide methods to retrieve all detected entities (`get_spans('ner')`) as a list of spans.  \n   - Each span can be iterated over for downstream processing, printing, or structured output.\n\nIn summary, the response is a structured representation of all named entities found in the input sentence, where each entity includes its text, location within the sentence, and predicted label. This enables downstream applications to extract, categorize, or highlight entities in the text.",
            "examples": [
                {
                    "user_intent": "I want to extract named entities from a German sentence using Flair's NER model.",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "George Washington ging nach Washington"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this sentence to identify any people, locations, or organizations in German text.",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "Angela Merkel besuchte Berlin im Jahr 2019."
                        }
                    }
                },
                {
                    "user_intent": "Can I extract named entities from the sentence 'Apple hat neue Produkte in Cupertino vorgestellt'?",
                    "tool_call": {
                        "name": "flair/ner-german",
                        "arguments": {
                            "Sentence": "Apple hat neue Produkte in Cupertino vorgestellt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 1: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 4: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 5: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 6: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 8: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 9: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 10: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 11: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 12: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 13: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 15: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 16: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 17: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 18: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 19: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities\\u2014person names, locations, organizations, and miscellaneous names\\u2014within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\", \"parameters\": {\"Sentence\": {\"type\": \"object\", \"description\": \"A Sentence object from the Flair library, which contains the text for Named Entity Recognition (NER) tagging.\", \"required\": true, \"example\": \"Sentence('George Washington ging nach Washington')\"}}, \"response\": \"The tool produces a response representing the results of Named Entity Recognition (NER) on a given input sentence. The response is associated with the `Sentence` object provided to the model and consists of one or more **entity spans** detected within the text. Each span contains structured information describing a recognized named entity. The typical structure and meaning of each component are as follows:\\n\\n1. **Sentence Object**  \\n   - Represents the input text that was analyzed.  \\n   - Contains the original tokens of the sentence.  \\n   - After prediction, it holds a collection of entity spans that the model detected.\\n\\n2. **Entity Spans**  \\n   Each entity span represents a contiguous sequence of tokens in the sentence that the model has labeled as a named entity. Each span typically includes:\\n   - **Text**: The substring of the sentence corresponding to the entity.  \\n   - **Start and End Positions**: Character indices or token positions marking where the entity occurs in the sentence.  \\n   - **NER Label**: The predicted entity type, which for this model can be one of `PER` (person), `LOC` (location), `ORG` (organization), or `MISC` (miscellaneous).  \\n   - **Confidence Score**: A numeric value indicating the model\\u2019s confidence in the prediction for that entity (optional depending on the implementation).  \\n\\n3. **Aggregated Information**  \\n   - The sentence object can provide methods to retrieve all detected entities (`get_spans('ner')`) as a list of spans.  \\n   - Each span can be iterated over for downstream processing, printing, or structured output.\\n\\nIn summary, the response is a structured representation of all named entities found in the input sentence, where each entity includes its text, location within the sentence, and predicted label. This enables downstream applications to extract, categorize, or highlight entities in the text.\", \"examples\": [{\"user_intent\": \"I want to extract named entities from a German sentence using Flair's NER model.\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"George Washington ging nach Washington\"}}}, {\"user_intent\": \"Analyze this sentence to identify any people, locations, or organizations in German text.\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"Angela Merkel besuchte Berlin im Jahr 2019.\"}}}, {\"user_intent\": \"Can I extract named entities from the sentence 'Apple hat neue Produkte in Cupertino vorgestellt'?\", \"tool_call\": {\"name\": \"flair/ner-german\", \"arguments\": {\"Sentence\": \"Apple hat neue Produkte in Cupertino vorgestellt\"}}}]}\n\n# User Query\nWe have a news article and we need to extract all the entities like the names of people, organizations, and locations.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract named entities from a news article.",
                    "tool_description": "Perform Named Entity Recognition (NER) to identify and extract structured information such as person names, locations, and organizations from the provided text.",
                    "expected_response": "A structured list of entity spans, each containing a recognized entity's text, type (person, location, organization), and position within the text."
                }
            ],
            "extracted_arguments": {
                "text": "The content of the news article that needs to be analyzed for named entities."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_192",
        "query": "We are building a fiction-sharing platform for multiple languages. Extract named entities from the given text.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform multilingual entity extraction from text by utilizing models capable of processing and identifying entities across multiple languages, aligning with the query's needs for a fiction-sharing platform.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)",
                        "tokenizer": "AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = My name is Wolfgang and I live in Berlin\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": "Babelscape/wikineural-multilingual-ner",
                        "accuracy": "span-based F1-score up to 6 points over previous state-of-the-art systems for data creation"
                    },
                    "description": "A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.",
                    "id": "huggingface_api_435",
                    "name": "Babelscape/wikineural-multilingual-ner"
                },
                "id": "gorilla_huggingface_tool_431",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 1: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 2: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 4: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 5: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 6: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 11: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 13: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 15: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 16: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 17: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 18: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 19: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries."
        ],
        "tool_view": {
            "description": "This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained model to use for Named Entity Recognition, loaded via the Transformers library.",
                    "required": true,
                    "example": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer corresponding to the pretrained model, used to convert text into token IDs for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')"
                }
            },
            "response": "The tool performs Named Entity Recognition (NER) using a pre-trained model based on the multilingual BERT architecture, specifically the model named \"Davlan/bert-base-multilingual-cased-ner-hrl\". The response from this tool would contain the following elements:\n\n1. **Entity Labels**: \n   The tool identifies and classifies entities in the provided input text. Each recognized entity is assigned a label that indicates its type. The possible entity labels are:\n   - **LOC** (Location)\n   - **ORG** (Organization)\n   - **PER** (Person)\n\n2. **Entities**: \n   The response will list the specific words or phrases from the input text that were identified as entities. Each entity will typically be paired with its corresponding label (e.g., \"PERSON\", \"LOCATION\", \"ORGANIZATION\").\n\n3. **Entity Boundaries**:\n   The recognized entities will likely include information about the token boundaries, i.e., where the entity starts and ends within the text. This allows users to map the detected entities back to the input text for further processing or analysis.\n\n4. **Confidence Scores (Optional)**:\n   While not explicitly mentioned, many Named Entity Recognition models, especially those based on transformer architectures, might provide confidence scores for each entity prediction. These scores would indicate the model's certainty regarding the classification of each entity.\n\n5. **Language**:\n   The model is capable of recognizing entities in multiple languages, so the response could potentially include a language identifier, specifying which language the text is in (e.g., Arabic, German, English). However, this is typically inferred based on the input and might not be explicitly provided in all cases.\n\nIn summary, the response will be a collection of entities identified in the input text, with each entity classified into one of the predefined categories (Location, Organization, or Person). Optionally, the response could also include confidence scores or other metadata related to the model's performance.",
            "examples": [
                {
                    "user_intent": "I want to extract names of people, locations, and organizations from an English news article.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "Apple announced a partnership with the German automotive company Volkswagen in Berlin yesterday."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities in a Spanish sentence for my multilingual NLP project.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "Lionel Messi jugó un partido increíble para el FC Barcelona en Madrid."
                        }
                    }
                },
                {
                    "user_intent": "Detect named entities in a Chinese sentence to test the model's multilingual capabilities.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "李小龍出生於美國舊金山，並在香港成長。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 1: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 2: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 4: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 5: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 6: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 7: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 10: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 11: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 13: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 14: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 15: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 16: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 17: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 18: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 19: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained model to use for Named Entity Recognition, loaded via the Transformers library.\", \"required\": true, \"example\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer corresponding to the pretrained model, used to convert text into token IDs for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\"}}, \"response\": \"The tool performs Named Entity Recognition (NER) using a pre-trained model based on the multilingual BERT architecture, specifically the model named \\\"Davlan/bert-base-multilingual-cased-ner-hrl\\\". The response from this tool would contain the following elements:\\n\\n1. **Entity Labels**: \\n   The tool identifies and classifies entities in the provided input text. Each recognized entity is assigned a label that indicates its type. The possible entity labels are:\\n   - **LOC** (Location)\\n   - **ORG** (Organization)\\n   - **PER** (Person)\\n\\n2. **Entities**: \\n   The response will list the specific words or phrases from the input text that were identified as entities. Each entity will typically be paired with its corresponding label (e.g., \\\"PERSON\\\", \\\"LOCATION\\\", \\\"ORGANIZATION\\\").\\n\\n3. **Entity Boundaries**:\\n   The recognized entities will likely include information about the token boundaries, i.e., where the entity starts and ends within the text. This allows users to map the detected entities back to the input text for further processing or analysis.\\n\\n4. **Confidence Scores (Optional)**:\\n   While not explicitly mentioned, many Named Entity Recognition models, especially those based on transformer architectures, might provide confidence scores for each entity prediction. These scores would indicate the model's certainty regarding the classification of each entity.\\n\\n5. **Language**:\\n   The model is capable of recognizing entities in multiple languages, so the response could potentially include a language identifier, specifying which language the text is in (e.g., Arabic, German, English). However, this is typically inferred based on the input and might not be explicitly provided in all cases.\\n\\nIn summary, the response will be a collection of entities identified in the input text, with each entity classified into one of the predefined categories (Location, Organization, or Person). Optionally, the response could also include confidence scores or other metadata related to the model's performance.\", \"examples\": [{\"user_intent\": \"I want to extract names of people, locations, and organizations from an English news article.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"Apple announced a partnership with the German automotive company Volkswagen in Berlin yesterday.\"}}}, {\"user_intent\": \"Identify entities in a Spanish sentence for my multilingual NLP project.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"Lionel Messi jug\\u00f3 un partido incre\\u00edble para el FC Barcelona en Madrid.\"}}}, {\"user_intent\": \"Detect named entities in a Chinese sentence to test the model's multilingual capabilities.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"\\u674e\\u5c0f\\u9f8d\\u51fa\\u751f\\u65bc\\u7f8e\\u570b\\u820a\\u91d1\\u5c71\\uff0c\\u4e26\\u5728\\u9999\\u6e2f\\u6210\\u9577\\u3002\"}}}]}\n\n# User Query\nWe are building a fiction-sharing platform for multiple languages. Extract named entities from the given text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract named entities from multilingual text for our fiction-sharing platform.",
                    "tool_description": "Use a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT to identify and classify person, organization, and location entities in text across multiple languages.",
                    "expected_response": "A list of entities detected in the input text, each labeled as a person, organization, or location, optionally including token boundaries and confidence scores."
                }
            ],
            "extracted_arguments": {
                "text": "The input text from which named entities should be extracted, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_237",
        "query": "Our client wants to create marketing slogans. Help them by completing this slogan \"Customer satisfaction is our top <mask>.\"",
        "instruction": "Given a `slogan completion` task, retrieve tools that perform masked language modeling by processing incomplete text inputs to generate suitable words or phrases to fill in the blanks according to the context and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='roberta-large')",
                    "api_arguments": {
                        "model": "roberta-large"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-large')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "GLUE",
                        "accuracy": {
                            "MNLI": 90.2,
                            "QQP": 92.2,
                            "QNLI": 94.7,
                            "SST-2": 96.4,
                            "CoLA": 68.0,
                            "STS-B": 96.4,
                            "MRPC": 90.9,
                            "RTE": 86.6
                        }
                    },
                    "description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using the Masked language modeling (MLM) objective. It can be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.",
                    "id": "huggingface_api_671",
                    "name": "roberta-large"
                },
                "id": "gorilla_huggingface_tool_666",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 2: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 3: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 7: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 9: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 10: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.",
            "Tool 11: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 12: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 14: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.",
            "Tool 15: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 16: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 17: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 19: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy."
        ],
        "tool_view": {
            "description": "This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path to the image that you want to classify using the zero-shot image classification model.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "labels": {
                    "type": "array",
                    "description": "A list of possible class names or labels to consider when performing zero-shot classification on the image.",
                    "required": true,
                    "example": [
                        "cat",
                        "dog",
                        "bird"
                    ]
                }
            },
            "response": "The tool’s response is a structured output representing the results of zero-shot image classification for a given image against a set of candidate labels. The response typically contains the following components:  \n\n1. **Predicted Labels** – A list of one or more class names from the provided label set that the model considers most likely for the input image. These labels indicate the categories the image most closely aligns with according to the model.  \n\n2. **Confidence Scores** – Corresponding numerical scores or probabilities for each predicted label, representing the model’s estimated likelihood or confidence that the image belongs to that class. These are usually normalized values summing to 1 across all candidate labels, though some implementations may provide raw logits instead.  \n\n3. **Optional Ranking Information** – In some implementations, the labels may be returned in descending order of confidence, effectively ranking the candidate classes from most to least likely.  \n\nOverall, the response conveys, for each input image, how likely it is to belong to each candidate class, enabling selection of the most probable category or further downstream processing such as threshold-based classification or retrieval.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a pet to see if it's a cat, dog, or rabbit without training a model.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/pet_photo.jpg",
                            "labels": [
                                "cat",
                                "dog",
                                "rabbit"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Determine the main object in this outdoor scene image among several possibilities.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/park_scene.png",
                            "labels": [
                                "tree",
                                "bench",
                                "car",
                                "person"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Check what kind of fruit is shown in this photo without manually labeling it.",
                    "tool_call": {
                        "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                        "arguments": {
                            "image_path": "images/fruit_bowl.jpeg",
                            "labels": [
                                "apple",
                                "banana",
                                "orange",
                                "grape"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 2: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 3: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 4: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 7: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 8: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 9: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 10: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.\nTool 11: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 12: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 14: The `microsoft/deberta-v3-base` model is a state-of-the-art DeBERTa V3 transformer designed for fill-mask tasks, allowing users to predict missing words in text. It enhances BERT and RoBERTa architectures through disentangled attention and an improved mask decoder, offering efficient and accurate language understanding. This model is suitable for NLP applications that require contextual word prediction, leveraging pre-trained knowledge to generate high-quality masked token completions.\nTool 15: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 16: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 17: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 19: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path to the image that you want to classify using the zero-shot image classification model.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"labels\": {\"type\": \"array\", \"description\": \"A list of possible class names or labels to consider when performing zero-shot classification on the image.\", \"required\": true, \"example\": [\"cat\", \"dog\", \"bird\"]}}, \"response\": \"The tool\\u2019s response is a structured output representing the results of zero-shot image classification for a given image against a set of candidate labels. The response typically contains the following components:  \\n\\n1. **Predicted Labels** \\u2013 A list of one or more class names from the provided label set that the model considers most likely for the input image. These labels indicate the categories the image most closely aligns with according to the model.  \\n\\n2. **Confidence Scores** \\u2013 Corresponding numerical scores or probabilities for each predicted label, representing the model\\u2019s estimated likelihood or confidence that the image belongs to that class. These are usually normalized values summing to 1 across all candidate labels, though some implementations may provide raw logits instead.  \\n\\n3. **Optional Ranking Information** \\u2013 In some implementations, the labels may be returned in descending order of confidence, effectively ranking the candidate classes from most to least likely.  \\n\\nOverall, the response conveys, for each input image, how likely it is to belong to each candidate class, enabling selection of the most probable category or further downstream processing such as threshold-based classification or retrieval.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a pet to see if it's a cat, dog, or rabbit without training a model.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/pet_photo.jpg\", \"labels\": [\"cat\", \"dog\", \"rabbit\"]}}}, {\"user_intent\": \"Determine the main object in this outdoor scene image among several possibilities.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/park_scene.png\", \"labels\": [\"tree\", \"bench\", \"car\", \"person\"]}}}, {\"user_intent\": \"Check what kind of fruit is shown in this photo without manually labeling it.\", \"tool_call\": {\"name\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"arguments\": {\"image_path\": \"images/fruit_bowl.jpeg\", \"labels\": [\"apple\", \"banana\", \"orange\", \"grape\"]}}}]}\n\n# User Query\nOur client wants to create marketing slogans. Help them by completing this slogan \"Customer satisfaction is our top <mask>.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Complete the given marketing slogan by predicting the masked word in the sentence.",
                    "tool_description": "Use a masked language model to predict the most likely word(s) that fit into the masked position within a text, based on context and linguistic patterns.",
                    "expected_response": "A list of predicted words or tokens with confidence scores indicating how likely each word fits the masked position in the input text."
                }
            ],
            "extracted_arguments": {
                "input_text": "The slogan containing a masked token (e.g., <mask>) that needs to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_303",
        "query": "We need a product description for an image-based online store platform that will help customers understand the specifics of the product.",
        "instruction": "Given a `product description generation` task, retrieve tools that process image inputs to produce descriptive and informative text outputs, helping customers understand product specifics in an online store environment.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')",
                    "api_arguments": "image, text",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.",
                    "id": "huggingface_api_77",
                    "name": "git-large-coco"
                },
                "id": "gorilla_huggingface_tool_77",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 4: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 7: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 8: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 10: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 13: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 14: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 17: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 19: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 3: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 4: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 7: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 8: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 10: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 13: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 14: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 17: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 19: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nWe need a product description for an image-based online store platform that will help customers understand the specifics of the product.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a descriptive text for a product image to help customers understand its details.",
                    "tool_description": "Use an image-to-text model to create a natural language description of the product depicted in an image, capturing key attributes, features, and relevant context.",
                    "expected_response": "A textual description of the product that highlights its main features, appearance, and any important specifications."
                }
            ],
            "extracted_arguments": {
                "image": "The image file or path representing the product to be described."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_443",
        "query": "We have a set of pictures for pets (dogs and cats). We need to offer an AI-based solution to classify the pictures given the pet name.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize AI and computer vision models to classify pet images by processing image inputs and identifying the specified pet names from a list of potential class labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')",
                    "api_arguments": {
                        "image_path": "path to the image file",
                        "labels": "list of possible class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "70.8 - 71.7%"
                    },
                    "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.",
                    "id": "huggingface_api_357",
                    "name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')"
                },
                "id": "gorilla_huggingface_tool_353",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 1: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 3: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 4: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 5: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 9: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 10: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 11: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 12: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 13: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 14: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 15: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 16: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 18: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 19: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations."
        ],
        "tool_view": {
            "description": "BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "parameters": {
                "raw_image": {
                    "type": "Image",
                    "description": "The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question about the content of the image that the model should answer.",
                    "required": true,
                    "example": "How many dogs are in the picture?"
                }
            },
            "response": "The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model’s predicted answer. Semantically, this string conveys the model’s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model’s internal generation.",
            "examples": [
                {
                    "user_intent": "How many people are standing in this photo?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1607746882042-944635dfe10e",
                            "question": "How many people are standing in this photo?"
                        }
                    }
                },
                {
                    "user_intent": "What color is the car in the image?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b",
                            "question": "What color is the car in the image?"
                        }
                    }
                },
                {
                    "user_intent": "Is there any food on the table in this picture?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c",
                            "question": "Is there any food on the table in this picture?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 1: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 3: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 4: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 5: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 6: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 9: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 10: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 11: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 12: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 13: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 14: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 15: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 16: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 18: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 19: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\", \"parameters\": {\"raw_image\": {\"type\": \"Image\", \"description\": \"The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question about the content of the image that the model should answer.\", \"required\": true, \"example\": \"How many dogs are in the picture?\"}}, \"response\": \"The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model\\u2019s predicted answer. Semantically, this string conveys the model\\u2019s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model\\u2019s internal generation.\", \"examples\": [{\"user_intent\": \"How many people are standing in this photo?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1607746882042-944635dfe10e\", \"question\": \"How many people are standing in this photo?\"}}}, {\"user_intent\": \"What color is the car in the image?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b\", \"question\": \"What color is the car in the image?\"}}}, {\"user_intent\": \"Is there any food on the table in this picture?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c\", \"question\": \"Is there any food on the table in this picture?\"}}}]}\n\n# User Query\nWe have a set of pictures for pets (dogs and cats). We need to offer an AI-based solution to classify the pictures given the pet name.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify an image of a pet as either a dog or a cat based on the picture.",
                    "tool_description": "A pre-trained image classification model that distinguishes between dogs and cats in images, using computer vision techniques to categorize each input photo.",
                    "expected_response": "A label indicating whether the image contains a dog or a cat."
                }
            ],
            "extracted_arguments": {
                "pet_image": "The input image of a pet to be classified, provided in a standard image format (e.g., JPEG or PNG).",
                "pet_name": "The name of the pet associated with the image, used to link classification results to the pet."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_76",
        "query": "We need a tool to help us generate textual descriptions for images and videos related to our product.",
        "instruction": "Given an `image and video captioning` task, retrieve tools that generate descriptive text by processing visual inputs, such as images or videos, to produce accurate and contextually relevant textual descriptions associated with the products depicted.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')",
                    "api_arguments": "image, text",
                    "python_environment_requirements": "transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "TextCaps",
                        "accuracy": "Refer to the paper"
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_84",
                    "name": "git-large-textcaps"
                },
                "id": "gorilla_huggingface_tool_84",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 1: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 2: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 3: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 4: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 5: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 7: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 8: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 11: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 12: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 13: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 14: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 15: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 16: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 17: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 18: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content."
        ],
        "tool_view": {
            "description": "FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "parameters": {
                "image": {
                    "type": "file",
                    "description": "The image file to be classified using the zero-shot fashion model.",
                    "required": true,
                    "example": "path/to/shoe.jpg"
                },
                "class_names": {
                    "type": "string",
                    "description": "A comma-separated string of class names or labels that the model will use for zero-shot classification.",
                    "required": true,
                    "example": "sneakers, boots, sandals, heels"
                }
            },
            "response": "The tool returns a structured prediction representing the likelihood that the input image belongs to each of the specified classes. The response is typically a list or array of numerical values, where each value corresponds to the probability or confidence score for a single class in the `class_names` list. These probabilities are normalized, usually summing to 1 across all classes, reflecting the model’s assessment of how well the image matches each textual class description.  \n\nKey components of the response:\n\n- **Class probabilities**: A numeric vector or list, with one entry per class provided in `class_names`. Each entry indicates the model’s predicted likelihood that the image belongs to that class.  \n- **Alignment with `class_names`**: The order of probabilities directly corresponds to the order of the input class names, so the first probability aligns with the first class name, the second with the second class name, and so on.  \n- **Optional logits**: In some implementations, the raw unnormalized scores (logits) may also be returned before applying the softmax transformation. These represent the relative affinity between the image and each class prior to normalization.  \n\nOverall, the response conveys a zero-shot classification result, allowing downstream code to identify the most probable class for the image or rank classes by likelihood.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a red dress and see if it matches categories like 'dress', 'shirt', or 'jacket'.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "red_dress.jpg",
                            "class_names": "dress,shirt,jacket"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this image of sneakers can be labeled as 'sneakers', 'boots', or 'sandals'.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "white_sneakers.png",
                            "class_names": "sneakers,boots,sandals"
                        }
                    }
                },
                {
                    "user_intent": "Determine if this photo belongs to 'handbag', 'backpack', or 'tote bag' category for my e-commerce inventory.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "leather_bag.jpeg",
                            "class_names": "handbag,backpack,tote bag"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 1: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 2: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 3: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 4: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 5: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 7: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 8: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 11: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 12: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 13: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 14: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 15: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 16: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 17: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 18: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\", \"parameters\": {\"image\": {\"type\": \"file\", \"description\": \"The image file to be classified using the zero-shot fashion model.\", \"required\": true, \"example\": \"path/to/shoe.jpg\"}, \"class_names\": {\"type\": \"string\", \"description\": \"A comma-separated string of class names or labels that the model will use for zero-shot classification.\", \"required\": true, \"example\": \"sneakers, boots, sandals, heels\"}}, \"response\": \"The tool returns a structured prediction representing the likelihood that the input image belongs to each of the specified classes. The response is typically a list or array of numerical values, where each value corresponds to the probability or confidence score for a single class in the `class_names` list. These probabilities are normalized, usually summing to 1 across all classes, reflecting the model\\u2019s assessment of how well the image matches each textual class description.  \\n\\nKey components of the response:\\n\\n- **Class probabilities**: A numeric vector or list, with one entry per class provided in `class_names`. Each entry indicates the model\\u2019s predicted likelihood that the image belongs to that class.  \\n- **Alignment with `class_names`**: The order of probabilities directly corresponds to the order of the input class names, so the first probability aligns with the first class name, the second with the second class name, and so on.  \\n- **Optional logits**: In some implementations, the raw unnormalized scores (logits) may also be returned before applying the softmax transformation. These represent the relative affinity between the image and each class prior to normalization.  \\n\\nOverall, the response conveys a zero-shot classification result, allowing downstream code to identify the most probable class for the image or rank classes by likelihood.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a red dress and see if it matches categories like 'dress', 'shirt', or 'jacket'.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"red_dress.jpg\", \"class_names\": \"dress,shirt,jacket\"}}}, {\"user_intent\": \"Check whether this image of sneakers can be labeled as 'sneakers', 'boots', or 'sandals'.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"white_sneakers.png\", \"class_names\": \"sneakers,boots,sandals\"}}}, {\"user_intent\": \"Determine if this photo belongs to 'handbag', 'backpack', or 'tote bag' category for my e-commerce inventory.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"leather_bag.jpeg\", \"class_names\": \"handbag,backpack,tote bag\"}}}]}\n\n# User Query\nWe need a tool to help us generate textual descriptions for images and videos related to our product.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate textual descriptions for visual content, such as images or videos, to describe the products.",
                    "tool_description": "Generate captions or textual descriptions from visual content, including images or videos, to enhance accessibility or content understanding.",
                    "expected_response": "A list of textual descriptions or captions generated from the input images or videos."
                }
            ],
            "extracted_arguments": {
                "visual_content": "The image or video file(s) from which textual descriptions will be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_404",
        "query": "We want to utilize the machine learning model for predicting molecular properties in the drug discovery domain.",
        "instruction": "Given a `molecular property prediction` task, retrieve tools designed for machine learning applications in the drug discovery domain by leveraging models that perform graph classification and representation to predict molecular properties accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Graph Machine Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('graphormer-base-pcqm4mv1')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "See the Graph Classification with Transformers tutorial",
                    "performance": {
                        "dataset": "PCQM4M-LSC",
                        "accuracy": "1st place on the KDD CUP 2021 (quantum prediction track)"
                    },
                    "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.",
                    "id": "huggingface_api_142",
                    "name": "graphormer-base-pcqm4mv1"
                },
                "id": "gorilla_huggingface_tool_138",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 1: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 2: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.",
            "Tool 3: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "Tool 4: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 5: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 6: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 7: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 8: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 10: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 11: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 12: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 14: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.",
            "Tool 15: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.",
            "Tool 16: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 17: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 18: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 19: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "parameters": {
                "algo": {
                    "type": "string",
                    "description": "The reinforcement learning algorithm to be used. It defines the method for training the agent.",
                    "required": true,
                    "example": "ppo"
                },
                "env": {
                    "type": "string",
                    "description": "The environment in which the agent will be trained or evaluated. In this case, it is the Pendulum-v1 environment.",
                    "required": true,
                    "example": "Pendulum-v1"
                },
                "f": {
                    "type": "string",
                    "description": "The directory path where logs or results should be saved during or after training.",
                    "required": true,
                    "example": "logs/"
                }
            },
            "response": "The tool’s response is not explicitly defined in the JSON, but based on the described functionality, a typical response would consist of a structured representation of a trained reinforcement learning model loaded from the hub. The response would likely include the following components:  \n\n1. **Model Object**:  \n   - Represents the loaded PPO agent trained on the `Pendulum-v1` environment.  \n   - Contains the agent’s policy network, value function, and associated parameters.  \n   - May include methods for inference, evaluation, or further training.  \n\n2. **Environment Information**:  \n   - Specifies the environment associated with the model (`Pendulum-v1`).  \n   - Could include environment metadata such as observation space, action space, and reward structure.  \n\n3. **Training Metadata** (optional but typical):  \n   - Hyperparameters used during training (learning rate, gamma, batch size, etc.).  \n   - Training performance metrics, e.g., mean return and standard deviation (`accuracy` in the JSON corresponds to evaluation reward).  \n   - Information about the training dataset or episodes.  \n\n4. **Source or Provenance Information**:  \n   - Repository ID or organization from which the model was loaded (`HumanCompatibleAI/ppo-Pendulum-v1`).  \n   - Optional logging or versioning information (`logs/` folder reference).  \n\n5. **Utility Methods** (if exposed in the response object):  \n   - Functions to evaluate the agent on the environment.  \n   - Functions to save or push the model to the hub.  \n   - Functions to modify or retrain the agent.  \n\nIn essence, the response provides a fully initialized, ready-to-use PPO agent compatible with the Stable-Baselines3 framework, along with metadata describing its environment, performance, and source.",
            "examples": [
                {
                    "user_intent": "I want to load a pre-trained PPO agent for the Pendulum-v1 environment from the Hugging Face hub.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "HumanCompatibleAI/ppo-Pendulum-v1",
                            "algo": "ppo",
                            "env": "Pendulum-v1",
                            "f": "logs/"
                        }
                    }
                },
                {
                    "user_intent": "Download the PPO Pendulum-v1 model so I can evaluate its performance in my custom RL script.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "HumanCompatibleAI/ppo-Pendulum-v1",
                            "algo": "ppo",
                            "env": "Pendulum-v1",
                            "f": "evaluation_logs/"
                        }
                    }
                },
                {
                    "user_intent": "Fetch the pre-trained PPO model for Pendulum-v1 to continue training from the saved checkpoint.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "HumanCompatibleAI/ppo-Pendulum-v1",
                            "algo": "ppo",
                            "env": "Pendulum-v1",
                            "f": "resume_training/"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 1: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 2: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.\nTool 3: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\nTool 4: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 5: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 6: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 7: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 8: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 10: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 11: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 12: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 14: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.\nTool 15: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.\nTool 16: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 17: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 18: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 19: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\", \"parameters\": {\"algo\": {\"type\": \"string\", \"description\": \"The reinforcement learning algorithm to be used. It defines the method for training the agent.\", \"required\": true, \"example\": \"ppo\"}, \"env\": {\"type\": \"string\", \"description\": \"The environment in which the agent will be trained or evaluated. In this case, it is the Pendulum-v1 environment.\", \"required\": true, \"example\": \"Pendulum-v1\"}, \"f\": {\"type\": \"string\", \"description\": \"The directory path where logs or results should be saved during or after training.\", \"required\": true, \"example\": \"logs/\"}}, \"response\": \"The tool\\u2019s response is not explicitly defined in the JSON, but based on the described functionality, a typical response would consist of a structured representation of a trained reinforcement learning model loaded from the hub. The response would likely include the following components:  \\n\\n1. **Model Object**:  \\n   - Represents the loaded PPO agent trained on the `Pendulum-v1` environment.  \\n   - Contains the agent\\u2019s policy network, value function, and associated parameters.  \\n   - May include methods for inference, evaluation, or further training.  \\n\\n2. **Environment Information**:  \\n   - Specifies the environment associated with the model (`Pendulum-v1`).  \\n   - Could include environment metadata such as observation space, action space, and reward structure.  \\n\\n3. **Training Metadata** (optional but typical):  \\n   - Hyperparameters used during training (learning rate, gamma, batch size, etc.).  \\n   - Training performance metrics, e.g., mean return and standard deviation (`accuracy` in the JSON corresponds to evaluation reward).  \\n   - Information about the training dataset or episodes.  \\n\\n4. **Source or Provenance Information**:  \\n   - Repository ID or organization from which the model was loaded (`HumanCompatibleAI/ppo-Pendulum-v1`).  \\n   - Optional logging or versioning information (`logs/` folder reference).  \\n\\n5. **Utility Methods** (if exposed in the response object):  \\n   - Functions to evaluate the agent on the environment.  \\n   - Functions to save or push the model to the hub.  \\n   - Functions to modify or retrain the agent.  \\n\\nIn essence, the response provides a fully initialized, ready-to-use PPO agent compatible with the Stable-Baselines3 framework, along with metadata describing its environment, performance, and source.\", \"examples\": [{\"user_intent\": \"I want to load a pre-trained PPO agent for the Pendulum-v1 environment from the Hugging Face hub.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"HumanCompatibleAI/ppo-Pendulum-v1\", \"algo\": \"ppo\", \"env\": \"Pendulum-v1\", \"f\": \"logs/\"}}}, {\"user_intent\": \"Download the PPO Pendulum-v1 model so I can evaluate its performance in my custom RL script.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"HumanCompatibleAI/ppo-Pendulum-v1\", \"algo\": \"ppo\", \"env\": \"Pendulum-v1\", \"f\": \"evaluation_logs/\"}}}, {\"user_intent\": \"Fetch the pre-trained PPO model for Pendulum-v1 to continue training from the saved checkpoint.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"HumanCompatibleAI/ppo-Pendulum-v1\", \"algo\": \"ppo\", \"env\": \"Pendulum-v1\", \"f\": \"resume_training/\"}}}]}\n\n# User Query\nWe want to utilize the machine learning model for predicting molecular properties in the drug discovery domain.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained model for predicting molecular properties in drug discovery.",
                    "tool_description": "Load a pretrained graph Transformer model designed for graph classification and representation tasks, particularly for molecular modeling, enabling property prediction and analysis of chemical structures.",
                    "expected_response": "A fully initialized graph Transformer model capable of performing molecular property prediction, including model weights, graph representation methods, and prediction utilities."
                },
                {
                    "user_intent": "Use the loaded model to predict properties of specific molecules for drug discovery applications.",
                    "tool_description": "Input molecular graphs into the pretrained model to obtain predictions for desired molecular properties relevant to drug discovery.",
                    "expected_response": "A set of predicted molecular property values for the input chemical structures, suitable for downstream analysis in drug discovery."
                }
            ],
            "extracted_arguments": {
                "model_type": "The pretrained graph Transformer model for molecular property prediction.",
                "input_molecules": "Graph-structured representations of molecules for which properties need to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_251",
        "query": "A medical research team requests an automated procedure for detecting blood cells in microscopic images of blood samples. Develop a solution to address this need.",
        "instruction": "Given an `image analysis` task for detecting blood cells in microscopic images, retrieve tools that specialize in computer vision and object detection to analyze and identify cellular structures within image inputs, supporting the research team's automated detection requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-blood-cell-detection')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic_nms": false,
                        "max_det": 1000
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": [
                        "from ultralyticsplus import YOLO, render_result",
                        "model = YOLO('keremberke/yolov8m-blood-cell-detection')",
                        "model.overrides['conf'] = 0.25",
                        "model.overrides['iou'] = 0.45",
                        "model.overrides['agnostic_nms'] = False",
                        "model.overrides['max_det'] = 1000",
                        "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model.predict(image)",
                        "print(results[0].boxes)",
                        "render = render_result(model=model, image=image, result=results[0])",
                        "render.show()"
                    ],
                    "performance": {
                        "dataset": "blood-cell-object-detection",
                        "accuracy": 0.927
                    },
                    "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.",
                    "id": "huggingface_api_224",
                    "name": "keremberke/yolov8m-blood-cell-detection"
                },
                "id": "gorilla_huggingface_tool_220",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.",
            "Tool 2: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.",
            "Tool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 4: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 5: A YOLOv8 model for detecting and extracting tables from images, capable of identifying both bordered and borderless tables. It is designed for computer vision applications that require automated table recognition and was trained on the keremberke/table-extraction dataset for high accuracy.",
            "Tool 6: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.",
            "Tool 7: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 8: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 9: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 10: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 11: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 12: The \"microsoft/trocr-small-handwritten\" model is a transformer-based Optical Character Recognition (OCR) tool fine-tuned on the IAM dataset. It is designed to convert handwritten text in images into machine-readable text, leveraging Hugging Face Transformers for integration. The model is particularly useful for extracting text from scanned handwritten documents and images, and can be used in Python environments with libraries such as `transformers`, `PIL`, and `requests`.",
            "Tool 13: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 14: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.",
            "Tool 15: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 16: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 17: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 18: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.",
            "Tool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Any detections beyond this number will be ignored.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The response from the tool would likely contain the following components, based on the provided documentation:\n\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\n   - **Label or Index**: A unique identifier or label associated with each detection.\n\n2. **Metadata**: Information regarding the detection process, which might include:\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\n   - **Inference Time**: The time taken to process the image and produce results.\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\n\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\n\n4. **Performance Information (optional)**: Information on the model's performance, including:\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\n\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.",
            "examples": [
                {
                    "user_intent": "Detect and count all red blood cells in this microscope image.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/microscope_rbc_sample.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a blood smear image to identify WBCs, RBCs, and platelets.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/blood_smear_full.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Quickly detect blood cells in a low-resolution image with a higher confidence threshold.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-blood-cell-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 300,
                            "image": "https://example.com/low_res_blood_sample.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 1: This tool uses a YOLOv8 model for detecting blood cells, including platelets, red blood cells (RBC), and white blood cells (WBC), trained on the blood-cell-object-detection dataset. It allows users to perform object detection with customizable settings such as confidence threshold, Intersection over Union (IoU), and maximum detection limits. This tool is designed to be used within the Hugging Face Transformers framework and requires specific Python packages like `ultralyticsplus` and `ultralytics`. It provides high accuracy (92.7%) for blood cell detection tasks.\nTool 2: A Hugging Face Transformers image classification model fine-tuned for detecting diabetic retinopathy from medical images, enabling automated analysis of retinal scans with an accuracy of 77.44%.\nTool 3: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 4: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 5: A YOLOv8 model for detecting and extracting tables from images, capable of identifying both bordered and borderless tables. It is designed for computer vision applications that require automated table recognition and was trained on the keremberke/table-extraction dataset for high accuracy.\nTool 6: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.\nTool 7: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 8: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 9: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 10: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 11: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 12: The \"microsoft/trocr-small-handwritten\" model is a transformer-based Optical Character Recognition (OCR) tool fine-tuned on the IAM dataset. It is designed to convert handwritten text in images into machine-readable text, leveraging Hugging Face Transformers for integration. The model is particularly useful for extracting text from scanned handwritten documents and images, and can be used in Python environments with libraries such as `transformers`, `PIL`, and `requests`.\nTool 13: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 14: This tool provides a YOLOv8s model for detecting and segmenting defects on printed circuit boards (PCBs), identifying issues such as dry joints, incorrect installation, PCB damage, and short circuits. It accepts an image from a URL or local path and outputs both bounding boxes and segmentation masks, making it useful for automated PCB quality inspection and defect analysis.\nTool 15: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 16: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 17: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 18: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.\nTool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8-based model to detect blood cells\\u2014including platelets, red blood cells, and white blood cells\\u2014in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting blood cells. Detections with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection Over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping detections.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"If set to true, NMS will be class-agnostic, meaning overlapping detections will be suppressed regardless of class. If false, NMS respects class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Any detections beyond this number will be ignored.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The response from the tool would likely contain the following components, based on the provided documentation:\\n\\n1. **Detections**: This is the primary output of the tool, listing the detected objects within the input image. The detected objects are expected to be blood cells, specifically platelets, RBCs (Red Blood Cells), and WBCs (White Blood Cells). Each detection would typically contain information such as:\\n   - **Object Class**: Identifies the type of blood cell detected (e.g., Platelet, RBC, WBC).\\n   - **Bounding Box Coordinates**: The coordinates that describe the location of the detected object within the image, usually in the form of a rectangle (e.g., xmin, ymin, xmax, ymax).\\n   - **Confidence Score**: A value indicating the model's certainty in the detection, typically constrained by the `conf` parameter (in this case, 0.25).\\n   - **IoU (Intersection over Union) Score**: Used to filter out poor-quality detections, based on the `iou` threshold (0.45).\\n   - **Label or Index**: A unique identifier or label associated with each detection.\\n\\n2. **Metadata**: Information regarding the detection process, which might include:\\n   - **Model Version**: The version of the YOLO model used (YOLOv8 in this case).\\n   - **Inference Time**: The time taken to process the image and produce results.\\n   - **Input Image Information**: Details about the image, such as the image URL or dimensions.\\n   - **Non-Maximum Suppression (NMS) Information**: Depending on the `agnostic_nms` flag, this could indicate whether non-maximum suppression was applied during the inference process.\\n\\n3. **Visual Output (if applicable)**: A rendered image that shows the detected blood cells overlaid on the original image, possibly including:\\n   - **Bounding Boxes**: Graphical representation of the detected blood cells, drawn on the input image.\\n   - **Confidence Labels**: Numerical labels indicating the confidence score for each detection, potentially displayed near the bounding boxes.\\n\\n4. **Performance Information (optional)**: Information on the model's performance, including:\\n   - **Accuracy**: The accuracy of the model, which is 0.893 as mentioned in the documentation. This would typically be presented as a percentage or ratio, indicating how often the model's predictions match the ground truth.\\n\\nIn summary, the response would likely be a structured set of detected objects (blood cells) with associated bounding boxes, class labels, confidence scores, and possibly a visual overlay of the detections on the original image, along with metadata and performance details.\", \"examples\": [{\"user_intent\": \"Detect and count all red blood cells in this microscope image.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/microscope_rbc_sample.jpg\"}}}, {\"user_intent\": \"Analyze a blood smear image to identify WBCs, RBCs, and platelets.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/blood_smear_full.jpg\"}}}, {\"user_intent\": \"Quickly detect blood cells in a low-resolution image with a higher confidence threshold.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 300, \"image\": \"https://example.com/low_res_blood_sample.jpg\"}}}]}\n\n# User Query\nA medical research team requests an automated procedure for detecting blood cells in microscopic images of blood samples. Develop a solution to address this need.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically detect and classify blood cells in microscopic blood sample images.",
                    "tool_description": "Use a YOLOv8-based object detection model trained on blood cell images to identify and classify platelets, red blood cells (RBCs), and white blood cells (WBCs) in input images, with configurable detection parameters for confidence threshold, IoU, class-agnostic NMS, and maximum detections.",
                    "expected_response": "A structured list of detected blood cells with class labels, bounding box coordinates, confidence scores, and optional visual overlays on the original images, along with metadata about the detection process."
                }
            ],
            "extracted_arguments": {
                "image": "The input microscopic blood sample image to be analyzed.",
                "conf": "The confidence threshold for considering a detection valid, expressed as a numeric value between 0 and 1.",
                "iou": "The Intersection over Union threshold used to filter overlapping detections, expressed as a numeric value between 0 and 1.",
                "agnostic_nms": "A boolean flag indicating whether Non-Maximum Suppression should ignore class labels (true) or respect them (false).",
                "max_det": "The maximum number of blood cell detections allowed per image, expressed as a numeric value."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_33",
        "query": "We are a company manufacturing AI-based toothbrushes for children. We want to analyze the emotion of children while they brush their teeth.",
        "instruction": "Given an `emotion analysis` task in an audio context, retrieve tools that process audio inputs to recognize and classify emotions, focusing on extracting emotional attributes from children's interactions as they brush their teeth.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')",
                    "api_arguments": {
                        "path": "/path/to/russian_audio_speech.wav",
                        "sampling_rate": 16000
                    },
                    "python_environment_requirements": [
                        "torch",
                        "torchaudio",
                        "transformers",
                        "librosa",
                        "numpy"
                    ],
                    "example_code": "result = predict('/path/to/russian_audio_speech.wav', 16000)\nprint(result)",
                    "performance": {
                        "dataset": "Russian Emotional Speech Dialogs",
                        "accuracy": "72%"
                    },
                    "description": "A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.",
                    "id": "huggingface_api_826",
                    "name": "wav2vec2-xlsr-53-russian-emotion-recognition"
                },
                "id": "gorilla_huggingface_tool_821",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.",
            "Tool 3: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.",
            "Tool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 5: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 6: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 7: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 8: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 9: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 11: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 12: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.",
            "Tool 13: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.",
            "Tool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 15: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 16: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 17: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 19: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications."
        ],
        "tool_view": {
            "description": "This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text string for which the emotion classification should be performed.",
                    "required": true,
                    "example": "I love using transformers. The best part is wide range of support and its easy to use"
                }
            },
            "response": "The tool produces a structured response containing predicted emotion scores for the input text. Since `return_all_scores=True` is specified, the response is typically a list of objects, where each object represents a possible emotion category and its associated confidence. The structure and meaning of each component are as follows:\n\n- **List of prediction objects**: The top-level response is a list containing one object per emotion class evaluated by the model.\n\n  - **`label`**: A string representing the emotion category predicted by the model (e.g., \"joy\", \"sadness\", \"anger\", etc.). Each object corresponds to a single emotion label.\n\n  - **`score`**: A numerical value (usually a float between 0 and 1) representing the model's confidence that the input text expresses the corresponding emotion. Higher values indicate higher confidence.\n\nThe overall response provides a full probability distribution over all supported emotion categories for the input text, allowing downstream applications to select the most likely emotion, rank emotions, or threshold scores as needed.",
            "examples": [
                {
                    "user_intent": "I want to analyze the emotions in a tweet: 'I'm so excited to start my new job tomorrow!'",
                    "tool_call": {
                        "name": "bhadresh-savani/distilbert-base-uncased-emotion",
                        "arguments": {
                            "text": "I'm so excited to start my new job tomorrow!"
                        }
                    }
                },
                {
                    "user_intent": "Determine the emotional tone of this customer review: 'The product broke after one day, very disappointing.'",
                    "tool_call": {
                        "name": "bhadresh-savani/distilbert-base-uncased-emotion",
                        "arguments": {
                            "text": "The product broke after one day, very disappointing."
                        }
                    }
                },
                {
                    "user_intent": "Analyze the feelings expressed in a short blog post: 'I feel a mix of joy and nervousness about moving to a new city.'",
                    "tool_call": {
                        "name": "bhadresh-savani/distilbert-base-uncased-emotion",
                        "arguments": {
                            "text": "I feel a mix of joy and nervousness about moving to a new city."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.\nTool 3: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.\nTool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 5: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 6: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 7: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 8: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 9: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 11: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 12: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.\nTool 13: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.\nTool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 15: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 16: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 17: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 19: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the \\\"bhadresh-savani/distilbert-base-uncased-emotion\\\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text string for which the emotion classification should be performed.\", \"required\": true, \"example\": \"I love using transformers. The best part is wide range of support and its easy to use\"}}, \"response\": \"The tool produces a structured response containing predicted emotion scores for the input text. Since `return_all_scores=True` is specified, the response is typically a list of objects, where each object represents a possible emotion category and its associated confidence. The structure and meaning of each component are as follows:\\n\\n- **List of prediction objects**: The top-level response is a list containing one object per emotion class evaluated by the model.\\n\\n  - **`label`**: A string representing the emotion category predicted by the model (e.g., \\\"joy\\\", \\\"sadness\\\", \\\"anger\\\", etc.). Each object corresponds to a single emotion label.\\n\\n  - **`score`**: A numerical value (usually a float between 0 and 1) representing the model's confidence that the input text expresses the corresponding emotion. Higher values indicate higher confidence.\\n\\nThe overall response provides a full probability distribution over all supported emotion categories for the input text, allowing downstream applications to select the most likely emotion, rank emotions, or threshold scores as needed.\", \"examples\": [{\"user_intent\": \"I want to analyze the emotions in a tweet: 'I'm so excited to start my new job tomorrow!'\", \"tool_call\": {\"name\": \"bhadresh-savani/distilbert-base-uncased-emotion\", \"arguments\": {\"text\": \"I'm so excited to start my new job tomorrow!\"}}}, {\"user_intent\": \"Determine the emotional tone of this customer review: 'The product broke after one day, very disappointing.'\", \"tool_call\": {\"name\": \"bhadresh-savani/distilbert-base-uncased-emotion\", \"arguments\": {\"text\": \"The product broke after one day, very disappointing.\"}}}, {\"user_intent\": \"Analyze the feelings expressed in a short blog post: 'I feel a mix of joy and nervousness about moving to a new city.'\", \"tool_call\": {\"name\": \"bhadresh-savani/distilbert-base-uncased-emotion\", \"arguments\": {\"text\": \"I feel a mix of joy and nervousness about moving to a new city.\"}}}]}\n\n# User Query\nWe are a company manufacturing AI-based toothbrushes for children. We want to analyze the emotion of children while they brush their teeth.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the emotions expressed by children while using the AI toothbrush.",
                    "tool_description": "Classify the emotional content of text or speech related to children's experiences while brushing teeth using an emotion recognition model.",
                    "expected_response": "A structured list of predicted emotion labels with confidence scores for each input sample, representing the emotional state of the children."
                }
            ],
            "extracted_arguments": {
                "input_data": "Textual or audio data capturing children's behavior or reactions while brushing teeth, used for emotion classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_414",
        "query": "They are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.",
        "instruction": "Given a `language translation` task, retrieve tools that can generate translations of text inputs from one language to another by processing the input text and providing the output in the desired language.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Language model",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')",
                    "api_arguments": [
                        "input_text",
                        "input_ids",
                        "outputs"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-large)\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-large)\ninput_text = translate English to German: How old are you?\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))",
                    "performance": {
                        "dataset": [
                            {
                                "name": "MMLU",
                                "accuracy": "75.2%"
                            }
                        ]
                    },
                    "description": "FLAN-T5 large is a language model fine-tuned on over 1000 tasks and multiple languages. It achieves state-of-the-art performance on several benchmarks, including 75.2% on five-shot MMLU. The model is based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. It can be used for research on language models, zero-shot NLP tasks, in-context few-shot learning NLP tasks, reasoning, question answering, and advancing fairness and safety research.",
                    "id": "huggingface_api_634",
                    "name": "google/flan-t5-large"
                },
                "id": "gorilla_huggingface_tool_629",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 2: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 11: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 12: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 14: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 15: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 16: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 17: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 18: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 19: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time."
        ],
        "tool_view": {
            "description": "JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "parameters": {},
            "response": "The tool is designed for **Visual Question Answering (VQA)**, which involves answering questions about images. The response to a VQA request typically consists of the answer to the question posed, based on the contents of the image provided.\n\nThe expected response from this tool would likely include the following components:\n\n1. **Answer**: A textual response that directly answers the question asked, based on the visual content of the image. This is the primary output of the tool.\n   \n2. **Confidence/Probability (optional)**: A numerical value indicating the model's confidence in the provided answer. This could be expressed as a probability score or a similar metric, although it is not explicitly defined in the documentation.\n\n3. **Language**: Since the model supports multiple languages (English, Chinese, Japanese, and German), the answer will likely be returned in the same language as the question posed.\n\n4. **Additional Context (optional)**: In some VQA systems, additional context or metadata may be included, such as the specific region of the image that contributed to the answer. This could be part of the response, though it is not specified in the documentation.\n\nThe response would focus on providing the most relevant answer based on the combination of the image input and the question posed.",
            "examples": [
                {
                    "user_intent": "I want to ask a question about an image in English, like 'What color is the car?'",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "What color is the car?"
                        }
                    }
                },
                {
                    "user_intent": "Can I ask about an image in Japanese? For example, 'この動物は何ですか？' (What animal is this?)",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "この動物は何ですか？"
                        }
                    }
                },
                {
                    "user_intent": "I have an image and I want to ask in German, 'Wie viele Menschen sind auf diesem Bild?' (How many people are in this image?)",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "Wie viele Menschen sind auf diesem Bild?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 2: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 11: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 12: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 14: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 15: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 16: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 17: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 18: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 19: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\", \"parameters\": {}, \"response\": \"The tool is designed for **Visual Question Answering (VQA)**, which involves answering questions about images. The response to a VQA request typically consists of the answer to the question posed, based on the contents of the image provided.\\n\\nThe expected response from this tool would likely include the following components:\\n\\n1. **Answer**: A textual response that directly answers the question asked, based on the visual content of the image. This is the primary output of the tool.\\n   \\n2. **Confidence/Probability (optional)**: A numerical value indicating the model's confidence in the provided answer. This could be expressed as a probability score or a similar metric, although it is not explicitly defined in the documentation.\\n\\n3. **Language**: Since the model supports multiple languages (English, Chinese, Japanese, and German), the answer will likely be returned in the same language as the question posed.\\n\\n4. **Additional Context (optional)**: In some VQA systems, additional context or metadata may be included, such as the specific region of the image that contributed to the answer. This could be part of the response, though it is not specified in the documentation.\\n\\nThe response would focus on providing the most relevant answer based on the combination of the image input and the question posed.\", \"examples\": [{\"user_intent\": \"I want to ask a question about an image in English, like 'What color is the car?'\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"What color is the car?\"}}}, {\"user_intent\": \"Can I ask about an image in Japanese? For example, '\\u3053\\u306e\\u52d5\\u7269\\u306f\\u4f55\\u3067\\u3059\\u304b\\uff1f' (What animal is this?)\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"\\u3053\\u306e\\u52d5\\u7269\\u306f\\u4f55\\u3067\\u3059\\u304b\\uff1f\"}}}, {\"user_intent\": \"I have an image and I want to ask in German, 'Wie viele Menschen sind auf diesem Bild?' (How many people are in this image?)\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"Wie viele Menschen sind auf diesem Bild?\"}}}]}\n\n# User Query\nThey are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate a question about park locations in Munich into German.",
                    "tool_description": "Translate a text-based query from English to German using a machine translation model.",
                    "expected_response": "The translated question in German that can be used to ask about park locations."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the original question, English, as a string.",
                "target_language": "The language to translate the question into, German, as a string.",
                "query_text": "The English text of the question regarding the location of parks in Munich, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_385",
        "query": "A customer wants to convert an input Korean text into a summary. Provide a solution for it.",
        "instruction": "Given a `text summarization` task, retrieve tools that process Korean text inputs to generate concise summaries aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')",
                    "api_arguments": {
                        "tokenizer": "BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import BertTokenizerFast, EncoderDecoderModel\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)",
                    "performance": {
                        "dataset": "70GB Korean text dataset",
                        "accuracy": "42000 lower-cased subwords"
                    },
                    "description": "Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.",
                    "id": "huggingface_api_638",
                    "name": "kykim/bertshared-kor-base"
                },
                "id": "gorilla_huggingface_tool_633",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 1: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.",
            "Tool 2: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 3: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 4: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 7: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 8: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 9: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 10: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 11: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 13: T5-Base is a versatile Text-to-Text Transfer Transformer model with 220 million parameters that can perform a wide range of NLP tasks, including translation, summarization, question answering, and text classification. Pre-trained on the Colossal Clean Crawled Corpus (C4), it can be easily integrated using the Transformers library to convert input text into task-specific output in a unified text-to-text format.",
            "Tool 14: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 15: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 17: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 18: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.",
            "Tool 19: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries."
        ],
        "tool_view": {
            "description": "A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Korean that will be converted into speech by the text-to-speech model.",
                    "required": true,
                    "example": "안녕하세요. 오늘 기분은 어떠신가요?"
                }
            },
            "response": "The tool produces synthesized speech audio from input text. The response typically consists of an audio waveform or audio data object representing the spoken version of the provided text. Key aspects of the response include:\n\n- **Audio waveform or tensor**: The primary content is the generated speech, usually represented as a multi-dimensional array (tensor) or waveform that can be converted to standard audio formats (e.g., WAV, MP3). This contains the amplitude values of the sound over time.  \n- **Sampling rate (optional but common)**: Some implementations may include metadata specifying the sampling rate used for the audio, which indicates how many samples per second the waveform contains.  \n- **Length/duration metadata (optional)**: The duration of the generated audio may also be returned or inferable from the waveform length and sampling rate.  \n- **Format compatibility**: The response is structured so it can be saved or played using standard audio libraries once converted to a suitable format.  \n\nIn principle, the response encapsulates all the information necessary to reconstruct or play the generated Korean speech corresponding to the input text.",
            "examples": [
                {
                    "user_intent": "Convert this Korean paragraph into speech for my audiobook project.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "오늘 아침, 나는 창문을 열고 상쾌한 공기를 마셨다."
                        }
                    }
                },
                {
                    "user_intent": "I want to generate a TTS version of a Korean dialogue between two characters.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "철수: 안녕, 영희! 오늘 기분 어때?\n영희: 안녕하세요, 철수! 저는 아주 좋아요."
                        }
                    }
                },
                {
                    "user_intent": "Produce a spoken version of a short Korean poem for language learning.",
                    "tool_call": {
                        "name": "lakahaga/novel_reading_tts",
                        "arguments": {
                            "text": "달빛 아래서 나는 조용히 노래를 부른다."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 1: A Korean BERT-based model designed for text-to-text generation tasks, trained on a 70GB Korean text dataset with 42,000 lower-cased subwords, suitable for encoding and decoding Korean text using the Hugging Face Transformers framework.\nTool 2: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 3: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 4: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 7: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 8: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 9: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 10: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 11: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 13: T5-Base is a versatile Text-to-Text Transfer Transformer model with 220 million parameters that can perform a wide range of NLP tasks, including translation, summarization, question answering, and text classification. Pre-trained on the Colossal Clean Crawled Corpus (C4), it can be easily integrated using the Transformers library to convert input text into task-specific output in a unified text-to-text format.\nTool 14: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 15: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 17: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 18: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\nTool 19: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Korean that will be converted into speech by the text-to-speech model.\", \"required\": true, \"example\": \"\\uc548\\ub155\\ud558\\uc138\\uc694. \\uc624\\ub298 \\uae30\\ubd84\\uc740 \\uc5b4\\ub5a0\\uc2e0\\uac00\\uc694?\"}}, \"response\": \"The tool produces synthesized speech audio from input text. The response typically consists of an audio waveform or audio data object representing the spoken version of the provided text. Key aspects of the response include:\\n\\n- **Audio waveform or tensor**: The primary content is the generated speech, usually represented as a multi-dimensional array (tensor) or waveform that can be converted to standard audio formats (e.g., WAV, MP3). This contains the amplitude values of the sound over time.  \\n- **Sampling rate (optional but common)**: Some implementations may include metadata specifying the sampling rate used for the audio, which indicates how many samples per second the waveform contains.  \\n- **Length/duration metadata (optional)**: The duration of the generated audio may also be returned or inferable from the waveform length and sampling rate.  \\n- **Format compatibility**: The response is structured so it can be saved or played using standard audio libraries once converted to a suitable format.  \\n\\nIn principle, the response encapsulates all the information necessary to reconstruct or play the generated Korean speech corresponding to the input text.\", \"examples\": [{\"user_intent\": \"Convert this Korean paragraph into speech for my audiobook project.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\uc624\\ub298 \\uc544\\uce68, \\ub098\\ub294 \\ucc3d\\ubb38\\uc744 \\uc5f4\\uace0 \\uc0c1\\ucf8c\\ud55c \\uacf5\\uae30\\ub97c \\ub9c8\\uc168\\ub2e4.\"}}}, {\"user_intent\": \"I want to generate a TTS version of a Korean dialogue between two characters.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\ucca0\\uc218: \\uc548\\ub155, \\uc601\\ud76c! \\uc624\\ub298 \\uae30\\ubd84 \\uc5b4\\ub54c?\\n\\uc601\\ud76c: \\uc548\\ub155\\ud558\\uc138\\uc694, \\ucca0\\uc218! \\uc800\\ub294 \\uc544\\uc8fc \\uc88b\\uc544\\uc694.\"}}}, {\"user_intent\": \"Produce a spoken version of a short Korean poem for language learning.\", \"tool_call\": {\"name\": \"lakahaga/novel_reading_tts\", \"arguments\": {\"text\": \"\\ub2ec\\ube5b \\uc544\\ub798\\uc11c \\ub098\\ub294 \\uc870\\uc6a9\\ud788 \\ub178\\ub798\\ub97c \\ubd80\\ub978\\ub2e4.\"}}}]}\n\n# User Query\nA customer wants to convert an input Korean text into a summary. Provide a solution for it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a concise summary from a Korean text input.",
                    "tool_description": "Use a Korean text summarization model to produce an abstractive summary of the input text, condensing the content while retaining key information.",
                    "expected_response": "A short, coherent text summary representing the main points of the original Korean input."
                }
            ],
            "extracted_arguments": {
                "input_text": "The Korean text content provided by the user that needs to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_267",
        "query": "I want to create a solution that can answer questions related to an image of my pet dogs.",
        "instruction": "Given an `image-based question answering` task, retrieve tools that address queries related to images by processing visual inputs and generating relevant textual answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 2: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 6: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 7: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 8: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 11: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 12: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 14: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 15: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 17: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 18: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 19: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want to ask about the table.",
                    "required": true,
                    "example": "How many goals did player A score?"
                },
                "table": {
                    "type": "array",
                    "description": "A two-dimensional array representing the table data, where the first row contains column headers and subsequent rows contain the data entries.",
                    "required": true,
                    "example": [
                        [
                            "Player",
                            "Goals"
                        ],
                        [
                            "Player A",
                            5
                        ],
                        [
                            "Player B",
                            3
                        ]
                    ]
                }
            },
            "response": "The tool’s response is a structured output representing the answer to a natural language question posed over a tabular dataset. The typical response structure includes the following components:  \n\n1. **`answer`** – A string or scalar value representing the model’s predicted answer to the input question. This could be a single value, a numeric count, or a textual summary derived from the table.  \n\n2. **`coordinates`** (optional) – A list of table cell locations or indices that the model used to compute or extract the answer. Each coordinate identifies a specific row and column in the input table.  \n\n3. **`aggregation`** (optional) – A string indicating the type of aggregation applied to the table data, such as “sum,” “average,” “count,” or “none,” if the answer was extracted directly without aggregation.  \n\n4. **`logits`** (optional) – Numerical scores or confidence values associated with potential answer cells or aggregation choices, representing the model’s internal confidence in its prediction.  \n\n5. **`units`** (optional) – If the table contains measurable quantities, this field may indicate the units associated with the predicted numeric answer.  \n\nIn practice, the response is typically a dictionary containing at least the `answer` key, with optional fields providing additional interpretability about how the answer was derived from the table.",
            "examples": [
                {
                    "user_intent": "I want to know how many goals Player B scored in the season.",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "How many goals did Player B score?",
                            "table": [
                                [
                                    "Player",
                                    "Goals"
                                ],
                                [
                                    "Player A",
                                    5
                                ],
                                [
                                    "Player B",
                                    3
                                ],
                                [
                                    "Player C",
                                    7
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Which department has the highest number of employees?",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "Which department has the most employees?",
                            "table": [
                                [
                                    "Department",
                                    "Employees"
                                ],
                                [
                                    "HR",
                                    12
                                ],
                                [
                                    "Engineering",
                                    45
                                ],
                                [
                                    "Sales",
                                    30
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Find the total revenue for Q2 from the financial table.",
                    "tool_call": {
                        "name": "google/tapas-base-finetuned-sqa",
                        "arguments": {
                            "question": "What is the total revenue for Q2?",
                            "table": [
                                [
                                    "Quarter",
                                    "Revenue"
                                ],
                                [
                                    "Q1",
                                    120000
                                ],
                                [
                                    "Q2",
                                    150000
                                ],
                                [
                                    "Q3",
                                    130000
                                ],
                                [
                                    "Q4",
                                    160000
                                ]
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 2: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 6: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 7: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 8: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 11: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 12: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 14: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 15: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 17: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 18: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 19: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"google/tapas-base-finetuned-sqa\\\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want to ask about the table.\", \"required\": true, \"example\": \"How many goals did player A score?\"}, \"table\": {\"type\": \"array\", \"description\": \"A two-dimensional array representing the table data, where the first row contains column headers and subsequent rows contain the data entries.\", \"required\": true, \"example\": [[\"Player\", \"Goals\"], [\"Player A\", 5], [\"Player B\", 3]]}}, \"response\": \"The tool\\u2019s response is a structured output representing the answer to a natural language question posed over a tabular dataset. The typical response structure includes the following components:  \\n\\n1. **`answer`** \\u2013 A string or scalar value representing the model\\u2019s predicted answer to the input question. This could be a single value, a numeric count, or a textual summary derived from the table.  \\n\\n2. **`coordinates`** (optional) \\u2013 A list of table cell locations or indices that the model used to compute or extract the answer. Each coordinate identifies a specific row and column in the input table.  \\n\\n3. **`aggregation`** (optional) \\u2013 A string indicating the type of aggregation applied to the table data, such as \\u201csum,\\u201d \\u201caverage,\\u201d \\u201ccount,\\u201d or \\u201cnone,\\u201d if the answer was extracted directly without aggregation.  \\n\\n4. **`logits`** (optional) \\u2013 Numerical scores or confidence values associated with potential answer cells or aggregation choices, representing the model\\u2019s internal confidence in its prediction.  \\n\\n5. **`units`** (optional) \\u2013 If the table contains measurable quantities, this field may indicate the units associated with the predicted numeric answer.  \\n\\nIn practice, the response is typically a dictionary containing at least the `answer` key, with optional fields providing additional interpretability about how the answer was derived from the table.\", \"examples\": [{\"user_intent\": \"I want to know how many goals Player B scored in the season.\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"How many goals did Player B score?\", \"table\": [[\"Player\", \"Goals\"], [\"Player A\", 5], [\"Player B\", 3], [\"Player C\", 7]]}}}, {\"user_intent\": \"Which department has the highest number of employees?\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"Which department has the most employees?\", \"table\": [[\"Department\", \"Employees\"], [\"HR\", 12], [\"Engineering\", 45], [\"Sales\", 30]]}}}, {\"user_intent\": \"Find the total revenue for Q2 from the financial table.\", \"tool_call\": {\"name\": \"google/tapas-base-finetuned-sqa\", \"arguments\": {\"question\": \"What is the total revenue for Q2?\", \"table\": [[\"Quarter\", \"Revenue\"], [\"Q1\", 120000], [\"Q2\", 150000], [\"Q3\", 130000], [\"Q4\", 160000]]}}}]}\n\n# User Query\nI want to create a solution that can answer questions related to an image of my pet dogs.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to ask questions about the content of an image of my pet dogs.",
                    "tool_description": "Process an input image and a natural language question to generate an answer about the visual content, enabling visual question answering (VQA) for pet images.",
                    "expected_response": "A textual answer describing or providing information about the image content relevant to the question."
                }
            ],
            "extracted_arguments": {
                "image": "A digital image file containing the pet dogs to be analyzed.",
                "question": "A natural language question about the content or attributes of the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_269",
        "query": "We have a list of documents written in multiple Romance languages, including texts in French, Spanish, and Italian. We want to make these texts accessible to our English-speaking audience by translating them.",
        "instruction": "Given a `translation` task, retrieve tools that translate texts from multiple Romance languages, such as French, Spanish, and Italian, to English, utilizing models capable of handling diverse source languages to meet the query's multilingual processing requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')",
                    "api_arguments": [
                        "source languages",
                        "target languages"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": 62.2,
                            "chr-F": 0.75
                        }
                    },
                    "description": "A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.",
                    "id": "huggingface_api_529",
                    "name": "opus-mt-ROMANCE-en"
                },
                "id": "gorilla_huggingface_tool_525",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 1: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 2: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 4: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 5: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 6: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 7: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 8: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 9: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 10: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 11: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.",
            "Tool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 13: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 14: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 15: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 16: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 17: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 18: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents."
        ],
        "tool_view": {
            "description": "A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "parameters": {
                "source_languages": {
                    "type": "array",
                    "description": "A list of source languages that you want to translate from. These should be specified using their standard language codes or names, e.g., 'fr' for French or 'es' for Spanish.",
                    "required": true,
                    "example": [
                        "fr",
                        "es",
                        "pt"
                    ]
                },
                "target_languages": {
                    "type": "array",
                    "description": "A list of target languages that you want to translate to. For this model, the target language is typically English ('en'), but multiple target languages can be specified if supported.",
                    "required": true,
                    "example": [
                        "en"
                    ]
                }
            },
            "response": "The tool is designed to translate Romance languages into English using a pre-trained MarianMT model. It relies on the Hugging Face Transformers framework and operates with the MarianMT model, specifically the `Helsinki-NLP/opus-mt-ROMANCE-en` variant. Given that the exact format of the response is not explicitly defined in the documentation, a reasonable and typical form of response can be inferred.\n\nThe response would likely contain the following components:\n\n1. **Translated Text**: The primary component of the response will be the translated text, which is the output of the translation process from the source language (one of the Romance languages) into English. This will typically be a string of text that represents the translation.\n\n2. **Source Language**: The tool may also return the source language code or name to indicate the language that was translated from. This is helpful for understanding the context of the translation and ensuring that the translation was made from the correct language.\n\n3. **Target Language**: The target language in this case is always English. However, the response may include an explicit mention of the target language, especially in the case of multi-language support or when multiple source languages are involved.\n\n4. **Model Information**: While the model's accuracy or version might not always be included in every response, in some cases, the response could contain metadata about the model used for the translation, such as the model name (`opus-mt-ROMANCE-en`), the dataset it was trained on (e.g., OPUS dataset), or its performance metrics (like BLEU score or chr-F score). These metrics indicate the quality of the translation based on standard evaluation methods.\n\n5. **Error Messages**: If there is an issue with translation, such as unsupported source language or invalid input, the response may include an error message to explain the problem.\n\nIn summary, the response would mainly consist of the translated English text, with additional metadata that could include the source language, model version, and possibly performance metrics or error messages, depending on the system's implementation and settings.",
            "examples": [
                {
                    "user_intent": "Translate this French text into English: 'Bonjour, comment ça va aujourd'hui ?'",
                    "tool_call": {
                        "name": "opus-mt-ROMANCE-en",
                        "arguments": {
                            "source languages": "fr",
                            "target languages": "en"
                        }
                    }
                },
                {
                    "user_intent": "I have a Portuguese paragraph that I need translated to English.",
                    "tool_call": {
                        "name": "opus-mt-ROMANCE-en",
                        "arguments": {
                            "source languages": "pt",
                            "target languages": "en"
                        }
                    }
                },
                {
                    "user_intent": "Convert this Spanish text into English: 'La tecnología está cambiando rápidamente el mundo.'",
                    "tool_call": {
                        "name": "opus-mt-ROMANCE-en",
                        "arguments": {
                            "source languages": "es",
                            "target languages": "en"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 1: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 2: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 4: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 5: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 6: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 7: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 8: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 9: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 10: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 11: The Stanford de-identifier is a natural language processing tool built with Hugging Face Transformers that automatically removes personally identifiable information from radiology and biomedical texts. It is designed for production use, achieving high accuracy across multiple datasets, making it suitable for safely de-identifying sensitive medical documents while preserving their utility for research or analysis.\nTool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 13: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 14: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 15: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 16: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 17: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 18: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 19: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\", \"parameters\": {\"source_languages\": {\"type\": \"array\", \"description\": \"A list of source languages that you want to translate from. These should be specified using their standard language codes or names, e.g., 'fr' for French or 'es' for Spanish.\", \"required\": true, \"example\": [\"fr\", \"es\", \"pt\"]}, \"target_languages\": {\"type\": \"array\", \"description\": \"A list of target languages that you want to translate to. For this model, the target language is typically English ('en'), but multiple target languages can be specified if supported.\", \"required\": true, \"example\": [\"en\"]}}, \"response\": \"The tool is designed to translate Romance languages into English using a pre-trained MarianMT model. It relies on the Hugging Face Transformers framework and operates with the MarianMT model, specifically the `Helsinki-NLP/opus-mt-ROMANCE-en` variant. Given that the exact format of the response is not explicitly defined in the documentation, a reasonable and typical form of response can be inferred.\\n\\nThe response would likely contain the following components:\\n\\n1. **Translated Text**: The primary component of the response will be the translated text, which is the output of the translation process from the source language (one of the Romance languages) into English. This will typically be a string of text that represents the translation.\\n\\n2. **Source Language**: The tool may also return the source language code or name to indicate the language that was translated from. This is helpful for understanding the context of the translation and ensuring that the translation was made from the correct language.\\n\\n3. **Target Language**: The target language in this case is always English. However, the response may include an explicit mention of the target language, especially in the case of multi-language support or when multiple source languages are involved.\\n\\n4. **Model Information**: While the model's accuracy or version might not always be included in every response, in some cases, the response could contain metadata about the model used for the translation, such as the model name (`opus-mt-ROMANCE-en`), the dataset it was trained on (e.g., OPUS dataset), or its performance metrics (like BLEU score or chr-F score). These metrics indicate the quality of the translation based on standard evaluation methods.\\n\\n5. **Error Messages**: If there is an issue with translation, such as unsupported source language or invalid input, the response may include an error message to explain the problem.\\n\\nIn summary, the response would mainly consist of the translated English text, with additional metadata that could include the source language, model version, and possibly performance metrics or error messages, depending on the system's implementation and settings.\", \"examples\": [{\"user_intent\": \"Translate this French text into English: 'Bonjour, comment \\u00e7a va aujourd'hui ?'\", \"tool_call\": {\"name\": \"opus-mt-ROMANCE-en\", \"arguments\": {\"source languages\": \"fr\", \"target languages\": \"en\"}}}, {\"user_intent\": \"I have a Portuguese paragraph that I need translated to English.\", \"tool_call\": {\"name\": \"opus-mt-ROMANCE-en\", \"arguments\": {\"source languages\": \"pt\", \"target languages\": \"en\"}}}, {\"user_intent\": \"Convert this Spanish text into English: 'La tecnolog\\u00eda est\\u00e1 cambiando r\\u00e1pidamente el mundo.'\", \"tool_call\": {\"name\": \"opus-mt-ROMANCE-en\", \"arguments\": {\"source languages\": \"es\", \"target languages\": \"en\"}}}]}\n\n# User Query\nWe have a list of documents written in multiple Romance languages, including texts in French, Spanish, and Italian. We want to make these texts accessible to our English-speaking audience by translating them.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate documents written in French, Spanish, and Italian into English for accessibility.",
                    "tool_description": "A transformer-based model for translating multiple Romance languages into English. It uses normalization and SentencePiece preprocessing to deliver high-quality translations, making multilingual texts understandable to English readers.",
                    "expected_response": "A collection of translated English texts corresponding to each input document, possibly including source language metadata."
                }
            ],
            "extracted_arguments": {
                "source_languages": "A list of Romance languages present in the documents, e.g., French, Spanish, Italian.",
                "target_languages": "The target language for translation, in this case, English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_183",
        "query": "Write a script to translate the following French sentence into English: \"Je t’aime.\"",
        "instruction": "Given a `translation` task, retrieve tools that utilize natural language processing to translate text from one language to another by processing language-specific inputs and returning accurate translations aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m')",
                    "api_arguments": {
                        "checkpoint": "bigscience/bloomz-560m",
                        "inputs": "Translate to English: Je t’aime."
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "accelerate",
                        "bitsandbytes"
                    ],
                    "example_code": {
                        "CPU": "from transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = bigscience/bloomz-560m\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\ninputs = tokenizer.encode(Translate to English: Je t’aime., return_tensors=pt)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))"
                    },
                    "performance": {
                        "dataset": "bigscience/xP3",
                        "accuracy": {
                            "Winogrande XL (xl) validation set": 52.41,
                            "XWinograd (en) test set": 51.01,
                            "XWinograd (fr) test set": 51.81,
                            "XWinograd (jp) test set": 52.03,
                            "XWinograd (pt) test set": 53.99,
                            "XWinograd (ru) test set": 53.97,
                            "XWinograd (zh) test set": 54.76,
                            "ANLI (r1) validation set": 33.4,
                            "ANLI (r2) validation set": 33.4,
                            "ANLI (r3) validation set": 33.5
                        }
                    },
                    "description": "BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.",
                    "id": "huggingface_api_627",
                    "name": "bigscience/bloomz-560m"
                },
                "id": "gorilla_huggingface_tool_622",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 2: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 6: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 7: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 8: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 9: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 10: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 11: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 12: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 13: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 14: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 15: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 16: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.",
            "Tool 17: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 18: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.",
            "Tool 19: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows."
        ],
        "tool_view": {
            "description": "The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in French that needs to be translated into English.",
                    "required": true,
                    "example": "Bonjour, comment ça va?"
                }
            },
            "response": "The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\n\nHere’s a description of the expected response:\n\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\n  \nIn principle, the response will consist of the following:\n\n1. **Translated Text**: A string containing the English translation of the input French text.\n\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\n\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool’s design or configuration.",
            "examples": [
                {
                    "user_intent": "Translate this French sentence to English: 'Je voudrais réserver une table pour deux ce soir.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Je voudrais réserver une table pour deux ce soir."
                        }
                    }
                },
                {
                    "user_intent": "I need the English version of this French paragraph about climate change: 'Le réchauffement climatique est une menace sérieuse pour notre planète.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Le réchauffement climatique est une menace sérieuse pour notre planète."
                        }
                    }
                },
                {
                    "user_intent": "Convert this casual French message into English: 'Salut! On se voit demain au café?'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Salut! On se voit demain au café?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 2: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 3: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 6: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 7: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 8: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 9: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 10: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 11: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 12: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 13: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 14: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 15: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 16: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.\nTool 17: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 18: A pre-trained English part-of-speech tagging model from Flair that assigns fine-grained POS tags to words in a sentence using Flair embeddings and an LSTM-CRF architecture, providing high-accuracy token-level syntactic analysis.\nTool 19: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in French that needs to be translated into English.\", \"required\": true, \"example\": \"Bonjour, comment \\u00e7a va?\"}}, \"response\": \"The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\\n\\nHere\\u2019s a description of the expected response:\\n\\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\\n  \\nIn principle, the response will consist of the following:\\n\\n1. **Translated Text**: A string containing the English translation of the input French text.\\n\\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\\n\\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool\\u2019s design or configuration.\", \"examples\": [{\"user_intent\": \"Translate this French sentence to English: 'Je voudrais r\\u00e9server une table pour deux ce soir.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Je voudrais r\\u00e9server une table pour deux ce soir.\"}}}, {\"user_intent\": \"I need the English version of this French paragraph about climate change: 'Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.\"}}}, {\"user_intent\": \"Convert this casual French message into English: 'Salut! On se voit demain au caf\\u00e9?'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Salut! On se voit demain au caf\\u00e9?\"}}}]}\n\n# User Query\nWrite a script to translate the following French sentence into English: \"Je t’aime.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate this French sentence to English: 'Je t’aime.'",
                    "tool_description": "Use a French-to-English machine translation model to convert the provided French text into English while preserving meaning and context.",
                    "expected_response": "A string containing the English translation of the input French sentence."
                }
            ],
            "extracted_arguments": {
                "text": "The input text in French that needs to be translated into English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_171",
        "query": "We are launching a new blog and need a paragraph with tips on how to take care of houseplants.",
        "instruction": "Given a `content creation` task, retrieve tools that generate coherent and informative text by processing inputs related to the specified topic, such as tips for taking care of houseplants, to produce a paragraph aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='bigscience/bloom-7b1')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-generation', model='bigscience/bloom-7b1')\nresult = model('Once upon a time')\nprint(result)",
                    "performance": {
                        "dataset": "Training Data",
                        "accuracy": {
                            "Training Loss": 2.3,
                            "Validation Loss": 2.9,
                            "Perplexity": 16
                        }
                    },
                    "description": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text.",
                    "id": "huggingface_api_606",
                    "name": "bigscience/bloom-7b1"
                },
                "id": "gorilla_huggingface_tool_601",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.",
            "Tool 1: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 2: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.",
            "Tool 3: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 6: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 7: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 8: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 11: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 12: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 13: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 14: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 16: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 18: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 19: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically."
        ],
        "tool_view": {
            "description": "Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.",
            "parameters": {
                "alpha": {
                    "type": "number",
                    "description": "The regularization strength parameter for the Ridge regression model. Higher values imply stronger regularization to prevent overfitting.",
                    "required": true,
                    "example": 10
                }
            },
            "response": "The tool’s response consists of a trained regression model pipeline loaded from a serialized file. The response is a single object representing the entire model, which is structured as a Scikit-learn `Pipeline` with the following components:\n\n1. **Preprocessing step (`EasyPreprocessor`)**  \n   - This subcomponent handles feature preprocessing for tabular data.  \n   - It encodes, imputes, or transforms features based on their types (e.g., continuous, categorical, low-cardinality integer, date, free string, or flagged as useless).  \n   - Each column of the dataset has associated boolean flags indicating which transformations are applied.\n\n2. **Regression step (`Ridge` model)**  \n   - The final step in the pipeline is a Ridge regression model.  \n   - It includes hyperparameters such as `alpha` (regularization strength) that control the model’s behavior.\n\n3. **Pipeline object (`Pipeline`)**  \n   - Encapsulates the sequence of steps from preprocessing to regression.  \n   - Provides methods for prediction, scoring, and accessing fitted parameters.\n\n**Semantic meaning of the response:**  \n- The object represents a ready-to-use baseline regression model trained on the `tips5wx_sbh5` dataset for predicting tips.  \n- It can accept new tabular input data, automatically preprocess it, and produce predicted target values.  \n- Performance metrics (`r2` and negative mean squared error`) reflect the model’s expected predictive quality on the original dataset.  \n\nIn short, the response is a fully trained, serialized Scikit-learn regression pipeline with preprocessing and Ridge regression, ready to perform regression predictions on tabular data.",
            "examples": [
                {
                    "user_intent": "I want to predict the tip amount for new restaurant bills using the pre-trained baseline model.",
                    "tool_call": {
                        "name": "merve/tips5wx_sbh5-tip-regression",
                        "arguments": {
                            "alpha": 10
                        }
                    }
                },
                {
                    "user_intent": "Load a regression model to quickly estimate tips from the tips5wx_sbh5 dataset features.",
                    "tool_call": {
                        "name": "merve/tips5wx_sbh5-tip-regression",
                        "arguments": {
                            "alpha": 10
                        }
                    }
                },
                {
                    "user_intent": "I need a baseline Ridge regression model for predicting tips that I can test on my own restaurant data.",
                    "tool_call": {
                        "name": "merve/tips5wx_sbh5-tip-regression",
                        "arguments": {
                            "alpha": 10
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.\nTool 1: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 2: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.\nTool 3: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 6: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 7: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 8: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 11: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 12: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 13: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 14: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 16: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 18: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 19: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Provides a pre-trained baseline regression model for predicting tips using the `tips5wx_sbh5` dataset, implemented in Scikit-learn, useful for quickly establishing a reference performance for tabular regression tasks.\", \"parameters\": {\"alpha\": {\"type\": \"number\", \"description\": \"The regularization strength parameter for the Ridge regression model. Higher values imply stronger regularization to prevent overfitting.\", \"required\": true, \"example\": 10}}, \"response\": \"The tool\\u2019s response consists of a trained regression model pipeline loaded from a serialized file. The response is a single object representing the entire model, which is structured as a Scikit-learn `Pipeline` with the following components:\\n\\n1. **Preprocessing step (`EasyPreprocessor`)**  \\n   - This subcomponent handles feature preprocessing for tabular data.  \\n   - It encodes, imputes, or transforms features based on their types (e.g., continuous, categorical, low-cardinality integer, date, free string, or flagged as useless).  \\n   - Each column of the dataset has associated boolean flags indicating which transformations are applied.\\n\\n2. **Regression step (`Ridge` model)**  \\n   - The final step in the pipeline is a Ridge regression model.  \\n   - It includes hyperparameters such as `alpha` (regularization strength) that control the model\\u2019s behavior.\\n\\n3. **Pipeline object (`Pipeline`)**  \\n   - Encapsulates the sequence of steps from preprocessing to regression.  \\n   - Provides methods for prediction, scoring, and accessing fitted parameters.\\n\\n**Semantic meaning of the response:**  \\n- The object represents a ready-to-use baseline regression model trained on the `tips5wx_sbh5` dataset for predicting tips.  \\n- It can accept new tabular input data, automatically preprocess it, and produce predicted target values.  \\n- Performance metrics (`r2` and negative mean squared error`) reflect the model\\u2019s expected predictive quality on the original dataset.  \\n\\nIn short, the response is a fully trained, serialized Scikit-learn regression pipeline with preprocessing and Ridge regression, ready to perform regression predictions on tabular data.\", \"examples\": [{\"user_intent\": \"I want to predict the tip amount for new restaurant bills using the pre-trained baseline model.\", \"tool_call\": {\"name\": \"merve/tips5wx_sbh5-tip-regression\", \"arguments\": {\"alpha\": 10}}}, {\"user_intent\": \"Load a regression model to quickly estimate tips from the tips5wx_sbh5 dataset features.\", \"tool_call\": {\"name\": \"merve/tips5wx_sbh5-tip-regression\", \"arguments\": {\"alpha\": 10}}}, {\"user_intent\": \"I need a baseline Ridge regression model for predicting tips that I can test on my own restaurant data.\", \"tool_call\": {\"name\": \"merve/tips5wx_sbh5-tip-regression\", \"arguments\": {\"alpha\": 10}}}]}\n\n# User Query\nWe are launching a new blog and need a paragraph with tips on how to take care of houseplants.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a paragraph with advice on taking care of houseplants for a blog.",
                    "tool_description": "Use a large-scale conversational or text generation model to produce a coherent and informative paragraph offering practical tips on houseplant care, suitable for publishing in a blog.",
                    "expected_response": "A text paragraph containing clear, engaging, and accurate tips for houseplant care."
                }
            ],
            "extracted_arguments": {
                "topic": "The subject matter for the paragraph, in this case focused on houseplant care.",
                "output_format": "The desired format of the content, here specified as a textual paragraph suitable for a blog."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_52",
        "query": "I am reading a book and, whenever I engage with an interesting topic, I write a short sentence summarizing that topic. I would like to have an assistant that, given a sentence, automatically classifies that topic among 'technology', 'literature', and 'science'.",
        "instruction": "Given a `topic classification` task, retrieve tools that automatically categorize text based on its content into predefined categories such as 'technology', 'literature', and 'science' using natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')",
                    "api_arguments": [
                        "sent",
                        "candidate_labels"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\nsent = 'Apple just announced the newest iPhone X'\ncandidate_labels = ['technology', 'sports', 'politics']\nres = classifier(sent, candidate_labels)\nprint(res)",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.64",
                            "MNLI_mismatched": "87.77"
                        }
                    },
                    "description": "This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.",
                    "id": "huggingface_api_513",
                    "name": "cross-encoder/nli-deberta-v3-xsmall"
                },
                "id": "gorilla_huggingface_tool_509",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 1: This tool uses the MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli model to perform zero-shot text classification, allowing users to assign a text sequence to one or more candidate labels without requiring task-specific training. It leverages a DeBERTa-v3-base model fine-tuned on MultiNLI, Fever-NLI, and Adversarial-NLI datasets, achieving strong performance on natural language inference benchmarks, and is suitable for quickly categorizing text into predefined topics in applications like content labeling, topic detection, or automated tagging.",
            "Tool 2: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 3: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 4: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 5: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 6: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 7: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 8: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 9: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 10: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 11: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 12: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 14: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 15: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 16: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.",
            "Tool 17: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.",
            "Tool 18: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "parameters": {
                "inputs": {
                    "type": "object",
                    "description": "A dictionary of input tensors for the model, typically produced by the DistilBERT tokenizer. This should include input IDs and attention masks formatted as PyTorch tensors.",
                    "required": true,
                    "example": {
                        "input_ids": [
                            [
                                101,
                                7592,
                                1010,
                                2026,
                                3899,
                                2003,
                                10140,
                                102
                            ]
                        ],
                        "attention_mask": [
                            [
                                1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                1,
                                1
                            ]
                        ]
                    }
                }
            },
            "response": "The tool returns the result of classifying a given text input into one of a predefined set of categories (labels) corresponding to sentiment or topic classes, as defined by the model’s fine-tuning. The typical response structure includes the following components:\n\n1. **Logits**: A tensor of numerical scores representing the raw, unnormalized predictions for each class. The length of this tensor equals the number of classes the model can predict. Higher values indicate stronger model confidence for that class.\n\n2. **Predicted Class ID**: An integer index corresponding to the class with the highest logit score. This identifies which category the model predicts for the input text.\n\n3. **Predicted Class Label**: The human-readable label associated with the predicted class ID, obtained via the model’s internal mapping (e.g., `model.config.id2label`). This provides a semantic interpretation of the model’s prediction.\n\n4. **Optional Probabilities**: Although not explicitly required, logits can be converted into normalized probabilities (e.g., using softmax) to quantify confidence for each possible class. This is commonly included in downstream processing.\n\nIn summary, the response semantically conveys the model’s prediction for the input text, both as a numerical class index and as a readable label, along with the underlying raw scores for all classes.",
            "examples": [
                {
                    "user_intent": "I want to analyze the sentiment of a movie review I wrote.",
                    "tool_call": {
                        "name": "distilbert-base-uncased-finetuned-sst-2-english",
                        "arguments": {
                            "inputs": "The movie was thrilling and kept me on the edge of my seat!"
                        }
                    }
                },
                {
                    "user_intent": "Classify this customer feedback as positive or negative.",
                    "tool_call": {
                        "name": "distilbert-base-uncased-finetuned-sst-2-english",
                        "arguments": {
                            "inputs": "The support team was unhelpful and the issue took weeks to resolve."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a tweet before posting it to ensure it sounds positive.",
                    "tool_call": {
                        "name": "distilbert-base-uncased-finetuned-sst-2-english",
                        "arguments": {
                            "inputs": "Excited to start my new job tomorrow! Feeling motivated and grateful."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 1: This tool uses the MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli model to perform zero-shot text classification, allowing users to assign a text sequence to one or more candidate labels without requiring task-specific training. It leverages a DeBERTa-v3-base model fine-tuned on MultiNLI, Fever-NLI, and Adversarial-NLI datasets, achieving strong performance on natural language inference benchmarks, and is suitable for quickly categorizing text into predefined topics in applications like content labeling, topic detection, or automated tagging.\nTool 2: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 3: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 4: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 5: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 6: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 7: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 8: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 9: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 10: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 11: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 12: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 14: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 15: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 16: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.\nTool 17: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\nTool 18: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\", \"parameters\": {\"inputs\": {\"type\": \"object\", \"description\": \"A dictionary of input tensors for the model, typically produced by the DistilBERT tokenizer. This should include input IDs and attention masks formatted as PyTorch tensors.\", \"required\": true, \"example\": {\"input_ids\": [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]], \"attention_mask\": [[1, 1, 1, 1, 1, 1, 1, 1]]}}}, \"response\": \"The tool returns the result of classifying a given text input into one of a predefined set of categories (labels) corresponding to sentiment or topic classes, as defined by the model\\u2019s fine-tuning. The typical response structure includes the following components:\\n\\n1. **Logits**: A tensor of numerical scores representing the raw, unnormalized predictions for each class. The length of this tensor equals the number of classes the model can predict. Higher values indicate stronger model confidence for that class.\\n\\n2. **Predicted Class ID**: An integer index corresponding to the class with the highest logit score. This identifies which category the model predicts for the input text.\\n\\n3. **Predicted Class Label**: The human-readable label associated with the predicted class ID, obtained via the model\\u2019s internal mapping (e.g., `model.config.id2label`). This provides a semantic interpretation of the model\\u2019s prediction.\\n\\n4. **Optional Probabilities**: Although not explicitly required, logits can be converted into normalized probabilities (e.g., using softmax) to quantify confidence for each possible class. This is commonly included in downstream processing.\\n\\nIn summary, the response semantically conveys the model\\u2019s prediction for the input text, both as a numerical class index and as a readable label, along with the underlying raw scores for all classes.\", \"examples\": [{\"user_intent\": \"I want to analyze the sentiment of a movie review I wrote.\", \"tool_call\": {\"name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"arguments\": {\"inputs\": \"The movie was thrilling and kept me on the edge of my seat!\"}}}, {\"user_intent\": \"Classify this customer feedback as positive or negative.\", \"tool_call\": {\"name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"arguments\": {\"inputs\": \"The support team was unhelpful and the issue took weeks to resolve.\"}}}, {\"user_intent\": \"Check the sentiment of a tweet before posting it to ensure it sounds positive.\", \"tool_call\": {\"name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"arguments\": {\"inputs\": \"Excited to start my new job tomorrow! Feeling motivated and grateful.\"}}}]}\n\n# User Query\nI am reading a book and, whenever I engage with an interesting topic, I write a short sentence summarizing that topic. I would like to have an assistant that, given a sentence, automatically classifies that topic among 'technology', 'literature', and 'science'.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the topic of a sentence into one of three categories: technology, literature, or science.",
                    "tool_description": "Perform zero-shot text classification to categorize the input sentence into one or more predefined categories (technology, literature, or science) based on the content of the text.",
                    "expected_response": "A classification result including a predicted class label, such as 'technology', 'literature', or 'science', based on the analysis of the input sentence."
                }
            ],
            "extracted_arguments": {
                "inputs": "The input sentence that needs to be classified into one of the predefined categories."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_66",
        "query": "Develop a voice command security system that distinguishes between specific command phrases like \"disarm security\" or \"activate alarm\".",
        "instruction": "Given a `voice command recognition` task, retrieve tools that specialize in audio classification to accurately process distinct spoken command phrases and identify specific commands such as \"disarm security\" and \"activate alarm\" based on their audio characteristics.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('audio-classification', model='superb/hubert-base-superb-ks')",
                    "api_arguments": [
                        "file",
                        "top_k"
                    ],
                    "python_environment_requirements": [
                        "datasets",
                        "transformers",
                        "torchaudio"
                    ],
                    "example_code": "from datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-ks)\nlabels = classifier(dataset[0][file], top_k=5)",
                    "performance": {
                        "dataset": "Speech Commands dataset v1.0",
                        "accuracy": 0.9672
                    },
                    "description": "This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_806",
                    "name": "superb/hubert-base-superb-ks"
                },
                "id": "gorilla_huggingface_tool_801",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a fine-tuned version of the Microsoft Unispeech-SAT model for audio classification tasks. It achieves high accuracy (99.79%) on the Google Speech Commands dataset, making it suitable for speech command recognition. The model is accessible through the Hugging Face Transformers pipeline and requires specific Python libraries like transformers, pytorch, and datasets for use.",
            "Tool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 2: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 3: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.",
            "Tool 4: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 9: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 11: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 12: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 13: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.",
            "Tool 14: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 15: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 16: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.",
            "Tool 17: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 18: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream."
        ],
        "tool_view": {
            "description": "This tool uses a fine-tuned version of the Microsoft Unispeech-SAT model for audio classification tasks. It achieves high accuracy (99.79%) on the Google Speech Commands dataset, making it suitable for speech command recognition. The model is accessible through the Hugging Face Transformers pipeline and requires specific Python libraries like transformers, pytorch, and datasets for use.",
            "parameters": {},
            "response": "The tool’s response is a structured collection of audio classification predictions generated by the Hugging Face Transformers pipeline. Each element in the response represents the model’s prediction for a single audio input and typically includes the following components:\n\n1. **`label`** – A string indicating the predicted class or category of the input audio. This corresponds to one of the labels the model has been trained to recognize.\n\n2. **`score`** – A numerical value representing the model’s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence.\n\nThe overall response is typically a list of such prediction objects, one for each processed audio sample. The order of predictions in the list matches the order of the input audio data. \n\nNo additional metadata is returned by default, although the structure could be extended in some implementations to include information such as input identifiers or timestamps.",
            "examples": [
                {
                    "user_intent": "I have a short audio clip of someone saying a number from 0 to 9. Can you identify which number it is?",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-unispeech-sat-base-ft",
                        "arguments": {
                            "audio": "path/to/number_audio.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a batch of audio recordings containing spoken digits to see which numbers are being said.",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-unispeech-sat-base-ft",
                        "arguments": {
                            "audio": [
                                "audio_samples/digit1.wav",
                                "audio_samples/digit2.wav",
                                "audio_samples/digit3.wav"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I recorded a command from my voice assistant that is just a number. Can the model tell me which digit it is?",
                    "tool_call": {
                        "name": "mazkooleg/0-9up-unispeech-sat-base-ft",
                        "arguments": {
                            "audio": "voice_commands/command_digit.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a fine-tuned version of the Microsoft Unispeech-SAT model for audio classification tasks. It achieves high accuracy (99.79%) on the Google Speech Commands dataset, making it suitable for speech command recognition. The model is accessible through the Hugging Face Transformers pipeline and requires specific Python libraries like transformers, pytorch, and datasets for use.\nTool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 2: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 3: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.\nTool 4: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 9: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 11: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 12: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 13: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.\nTool 14: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 15: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 16: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.\nTool 17: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 18: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a fine-tuned version of the Microsoft Unispeech-SAT model for audio classification tasks. It achieves high accuracy (99.79%) on the Google Speech Commands dataset, making it suitable for speech command recognition. The model is accessible through the Hugging Face Transformers pipeline and requires specific Python libraries like transformers, pytorch, and datasets for use.\", \"parameters\": {}, \"response\": \"The tool\\u2019s response is a structured collection of audio classification predictions generated by the Hugging Face Transformers pipeline. Each element in the response represents the model\\u2019s prediction for a single audio input and typically includes the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted class or category of the input audio. This corresponds to one of the labels the model has been trained to recognize.\\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence in the predicted label. This is usually a probability between 0 and 1, with higher values indicating greater confidence.\\n\\nThe overall response is typically a list of such prediction objects, one for each processed audio sample. The order of predictions in the list matches the order of the input audio data. \\n\\nNo additional metadata is returned by default, although the structure could be extended in some implementations to include information such as input identifiers or timestamps.\", \"examples\": [{\"user_intent\": \"I have a short audio clip of someone saying a number from 0 to 9. Can you identify which number it is?\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-unispeech-sat-base-ft\", \"arguments\": {\"audio\": \"path/to/number_audio.wav\"}}}, {\"user_intent\": \"I want to classify a batch of audio recordings containing spoken digits to see which numbers are being said.\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-unispeech-sat-base-ft\", \"arguments\": {\"audio\": [\"audio_samples/digit1.wav\", \"audio_samples/digit2.wav\", \"audio_samples/digit3.wav\"]}}}, {\"user_intent\": \"I recorded a command from my voice assistant that is just a number. Can the model tell me which digit it is?\", \"tool_call\": {\"name\": \"mazkooleg/0-9up-unispeech-sat-base-ft\", \"arguments\": {\"audio\": \"voice_commands/command_digit.wav\"}}}]}\n\n# User Query\nDevelop a voice command security system that distinguishes between specific command phrases like \"disarm security\" or \"activate alarm\".\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify spoken voice commands to identify specific phrases like 'disarm security' or 'activate alarm'.",
                    "tool_description": "Use a fine-tuned audio classification model to recognize predefined spoken command phrases from audio inputs, suitable for security system command detection.",
                    "expected_response": "A structured list of predictions for each audio input, including the predicted command phrase label and the confidence score."
                },
                {
                    "user_intent": "Trigger specific actions in the security system based on the recognized command phrases.",
                    "tool_description": "Map recognized audio command labels to corresponding security system actions, enabling automated response to voice commands.",
                    "expected_response": "Don't need response."
                }
            ],
            "extracted_arguments": {
                "audio_input": "Audio file(s) containing spoken commands to be classified.",
                "command_labels": "A predefined set of command phrases that the system should recognize and differentiate.",
                "action_mapping": "A mapping between recognized command labels and the security system actions they should trigger."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_350",
        "query": "We would like to build a semantic text search system that can find similar documents in a repository based on a given description.",
        "instruction": "Given a `semantic text search` task, retrieve tools that facilitate finding similar documents by utilizing sentence embeddings and semantic similarity measures to compare and rank document descriptions based on query relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_695",
                    "name": "sentence-transformers/bert-base-nli-mean-tokens"
                },
                "id": "gorilla_huggingface_tool_690",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 2: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 4: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 5: This tool uses the \"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\" model from Hugging Face Transformers to map sentences and paragraphs to a 768-dimensional dense vector space. It is designed for tasks like semantic search and clustering by converting text into numerical embeddings that capture semantic meaning.",
            "Tool 6: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 7: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 8: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 9: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.",
            "Tool 10: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.",
            "Tool 11: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 12: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 13: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 15: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 16: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 17: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.",
            "Tool 18: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 19: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations."
        ],
        "tool_view": {
            "description": "Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained question answering model to load from the Transformers library.",
                    "required": true,
                    "example": "deepset/roberta-base-squad2"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or identifier of the tokenizer corresponding to the pre-trained model, used for processing input text.",
                    "required": true,
                    "example": "deepset/roberta-base-squad2"
                }
            },
            "response": "The tool produces a response containing the answer to a user-provided question based on a given context passage. The response is typically structured as a dictionary (or JSON-like object) with the following components:\n\n1. **`answer`** – A string representing the extracted text from the context that directly answers the input question. This is the primary output of the model.  \n\n2. **`score`** – A numerical value indicating the model’s confidence in the predicted answer, often represented as a probability or likelihood between 0 and 1. Higher values indicate greater confidence.  \n\n3. **`start`** – An integer representing the character index in the context where the answer begins. This allows precise localization of the answer within the context passage.  \n\n4. **`end`** – An integer representing the character index in the context where the answer ends (exclusive), complementing the `start` index to delineate the exact span of the answer.  \n\nThis structure allows downstream applications to not only retrieve the predicted answer text but also assess confidence and locate the answer within the original context. In cases where the model determines no answer is appropriate (as in SQuAD 2.0 “unanswerable” questions), the `answer` may be an empty string, and the `score` will reflect low confidence.",
            "examples": [
                {
                    "user_intent": "I want to find the answer to a question in a given text using a pre-trained Roberta QA model.",
                    "tool_call": {
                        "name": "optimum/roberta-base-squad2",
                        "arguments": {
                            "model_name": "deepset/roberta-base-squad2",
                            "tokenizer": "deepset/roberta-base-squad2",
                            "question": "What is the main benefit of converting models between frameworks?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                        }
                    }
                },
                {
                    "user_intent": "Check who developed the Roberta QA model and answer it using the model.",
                    "tool_call": {
                        "name": "optimum/roberta-base-squad2",
                        "arguments": {
                            "model_name": "deepset/roberta-base-squad2",
                            "tokenizer": "deepset/roberta-base-squad2",
                            "question": "Who developed the Roberta-base-squad2 model?",
                            "context": "This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library."
                        }
                    }
                },
                {
                    "user_intent": "I need to know the performance of this model on the SQuAD v2 dataset.",
                    "tool_call": {
                        "name": "optimum/roberta-base-squad2",
                        "arguments": {
                            "model_name": "deepset/roberta-base-squad2",
                            "tokenizer": "deepset/roberta-base-squad2",
                            "question": "What is the exact match accuracy and F1 score of the model?",
                            "context": "The model was evaluated on the SQuAD v2 dataset and achieved an exact match score of 79.87% and an F1 score of 82.91%."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 2: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 3: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 4: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 5: This tool uses the \"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\" model from Hugging Face Transformers to map sentences and paragraphs to a 768-dimensional dense vector space. It is designed for tasks like semantic search and clustering by converting text into numerical embeddings that capture semantic meaning.\nTool 6: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 7: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 8: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 9: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.\nTool 10: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\nTool 11: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 12: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 13: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 15: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 16: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 17: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.\nTool 18: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 19: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained question answering model to load from the Transformers library.\", \"required\": true, \"example\": \"deepset/roberta-base-squad2\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or identifier of the tokenizer corresponding to the pre-trained model, used for processing input text.\", \"required\": true, \"example\": \"deepset/roberta-base-squad2\"}}, \"response\": \"The tool produces a response containing the answer to a user-provided question based on a given context passage. The response is typically structured as a dictionary (or JSON-like object) with the following components:\\n\\n1. **`answer`** \\u2013 A string representing the extracted text from the context that directly answers the input question. This is the primary output of the model.  \\n\\n2. **`score`** \\u2013 A numerical value indicating the model\\u2019s confidence in the predicted answer, often represented as a probability or likelihood between 0 and 1. Higher values indicate greater confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the character index in the context where the answer begins. This allows precise localization of the answer within the context passage.  \\n\\n4. **`end`** \\u2013 An integer representing the character index in the context where the answer ends (exclusive), complementing the `start` index to delineate the exact span of the answer.  \\n\\nThis structure allows downstream applications to not only retrieve the predicted answer text but also assess confidence and locate the answer within the original context. In cases where the model determines no answer is appropriate (as in SQuAD 2.0 \\u201cunanswerable\\u201d questions), the `answer` may be an empty string, and the `score` will reflect low confidence.\", \"examples\": [{\"user_intent\": \"I want to find the answer to a question in a given text using a pre-trained Roberta QA model.\", \"tool_call\": {\"name\": \"optimum/roberta-base-squad2\", \"arguments\": {\"model_name\": \"deepset/roberta-base-squad2\", \"tokenizer\": \"deepset/roberta-base-squad2\", \"question\": \"What is the main benefit of converting models between frameworks?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}}, {\"user_intent\": \"Check who developed the Roberta QA model and answer it using the model.\", \"tool_call\": {\"name\": \"optimum/roberta-base-squad2\", \"arguments\": {\"model_name\": \"deepset/roberta-base-squad2\", \"tokenizer\": \"deepset/roberta-base-squad2\", \"question\": \"Who developed the Roberta-base-squad2 model?\", \"context\": \"This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library.\"}}}, {\"user_intent\": \"I need to know the performance of this model on the SQuAD v2 dataset.\", \"tool_call\": {\"name\": \"optimum/roberta-base-squad2\", \"arguments\": {\"model_name\": \"deepset/roberta-base-squad2\", \"tokenizer\": \"deepset/roberta-base-squad2\", \"question\": \"What is the exact match accuracy and F1 score of the model?\", \"context\": \"The model was evaluated on the SQuAD v2 dataset and achieved an exact match score of 79.87% and an F1 score of 82.91%.\"}}}]}\n\n# User Query\nWe would like to build a semantic text search system that can find similar documents in a repository based on a given description.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to encode all documents in our repository into vector embeddings for semantic search.",
                    "tool_description": "Convert each document in the repository into dense vector embeddings using a sentence-transformers model to capture semantic meaning.",
                    "expected_response": "A collection of vector representations corresponding to each document, suitable for similarity comparison."
                },
                {
                    "user_intent": "I want to encode a query description into a vector embedding to search for similar documents.",
                    "tool_description": "Convert the user-provided description into a dense vector embedding using the same sentence-transformers model to enable semantic comparison with repository documents.",
                    "expected_response": "A single vector representation of the query description."
                },
                {
                    "user_intent": "I want to find and rank documents that are most similar to my query description.",
                    "tool_description": "Compute similarity scores between the query vector and each document vector, then rank documents based on semantic similarity.",
                    "expected_response": "A ranked list of documents with their corresponding similarity scores."
                }
            ],
            "extracted_arguments": {
                "document_repository": "A collection of textual documents to be searched, stored as strings or structured text objects.",
                "query_description": "The textual description used as a search query, provided as a string.",
                "embedding_model": "The pre-trained sentence-transformers model used to generate vector embeddings for semantic comparison."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_493",
        "query": "Determine the electricity consumption of a residential area based on historical data.",
        "instruction": "Given an `electricity consumption prediction` task, retrieve tools that utilize regression models to analyze historical data and predict future consumption, focusing on tools capable of processing tabular data and implementing predictive algorithms effectively.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Scikit-learn",
                    "functionality": "skops",
                    "api_call": "RandomForestRegressor()",
                    "api_arguments": {
                        "bootstrap": "True",
                        "ccp_alpha": "0.0",
                        "criterion": "squared_error",
                        "max_depth": "10",
                        "max_features": "1.0",
                        "max_leaf_nodes": "",
                        "max_samples": "",
                        "min_impurity_decrease": "0.0",
                        "min_samples_leaf": "1",
                        "min_samples_split": "2",
                        "min_weight_fraction_leaf": "0.0",
                        "n_estimators": "50",
                        "n_jobs": "",
                        "oob_score": "False",
                        "random_state": "59",
                        "verbose": "0",
                        "warm_start": "False"
                    },
                    "python_environment_requirements": "",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A RandomForestRegressor model for electricity consumption prediction.",
                    "id": "huggingface_api_888",
                    "name": "rajistics/MAPIE-TS-Electricity"
                },
                "id": "gorilla_huggingface_tool_881",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 1: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 4: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 5: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 6: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 7: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 9: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 10: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 12: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 13: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 14: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 15: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 16: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 17: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 18: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 19: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features."
        ],
        "tool_view": {
            "description": "A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "parameters": {
                "bootstrap": {
                    "type": "boolean",
                    "description": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
                    "required": true,
                    "default": "True",
                    "example": true
                },
                "ccp_alpha": {
                    "type": "number",
                    "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "criterion": {
                    "type": "string",
                    "description": "The function used to measure the quality of a split. 'squared_error' uses mean squared error.",
                    "required": true,
                    "default": "squared_error",
                    "example": "squared_error"
                },
                "max_depth": {
                    "type": "integer",
                    "description": "The maximum depth of the tree. Limits how deep each tree can grow.",
                    "required": true,
                    "default": "10",
                    "example": 10
                },
                "max_features": {
                    "type": "number",
                    "description": "The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.",
                    "required": true,
                    "default": "1.0",
                    "example": 1.0
                },
                "max_leaf_nodes": {
                    "type": "string",
                    "description": "Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.",
                    "required": true,
                    "example": ""
                },
                "max_samples": {
                    "type": "string",
                    "description": "If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.",
                    "required": true,
                    "example": ""
                },
                "min_impurity_decrease": {
                    "type": "number",
                    "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "min_samples_leaf": {
                    "type": "integer",
                    "description": "The minimum number of samples required to be at a leaf node.",
                    "required": true,
                    "default": "1",
                    "example": 1
                },
                "min_samples_split": {
                    "type": "integer",
                    "description": "The minimum number of samples required to split an internal node.",
                    "required": true,
                    "default": "2",
                    "example": 2
                },
                "min_weight_fraction_leaf": {
                    "type": "number",
                    "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "n_estimators": {
                    "type": "integer",
                    "description": "The number of trees in the forest.",
                    "required": true,
                    "default": "50",
                    "example": 50
                },
                "n_jobs": {
                    "type": "string",
                    "description": "The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.",
                    "required": true,
                    "example": ""
                },
                "oob_score": {
                    "type": "boolean",
                    "description": "Whether to use out-of-bag samples to estimate the R^2 on unseen data.",
                    "required": true,
                    "default": "False",
                    "example": false
                },
                "random_state": {
                    "type": "integer",
                    "description": "Controls the randomness of the estimator for reproducibility.",
                    "required": true,
                    "default": "59",
                    "example": 59
                },
                "verbose": {
                    "type": "integer",
                    "description": "Controls the verbosity when fitting and predicting.",
                    "required": true,
                    "default": "0",
                    "example": 0
                },
                "warm_start": {
                    "type": "boolean",
                    "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.",
                    "required": true,
                    "default": "False",
                    "example": false
                }
            },
            "response": "The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\n\n1. **model description**: \n   - **Domain**: \"Tabular Tabular Regression\" - This indicates that the model is designed for regression tasks involving tabular data.\n   - **Framework**: \"Scikit-learn\" - Specifies that the model is built using the Scikit-learn machine learning library.\n   - **Functionality**: \"skops\" - Refers to the model's compatibility with or use of the Skops library.\n   - **Description**: \"A RandomForestRegressor model for electricity consumption prediction\" - A brief description of what the model is used for.\n\n2. **API call**: \n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\n\n3. **Performance**: \n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\n\n4. **Example Code**: \n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\n\n5. **Python Environment Requirements**: \n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\n\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.",
            "examples": [
                {
                    "user_intent": "I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 100,
                            "n_jobs": -1,
                            "oob_score": true,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 20,
                            "max_features": 1.5,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 1: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 2: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 4: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 5: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 6: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 7: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 9: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 10: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 12: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 13: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 14: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 15: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 16: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 17: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 18: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 19: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\", \"parameters\": {\"bootstrap\": {\"type\": \"boolean\", \"description\": \"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\", \"required\": true, \"default\": \"True\", \"example\": true}, \"ccp_alpha\": {\"type\": \"number\", \"description\": \"Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"criterion\": {\"type\": \"string\", \"description\": \"The function used to measure the quality of a split. 'squared_error' uses mean squared error.\", \"required\": true, \"default\": \"squared_error\", \"example\": \"squared_error\"}, \"max_depth\": {\"type\": \"integer\", \"description\": \"The maximum depth of the tree. Limits how deep each tree can grow.\", \"required\": true, \"default\": \"10\", \"example\": 10}, \"max_features\": {\"type\": \"number\", \"description\": \"The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.\", \"required\": true, \"default\": \"1.0\", \"example\": 1.0}, \"max_leaf_nodes\": {\"type\": \"string\", \"description\": \"Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\", \"required\": true, \"example\": \"\"}, \"max_samples\": {\"type\": \"string\", \"description\": \"If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.\", \"required\": true, \"example\": \"\"}, \"min_impurity_decrease\": {\"type\": \"number\", \"description\": \"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"min_samples_leaf\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to be at a leaf node.\", \"required\": true, \"default\": \"1\", \"example\": 1}, \"min_samples_split\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to split an internal node.\", \"required\": true, \"default\": \"2\", \"example\": 2}, \"min_weight_fraction_leaf\": {\"type\": \"number\", \"description\": \"The minimum weighted fraction of the sum total of weights required to be at a leaf node.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"n_estimators\": {\"type\": \"integer\", \"description\": \"The number of trees in the forest.\", \"required\": true, \"default\": \"50\", \"example\": 50}, \"n_jobs\": {\"type\": \"string\", \"description\": \"The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.\", \"required\": true, \"example\": \"\"}, \"oob_score\": {\"type\": \"boolean\", \"description\": \"Whether to use out-of-bag samples to estimate the R^2 on unseen data.\", \"required\": true, \"default\": \"False\", \"example\": false}, \"random_state\": {\"type\": \"integer\", \"description\": \"Controls the randomness of the estimator for reproducibility.\", \"required\": true, \"default\": \"59\", \"example\": 59}, \"verbose\": {\"type\": \"integer\", \"description\": \"Controls the verbosity when fitting and predicting.\", \"required\": true, \"default\": \"0\", \"example\": 0}, \"warm_start\": {\"type\": \"boolean\", \"description\": \"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.\", \"required\": true, \"default\": \"False\", \"example\": false}}, \"response\": \"The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\\n\\n1. **model description**: \\n   - **Domain**: \\\"Tabular Tabular Regression\\\" - This indicates that the model is designed for regression tasks involving tabular data.\\n   - **Framework**: \\\"Scikit-learn\\\" - Specifies that the model is built using the Scikit-learn machine learning library.\\n   - **Functionality**: \\\"skops\\\" - Refers to the model's compatibility with or use of the Skops library.\\n   - **Description**: \\\"A RandomForestRegressor model for electricity consumption prediction\\\" - A brief description of what the model is used for.\\n\\n2. **API call**: \\n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \\n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\\n\\n3. **Performance**: \\n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\\n\\n4. **Example Code**: \\n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\\n\\n5. **Python Environment Requirements**: \\n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\\n\\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.\", \"examples\": [{\"user_intent\": \"I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": -1, \"oob_score\": true, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 20, \"max_features\": 1.5, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}]}\n\n# User Query\nDetermine the electricity consumption of a residential area based on historical data.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict electricity consumption using historical data for a residential area.",
                    "tool_description": "Use a RandomForestRegressor model to estimate electricity consumption based on tabular historical data. The model leverages an ensemble of decision trees to handle complex patterns and improve regression accuracy.",
                    "expected_response": "A numeric prediction representing the estimated electricity consumption for the given residential area."
                }
            ],
            "extracted_arguments": {
                "input_data": "A tabular dataset containing historical electricity usage and relevant features for the residential area."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_214",
        "query": "We are worried about price inflation in our country. Can you answer our questions on price inflation using the BERT large cased whole word masking finetuned model on SQuAD?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing models, specifically those fine-tuned for accurate comprehension and response generation about the topic of price inflation, using a specified model and dataset such as BERT large cased whole word masking fine-tuned on SQuAD.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')",
                    "api_arguments": {
                        "model_name_or_path": "bert-large-cased-whole-word-masking",
                        "dataset_name": "squad",
                        "do_train": true,
                        "do_eval": true,
                        "learning_rate": 3e-05,
                        "num_train_epochs": 2,
                        "max_seq_length": 384,
                        "doc_stride": 128,
                        "output_dir": "./examples/models/wwm_cased_finetuned_squad/",
                        "per_device_eval_batch_size": 3,
                        "per_device_train_batch_size": 3
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\nprint(result)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BookCorpus",
                                "accuracy": "N/A"
                            },
                            {
                                "name": "English Wikipedia",
                                "accuracy": "N/A"
                            }
                        ]
                    },
                    "description": "BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.",
                    "id": "huggingface_api_486",
                    "name": "bert-large-cased-whole-word-masking-finetuned-squad"
                },
                "id": "gorilla_huggingface_tool_482",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 1: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 2: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 3: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 4: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 5: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 7: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 8: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 9: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 12: A BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification, enabling the identification of entities or labels at the token level in text using Hugging Face Transformers.",
            "Tool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 14: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 15: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 16: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.",
            "Tool 17: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries."
        ],
        "tool_view": {
            "description": "A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "parameters": {
                "model_name_or_path": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained model to load for question answering.",
                    "required": true,
                    "example": "bert-large-cased-whole-word-masking"
                },
                "dataset_name": {
                    "type": "string",
                    "description": "The name of the dataset to use for training and evaluation.",
                    "required": true,
                    "example": "squad"
                },
                "do_train": {
                    "type": "boolean",
                    "description": "Whether to perform training on the specified dataset.",
                    "required": true,
                    "example": true
                },
                "do_eval": {
                    "type": "boolean",
                    "description": "Whether to perform evaluation on the specified dataset.",
                    "required": true,
                    "example": true
                },
                "learning_rate": {
                    "type": "number",
                    "description": "The learning rate to use during training.",
                    "required": true,
                    "example": 3e-05
                },
                "num_train_epochs": {
                    "type": "number",
                    "description": "The number of epochs to train the model.",
                    "required": true,
                    "example": 2
                },
                "max_seq_length": {
                    "type": "number",
                    "description": "The maximum total input sequence length after tokenization.",
                    "required": true,
                    "example": 384
                },
                "doc_stride": {
                    "type": "number",
                    "description": "The stride to take when splitting long documents into chunks for training.",
                    "required": true,
                    "example": 128
                },
                "output_dir": {
                    "type": "string",
                    "description": "The directory where the trained model and checkpoints will be saved.",
                    "required": true,
                    "example": "./examples/models/wwm_cased_finetuned_squad/"
                },
                "per_device_eval_batch_size": {
                    "type": "number",
                    "description": "The batch size for evaluation per device (GPU/CPU).",
                    "required": true,
                    "example": 3
                },
                "per_device_train_batch_size": {
                    "type": "number",
                    "description": "The batch size for training per device (GPU/CPU).",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool’s response is a structured object representing the answer to a given natural language question based on a provided context. The response typically contains the following components:  \n\n1. **`answer`** – A string containing the text span from the context that the model predicts as the answer to the question. This is the primary output of the question-answering task.  \n\n2. **`score`** – A numerical value (usually a floating-point probability) representing the model’s confidence in the predicted answer. Higher values indicate greater confidence.  \n\n3. **`start`** – An integer representing the character index in the context where the predicted answer begins. This allows precise localization of the answer within the input text.  \n\n4. **`end`** – An integer representing the character index in the context where the predicted answer ends. This, together with `start`, defines the exact span of the predicted answer.  \n\nOptionally, in some implementations or extended pipelines, the response may also include additional metadata, such as multiple answer candidates or attention scores, but the core output always provides the text of the answer and its position within the context along with a confidence score.  \n\nThis response structure allows downstream applications to extract the answer, evaluate its reliability, and map it back to the original input text.",
            "examples": [
                {
                    "user_intent": "I want to find the answer to a question about a paragraph of text I have.",
                    "tool_call": {
                        "name": "bert-large-cased-whole-word-masking-finetuned-squad",
                        "arguments": {
                            "context": "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel.",
                            "question": "Who was the Eiffel Tower named after?"
                        }
                    }
                },
                {
                    "user_intent": "Can you tell me the main ingredient in this recipe passage?",
                    "tool_call": {
                        "name": "bert-large-cased-whole-word-masking-finetuned-squad",
                        "arguments": {
                            "context": "To make the perfect guacamole, you need ripe avocados, lime juice, chopped onions, and a pinch of salt.",
                            "question": "What is the main ingredient in guacamole?"
                        }
                    }
                },
                {
                    "user_intent": "I have a technical paragraph and I want to extract the key fact about a software library mentioned.",
                    "tool_call": {
                        "name": "bert-large-cased-whole-word-masking-finetuned-squad",
                        "arguments": {
                            "context": "The PyTorch library is widely used for machine learning and deep learning applications. It provides flexible tensor computation and dynamic neural networks.",
                            "question": "What is PyTorch used for?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 1: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 2: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 3: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 4: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 5: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 7: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 8: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 9: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 12: A BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification, enabling the identification of entities or labels at the token level in text using Hugging Face Transformers.\nTool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 14: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 15: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 16: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.\nTool 17: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\", \"parameters\": {\"model_name_or_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained model to load for question answering.\", \"required\": true, \"example\": \"bert-large-cased-whole-word-masking\"}, \"dataset_name\": {\"type\": \"string\", \"description\": \"The name of the dataset to use for training and evaluation.\", \"required\": true, \"example\": \"squad\"}, \"do_train\": {\"type\": \"boolean\", \"description\": \"Whether to perform training on the specified dataset.\", \"required\": true, \"example\": true}, \"do_eval\": {\"type\": \"boolean\", \"description\": \"Whether to perform evaluation on the specified dataset.\", \"required\": true, \"example\": true}, \"learning_rate\": {\"type\": \"number\", \"description\": \"The learning rate to use during training.\", \"required\": true, \"example\": 3e-05}, \"num_train_epochs\": {\"type\": \"number\", \"description\": \"The number of epochs to train the model.\", \"required\": true, \"example\": 2}, \"max_seq_length\": {\"type\": \"number\", \"description\": \"The maximum total input sequence length after tokenization.\", \"required\": true, \"example\": 384}, \"doc_stride\": {\"type\": \"number\", \"description\": \"The stride to take when splitting long documents into chunks for training.\", \"required\": true, \"example\": 128}, \"output_dir\": {\"type\": \"string\", \"description\": \"The directory where the trained model and checkpoints will be saved.\", \"required\": true, \"example\": \"./examples/models/wwm_cased_finetuned_squad/\"}, \"per_device_eval_batch_size\": {\"type\": \"number\", \"description\": \"The batch size for evaluation per device (GPU/CPU).\", \"required\": true, \"example\": 3}, \"per_device_train_batch_size\": {\"type\": \"number\", \"description\": \"The batch size for training per device (GPU/CPU).\", \"required\": true, \"example\": 3}}, \"response\": \"The tool\\u2019s response is a structured object representing the answer to a given natural language question based on a provided context. The response typically contains the following components:  \\n\\n1. **`answer`** \\u2013 A string containing the text span from the context that the model predicts as the answer to the question. This is the primary output of the question-answering task.  \\n\\n2. **`score`** \\u2013 A numerical value (usually a floating-point probability) representing the model\\u2019s confidence in the predicted answer. Higher values indicate greater confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the character index in the context where the predicted answer begins. This allows precise localization of the answer within the input text.  \\n\\n4. **`end`** \\u2013 An integer representing the character index in the context where the predicted answer ends. This, together with `start`, defines the exact span of the predicted answer.  \\n\\nOptionally, in some implementations or extended pipelines, the response may also include additional metadata, such as multiple answer candidates or attention scores, but the core output always provides the text of the answer and its position within the context along with a confidence score.  \\n\\nThis response structure allows downstream applications to extract the answer, evaluate its reliability, and map it back to the original input text.\", \"examples\": [{\"user_intent\": \"I want to find the answer to a question about a paragraph of text I have.\", \"tool_call\": {\"name\": \"bert-large-cased-whole-word-masking-finetuned-squad\", \"arguments\": {\"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel.\", \"question\": \"Who was the Eiffel Tower named after?\"}}}, {\"user_intent\": \"Can you tell me the main ingredient in this recipe passage?\", \"tool_call\": {\"name\": \"bert-large-cased-whole-word-masking-finetuned-squad\", \"arguments\": {\"context\": \"To make the perfect guacamole, you need ripe avocados, lime juice, chopped onions, and a pinch of salt.\", \"question\": \"What is the main ingredient in guacamole?\"}}}, {\"user_intent\": \"I have a technical paragraph and I want to extract the key fact about a software library mentioned.\", \"tool_call\": {\"name\": \"bert-large-cased-whole-word-masking-finetuned-squad\", \"arguments\": {\"context\": \"The PyTorch library is widely used for machine learning and deep learning applications. It provides flexible tensor computation and dynamic neural networks.\", \"question\": \"What is PyTorch used for?\"}}}]}\n\n# User Query\nWe are worried about price inflation in our country. Can you answer our questions on price inflation using the BERT large cased whole word masking finetuned model on SQuAD?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to get answers to questions about price inflation using a BERT-based model.",
                    "tool_description": "Use a BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset to extract precise answers from a given text context related to price inflation.",
                    "expected_response": "A structured object containing the predicted answer text, confidence score, and the start and end positions of the answer within the provided context."
                }
            ],
            "extracted_arguments": {
                "model_name_or_path": "The identifier or path of the pretrained BERT large cased whole word masking model fine-tuned on SQuAD.",
                "dataset_name": "The name of the dataset used for model fine-tuning, e.g., SQuAD.",
                "context": "The textual passage containing information about price inflation.",
                "question": "The specific natural language question about price inflation.",
                "do_train": "Boolean indicating whether to train the model on the dataset.",
                "do_eval": "Boolean indicating whether to evaluate the model on the dataset.",
                "learning_rate": "The learning rate to use if training the model, as a numeric value.",
                "num_train_epochs": "The number of training epochs if training the model, as a numeric value.",
                "max_seq_length": "The maximum sequence length for tokenization, as a numeric value.",
                "doc_stride": "The stride to take when splitting long documents into chunks for training, as a numeric value.",
                "output_dir": "Directory path where the trained model and checkpoints will be saved.",
                "per_device_eval_batch_size": "Batch size per device for evaluation, as a numeric value.",
                "per_device_train_batch_size": "Batch size per device for training, as a numeric value."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_266",
        "query": "We are building a content moderation system. Our clients upload the content, it can be generated by human or AI. We want to have a filtering API to advise on the original text if it is generated by GPT-2.",
        "instruction": "Given a `content moderation` task, retrieve tools that utilize text classification models to analyze and determine whether input text has been generated by GPT-2, thereby enabling effective content filtering and moderation based on the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Detect GPT-2 generated text",
                    "api_call": "pipeline('text-classification', model='roberta-base-openai-detector')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\nprint(pipe(Hello world! Is this content AI-generated?))",
                    "performance": {
                        "dataset": "WebText",
                        "accuracy": "95%"
                    },
                    "description": "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.",
                    "id": "huggingface_api_382",
                    "name": "roberta-base-openai-detector"
                },
                "id": "gorilla_huggingface_tool_378",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 1: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 3: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 4: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 5: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.",
            "Tool 6: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 8: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 9: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 11: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 12: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 14: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 15: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 16: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 17: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 18: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 19: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation."
        ],
        "tool_view": {
            "description": "The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "parameters": {},
            "response": "The tool’s response is a structured output indicating whether the input text is likely to have been generated by a GPT-2 model. The response is typically a list of dictionaries (or objects), where each dictionary corresponds to a single text input and contains the following components:\n\n1. **label** – A string representing the predicted class for the input text. For this detector, the possible values are typically along the lines of `\"GPT-2 generated\"` or `\"Human-written\"` (or similar phrasing depending on the implementation). This indicates the model’s categorical prediction.\n\n2. **score** – A numerical value representing the model’s confidence in its prediction. This is usually a probability between 0 and 1, quantifying how strongly the model believes the text belongs to the predicted class. Higher values indicate higher confidence.\n\n3. **additional metadata (optional)** – Some text-classification pipelines may include extra fields, such as the input text or model-specific information, but the core components are the `label` and `score`.\n\nIn summary, for each input text, the response provides a classification label and a confidence score that together describe whether the text is likely AI-generated or human-written. The list structure allows the tool to handle multiple texts in a single call.",
            "examples": [
                {
                    "user_intent": "I want to check if this paragraph was written by GPT-2: 'In the future, AI will revolutionize the way we work and live.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "In the future, AI will revolutionize the way we work and live."
                        }
                    }
                },
                {
                    "user_intent": "Detect whether the following email is AI-generated: 'Dear user, your account has been updated successfully.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "Dear user, your account has been updated successfully."
                        }
                    }
                },
                {
                    "user_intent": "Analyze this article snippet to see if it was produced by a GPT-2 model: 'Climate change impacts are becoming increasingly severe worldwide.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "Climate change impacts are becoming increasingly severe worldwide."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 1: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 3: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 4: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 5: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.\nTool 6: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 8: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 9: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 11: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 12: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 13: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 14: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 15: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 16: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 17: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 18: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 19: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\", \"parameters\": {}, \"response\": \"The tool\\u2019s response is a structured output indicating whether the input text is likely to have been generated by a GPT-2 model. The response is typically a list of dictionaries (or objects), where each dictionary corresponds to a single text input and contains the following components:\\n\\n1. **label** \\u2013 A string representing the predicted class for the input text. For this detector, the possible values are typically along the lines of `\\\"GPT-2 generated\\\"` or `\\\"Human-written\\\"` (or similar phrasing depending on the implementation). This indicates the model\\u2019s categorical prediction.\\n\\n2. **score** \\u2013 A numerical value representing the model\\u2019s confidence in its prediction. This is usually a probability between 0 and 1, quantifying how strongly the model believes the text belongs to the predicted class. Higher values indicate higher confidence.\\n\\n3. **additional metadata (optional)** \\u2013 Some text-classification pipelines may include extra fields, such as the input text or model-specific information, but the core components are the `label` and `score`.\\n\\nIn summary, for each input text, the response provides a classification label and a confidence score that together describe whether the text is likely AI-generated or human-written. The list structure allows the tool to handle multiple texts in a single call.\", \"examples\": [{\"user_intent\": \"I want to check if this paragraph was written by GPT-2: 'In the future, AI will revolutionize the way we work and live.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"In the future, AI will revolutionize the way we work and live.\"}}}, {\"user_intent\": \"Detect whether the following email is AI-generated: 'Dear user, your account has been updated successfully.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"Dear user, your account has been updated successfully.\"}}}, {\"user_intent\": \"Analyze this article snippet to see if it was produced by a GPT-2 model: 'Climate change impacts are becoming increasingly severe worldwide.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"Climate change impacts are becoming increasingly severe worldwide.\"}}}]}\n\n# User Query\nWe are building a content moderation system. Our clients upload the content, it can be generated by human or AI. We want to have a filtering API to advise on the original text if it is generated by GPT-2.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze content to determine if it was generated by GPT-2.",
                    "tool_description": "Classify whether the uploaded content was generated by GPT-2 or written by a human, using a pre-trained text classification model.",
                    "expected_response": "A list of dictionaries, each containing a classification label ('GPT-2 generated' or 'Human-written') and a confidence score indicating the likelihood of the classification."
                }
            ],
            "extracted_arguments": {
                "text": "The content to be analyzed for AI or human authorship, typically provided as a string of text."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_20",
        "query": "Our company receives invoices in different formats. We need to extract specific information from these documents to process payments and keep records.",
        "instruction": "Given a `document information extraction` task, retrieve tools designed to process multimodal documents, extracting specific data for payment processing and record-keeping by utilizing machine learning models capable of understanding various document formats.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.12.2",
                        "torch==1.8.0+cu101",
                        "datasets==1.14.0",
                        "tokenizers==0.10.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 1.194
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_115",
                    "name": "layoutlmv2-base-uncased-finetuned-docvqa"
                },
                "id": "gorilla_huggingface_tool_114",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 1: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 5: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 8: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 9: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 10: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 11: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.",
            "Tool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 13: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 14: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 15: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 16: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 18: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information."
        ],
        "tool_view": {
            "description": "CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "parameters": {
                "url": {
                    "type": "string",
                    "description": "The URL of the document image or PDF from which information will be extracted to answer the question.",
                    "required": true,
                    "example": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
                },
                "question": {
                    "type": "string",
                    "description": "The question to ask about the content of the document, which the model will attempt to answer.",
                    "required": true,
                    "example": "What is the invoice number?"
                }
            },
            "response": "The tool returns an answer to the question posed, based on a multimodal document (such as a PDF or image) provided in the `url` argument. The response consists of the following elements:\n\n1. **Answer**: The extracted information or the response to the user’s query based on the contents of the document provided via the `url`. This is typically a string of text.\n\n2. **Context**: The portion of the document that was used to derive the answer. This may include key excerpts or text passages from the document that directly relate to the answer. (Note: While not explicitly mentioned in the tool documentation, such context is often included in similar multimodal document question-answering systems to provide transparency on the answer derivation process.)\n\n3. **Accuracy (Optional)**: Although the performance section indicates accuracy metrics from evaluations on a dataset, it is not clear whether this metric is included in the direct response. However, it could be part of internal evaluation or debugging.\n\n4. **Error or No Answer (Optional)**: If the model is unable to find a relevant answer or cannot process the document properly, it may return an error message or indicate that no answer was found.\n\nThis response format is designed to provide the answer to a specific question based on the input document URL (which could be an image, PDF, etc.), leveraging the model's capability to interpret both the textual and visual content.",
            "examples": [
                {
                    "user_intent": "I want to extract the invoice number from an image of an invoice.",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",
                            "question": "What is the invoice number?"
                        }
                    }
                },
                {
                    "user_intent": "Can you tell me the purchase amount from this receipt image?",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg",
                            "question": "What is the purchase amount?"
                        }
                    }
                },
                {
                    "user_intent": "I need to know the net sales for the year 2020 from this income statement.",
                    "tool_call": {
                        "name": "CQI_Visual_Question_Awnser_PT_v0",
                        "arguments": {
                            "url": "https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png",
                            "question": "What are the 2020 net sales?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 1: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 5: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 8: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 9: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 10: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 11: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.\nTool 12: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 13: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 14: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 15: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 16: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 18: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 19: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\", \"parameters\": {\"url\": {\"type\": \"string\", \"description\": \"The URL of the document image or PDF from which information will be extracted to answer the question.\", \"required\": true, \"example\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to ask about the content of the document, which the model will attempt to answer.\", \"required\": true, \"example\": \"What is the invoice number?\"}}, \"response\": \"The tool returns an answer to the question posed, based on a multimodal document (such as a PDF or image) provided in the `url` argument. The response consists of the following elements:\\n\\n1. **Answer**: The extracted information or the response to the user\\u2019s query based on the contents of the document provided via the `url`. This is typically a string of text.\\n\\n2. **Context**: The portion of the document that was used to derive the answer. This may include key excerpts or text passages from the document that directly relate to the answer. (Note: While not explicitly mentioned in the tool documentation, such context is often included in similar multimodal document question-answering systems to provide transparency on the answer derivation process.)\\n\\n3. **Accuracy (Optional)**: Although the performance section indicates accuracy metrics from evaluations on a dataset, it is not clear whether this metric is included in the direct response. However, it could be part of internal evaluation or debugging.\\n\\n4. **Error or No Answer (Optional)**: If the model is unable to find a relevant answer or cannot process the document properly, it may return an error message or indicate that no answer was found.\\n\\nThis response format is designed to provide the answer to a specific question based on the input document URL (which could be an image, PDF, etc.), leveraging the model's capability to interpret both the textual and visual content.\", \"examples\": [{\"user_intent\": \"I want to extract the invoice number from an image of an invoice.\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\", \"question\": \"What is the invoice number?\"}}}, {\"user_intent\": \"Can you tell me the purchase amount from this receipt image?\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\", \"question\": \"What is the purchase amount?\"}}}, {\"user_intent\": \"I need to know the net sales for the year 2020 from this income statement.\", \"tool_call\": {\"name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"arguments\": {\"url\": \"https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\", \"question\": \"What are the 2020 net sales?\"}}}]}\n\n# User Query\nOur company receives invoices in different formats. We need to extract specific information from these documents to process payments and keep records.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract specific information from invoices and other document images for processing and record-keeping.",
                    "tool_description": "Use a multimodal document question-answering tool to process images or PDFs of invoices and extract targeted information by asking specific questions.",
                    "expected_response": "A structured answer containing the requested information from the document, possibly including context excerpts from the document relevant to the answer."
                }
            ],
            "extracted_arguments": {
                "url": "The URL or file path of the document image or PDF to be processed.",
                "question": "The specific information to extract from the document, framed as a question (e.g., invoice number, payment amount, or date)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_362",
        "query": "We are developing an interactive story app that would display a picture of each character as mentioned in the story given to our GPT-3 model.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that convert narrative descriptions into visual representations by processing text prompts to produce images that match characters or scenes from the story provided to the model.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0')",
                    "api_arguments": [
                        "prompt",
                        "negative_prompt"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "torch"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-anime-1.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.",
                    "id": "huggingface_api_51",
                    "name": "dreamlike-art/dreamlike-anime-1.0"
                },
                "id": "gorilla_huggingface_tool_51",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 1: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 2: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 3: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 4: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 5: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 8: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 9: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 10: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 11: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 12: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.",
            "Tool 13: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 14: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 15: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.",
            "Tool 16: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 17: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 18: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 19: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts."
        ],
        "tool_view": {
            "description": "GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will continue or generate text from.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "example": 50
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of independent text sequences to generate for the given input prompt.",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\n\nHere’s a description of the response structure:\n\n1. **Generated Texts**: \n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \n   - Each sequence will represent a continuation or completion of the input text (e.g., \"Hello, I'm a language model,\").\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\n\n2. **Content of Each Text Generation**: \n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\n\n### Key Components:\n\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\n   - Each element in the array is a string, representing a complete generated text.\n\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\n\n### Example of Expected Output Content (Abstract):\n\n- A list of 5 strings, each one being a text sequence that continues from the input (\"Hello, I'm a language model,\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\n\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a distant galaxy,'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "Once upon a time in a distant galaxy,",
                            "max_length": 50,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Continue this sentence realistically: 'The stock market crashed yesterday because'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "The stock market crashed yesterday because",
                            "max_length": 40,
                            "num_return_sequences": 2
                        }
                    }
                },
                {
                    "user_intent": "Write multiple humorous completions for: 'I never expected my cat to'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "I never expected my cat to",
                            "max_length": 30,
                            "num_return_sequences": 5
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 1: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 2: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 3: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 4: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 5: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 8: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 9: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 10: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 11: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 12: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.\nTool 13: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 14: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 15: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.\nTool 16: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 17: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 18: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 19: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will continue or generate text from.\", \"required\": true, \"example\": \"Once upon a time\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"example\": 50}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of independent text sequences to generate for the given input prompt.\", \"required\": true, \"example\": 3}}, \"response\": \"The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\\n\\nHere\\u2019s a description of the response structure:\\n\\n1. **Generated Texts**: \\n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \\n   - Each sequence will represent a continuation or completion of the input text (e.g., \\\"Hello, I'm a language model,\\\").\\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\\n\\n2. **Content of Each Text Generation**: \\n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\\n\\n### Key Components:\\n\\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \\n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\\n   - Each element in the array is a string, representing a complete generated text.\\n\\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\\n\\n### Example of Expected Output Content (Abstract):\\n\\n- A list of 5 strings, each one being a text sequence that continues from the input (\\\"Hello, I'm a language model,\\\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\\n\\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a distant galaxy,'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"Once upon a time in a distant galaxy,\", \"max_length\": 50, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Continue this sentence realistically: 'The stock market crashed yesterday because'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"The stock market crashed yesterday because\", \"max_length\": 40, \"num_return_sequences\": 2}}}, {\"user_intent\": \"Write multiple humorous completions for: 'I never expected my cat to'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"I never expected my cat to\", \"max_length\": 30, \"num_return_sequences\": 5}}}]}\n\n# User Query\nWe are developing an interactive story app that would display a picture of each character as mentioned in the story given to our GPT-3 model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a story using a GPT-3 model based on character descriptions.",
                    "tool_description": "Generate a text-based continuation or narrative using a GPT-3 model that builds on input prompts, incorporating character-based storytelling.",
                    "expected_response": "A sequence of text that continues the story, potentially including dialogue and character actions, based on the provided character descriptions."
                },
                {
                    "user_intent": "I need to display a character image along with the generated story.",
                    "tool_description": "Retrieve or generate a visual representation of the character based on the story description or character persona provided.",
                    "expected_response": "A set of image URLs or files corresponding to the characters mentioned in the story."
                }
            ],
            "extracted_arguments": {
                "story_prompt": "The input text prompt or character description that the model will use to generate the story.",
                "num_return_sequences": "The number of independent story sequences to generate from the provided input.",
                "character_description": "A textual description or persona of the character(s) for visual generation or display.",
                "image_type": "The desired format or specification for character images to be displayed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_363",
        "query": "We want to implement a chatbot on our website to provide quick responses to customer inquiries.",
        "instruction": "Given a `chatbot implementation` task, retrieve tools that facilitate the deployment of conversational AI on websites by generating dialogue responses based on user inquiries, ensuring quick and relevant interactions aligned with the functionality requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face",
                    "functionality": "Dialogue Response Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')",
                    "api_arguments": [
                        "bot_input_ids",
                        "max_length",
                        "pad_token_id"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\nfor step in range(5):\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))",
                    "performance": {
                        "dataset": "Reddit discussion thread",
                        "accuracy": "Comparable to human response quality under a single-turn conversation Turing test"
                    },
                    "description": "DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.",
                    "id": "huggingface_api_578",
                    "name": "microsoft/DialoGPT-small"
                },
                "id": "gorilla_huggingface_tool_574",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 2: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 3: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 4: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 8: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 9: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 10: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.",
            "Tool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 12: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 15: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 17: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.",
            "Tool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 19: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights."
        ],
        "tool_view": {
            "description": "Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "parameters": {
                "input": {
                    "type": "string",
                    "description": "The input message that the model will generate a response for. This serves as the starting point for the conversation.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model’s continuation, reply, or conversational output. Specifically:\n\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\n  - `generated_text`: The actual text output produced by the model.\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model’s confidence or token-level details.  \n\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.",
            "examples": [
                {
                    "user_intent": "Start a conversation with the Zixtrauce chatbot and ask about the weather.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What's the weather like today?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a response from BrandonBot4Epochs about AI advancements.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "Tell me about the latest advancements in AI."
                        }
                    }
                },
                {
                    "user_intent": "Chat with the bot about hobbies and interests.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What do you think is the best hobby to start?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 2: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 3: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 4: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 8: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 9: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 10: A lightweight Visual Question Answering model based on the VILT framework that takes an image and a text question as input to generate answers, designed for quick experimentation and testing within the Hugging Face Transformers environment.\nTool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 12: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 15: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 17: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\nTool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 19: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\", \"parameters\": {\"input\": {\"type\": \"string\", \"description\": \"The input message that the model will generate a response for. This serves as the starting point for the conversation.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model\\u2019s continuation, reply, or conversational output. Specifically:\\n\\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \\n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\\n  - `generated_text`: The actual text output produced by the model.\\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model\\u2019s confidence or token-level details.  \\n\\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.\", \"examples\": [{\"user_intent\": \"Start a conversation with the Zixtrauce chatbot and ask about the weather.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What's the weather like today?\"}}}, {\"user_intent\": \"Generate a response from BrandonBot4Epochs about AI advancements.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"Tell me about the latest advancements in AI.\"}}}, {\"user_intent\": \"Chat with the bot about hobbies and interests.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What do you think is the best hobby to start?\"}}}]}\n\n# User Query\nWe want to implement a chatbot on our website to provide quick responses to customer inquiries.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Integrate a conversational AI chatbot on our website to handle customer inquiries.",
                    "tool_description": "Deploy a GPT-based conversational AI model capable of generating context-aware text responses for multi-turn dialogue, enabling real-time interaction with website visitors.",
                    "expected_response": "Generated text responses from the AI model corresponding to user inputs, formatted for display in the web chat interface."
                }
            ],
            "extracted_arguments": {
                "input_message": "The text message or customer inquiry submitted by a website visitor that requires a chatbot response."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_319",
        "query": "We have a picture of a landscape, and we'd like to add a building and a river in this picture.",
        "instruction": "Given an `image editing` task, retrieve tools that utilize image-to-image transformations by processing existing images and applying additional features or elements specified in the query to achieve the desired modifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Diffusion-based text-to-image generation model",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11e_sd15_ip2p')",
                    "api_arguments": [
                        "checkpoint",
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\nprompt = make it on fire\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "Stable Diffusion v1-5",
                        "accuracy": "Not provided"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.",
                    "id": "huggingface_api_274",
                    "name": "lllyasviel/control_v11e_sd15_ip2p"
                },
                "id": "gorilla_huggingface_tool_270",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 1: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 2: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 5: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 6: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 7: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 8: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 9: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 12: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 13: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 14: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 15: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 16: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 18: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 19: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on."
        ],
        "tool_view": {
            "description": "Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "parameters": {},
            "response": "The tool produces a conversational response in the context of a dialogue. The response is typically structured as an object representing the model’s reply to the provided input message. Key components of the response generally include:\n\n1. **Generated Text / Response**: The main content of the tool’s output, representing the model’s natural language reply to the input message. This is the primary field that contains the conversational answer.  \n\n2. **Conversation Context (Optional)**: Some implementations may include metadata about the ongoing conversation, such as previous messages or a history of turns, to maintain context across multiple exchanges.  \n\n3. **Metadata / Additional Info (Optional)**: Depending on the pipeline configuration, the response may also include auxiliary fields like message identifiers, timestamps, or flags indicating special tokens, but these are not guaranteed by the documented JSON.  \n\nOverall, the response is focused on providing a coherent and contextually appropriate conversational reply based on the input message, potentially maintaining dialogue history if multiple exchanges are processed.",
            "examples": [
                {
                    "user_intent": "I want to have a conversation with an AI. Can you start by asking how I'm doing?",
                    "tool_call": {
                        "name": "ingen51/DialoGPT-medium-GPT4",
                        "arguments": {
                            "message": "Hello, how are you?"
                        }
                    }
                },
                {
                    "user_intent": "Chat with me about the weather today.",
                    "tool_call": {
                        "name": "ingen51/DialoGPT-medium-GPT4",
                        "arguments": {
                            "message": "What's the weather like today?"
                        }
                    }
                },
                {
                    "user_intent": "Can you talk about the latest trends in technology?",
                    "tool_call": {
                        "name": "ingen51/DialoGPT-medium-GPT4",
                        "arguments": {
                            "message": "What are the latest trends in technology?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 1: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 2: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 5: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 6: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 7: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 8: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 9: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 12: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 13: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 14: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 15: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 16: A tool that uses the BioBERT model to predict missing words in biomedical text, enabling tasks like entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 18: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 19: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate human-like conversational responses using a GPT-4\\u2013based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\", \"parameters\": {}, \"response\": \"The tool produces a conversational response in the context of a dialogue. The response is typically structured as an object representing the model\\u2019s reply to the provided input message. Key components of the response generally include:\\n\\n1. **Generated Text / Response**: The main content of the tool\\u2019s output, representing the model\\u2019s natural language reply to the input message. This is the primary field that contains the conversational answer.  \\n\\n2. **Conversation Context (Optional)**: Some implementations may include metadata about the ongoing conversation, such as previous messages or a history of turns, to maintain context across multiple exchanges.  \\n\\n3. **Metadata / Additional Info (Optional)**: Depending on the pipeline configuration, the response may also include auxiliary fields like message identifiers, timestamps, or flags indicating special tokens, but these are not guaranteed by the documented JSON.  \\n\\nOverall, the response is focused on providing a coherent and contextually appropriate conversational reply based on the input message, potentially maintaining dialogue history if multiple exchanges are processed.\", \"examples\": [{\"user_intent\": \"I want to have a conversation with an AI. Can you start by asking how I'm doing?\", \"tool_call\": {\"name\": \"ingen51/DialoGPT-medium-GPT4\", \"arguments\": {\"message\": \"Hello, how are you?\"}}}, {\"user_intent\": \"Chat with me about the weather today.\", \"tool_call\": {\"name\": \"ingen51/DialoGPT-medium-GPT4\", \"arguments\": {\"message\": \"What's the weather like today?\"}}}, {\"user_intent\": \"Can you talk about the latest trends in technology?\", \"tool_call\": {\"name\": \"ingen51/DialoGPT-medium-GPT4\", \"arguments\": {\"message\": \"What are the latest trends in technology?\"}}}]}\n\n# User Query\nWe have a picture of a landscape, and we'd like to add a building and a river in this picture.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a landscape image based on a textual description.",
                    "tool_description": "Create a high-quality image from a textual prompt, supporting the generation of landscapes and objects such as buildings and rivers using a text-to-image model.",
                    "expected_response": "A generated image object containing the requested landscape with the specified elements (building and river)."
                }
            ],
            "extracted_arguments": {
                "image_type": "The type or style of image to generate, e.g., landscape.",
                "elements_to_add": "A list of objects or features to include in the image, e.g., building and river."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_120",
        "query": "We received a scanned document with a lot of information. We need to go through it and find answers to specific questions. Create a program to extract information from the document.",
        "instruction": "Given a `document information extraction` task, retrieve tools that specialize in document question answering by processing scanned documents and leveraging multimodal capabilities to accurately provide answers to specific queries.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')",
                    "api_arguments": {
                        "image": "path/to/image/file"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_125",
                    "name": "tiny-random-LayoutLMv3ForQuestionAnswering"
                },
                "id": "gorilla_huggingface_tool_124",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 2: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 3: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 6: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 8: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 13: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 15: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 16: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.",
            "Tool 17: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 18: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 19: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "parameters": {
                "image": {
                    "type": "image",
                    "description": "The input document image on which the question answering model will operate. This should be a visual representation of the page(s) to analyze.",
                    "required": true,
                    "example": "Image.open('example.jpg').convert('RGB')"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question to ask about the content of the document image.",
                    "required": true,
                    "example": "Is this a question?"
                },
                "context": {
                    "type": "array",
                    "description": "An optional list of text strings providing additional context or reference information to help the model answer the question.",
                    "required": true,
                    "example": [
                        "Example text from the document."
                    ]
                },
                "boxes": {
                    "type": "array",
                    "description": "A list of bounding box coordinates corresponding to text regions in the document image, formatted as [x0, y0, x1, y1]. These boxes help the model locate relevant text areas.",
                    "required": true,
                    "example": [
                        [
                            0,
                            0,
                            1000,
                            1000
                        ]
                    ]
                }
            },
            "response": "The tool returns a structured output representing the model’s predicted answer to a question about a document image, based on the LayoutLMv3 architecture fine-tuned for multi-page document question answering. The response typically contains the following components:\n\n1. **start_logits**  \n   - A tensor of numerical scores corresponding to each token in the encoded input.  \n   - Each score represents the model’s confidence that the token is the **start** of the answer span.\n\n2. **end_logits**  \n   - A tensor of numerical scores for each token, representing the model’s confidence that the token is the **end** of the answer span.\n\n3. **answer_text** (derived from decoding the token span)  \n   - A string representing the predicted answer, obtained by taking the tokens between the indices of the highest start and end logits and decoding them using the tokenizer.  \n   - This is the human-readable answer extracted from the document content.\n\n4. **Optional auxiliary outputs** (depending on processor and model usage)  \n   - Token-level encodings or IDs of the document text.  \n   - Possibly attention scores or other intermediate features used for interpretability.\n\nIn essence, the response provides both **token-level prediction scores** (start and end logits) and the **final extracted answer** as text, allowing the user to locate and interpret the answer within the document context.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "scanned_invoice_page1.jpg",
                            "question": "What is the total amount due?",
                            "context": [
                                "Invoice from ABC Corp, dated 12/15/2025, containing itemized charges and totals."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1000,
                                    1400
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Extract the date of birth from a scanned government ID.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "passport_scan.jpg",
                            "question": "What is the date of birth?",
                            "context": [
                                "Passport ID page with name, date of birth, nationality, and passport number."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1200,
                                    800
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Find the page number and section title of a specific chapter in a PDF screenshot.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "pdf_chapter_screenshot.png",
                            "question": "What is the section title and page number of Chapter 3?",
                            "context": [
                                "PDF contains chapters, headings, and page numbers formatted in a multi-column layout."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1600,
                                    2000
                                ]
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 2: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 3: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 6: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 8: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 9: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 12: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 13: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 15: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 16: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.\nTool 17: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 18: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 19: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\", \"parameters\": {\"image\": {\"type\": \"image\", \"description\": \"The input document image on which the question answering model will operate. This should be a visual representation of the page(s) to analyze.\", \"required\": true, \"example\": \"Image.open('example.jpg').convert('RGB')\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question to ask about the content of the document image.\", \"required\": true, \"example\": \"Is this a question?\"}, \"context\": {\"type\": \"array\", \"description\": \"An optional list of text strings providing additional context or reference information to help the model answer the question.\", \"required\": true, \"example\": [\"Example text from the document.\"]}, \"boxes\": {\"type\": \"array\", \"description\": \"A list of bounding box coordinates corresponding to text regions in the document image, formatted as [x0, y0, x1, y1]. These boxes help the model locate relevant text areas.\", \"required\": true, \"example\": [[0, 0, 1000, 1000]]}}, \"response\": \"The tool returns a structured output representing the model\\u2019s predicted answer to a question about a document image, based on the LayoutLMv3 architecture fine-tuned for multi-page document question answering. The response typically contains the following components:\\n\\n1. **start_logits**  \\n   - A tensor of numerical scores corresponding to each token in the encoded input.  \\n   - Each score represents the model\\u2019s confidence that the token is the **start** of the answer span.\\n\\n2. **end_logits**  \\n   - A tensor of numerical scores for each token, representing the model\\u2019s confidence that the token is the **end** of the answer span.\\n\\n3. **answer_text** (derived from decoding the token span)  \\n   - A string representing the predicted answer, obtained by taking the tokens between the indices of the highest start and end logits and decoding them using the tokenizer.  \\n   - This is the human-readable answer extracted from the document content.\\n\\n4. **Optional auxiliary outputs** (depending on processor and model usage)  \\n   - Token-level encodings or IDs of the document text.  \\n   - Possibly attention scores or other intermediate features used for interpretability.\\n\\nIn essence, the response provides both **token-level prediction scores** (start and end logits) and the **final extracted answer** as text, allowing the user to locate and interpret the answer within the document context.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"scanned_invoice_page1.jpg\", \"question\": \"What is the total amount due?\", \"context\": [\"Invoice from ABC Corp, dated 12/15/2025, containing itemized charges and totals.\"], \"boxes\": [[0, 0, 1000, 1400]]}}}, {\"user_intent\": \"Extract the date of birth from a scanned government ID.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"passport_scan.jpg\", \"question\": \"What is the date of birth?\", \"context\": [\"Passport ID page with name, date of birth, nationality, and passport number.\"], \"boxes\": [[0, 0, 1200, 800]]}}}, {\"user_intent\": \"Find the page number and section title of a specific chapter in a PDF screenshot.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"pdf_chapter_screenshot.png\", \"question\": \"What is the section title and page number of Chapter 3?\", \"context\": [\"PDF contains chapters, headings, and page numbers formatted in a multi-column layout.\"], \"boxes\": [[0, 0, 1600, 2000]]}}}]}\n\n# User Query\nWe received a scanned document with a lot of information. We need to go through it and find answers to specific questions. Create a program to extract information from the document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract answers to specific questions from a scanned document.",
                    "tool_description": "Use a multimodal document question-answering model to process document images along with text, layout information, and bounding boxes, extracting precise answers to user-provided questions.",
                    "expected_response": "A structured output containing predicted answers, including token-level start and end logits and the final extracted answer text for each question."
                }
            ],
            "extracted_arguments": {
                "image": "The scanned document image to analyze, representing the page(s) containing information.",
                "question": "The natural language question to ask about the content of the document.",
                "context": "Optional textual context from the document to assist in answering the question.",
                "boxes": "Bounding box coordinates corresponding to text regions in the document image, formatted as [x0, y0, x1, y1]."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_347",
        "query": "I am a movie director and I need to detect the genre of a movie based on its actions.",
        "instruction": "Given a `movie genre detection` task, retrieve tools that utilize video action recognition capabilities by processing video inputs to classify the genre based on observed actions and sequences within the footage.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Action Recognition",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "decord",
                        "huggingface_hub"
                    ],
                    "example_code": "from decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n converted_len = int(clip_len * frame_sample_rate)\n end_idx = np.random.randint(converted_len, seg_len)\n start_idx = end_idx - converted_len\n indices = np.linspace(start_idx, end_idx, num=clip_len)\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n return indices\nfile_path = hf_hub_download(\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\ninputs = feature_extractor(list(video), return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "UCF101",
                        "accuracy": 0.758209764957428
                    },
                    "description": "VideoMAE Base model fine tuned on UCF101 for Video Action Recognition",
                    "id": "huggingface_api_342",
                    "name": "videomae-base-finetuned-ucf101"
                },
                "id": "gorilla_huggingface_tool_338",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 2: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 3: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 4: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.",
            "Tool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 6: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 7: This tool utilizes the uncased DistilBERT model, fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, for zero-shot text classification tasks. It allows for classifying text without requiring task-specific training, making it useful for a wide range of natural language processing applications where labeled data is scarce.",
            "Tool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 9: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 10: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 11: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 12: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 13: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 15: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 16: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 17: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 18: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.",
            "Tool 19: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "parameters": {},
            "response": "The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \n\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \n- **score**: A numeric value representing the model’s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \n\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers’ sentiment-analysis pipeline.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "The plot was captivating and the acting was superb."
                        }
                    }
                },
                {
                    "user_intent": "I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "I couldn't stand the boring dialogue and slow pacing."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a short review: 'It was an average movie, nothing special.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "It was an average movie, nothing special."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 2: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 3: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 4: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.\nTool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 6: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 7: This tool utilizes the uncased DistilBERT model, fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, for zero-shot text classification tasks. It allows for classifying text without requiring task-specific training, making it useful for a wide range of natural language processing applications where labeled data is scarce.\nTool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 9: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 10: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 11: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 12: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 13: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 14: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 15: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 16: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 17: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 18: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.\nTool 19: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\", \"parameters\": {}, \"response\": \"The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \\n\\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \\n- **score**: A numeric value representing the model\\u2019s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \\n\\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers\\u2019 sentiment-analysis pipeline.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"The plot was captivating and the acting was superb.\"}}}, {\"user_intent\": \"I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"I couldn't stand the boring dialogue and slow pacing.\"}}}, {\"user_intent\": \"Check the sentiment of a short review: 'It was an average movie, nothing special.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"It was an average movie, nothing special.\"}}}]}\n\n# User Query\nI am a movie director and I need to detect the genre of a movie based on its actions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify a movie's genre based on its action sequences.",
                    "tool_description": "Analyze text or video content representing the movie's action sequences to predict the movie's genre using a classification model trained on labeled movie genres.",
                    "expected_response": "A predicted genre label for the movie along with a confidence score."
                }
            ],
            "extracted_arguments": {
                "movie_actions": "Textual description or video representation of the movie's action scenes used to determine the genre."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_411",
        "query": "We have an international audience for our website and require our website content to be translated into multiple languages for better understanding.",
        "instruction": "Given a `website translation` task, retrieve tools that perform multilingual translation by processing text inputs to produce translations in multiple languages for enhanced understanding across an international audience.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "PyTorch Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')",
                    "api_arguments": [
                        "model",
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')",
                    "performance": {
                        "dataset": "Flores-200",
                        "accuracy": "BLEU, spBLEU, chrF++"
                    },
                    "description": "NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.",
                    "id": "huggingface_api_531",
                    "name": "facebook/nllb-200-distilled-600M"
                },
                "id": "gorilla_huggingface_tool_527",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 1: This tool uses the DistilBERT multilingual model to perform masked language modeling, predicting missing words in text across multiple languages. It is optimized for efficiency, offering faster inference than the full multilingual BERT model, and supports text inputs in over 100 languages, making it useful for tasks like language understanding, text completion, and multilingual NLP applications.",
            "Tool 2: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.",
            "Tool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 4: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.",
            "Tool 5: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.",
            "Tool 6: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 7: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.",
            "Tool 8: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 9: bigscience/bloom-7b1 is a large, multilingual transformer-based language model designed for generating text. It supports 48 natural languages and multiple programming languages, making it suitable for producing coherent and contextually relevant text or serving as a pretrained base for fine-tuning on specific NLP tasks.",
            "Tool 10: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.",
            "Tool 11: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 12: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 13: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.",
            "Tool 14: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.",
            "Tool 15: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 16: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.",
            "Tool 17: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 19: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets."
        ],
        "tool_view": {
            "description": "A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "parameters": {
                "audio_file": {
                    "type": "string",
                    "description": "The path to the audio file to be transcribed by the Whisper large-v2 model. This file should contain speech in a supported language.",
                    "required": true,
                    "example": "audio.mp3"
                }
            },
            "response": "The tool produces a structured transcription of an audio input, typically consisting of two main components:\n\n1. **Segments**: A collection of sequential portions of the audio, where each segment represents a contiguous spoken portion detected by the model. Each segment usually includes:\n   - **start**: The timestamp (in seconds) indicating the beginning of the segment within the audio.\n   - **end**: The timestamp (in seconds) indicating the end of the segment within the audio.\n   - **text**: The transcribed text corresponding to the spoken content within that segment.\n\n2. **Info**: Metadata about the transcription process. This may include details such as the total duration of the audio, language detected, average confidence scores, or other diagnostic information provided by the model.  \n\nIn essence, the response provides a time-aligned transcription of the input audio, allowing users to retrieve both the text content and the exact time intervals where each portion of speech occurs.",
            "examples": [
                {
                    "user_intent": "Transcribe my recorded lecture from yesterday into text.",
                    "tool_call": {
                        "name": "guillaumekln/faster-whisper-large-v2",
                        "arguments": {
                            "audio.mp3": "lecture_recording.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Convert this podcast episode audio file into a readable transcript.",
                    "tool_call": {
                        "name": "guillaumekln/faster-whisper-large-v2",
                        "arguments": {
                            "audio.mp3": "podcast_episode_12.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I have a short interview in Spanish that I want transcribed to text.",
                    "tool_call": {
                        "name": "guillaumekln/faster-whisper-large-v2",
                        "arguments": {
                            "audio.mp3": "spanish_interview.mp3"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 1: This tool uses the DistilBERT multilingual model to perform masked language modeling, predicting missing words in text across multiple languages. It is optimized for efficiency, offering faster inference than the full multilingual BERT model, and supports text inputs in over 100 languages, making it useful for tasks like language understanding, text completion, and multilingual NLP applications.\nTool 2: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.\nTool 3: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 4: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.\nTool 5: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.\nTool 6: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 7: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.\nTool 8: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 9: bigscience/bloom-7b1 is a large, multilingual transformer-based language model designed for generating text. It supports 48 natural languages and multiple programming languages, making it suitable for producing coherent and contextually relevant text or serving as a pretrained base for fine-tuning on specific NLP tasks.\nTool 10: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.\nTool 11: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 12: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 13: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.\nTool 14: A multilingual zero-shot text classification tool that uses the XLM-RoBERTa large model fine-tuned on natural language inference data across 15 languages. It allows users to classify text into custom categories without prior task-specific training by providing candidate labels and an optional hypothesis template, making it useful for quickly categorizing text in multiple languages.\nTool 15: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 16: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.\nTool 17: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 19: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\", \"parameters\": {\"audio_file\": {\"type\": \"string\", \"description\": \"The path to the audio file to be transcribed by the Whisper large-v2 model. This file should contain speech in a supported language.\", \"required\": true, \"example\": \"audio.mp3\"}}, \"response\": \"The tool produces a structured transcription of an audio input, typically consisting of two main components:\\n\\n1. **Segments**: A collection of sequential portions of the audio, where each segment represents a contiguous spoken portion detected by the model. Each segment usually includes:\\n   - **start**: The timestamp (in seconds) indicating the beginning of the segment within the audio.\\n   - **end**: The timestamp (in seconds) indicating the end of the segment within the audio.\\n   - **text**: The transcribed text corresponding to the spoken content within that segment.\\n\\n2. **Info**: Metadata about the transcription process. This may include details such as the total duration of the audio, language detected, average confidence scores, or other diagnostic information provided by the model.  \\n\\nIn essence, the response provides a time-aligned transcription of the input audio, allowing users to retrieve both the text content and the exact time intervals where each portion of speech occurs.\", \"examples\": [{\"user_intent\": \"Transcribe my recorded lecture from yesterday into text.\", \"tool_call\": {\"name\": \"guillaumekln/faster-whisper-large-v2\", \"arguments\": {\"audio.mp3\": \"lecture_recording.mp3\"}}}, {\"user_intent\": \"Convert this podcast episode audio file into a readable transcript.\", \"tool_call\": {\"name\": \"guillaumekln/faster-whisper-large-v2\", \"arguments\": {\"audio.mp3\": \"podcast_episode_12.mp3\"}}}, {\"user_intent\": \"I have a short interview in Spanish that I want transcribed to text.\", \"tool_call\": {\"name\": \"guillaumekln/faster-whisper-large-v2\", \"arguments\": {\"audio.mp3\": \"spanish_interview.mp3\"}}}]}\n\n# User Query\nWe have an international audience for our website and require our website content to be translated into multiple languages for better understanding.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate website content into multiple languages to reach an international audience.",
                    "tool_description": "Use a multilingual sequence-to-sequence model to translate text from the website into the target languages specified by the user.",
                    "expected_response": "A collection of translated text in the desired languages, with each translated version corresponding to the original content."
                }
            ],
            "extracted_arguments": {
                "source_text": "The content of the website to be translated.",
                "target_languages": "A list of target languages to which the website content should be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_95",
        "query": "The company wants to create a social media application like Pinterest that generates captions for users' images. Show us how to create this functionality.",
        "instruction": "Given an `image captioning` task, retrieve tools that enable the generation of textual captions by processing user-provided images and, optionally, accompanying text. These tools should effectively leverage pre-trained models to produce descriptive and coherent captions aligned with the application's functionality and user requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')",
                    "api_arguments": "image, optional text",
                    "python_environment_requirements": "transformers",
                    "example_code": "Refer to the documentation",
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_74",
                    "name": "blip2-opt-6.7b"
                },
                "id": "gorilla_huggingface_tool_74",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 1: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 3: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 4: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 6: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 8: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 9: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 10: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 12: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 14: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 15: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 16: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 17: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 18: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 19: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums."
        ],
        "tool_view": {
            "description": "Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that specifies the content, style, and details of the image to be generated by the model.",
                    "required": true,
                    "example": "A futuristic cityscape at sunset with flying cars, in a cyberpunk art style"
                }
            },
            "response": "The tool is designed to generate images from text prompts using the \"StableDiffusionPipeline\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \n\nThe expected response format includes the following components:\n\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\n\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\n\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\n\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.",
            "examples": [
                {
                    "user_intent": "Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "futuristic city skyline at sunset, retro 80s art style"
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a medieval knight in full armor, riding a horse.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "medieval knight in full armor, riding a horse"
                        }
                    }
                },
                {
                    "user_intent": "I want a retro-style image of different cars with various colors and shapes.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "retro series of different cars with different colors and shapes, mdjrny-v4 style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 1: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 2: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 3: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 4: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 6: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 8: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 9: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 10: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 12: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 14: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 15: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 16: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 17: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 18: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 19: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that specifies the content, style, and details of the image to be generated by the model.\", \"required\": true, \"example\": \"A futuristic cityscape at sunset with flying cars, in a cyberpunk art style\"}}, \"response\": \"The tool is designed to generate images from text prompts using the \\\"StableDiffusionPipeline\\\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \\n\\nThe expected response format includes the following components:\\n\\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\\n\\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\\n\\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\\n\\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.\", \"examples\": [{\"user_intent\": \"Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"futuristic city skyline at sunset, retro 80s art style\"}}}, {\"user_intent\": \"Create an illustration of a medieval knight in full armor, riding a horse.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"medieval knight in full armor, riding a horse\"}}}, {\"user_intent\": \"I want a retro-style image of different cars with various colors and shapes.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"retro series of different cars with different colors and shapes, mdjrny-v4 style\"}}}]}\n\n# User Query\nThe company wants to create a social media application like Pinterest that generates captions for users' images. Show us how to create this functionality.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate descriptive captions for user-uploaded images to display on the social media platform.",
                    "tool_description": "Use an image-to-text model to analyze uploaded images and generate natural language captions that describe the visual content, suitable for social media sharing.",
                    "expected_response": "A text string or list of strings providing descriptive captions for each uploaded image."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file or image data uploaded by the user for which a caption needs to be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_41",
        "query": "Our team wants to create a new app for autonomous vehicles. For that, we need to estimate the depth of the field from images.",
        "instruction": "Given a `depth estimation` task, retrieve tools that facilitate the analysis of images to estimate the depth of the field, specifically for use cases like autonomous vehicle applications, leveraging computer vision models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3533,
                            "Mae": 0.2668,
                            "Rmse": 0.3716,
                            "Abs Rel": 0.3427,
                            "Log Mae": 0.1167,
                            "Log Rmse": 0.1703,
                            "Delta1": 0.5522,
                            "Delta2": 0.8362,
                            "Delta3": 0.9382
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.",
                    "id": "huggingface_api_163",
                    "name": "glpn-nyu-finetuned-diode-221121-063504"
                },
                "id": "gorilla_huggingface_tool_159",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 3: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 4: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 5: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 6: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 7: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 8: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 9: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 12: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 15: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 16: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 17: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.",
            "Tool 18: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 19: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs."
        ],
        "tool_view": {
            "description": "A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "parameters": {},
            "response": "The tool is a depth estimation model implemented using the Hugging Face Transformers framework. Its response, when invoked on an input image, would typically consist of a dense depth map corresponding to the scene depicted in the image. The expected structure and components of the response are as follows:\n\n- **Depth Map Tensor/Array**: The core output is a multi-dimensional numerical array (tensor) representing per-pixel depth values. Each element corresponds to the estimated distance of the scene point from the camera.  \n- **Shape/Dimensions**: The depth map usually has the same spatial dimensions as the input image or a fixed resolution determined by the model. The structure is generally `[height, width]` for a single-channel depth map, or `[batch_size, height, width]` if multiple images are processed at once.  \n- **Data Type**: The array contains continuous numeric values, typically floating-point numbers, representing relative or absolute depth measurements.  \n- **Optional Metadata**: Depending on the model implementation, the response may include auxiliary information such as preprocessing parameters, scaling factors, or confidence scores, though this is not guaranteed.  \n\nOverall, the response provides a pixel-wise representation of scene depth that can be used for tasks such as 3D reconstruction, augmented reality, or robotics perception.",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth map of an indoor scene using a pre-trained model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_path": "images/living_room.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Generate a depth map for an outdoor landscape photo with a pre-trained DIODE model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_path": "images/mountain_landscape.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to process multiple images and get their depth estimations using the Hugging Face GLPN model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_paths": [
                                "images/office.jpg",
                                "images/kitchen.png",
                                "images/park.jpg"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 3: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 4: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 5: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 6: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 7: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 8: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 9: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 12: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 15: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 16: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 17: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\nTool 18: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 19: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\", \"parameters\": {}, \"response\": \"The tool is a depth estimation model implemented using the Hugging Face Transformers framework. Its response, when invoked on an input image, would typically consist of a dense depth map corresponding to the scene depicted in the image. The expected structure and components of the response are as follows:\\n\\n- **Depth Map Tensor/Array**: The core output is a multi-dimensional numerical array (tensor) representing per-pixel depth values. Each element corresponds to the estimated distance of the scene point from the camera.  \\n- **Shape/Dimensions**: The depth map usually has the same spatial dimensions as the input image or a fixed resolution determined by the model. The structure is generally `[height, width]` for a single-channel depth map, or `[batch_size, height, width]` if multiple images are processed at once.  \\n- **Data Type**: The array contains continuous numeric values, typically floating-point numbers, representing relative or absolute depth measurements.  \\n- **Optional Metadata**: Depending on the model implementation, the response may include auxiliary information such as preprocessing parameters, scaling factors, or confidence scores, though this is not guaranteed.  \\n\\nOverall, the response provides a pixel-wise representation of scene depth that can be used for tasks such as 3D reconstruction, augmented reality, or robotics perception.\", \"examples\": [{\"user_intent\": \"I want to estimate the depth map of an indoor scene using a pre-trained model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_path\": \"images/living_room.jpg\"}}}, {\"user_intent\": \"Generate a depth map for an outdoor landscape photo with a pre-trained DIODE model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_path\": \"images/mountain_landscape.png\"}}}, {\"user_intent\": \"I want to process multiple images and get their depth estimations using the Hugging Face GLPN model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_paths\": [\"images/office.jpg\", \"images/kitchen.png\", \"images/park.jpg\"]}}}]}\n\n# User Query\nOur team wants to create a new app for autonomous vehicles. For that, we need to estimate the depth of the field from images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth map of images for autonomous vehicle applications.",
                    "tool_description": "Use a pre-trained depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, to predict per-pixel depth from input images.",
                    "expected_response": "A dense depth map for each input image, represented as a multi-dimensional array of floating-point values corresponding to per-pixel distances from the camera, suitable for tasks like 3D scene understanding and autonomous navigation."
                }
            ],
            "extracted_arguments": {
                "input_images": "One or more images captured from the vehicle's camera system, used as input for depth estimation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_498",
        "query": "Develop an emotion analysis system to understand customer satisfaction over the phone for a telecommunication company in Russia.",
        "instruction": "Given an `emotion analysis` task, retrieve tools that can analyze audio inputs to identify and classify emotions, focusing on customer satisfaction in a specific linguistic and cultural context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53')",
                    "api_arguments": {
                        "path": "/path/to/russian_audio_speech.wav",
                        "sampling_rate": 16000
                    },
                    "python_environment_requirements": [
                        "torch",
                        "torchaudio",
                        "transformers",
                        "librosa",
                        "numpy"
                    ],
                    "example_code": "result = predict('/path/to/russian_audio_speech.wav', 16000)\nprint(result)",
                    "performance": {
                        "dataset": "Russian Emotional Speech Dialogs",
                        "accuracy": "72%"
                    },
                    "description": "A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.",
                    "id": "huggingface_api_826",
                    "name": "wav2vec2-xlsr-53-russian-emotion-recognition"
                },
                "id": "gorilla_huggingface_tool_821",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.",
            "Tool 2: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 3: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 5: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 6: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 7: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 9: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 10: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 11: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.",
            "Tool 12: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 14: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 15: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 16: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.",
            "Tool 18: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.",
            "Tool 19: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: This tool uses the `superb/hubert-base-superb-er` model to classify emotions in speech audio. Built on the Hubert-Base architecture and trained on the IEMOCAP dataset, it predicts emotion labels for individual utterances, enabling applications in speech analysis, affective computing, and audio-based emotion recognition.\nTool 2: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 3: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 4: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 5: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 6: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 7: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 9: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 10: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 11: This tool performs emotion recognition on audio data using the wav2vec2 model fine-tuned for the SUPERB Emotion Recognition task. It requires a 16kHz sampled speech input and provides classification results based on the IEMOCAP dataset. The model is implemented using PyTorch and Transformers, and it can be accessed via the `pipeline('audio-classification', model='superb/wav2vec2-base-superb-er')` API call.\nTool 12: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 13: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 14: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 15: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 16: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.\nTool 18: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.\nTool 19: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nDevelop an emotion analysis system to understand customer satisfaction over the phone for a telecommunication company in Russia.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the emotional content of customer speech audio to assess satisfaction levels.",
                    "tool_description": "Perform speech emotion recognition on audio recordings to classify emotions such as anger, happiness, sadness, fear, disgust, and neutral states using a pretrained speech emotion recognition model.",
                    "expected_response": "A structured output containing predicted emotion labels and associated confidence scores for each audio input."
                },
                {
                    "user_intent": "Aggregate and interpret emotion data to evaluate overall customer satisfaction for the telecommunication company.",
                    "tool_description": "Summarize emotion predictions across multiple audio interactions to provide insights into customer satisfaction trends and patterns.",
                    "expected_response": "An analysis report or dataset showing aggregated emotion statistics, trends, and indicators of customer satisfaction."
                }
            ],
            "extracted_arguments": {
                "audio_input": "Audio recordings of customer interactions over the phone, sampled at an appropriate frequency for emotion recognition.",
                "language": "The language spoken in the audio recordings, in this case Russian, to ensure correct emotion interpretation.",
                "customer_context": "Metadata about the customer interactions, such as call duration or agent ID, to contextualize emotion analysis."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_273",
        "query": "We have an AI-powered document management system that needs to answer questions based on the content of a given document.",
        "instruction": "Given a `document-based question answering` task, retrieve tools that facilitate AI-powered systems in extracting answers from documents by processing text inputs and leveraging machine learning models to interpret and correctly respond to queries based on document content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": "4.12.2",
                        "pytorch": "1.8.0+cu101",
                        "datasets": "1.14.0",
                        "tokenizers": "0.10.3"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 2.087
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_131",
                    "name": "layoutlmv2-base-uncased-finetuned-infovqa"
                },
                "id": "gorilla_huggingface_tool_130",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 1: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 3: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 4: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 8: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 9: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 12: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 13: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 15: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.",
            "Tool 16: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 18: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents."
        ],
        "tool_view": {
            "description": "This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path to the pre-trained model to be used for document question answering. This model should be fine-tuned for the task of answering questions based on document input.",
                    "required": true,
                    "example": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path to the tokenizer associated with the pre-trained model. It is used to convert text into tokenized input for the model.",
                    "required": true,
                    "example": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa"
                }
            },
            "response": "The tool is designed for document question answering using a fine-tuned version of the LayoutLMv2 model on the DocVQA dataset. The expected response from the tool would typically contain:\n\n1. **Answer**: A string representing the answer to the question provided, extracted from the input document. This would be the model’s best interpretation of the query based on the context of the document it has been applied to.\n\n2. **Context**: Optionally, the response might include the specific part of the document or text that was most relevant in helping the model derive the answer. This could be a portion or span of the original document that helped to identify the correct response.\n\n3. **Score (Confidence)**: An optional numeric value indicating the model's confidence in the accuracy of its answer. This score typically ranges between 0 and 1, with higher values indicating greater confidence.\n\n4. **Question**: The original question provided to the model, which can be echoed in the response for clarity or context.\n\nGiven that the tool is based on a Hugging Face pipeline, the response would likely be in a structured format that can easily be interpreted by downstream applications. The model's output may vary depending on the specific implementation and any additional configurations set by the user or the underlying API.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "What is the total amount due?",
                            "context": "Image or PDF of the invoice document"
                        }
                    }
                },
                {
                    "user_intent": "Check the date of issue on this official letter.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "What is the date of issue?",
                            "context": "Scanned letter document"
                        }
                    }
                },
                {
                    "user_intent": "I want to extract the recipient's name from this filled-out form.",
                    "tool_call": {
                        "name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa",
                        "arguments": {
                            "question": "Who is the recipient?",
                            "context": "Image or PDF of the completed form"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 1: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 3: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 4: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 5: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 8: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 9: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 12: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 13: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 14: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 15: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.\nTool 16: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 18: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path to the pre-trained model to be used for document question answering. This model should be fine-tuned for the task of answering questions based on document input.\", \"required\": true, \"example\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path to the tokenizer associated with the pre-trained model. It is used to convert text into tokenized input for the model.\", \"required\": true, \"example\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\"}}, \"response\": \"The tool is designed for document question answering using a fine-tuned version of the LayoutLMv2 model on the DocVQA dataset. The expected response from the tool would typically contain:\\n\\n1. **Answer**: A string representing the answer to the question provided, extracted from the input document. This would be the model\\u2019s best interpretation of the query based on the context of the document it has been applied to.\\n\\n2. **Context**: Optionally, the response might include the specific part of the document or text that was most relevant in helping the model derive the answer. This could be a portion or span of the original document that helped to identify the correct response.\\n\\n3. **Score (Confidence)**: An optional numeric value indicating the model's confidence in the accuracy of its answer. This score typically ranges between 0 and 1, with higher values indicating greater confidence.\\n\\n4. **Question**: The original question provided to the model, which can be echoed in the response for clarity or context.\\n\\nGiven that the tool is based on a Hugging Face pipeline, the response would likely be in a structured format that can easily be interpreted by downstream applications. The model's output may vary depending on the specific implementation and any additional configurations set by the user or the underlying API.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"What is the total amount due?\", \"context\": \"Image or PDF of the invoice document\"}}}, {\"user_intent\": \"Check the date of issue on this official letter.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"What is the date of issue?\", \"context\": \"Scanned letter document\"}}}, {\"user_intent\": \"I want to extract the recipient's name from this filled-out form.\", \"tool_call\": {\"name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"arguments\": {\"question\": \"Who is the recipient?\", \"context\": \"Image or PDF of the completed form\"}}}]}\n\n# User Query\nWe have an AI-powered document management system that needs to answer questions based on the content of a given document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract information from a document based on specific questions.",
                    "tool_description": "Use a fine-tuned document question-answering model to process the content of a document and answer user-provided questions.",
                    "expected_response": "A structured response containing the extracted answer(s), relevant context from the document, and a confidence score."
                }
            ],
            "extracted_arguments": {
                "question": "The specific query the user wants answered from the document.",
                "context": "The document content (image or text) that the model will process to find the answer."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_430",
        "query": "Develop a system that can detect voices in a podcast to find out if guests are speaking or not.",
        "instruction": "Given a `voice detection and segmentation` task, retrieve tools that specialize in analyzing audio inputs to identify and segment different speakers and voice activities to determine when guests are speaking in a podcast.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker segmentation, Voice activity detection, Overlapped speech detection, Resegmentation, Raw scores",
                    "api_call": "Model.from_pretrained('pyannote/segmentation')",
                    "api_arguments": {
                        "use_auth_token": "ACCESS_TOKEN_GOES_HERE"
                    },
                    "python_environment_requirements": "pyannote.audio 2.1.1",
                    "example_code": {
                        "voice_activity_detection": "from pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n onset: 0.5, offset: 0.5,\n min_duration_on: 0.0,\n min_duration_off: 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(audio.wav)",
                        "overlapped_speech_detection": "from pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(audio.wav)",
                        "resegmentation": "from pyannote.audio.pipelines import Resegmentation\npipeline = Resegmentation(segmentation=model, diarization=baseline)\npipeline.instantiate(HYPER_PARAMETERS)\nresegmented_baseline = pipeline({audio: audio.wav, baseline: baseline})"
                    },
                    "performance": {
                        "dataset": {
                            "AMI Mix-Headset": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.684,
                                    "offset": 0.577,
                                    "min_duration_on": 0.181,
                                    "min_duration_off": 0.037
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.448,
                                    "offset": 0.362,
                                    "min_duration_on": 0.116,
                                    "min_duration_off": 0.187
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.542,
                                    "offset": 0.527,
                                    "min_duration_on": 0.044,
                                    "min_duration_off": 0.705
                                }
                            },
                            "DIHARD3": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.767,
                                    "offset": 0.377,
                                    "min_duration_on": 0.136,
                                    "min_duration_off": 0.067
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.43,
                                    "offset": 0.32,
                                    "min_duration_on": 0.091,
                                    "min_duration_off": 0.144
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.592,
                                    "offset": 0.489,
                                    "min_duration_on": 0.163,
                                    "min_duration_off": 0.182
                                }
                            },
                            "VoxConverse": {
                                "voice_activity_detection_accuracy": {
                                    "onset": 0.767,
                                    "offset": 0.713,
                                    "min_duration_on": 0.182,
                                    "min_duration_off": 0.501
                                },
                                "overlapped_speech_detection_accuracy": {
                                    "onset": 0.587,
                                    "offset": 0.426,
                                    "min_duration_on": 0.337,
                                    "min_duration_off": 0.112
                                },
                                "resegmentation_accuracy": {
                                    "onset": 0.537,
                                    "offset": 0.724,
                                    "min_duration_on": 0.41,
                                    "min_duration_off": 0.563
                                }
                            }
                        }
                    },
                    "description": "A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework.",
                    "id": "huggingface_api_844",
                    "name": "pyannote/segmentation"
                },
                "id": "gorilla_huggingface_tool_832",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 1: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 2: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 3: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 5: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 6: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 7: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 8: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 9: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 10: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 11: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 14: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.",
            "Tool 15: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.",
            "Tool 16: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 17: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream."
        ],
        "tool_view": {
            "description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "parameters": {
                "audio.wav": {
                    "type": "string",
                    "description": "Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\n\n1. **Timeline object**:  \n   - Represents the sequence of time intervals in the audio.  \n   - Each interval corresponds to a period where overlapped speech occurs.\n\n2. **Speech segments** (within the timeline):  \n   - Each segment has at least two key attributes:  \n     - **start**: The timestamp indicating when the overlapped speech begins.  \n     - **end**: The timestamp indicating when the overlapped speech ends.  \n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\n\n3. **Support or confidence information** (optional, depending on the output object):  \n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\n\n4. **Methods for iteration and access**:  \n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\n\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.",
            "examples": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recording.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "conference_call.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a meeting recording to identify parts with overlapping speech.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "team_meeting.wav"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any overlapping speech segments in this podcast episode.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_3.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 1: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 2: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 3: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 5: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 6: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 7: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 8: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 9: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 10: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 11: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 12: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 14: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.\nTool 15: This tool is a multi-class text classification model designed to detect gibberish text. It is built using AutoNLP and DistilBERT, and offers high performance, with an accuracy of 97.36% on the \"madhurjindal/autonlp-data-Gibberish-Detector\" dataset. The model can be accessed through the Hugging Face Transformers framework and is useful for identifying nonsensical or irrelevant text in natural language processing tasks.\nTool 16: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 17: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\", \"parameters\": {\"audio.wav\": {\"type\": \"string\", \"description\": \"Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\\n\\n1. **Timeline object**:  \\n   - Represents the sequence of time intervals in the audio.  \\n   - Each interval corresponds to a period where overlapped speech occurs.\\n\\n2. **Speech segments** (within the timeline):  \\n   - Each segment has at least two key attributes:  \\n     - **start**: The timestamp indicating when the overlapped speech begins.  \\n     - **end**: The timestamp indicating when the overlapped speech ends.  \\n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\\n\\n3. **Support or confidence information** (optional, depending on the output object):  \\n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\\n\\n4. **Methods for iteration and access**:  \\n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\\n\\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.\", \"examples\": [{\"user_intent\": \"Detect when multiple speakers are talking simultaneously in a recording.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"conference_call.wav\"}}}, {\"user_intent\": \"I need to analyze a meeting recording to identify parts with overlapping speech.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"team_meeting.wav\"}}}, {\"user_intent\": \"Check if there are any overlapping speech segments in this podcast episode.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_3.wav\"}}}]}\n\n# User Query\nDevelop a system that can detect voices in a podcast to find out if guests are speaking or not.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect the segments in a podcast where voices are present.",
                    "tool_description": "Use a Voice Activity Detection (VAD) model to identify periods of active speech in an audio recording.",
                    "expected_response": "A timeline of speech segments indicating start and end times where voices are detected."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the podcast audio file in a standard audio format (e.g., .wav, .mp3)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_257",
        "query": "As a translation company, we are translating messages between co-workers in a multinational company. Translate the message from Hindi to French.",
        "instruction": "Given a `translation task`, retrieve tools that facilitate multilingual machine translation by processing text inputs in one language and accurately converting them to another specified language based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Text2Text Generation",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mbart-large-50-many-to-many-mmt"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.0"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi model = MBartForConditionalGeneration.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer = MBart50TokenizerFast.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer.src_lang = hi_IN\nencoded_hi = tokenizer(article_hi, return_tensors=pt)\ngenerated_tokens = model.generate(\n **encoded_hi,\n forced_bos_token_id=tokenizer.lang_code_to_id[fr_XX]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "Multilingual Translation",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_661",
                    "name": "facebook/mbart-large-50-many-to-many-mmt"
                },
                "id": "gorilla_huggingface_tool_656",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 3: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 4: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 5: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 6: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.",
            "Tool 7: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 8: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 9: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 10: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 11: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 12: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 13: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.",
            "Tool 14: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 15: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 16: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.",
            "Tool 17: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 18: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.",
            "Tool 19: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba."
        ],
        "tool_view": {
            "description": "A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "parameters": {
                "source_languages": {
                    "type": "string",
                    "description": "The language code(s) of the input text that will be translated. This specifies the source language for the translation model.",
                    "required": true,
                    "example": "fr"
                },
                "target_languages": {
                    "type": "string",
                    "description": "The language code(s) into which the input text will be translated. This specifies the target language for the translation model.",
                    "required": true,
                    "example": "es"
                }
            },
            "response": "The tool produces a textual translation of input text from French (`fr`) to Spanish (`es`). The response is a string containing the translated sentence or passage, preserving the semantic meaning of the input while rendering it in the target language.  \n\nKey aspects of the response:  \n\n- **Type:** String  \n- **Content:** The translated text in Spanish. This is the primary output and the only required component.  \n- **Length/Structure:** Can vary depending on the input; for single sentences, it will be a single string; for multiple sentences or longer passages, the response may include the full translated text concatenated in sequence.  \n- **Preservation of Meaning:** The translation aims to retain the meaning, syntax, and style of the original French input.  \n- **Optional Metadata (not explicitly defined but typical in transformer-based outputs):** In some frameworks, additional fields such as token IDs, attention weights, or confidence scores may be available internally, but in the standard usage (as in the `translation()` call shown), the returned response is generally just the translated string.  \n\nIn summary, the response is a single string representing the French input text translated accurately into Spanish.",
            "examples": [
                {
                    "user_intent": "Translate a simple French greeting into Spanish.",
                    "tool_call": {
                        "name": "opus-mt-fr-es",
                        "arguments": {
                            "text": "Bonjour, comment ça va?"
                        }
                    }
                },
                {
                    "user_intent": "I have a French paragraph from a news article that I need in Spanish.",
                    "tool_call": {
                        "name": "opus-mt-fr-es",
                        "arguments": {
                            "text": "Le gouvernement français a annoncé de nouvelles mesures pour soutenir l'économie locale face à la crise."
                        }
                    }
                },
                {
                    "user_intent": "Translate a set of French sentences about travel tips into Spanish.",
                    "tool_call": {
                        "name": "opus-mt-fr-es",
                        "arguments": {
                            "text": "Pour visiter Paris, il est conseillé de prendre le métro et de visiter les musées tôt le matin pour éviter la foule."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 3: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 4: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 5: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 6: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.\nTool 7: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 8: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 9: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 10: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 11: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 12: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 13: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.\nTool 14: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 15: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 16: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.\nTool 17: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 18: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.\nTool 19: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\", \"parameters\": {\"source_languages\": {\"type\": \"string\", \"description\": \"The language code(s) of the input text that will be translated. This specifies the source language for the translation model.\", \"required\": true, \"example\": \"fr\"}, \"target_languages\": {\"type\": \"string\", \"description\": \"The language code(s) into which the input text will be translated. This specifies the target language for the translation model.\", \"required\": true, \"example\": \"es\"}}, \"response\": \"The tool produces a textual translation of input text from French (`fr`) to Spanish (`es`). The response is a string containing the translated sentence or passage, preserving the semantic meaning of the input while rendering it in the target language.  \\n\\nKey aspects of the response:  \\n\\n- **Type:** String  \\n- **Content:** The translated text in Spanish. This is the primary output and the only required component.  \\n- **Length/Structure:** Can vary depending on the input; for single sentences, it will be a single string; for multiple sentences or longer passages, the response may include the full translated text concatenated in sequence.  \\n- **Preservation of Meaning:** The translation aims to retain the meaning, syntax, and style of the original French input.  \\n- **Optional Metadata (not explicitly defined but typical in transformer-based outputs):** In some frameworks, additional fields such as token IDs, attention weights, or confidence scores may be available internally, but in the standard usage (as in the `translation()` call shown), the returned response is generally just the translated string.  \\n\\nIn summary, the response is a single string representing the French input text translated accurately into Spanish.\", \"examples\": [{\"user_intent\": \"Translate a simple French greeting into Spanish.\", \"tool_call\": {\"name\": \"opus-mt-fr-es\", \"arguments\": {\"text\": \"Bonjour, comment \\u00e7a va?\"}}}, {\"user_intent\": \"I have a French paragraph from a news article that I need in Spanish.\", \"tool_call\": {\"name\": \"opus-mt-fr-es\", \"arguments\": {\"text\": \"Le gouvernement fran\\u00e7ais a annonc\\u00e9 de nouvelles mesures pour soutenir l'\\u00e9conomie locale face \\u00e0 la crise.\"}}}, {\"user_intent\": \"Translate a set of French sentences about travel tips into Spanish.\", \"tool_call\": {\"name\": \"opus-mt-fr-es\", \"arguments\": {\"text\": \"Pour visiter Paris, il est conseill\\u00e9 de prendre le m\\u00e9tro et de visiter les mus\\u00e9es t\\u00f4t le matin pour \\u00e9viter la foule.\"}}}]}\n\n# User Query\nAs a translation company, we are translating messages between co-workers in a multinational company. Translate the message from Hindi to French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate a message from Hindi to French for internal company communication.",
                    "tool_description": "Use a multilingual machine translation model to convert text from Hindi into French while preserving meaning, style, and context appropriate for corporate messages.",
                    "expected_response": "A string containing the message accurately translated from Hindi to French."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language code of the original message text (Hindi).",
                "target_language": "The language code of the desired translated text (French).",
                "message_text": "The content of the message to be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_244",
        "query": "We're trying to help out a friend who's developing an application for composing text. He is trying to create a bot that comes up with creative ideas for your paragraph.",
        "instruction": "Given a `creative text generation` task, retrieve tools that assist in generating innovative and engaging text by processing input prompts to inspire creative writing or ideas, aligned with the application's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation')",
                    "api_arguments": [
                        "model"
                    ],
                    "python_environment_requirements": [
                        "from transformers import pipeline, set_seed"
                    ],
                    "example_code": "set_seed(42)\ngenerator(Hello, I’m a language model, max_length=20, num_return_sequences=5)",
                    "performance": {
                        "dataset": "WikiText-103",
                        "accuracy": "21.100"
                    },
                    "description": "DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.",
                    "id": "huggingface_api_916",
                    "name": "distilgpt2"
                },
                "id": "gorilla_huggingface_tool_596",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 3: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 5: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 7: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 8: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 10: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 14: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 15: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.",
            "Tool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 17: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 18: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.",
            "Tool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use."
        ],
        "tool_view": {
            "description": "EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will use as the starting point for text generation.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "do_sample": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use sampling for text generation. If true, the model will randomly sample from the predicted token probabilities; if false, it will use deterministic greedy decoding.",
                    "required": true,
                    "example": true
                },
                "min_length": {
                    "type": "number",
                    "description": "The minimum number of tokens that the generated text should contain.",
                    "required": true,
                    "example": 50
                }
            },
            "response": "The tool produces a generated text response based on a given prompt. The typical structure of the response is a list of one or more generated outputs, where each output contains at least the following components:  \n\n1. **Generated text**: The main textual content produced by the model in continuation of the input prompt. This is the core output that reflects the model’s completion or elaboration.  \n\n2. **Optional metadata**: Depending on the pipeline and arguments used, each generated output may include additional information such as:  \n   - **Score or probability**: A numeric value representing the model’s confidence or likelihood of the generated text.  \n   - **Token-level details**: If enabled, information about individual tokens generated, including log probabilities or token IDs.  \n\nThe list can contain multiple entries if the generation parameters request multiple completions (e.g., `num_return_sequences > 1`). Each entry corresponds to a single independent text continuation from the same prompt, allowing for sampling diversity when `do_sample=True` is used.  \n\nOverall, the response is a structured collection of text continuations along with optional model-internal scoring, suitable for programmatic use or further processing.",
            "examples": [
                {
                    "user_intent": "Generate a creative continuation for a short story starting with 'Once upon a time, there was a small village by the sea.'",
                    "tool_call": {
                        "name": "EleutherAI/gpt-neo-2.7B",
                        "arguments": {
                            "text": "Once upon a time, there was a small village by the sea.",
                            "do_sample": true,
                            "min_length": 100
                        }
                    }
                },
                {
                    "user_intent": "I want a text generation model to expand on the phrase 'The future of AI is' with some creative possibilities.",
                    "tool_call": {
                        "name": "EleutherAI/gpt-neo-2.7B",
                        "arguments": {
                            "text": "The future of AI is",
                            "do_sample": true,
                            "min_length": 80
                        }
                    }
                },
                {
                    "user_intent": "Generate a few paragraphs about the impact of renewable energy on the economy.",
                    "tool_call": {
                        "name": "EleutherAI/gpt-neo-2.7B",
                        "arguments": {
                            "text": "The impact of renewable energy on the economy",
                            "do_sample": true,
                            "min_length": 150
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 2: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 3: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 5: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 6: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 7: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 8: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 10: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 14: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 15: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\nTool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 17: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 18: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.\nTool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will use as the starting point for text generation.\", \"required\": true, \"example\": \"Once upon a time\"}, \"do_sample\": {\"type\": \"boolean\", \"description\": \"A flag indicating whether to use sampling for text generation. If true, the model will randomly sample from the predicted token probabilities; if false, it will use deterministic greedy decoding.\", \"required\": true, \"example\": true}, \"min_length\": {\"type\": \"number\", \"description\": \"The minimum number of tokens that the generated text should contain.\", \"required\": true, \"example\": 50}}, \"response\": \"The tool produces a generated text response based on a given prompt. The typical structure of the response is a list of one or more generated outputs, where each output contains at least the following components:  \\n\\n1. **Generated text**: The main textual content produced by the model in continuation of the input prompt. This is the core output that reflects the model\\u2019s completion or elaboration.  \\n\\n2. **Optional metadata**: Depending on the pipeline and arguments used, each generated output may include additional information such as:  \\n   - **Score or probability**: A numeric value representing the model\\u2019s confidence or likelihood of the generated text.  \\n   - **Token-level details**: If enabled, information about individual tokens generated, including log probabilities or token IDs.  \\n\\nThe list can contain multiple entries if the generation parameters request multiple completions (e.g., `num_return_sequences > 1`). Each entry corresponds to a single independent text continuation from the same prompt, allowing for sampling diversity when `do_sample=True` is used.  \\n\\nOverall, the response is a structured collection of text continuations along with optional model-internal scoring, suitable for programmatic use or further processing.\", \"examples\": [{\"user_intent\": \"Generate a creative continuation for a short story starting with 'Once upon a time, there was a small village by the sea.'\", \"tool_call\": {\"name\": \"EleutherAI/gpt-neo-2.7B\", \"arguments\": {\"text\": \"Once upon a time, there was a small village by the sea.\", \"do_sample\": true, \"min_length\": 100}}}, {\"user_intent\": \"I want a text generation model to expand on the phrase 'The future of AI is' with some creative possibilities.\", \"tool_call\": {\"name\": \"EleutherAI/gpt-neo-2.7B\", \"arguments\": {\"text\": \"The future of AI is\", \"do_sample\": true, \"min_length\": 80}}}, {\"user_intent\": \"Generate a few paragraphs about the impact of renewable energy on the economy.\", \"tool_call\": {\"name\": \"EleutherAI/gpt-neo-2.7B\", \"arguments\": {\"text\": \"The impact of renewable energy on the economy\", \"do_sample\": true, \"min_length\": 150}}}]}\n\n# User Query\nWe're trying to help out a friend who's developing an application for composing text. He is trying to create a bot that comes up with creative ideas for your paragraph.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate creative text ideas or paragraph continuations based on a given prompt.",
                    "tool_description": "Use a transformer-based language model to produce coherent and contextually relevant text from an input prompt, enabling creative content generation for paragraphs or story ideas.",
                    "expected_response": "A generated text output containing one or more paragraph continuations or creative ideas, optionally including metadata such as confidence scores or token-level details."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "The starting text or idea that the bot will use to generate paragraph continuations.",
                "do_sample": "A boolean flag indicating whether to sample randomly for creative variation or use deterministic generation.",
                "min_length": "The minimum number of tokens the generated text should contain."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_354",
        "query": "We need to build an AI-powered tool to assist visually impaired users in understanding their surroundings by answering questions about images.",
        "instruction": "Given an `AI visual assistance` task, retrieve tools that process image and text inputs to provide answers to questions related to visual content, enabling visually impaired users to understand their surroundings by generating informative textual descriptions based on the AI model's analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')",
                    "api_arguments": "image, question",
                    "python_environment_requirements": "transformers",
                    "example_code": "For code examples, we refer to the documentation.",
                    "performance": {
                        "dataset": "TextVQA",
                        "accuracy": "See table 11 in the paper for more details."
                    },
                    "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).",
                    "id": "huggingface_api_106",
                    "name": "git-large-textvqa"
                },
                "id": "gorilla_huggingface_tool_105",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 2: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 3: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 6: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 8: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 9: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 11: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 14: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 15: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 16: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 17: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 18: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables."
        ],
        "tool_view": {
            "description": "A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "parameters": {},
            "response": "The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \n\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \n\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.",
            "examples": [
                {
                    "user_intent": "I have an image of a street scene and I want to know how many cars are visible in it.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/street_scene.jpg",
                            "question": "How many cars are there in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this photo of a kitchen and tell me what color the refrigerator is.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/kitchen.jpg",
                            "question": "What color is the refrigerator?"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a picture of a park and find out if there are any people sitting on benches.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/park.jpg",
                            "question": "Are there people sitting on benches?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 2: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 3: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 6: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 8: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 9: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 11: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 13: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 14: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 15: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 16: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 17: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 18: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\", \"parameters\": {}, \"response\": \"The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \\n\\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \\n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \\n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \\n\\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.\", \"examples\": [{\"user_intent\": \"I have an image of a street scene and I want to know how many cars are visible in it.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/street_scene.jpg\", \"question\": \"How many cars are there in this image?\"}}}, {\"user_intent\": \"Look at this photo of a kitchen and tell me what color the refrigerator is.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/kitchen.jpg\", \"question\": \"What color is the refrigerator?\"}}}, {\"user_intent\": \"I want to analyze a picture of a park and find out if there are any people sitting on benches.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/park.jpg\", \"question\": \"Are there people sitting on benches?\"}}}]}\n\n# User Query\nWe need to build an AI-powered tool to assist visually impaired users in understanding their surroundings by answering questions about images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze an image and provide descriptive answers to user questions about its content.",
                    "tool_description": "Use a multimodal Visual Question Answering (VQA) model to process images and textual questions simultaneously, generating textual responses that describe visual elements in the image.",
                    "expected_response": "A textual answer describing the content of the image, optionally with confidence scores and metadata about the analyzed image and question."
                },
                {
                    "user_intent": "Enable visually impaired users to query their surroundings through images.",
                    "tool_description": "Integrate the VQA model into an accessible interface that accepts images and natural language questions from users, and returns clear, interpretable answers about visual content.",
                    "expected_response": "User-facing responses that convey information about the image in an understandable format for visually impaired users."
                }
            ],
            "extracted_arguments": {
                "image_input": "The digital image representing the user's surroundings to be analyzed.",
                "user_question": "A natural language question posed by the user regarding the content of the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_300",
        "query": "I need to extract biomedical entities from a given set of case reports to make it easier for researchers to analyze the data.",
        "instruction": "Given a `biomedical entity extraction` task, retrieve tools that specialize in Named Entity Recognition (NER) to process text inputs and identify relevant biomedical entities from case reports, supporting the query's data analysis objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                    "api_arguments": {
                        "model": "AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)",
                        "tokenizer": "AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)",
                        "aggregation_strategy": "simple"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
                    },
                    "example_code": "pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)",
                    "performance": {
                        "dataset": "Maccrobat",
                        "accuracy": "Not provided"
                    },
                    "description": "An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.",
                    "id": "huggingface_api_404",
                    "name": "d4data/biomedical-ner-all"
                },
                "id": "gorilla_huggingface_tool_400",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 1: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 2: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 3: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 6: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 9: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.",
            "Tool 10: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 11: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 12: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 13: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 14: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 15: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 16: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 17: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.",
            "Tool 18: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 19: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data."
        ],
        "tool_view": {
            "description": "A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained token classification model to load for named entity recognition, specified by its model identifier.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.",
                    "required": true,
                    "example": "d4data/biomedical-ner-all"
                },
                "aggregation_strategy": {
                    "type": "string",
                    "description": "The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.",
                    "required": true,
                    "example": "simple"
                }
            },
            "response": "The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\n\n1. **Entity text or span** – The exact substring from the input text that has been recognized as an entity.  \n2. **Entity type or label** – The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \n3. **Start and end positions** – Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \n4. **Score or confidence** – A numeric value representing the model’s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \n\nBecause the tool is used with an **aggregation strategy of \"simple\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\n\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.",
            "examples": [
                {
                    "user_intent": "I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks."
                        }
                    }
                },
                {
                    "user_intent": "Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy."
                        }
                    }
                },
                {
                    "user_intent": "Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.",
                    "tool_call": {
                        "name": "d4data/biomedical-ner-all",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')",
                            "tokenizer": "AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')",
                            "aggregation_strategy": "simple",
                            "text": "After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 1: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 2: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 3: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 6: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 9: BioBERT is a pre-trained biomedical language model designed for extracting meaningful features from biomedical text, enabling tasks such as named entity recognition, relation extraction, and question answering in the biomedical domain.\nTool 10: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 11: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 12: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 13: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 14: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 15: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 16: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 17: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.\nTool 18: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 19: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained token classification model to load for named entity recognition, specified by its model identifier.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.\", \"required\": true, \"example\": \"d4data/biomedical-ner-all\"}, \"aggregation_strategy\": {\"type\": \"string\", \"description\": \"The strategy to aggregate token-level predictions into entity-level results. Common strategies include 'simple', 'first', 'average', etc.\", \"required\": true, \"example\": \"simple\"}}, \"response\": \"The tool produces a **Named Entity Recognition (NER) response** for a given input text. The response is typically structured as a sequence of entities detected in the text, with each entity containing several key pieces of information:\\n\\n1. **Entity text or span** \\u2013 The exact substring from the input text that has been recognized as an entity.  \\n2. **Entity type or label** \\u2013 The category or class of the entity, corresponding to one of the 107 biomedical entity types the model was trained to recognize. Examples include medical conditions, medications, procedures, or anatomical terms.  \\n3. **Start and end positions** \\u2013 Integer indices indicating the position of the entity text within the input string. The `start` index marks the first character of the entity, and the `end` index marks the position immediately after the last character of the entity.  \\n4. **Score or confidence** \\u2013 A numeric value representing the model\\u2019s confidence in the predicted entity label. This is usually a probability between 0 and 1.  \\n\\nBecause the tool is used with an **aggregation strategy of \\\"simple\\\"**, overlapping or subword tokens may be merged so that each entity in the response corresponds to a coherent, human-readable span rather than fragmented subword tokens.\\n\\nOverall, the response is a list of such entity dictionaries, providing a structured identification of biomedical entities present in the input text.\", \"examples\": [{\"user_intent\": \"I want to extract all biomedical entities like diseases, medications, and procedures from a patient's clinical note.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"The patient was prescribed 5 mg of amlodipine daily and showed improvement in hypertension symptoms within 2 weeks.\"}}}, {\"user_intent\": \"Analyze a set of clinical trial abstracts to automatically identify genes, proteins, and other biomedical entities.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"BRCA1 mutations are linked to a higher risk of breast cancer, and patients often undergo genetic counseling before starting therapy.\"}}}, {\"user_intent\": \"Detect and highlight any medical procedures and conditions mentioned in a cardiology case report.\", \"tool_call\": {\"name\": \"d4data/biomedical-ner-all\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\", \"aggregation_strategy\": \"simple\", \"text\": \"After undergoing catheter ablation, the patient reported no recurrence of palpitations at the 6-month follow-up.\"}}}]}\n\n# User Query\nI need to extract biomedical entities from a given set of case reports to make it easier for researchers to analyze the data.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract biomedical entities like diseases, medications, and procedures from case reports.",
                    "tool_description": "Retrieve structured biomedical information by identifying entities such as diseases, medications, procedures, or anatomical terms from a given set of case reports using a pretrained token classification model.",
                    "expected_response": "A list of biomedical entities identified in the input text, including their entity type, position within the text, and confidence score."
                }
            ],
            "extracted_arguments": {
                "model": "The pretrained token classification model to load for named entity recognition.",
                "tokenizer": "The pretrained tokenizer corresponding to the model, used to convert text into token inputs for the model.",
                "aggregation_strategy": "The strategy to aggregate token-level predictions into entity-level results.",
                "text": "The input case report or text containing biomedical information to extract."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_160",
        "query": "Organizing a special event and want to identify the place where street pictures were taken for invitations.",
        "instruction": "Given an `image geolocalization` task, retrieve tools that can identify the locations of street pictures by processing image inputs to provide their geographic details and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Geolocalization",
                    "api_call": "CLIPModel.from_pretrained('geolocal/StreetCLIP')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "geolocal/StreetCLIP"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "IM2GPS",
                                "accuracy": {
                                    "25km": 28.3,
                                    "200km": 45.1,
                                    "750km": 74.7,
                                    "2500km": 88.2
                                }
                            },
                            {
                                "name": "IM2GPS3K",
                                "accuracy": {
                                    "25km": 22.4,
                                    "200km": 37.4,
                                    "750km": 61.3,
                                    "2500km": 80.4
                                }
                            }
                        ]
                    },
                    "description": "StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.",
                    "id": "huggingface_api_374",
                    "name": "geolocal/StreetCLIP"
                },
                "id": "gorilla_huggingface_tool_370",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 1: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 2: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 4: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.",
            "Tool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 8: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 9: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 10: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 11: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 12: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 15: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.",
            "Tool 16: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 18: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 19: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "parameters": {
                "use_auth_token": {
                    "type": "string",
                    "description": "The authentication token used to access the pre-trained pyannote segmentation model from Hugging Face. This is required if the model is private or requires user authentication.",
                    "required": true,
                    "example": "hf_1234567890abcdef"
                }
            },
            "response": "The tool produces structured output representing the analysis of an audio signal in terms of speaker activity and speech characteristics. A typical response would include the following components:\n\n1. **Voice Activity Detection (VAD) results**  \n   - Indicates when speech is present versus silence in the audio.  \n   - Usually represented as a timeline or sequence of segments, where each segment has:\n     - `start` and `end` timestamps marking the speech interval.\n     - A confidence score or probability indicating the likelihood of speech presence.\n   - May include optional thresholds applied (`onset`, `offset`, `min_duration_on`, `min_duration_off`) to control detection sensitivity.\n\n2. **Overlapped Speech Detection (OSD) results**  \n   - Identifies regions where multiple speakers are talking simultaneously.  \n   - Each segment includes:\n     - `start` and `end` timestamps for the overlap region.\n     - A confidence score representing the probability of overlapped speech.\n   - Parameters like `onset`, `offset`, and minimum durations can be applied similarly to VAD.\n\n3. **Speaker Segmentation / Diarization (optional, via Resegmentation)**  \n   - Assigns speech segments to individual speakers.  \n   - Each segment includes:\n     - `start` and `end` timestamps.\n     - A `speaker` label or ID.\n     - Optional confidence scores or resegmentation-adjusted probabilities.\n   - Can incorporate a baseline diarization to refine speaker boundaries.\n\n4. **Raw scores / probabilities**  \n   - Some pipelines can return continuous frame-level scores instead of hard segments.\n   - Each frame has a timestamp and a score indicating the likelihood of speech or overlap.\n\n5. **Pipeline metadata (optional)**  \n   - Information about hyperparameters used (e.g., onset/offset thresholds, minimum durations).  \n   - May include model identifiers or versioning information.\n\nOverall, the response is a time-aligned representation of detected speech activity, speaker segments, and overlapped speech, optionally supplemented with confidence scores or raw probability values, structured in a way that supports both segment-level and frame-level analysis.",
            "examples": [
                {
                    "user_intent": "I have a meeting recording and I want to detect when people are speaking and when it's silent.",
                    "tool_call": {
                        "name": "pyannote/segmentation",
                        "arguments": {
                            "use_auth_token": "hf_ABC123TOKEN"
                        },
                        "pipeline": "VoiceActivityDetection",
                        "audio_file": "meeting_recording.wav",
                        "hyper_parameters": {
                            "onset": 0.5,
                            "offset": 0.5,
                            "min_duration_on": 0.0,
                            "min_duration_off": 0.0
                        }
                    }
                },
                {
                    "user_intent": "I want to identify segments where multiple people are speaking at the same time in a podcast audio file.",
                    "tool_call": {
                        "name": "pyannote/segmentation",
                        "arguments": {
                            "use_auth_token": "hf_ABC123TOKEN"
                        },
                        "pipeline": "OverlappedSpeechDetection",
                        "audio_file": "podcast_episode.wav",
                        "hyper_parameters": {
                            "onset": 0.4,
                            "offset": 0.4,
                            "min_duration_on": 0.1,
                            "min_duration_off": 0.1
                        }
                    }
                },
                {
                    "user_intent": "I already have a speaker diarization output for a conference recording, but I want to refine it for more accurate segment boundaries.",
                    "tool_call": {
                        "name": "pyannote/segmentation",
                        "arguments": {
                            "use_auth_token": "hf_ABC123TOKEN"
                        },
                        "pipeline": "Resegmentation",
                        "audio_file": "conference.wav",
                        "baseline_diarization_file": "conference_diarization.rttm",
                        "hyper_parameters": {
                            "onset": 0.5,
                            "offset": 0.5,
                            "min_duration_on": 0.05,
                            "min_duration_off": 0.05
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 1: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 2: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 4: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.\nTool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 8: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 9: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 10: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 11: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 12: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 15: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.\nTool 16: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 18: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 19: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\", \"parameters\": {\"use_auth_token\": {\"type\": \"string\", \"description\": \"The authentication token used to access the pre-trained pyannote segmentation model from Hugging Face. This is required if the model is private or requires user authentication.\", \"required\": true, \"example\": \"hf_1234567890abcdef\"}}, \"response\": \"The tool produces structured output representing the analysis of an audio signal in terms of speaker activity and speech characteristics. A typical response would include the following components:\\n\\n1. **Voice Activity Detection (VAD) results**  \\n   - Indicates when speech is present versus silence in the audio.  \\n   - Usually represented as a timeline or sequence of segments, where each segment has:\\n     - `start` and `end` timestamps marking the speech interval.\\n     - A confidence score or probability indicating the likelihood of speech presence.\\n   - May include optional thresholds applied (`onset`, `offset`, `min_duration_on`, `min_duration_off`) to control detection sensitivity.\\n\\n2. **Overlapped Speech Detection (OSD) results**  \\n   - Identifies regions where multiple speakers are talking simultaneously.  \\n   - Each segment includes:\\n     - `start` and `end` timestamps for the overlap region.\\n     - A confidence score representing the probability of overlapped speech.\\n   - Parameters like `onset`, `offset`, and minimum durations can be applied similarly to VAD.\\n\\n3. **Speaker Segmentation / Diarization (optional, via Resegmentation)**  \\n   - Assigns speech segments to individual speakers.  \\n   - Each segment includes:\\n     - `start` and `end` timestamps.\\n     - A `speaker` label or ID.\\n     - Optional confidence scores or resegmentation-adjusted probabilities.\\n   - Can incorporate a baseline diarization to refine speaker boundaries.\\n\\n4. **Raw scores / probabilities**  \\n   - Some pipelines can return continuous frame-level scores instead of hard segments.\\n   - Each frame has a timestamp and a score indicating the likelihood of speech or overlap.\\n\\n5. **Pipeline metadata (optional)**  \\n   - Information about hyperparameters used (e.g., onset/offset thresholds, minimum durations).  \\n   - May include model identifiers or versioning information.\\n\\nOverall, the response is a time-aligned representation of detected speech activity, speaker segments, and overlapped speech, optionally supplemented with confidence scores or raw probability values, structured in a way that supports both segment-level and frame-level analysis.\", \"examples\": [{\"user_intent\": \"I have a meeting recording and I want to detect when people are speaking and when it's silent.\", \"tool_call\": {\"name\": \"pyannote/segmentation\", \"arguments\": {\"use_auth_token\": \"hf_ABC123TOKEN\"}, \"pipeline\": \"VoiceActivityDetection\", \"audio_file\": \"meeting_recording.wav\", \"hyper_parameters\": {\"onset\": 0.5, \"offset\": 0.5, \"min_duration_on\": 0.0, \"min_duration_off\": 0.0}}}, {\"user_intent\": \"I want to identify segments where multiple people are speaking at the same time in a podcast audio file.\", \"tool_call\": {\"name\": \"pyannote/segmentation\", \"arguments\": {\"use_auth_token\": \"hf_ABC123TOKEN\"}, \"pipeline\": \"OverlappedSpeechDetection\", \"audio_file\": \"podcast_episode.wav\", \"hyper_parameters\": {\"onset\": 0.4, \"offset\": 0.4, \"min_duration_on\": 0.1, \"min_duration_off\": 0.1}}}, {\"user_intent\": \"I already have a speaker diarization output for a conference recording, but I want to refine it for more accurate segment boundaries.\", \"tool_call\": {\"name\": \"pyannote/segmentation\", \"arguments\": {\"use_auth_token\": \"hf_ABC123TOKEN\"}, \"pipeline\": \"Resegmentation\", \"audio_file\": \"conference.wav\", \"baseline_diarization_file\": \"conference_diarization.rttm\", \"hyper_parameters\": {\"onset\": 0.5, \"offset\": 0.5, \"min_duration_on\": 0.05, \"min_duration_off\": 0.05}}}]}\n\n# User Query\nOrganizing a special event and want to identify the place where street pictures were taken for invitations.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify the geographic location of street images for event invitations.",
                    "tool_description": "This tool identifies the geographic location of street-level images based on visual features, leveraging a foundation model for zero-shot image geolocalization.",
                    "expected_response": "A geolocation prediction for each image, indicating the geographic coordinates or location of the street scene."
                }
            ],
            "extracted_arguments": {
                "image": "The image or URL of the street-level image to be analyzed for geolocation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_419",
        "query": "I am a real-estate agent working on a project where I need to convert images of room plans to a better visual representation.",
        "instruction": "Given an `image enhancement` task, retrieve tools that process visual inputs to transform and enhance image representations, particularly focusing on room plans to produce improved visual outputs with specific enhancement techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image-to-Image",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "opencv": "pip install opencv-contrib-python",
                        "diffusers": "pip install diffusers transformers accelerate"
                    },
                    "example_code": "import cv2\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nimport numpy as np\nfrom diffusers.utils import load_image\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\nimage = np.array(image)\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(bird, image, num_inference_steps=20).images[0]\nimage.save('images/bird_canny_out.png')",
                    "performance": {
                        "dataset": "3M edge-image, caption pairs",
                        "accuracy": "600 GPU-hours with Nvidia A100 80G"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_261",
                    "name": "lllyasviel/sd-controlnet-canny"
                },
                "id": "gorilla_huggingface_tool_257",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 2: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 3: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.",
            "Tool 4: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 5: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 8: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 9: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 10: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.",
            "Tool 11: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 12: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 13: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 14: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 16: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 17: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 18: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 2: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 3: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.\nTool 4: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 5: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 8: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 9: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 10: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.\nTool 11: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 12: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 13: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 14: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 16: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 17: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 18: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 19: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nI am a real-estate agent working on a project where I need to convert images of room plans to a better visual representation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to convert images of room plans into enhanced visual representations.",
                    "tool_description": "Generate high-quality, visually appealing images based on input images of room plans, improving clarity, style, or realism.",
                    "expected_response": "A generated image or set of images representing an enhanced version of the input room plan."
                }
            ],
            "extracted_arguments": {
                "input_image": "The original image file or tensor representing the room plan to be visually enhanced."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_397",
        "query": "I have a table with data about different types of bards and their magical abilities. Let me see if I can find the best bard given the data in the table.",
        "instruction": "Given a `table question answering` task, retrieve tools that utilize table data to answer queries by analyzing and processing the information contained within the table to identify the best option based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')",
                    "api_arguments": "tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\n\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.2854
                    },
                    "description": "TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.",
                    "id": "huggingface_api_453",
                    "name": "google/tapas-mini-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_449",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 3: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 4: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.",
            "Tool 5: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 6: A computer vision model built on Hugging Face Transformers for detecting and extracting tables from documents, supporting both bordered and borderless table types, and providing high-accuracy table localization for downstream processing or analysis.",
            "Tool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 8: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 9: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 10: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 11: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 12: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 13: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 14: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 17: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 18: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 19: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables."
        ],
        "tool_view": {
            "description": "This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The identifier or name of the pre-trained model to be used for the table question answering task.",
                    "required": true,
                    "default": "Meena/table-question-answering-tapas",
                    "example": "Meena/table-question-answering-tapas"
                }
            },
            "response": "The tool described is a model for answering questions about tabular data using natural language processing, specifically through a transformer-based architecture (PyTorch Transformers). This model, called TAPAS, is designed to understand and reason over tables alongside natural language text.\n\n### Expected Response Description:\n\nThe tool response would likely contain the answer to a user-provided question, based on a table input. Here’s a breakdown of the typical content and structure:\n\n1. **Answer**: The primary content of the response will be the model's answer to the question provided in natural language, with respect to the table data.\n\n2. **Confidence** (optional): If supported, the response might include a confidence score or probability associated with the provided answer, indicating the model's certainty.\n\n3. **Supporting Information** (optional): The model might return additional details, such as the relevant rows or columns in the table that led to the final answer, or other reasoning steps. This is especially useful for transparency and interpretability.\n\n4. **Metadata**: The response might also include metadata about the question, such as:\n   - **Question ID** (if applicable)\n   - **Table ID** (if the table is referenced from a specific dataset)\n   - **Processing time** (optional, may be included to inform the user of the time taken for processing)\n\n### Key Components:\n- **Answer**: A direct, concise response to the question posed.\n- **Confidence**: A numeric value indicating the model's certainty in its answer (if provided).\n- **Supporting Information**: Details on which part of the table was used to generate the answer (if applicable).\n- **Metadata**: Information such as the question and table identifier, processing time, etc., for tracking or debugging purposes.\n\nThe response is expected to be well-structured for downstream use in an NLP pipeline, focusing on the answer and any supporting evidence necessary for the question-answering task.",
            "examples": [
                {
                    "user_intent": "I have a table of sales data and I want to know the total revenue for Q1.",
                    "tool_call": {
                        "name": "table-question-answering-tapas",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Quarter",
                                    "Revenue",
                                    "Expenses"
                                ],
                                "rows": [
                                    [
                                        "Q1",
                                        150000,
                                        50000
                                    ],
                                    [
                                        "Q2",
                                        200000,
                                        70000
                                    ],
                                    [
                                        "Q3",
                                        180000,
                                        60000
                                    ],
                                    [
                                        "Q4",
                                        220000,
                                        80000
                                    ]
                                ]
                            },
                            "query": "What is the total revenue for Q1?"
                        }
                    }
                },
                {
                    "user_intent": "From this sports stats table, tell me who scored the most goals.",
                    "tool_call": {
                        "name": "table-question-answering-tapas",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Player",
                                    "Team",
                                    "Goals"
                                ],
                                "rows": [
                                    [
                                        "Alice",
                                        "Red Lions",
                                        12
                                    ],
                                    [
                                        "Bob",
                                        "Blue Tigers",
                                        15
                                    ],
                                    [
                                        "Charlie",
                                        "Green Hawks",
                                        10
                                    ]
                                ]
                            },
                            "query": "Which player scored the highest number of goals?"
                        }
                    }
                },
                {
                    "user_intent": "I need to check if 'France' is listed as a country with a population over 60 million in this table.",
                    "tool_call": {
                        "name": "table-question-answering-tapas",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Country",
                                    "Population (millions)"
                                ],
                                "rows": [
                                    [
                                        "Germany",
                                        83
                                    ],
                                    [
                                        "France",
                                        67
                                    ],
                                    [
                                        "Italy",
                                        60
                                    ],
                                    [
                                        "Spain",
                                        47
                                    ]
                                ]
                            },
                            "query": "Is France listed as having a population over 60 million?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 3: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 4: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.\nTool 5: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 6: A computer vision model built on Hugging Face Transformers for detecting and extracting tables from documents, supporting both bordered and borderless table types, and providing high-accuracy table localization for downstream processing or analysis.\nTool 7: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 8: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 9: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 10: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 11: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 12: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 13: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 14: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 16: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 17: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 18: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 19: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The identifier or name of the pre-trained model to be used for the table question answering task.\", \"required\": true, \"default\": \"Meena/table-question-answering-tapas\", \"example\": \"Meena/table-question-answering-tapas\"}}, \"response\": \"The tool described is a model for answering questions about tabular data using natural language processing, specifically through a transformer-based architecture (PyTorch Transformers). This model, called TAPAS, is designed to understand and reason over tables alongside natural language text.\\n\\n### Expected Response Description:\\n\\nThe tool response would likely contain the answer to a user-provided question, based on a table input. Here\\u2019s a breakdown of the typical content and structure:\\n\\n1. **Answer**: The primary content of the response will be the model's answer to the question provided in natural language, with respect to the table data.\\n\\n2. **Confidence** (optional): If supported, the response might include a confidence score or probability associated with the provided answer, indicating the model's certainty.\\n\\n3. **Supporting Information** (optional): The model might return additional details, such as the relevant rows or columns in the table that led to the final answer, or other reasoning steps. This is especially useful for transparency and interpretability.\\n\\n4. **Metadata**: The response might also include metadata about the question, such as:\\n   - **Question ID** (if applicable)\\n   - **Table ID** (if the table is referenced from a specific dataset)\\n   - **Processing time** (optional, may be included to inform the user of the time taken for processing)\\n\\n### Key Components:\\n- **Answer**: A direct, concise response to the question posed.\\n- **Confidence**: A numeric value indicating the model's certainty in its answer (if provided).\\n- **Supporting Information**: Details on which part of the table was used to generate the answer (if applicable).\\n- **Metadata**: Information such as the question and table identifier, processing time, etc., for tracking or debugging purposes.\\n\\nThe response is expected to be well-structured for downstream use in an NLP pipeline, focusing on the answer and any supporting evidence necessary for the question-answering task.\", \"examples\": [{\"user_intent\": \"I have a table of sales data and I want to know the total revenue for Q1.\", \"tool_call\": {\"name\": \"table-question-answering-tapas\", \"arguments\": {\"table\": {\"columns\": [\"Quarter\", \"Revenue\", \"Expenses\"], \"rows\": [[\"Q1\", 150000, 50000], [\"Q2\", 200000, 70000], [\"Q3\", 180000, 60000], [\"Q4\", 220000, 80000]]}, \"query\": \"What is the total revenue for Q1?\"}}}, {\"user_intent\": \"From this sports stats table, tell me who scored the most goals.\", \"tool_call\": {\"name\": \"table-question-answering-tapas\", \"arguments\": {\"table\": {\"columns\": [\"Player\", \"Team\", \"Goals\"], \"rows\": [[\"Alice\", \"Red Lions\", 12], [\"Bob\", \"Blue Tigers\", 15], [\"Charlie\", \"Green Hawks\", 10]]}, \"query\": \"Which player scored the highest number of goals?\"}}}, {\"user_intent\": \"I need to check if 'France' is listed as a country with a population over 60 million in this table.\", \"tool_call\": {\"name\": \"table-question-answering-tapas\", \"arguments\": {\"table\": {\"columns\": [\"Country\", \"Population (millions)\"], \"rows\": [[\"Germany\", 83], [\"France\", 67], [\"Italy\", 60], [\"Spain\", 47]]}, \"query\": \"Is France listed as having a population over 60 million?\"}}}]}\n\n# User Query\nI have a table with data about different types of bards and their magical abilities. Let me see if I can find the best bard given the data in the table.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find out which bard has the best magical abilities from my table.",
                    "tool_description": "Use a table question-answering model to analyze tabular data about bards and their magical abilities and determine which bard ranks highest based on specified criteria.",
                    "expected_response": "A concise answer identifying the bard with the best magical abilities, optionally including supporting table rows or reasoning steps."
                }
            ],
            "extracted_arguments": {
                "table": "A structured representation of bard data including columns and rows.",
                "query": "A natural language question asking which bard has the best magical abilities."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_111",
        "query": "The publisher has sent us a draft of an article, but some of the words have been masked. We need to identify the masked words.",
        "instruction": "Given a `fill-mask language modeling` task, retrieve tools that identify and replace masked words in text using advanced natural language processing techniques to complete and enhance the text content accurately and coherently.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='xlm-roberta-large')",
                    "api_arguments": {
                        "model": "xlm-roberta-large"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='xlm-roberta-large')\nunmasker(Hello I'm a <mask> model.)",
                    "performance": {
                        "dataset": "CommonCrawl",
                        "accuracy": "N/A"
                    },
                    "description": "XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It is designed for masked language modeling and can be fine-tuned on downstream tasks such as sequence classification, token classification, or question answering.",
                    "id": "huggingface_api_663",
                    "name": "xlm-roberta-large"
                },
                "id": "gorilla_huggingface_tool_658",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 1: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 2: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.",
            "Tool 3: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.",
            "Tool 4: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 5: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 6: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 7: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 9: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 10: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 11: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 12: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 13: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 14: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 16: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 17: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 18: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 19: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks."
        ],
        "tool_view": {
            "description": "The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text containing a [MASK] token where the model should predict the missing word or phrase.",
                    "required": true,
                    "example": "Hello I'm a [MASK] model."
                }
            },
            "response": "The tool returns a list of predictions for a masked token in the input text. Each element in the response represents a single candidate for the masked position and typically contains the following components:  \n\n1. **`sequence`** – The full input text with the `[MASK]` token replaced by the predicted word or token. This shows how the input would read if this candidate were inserted.  \n2. **`score`** – A numerical probability or confidence value representing the model’s likelihood that this token correctly fills the masked position. Higher scores indicate higher confidence.  \n3. **`token`** – The internal token ID corresponding to the predicted word or subword from the model’s vocabulary.  \n4. **`token_str`** – The textual form of the predicted token, i.e., the actual word or subword string that fills the mask.  \n\nThe response is typically ordered by descending confidence, so the first element is the model’s most likely prediction for the masked position. Multiple predictions allow exploration of alternative plausible words in context.",
            "examples": [
                {
                    "user_intent": "I want to guess the missing word in this sentence: 'The quick brown fox jumps over the [MASK] dog.'",
                    "tool_call": {
                        "name": "albert-base-v2",
                        "arguments": {
                            "text": "The quick brown fox jumps over the [MASK] dog."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for: 'Artificial intelligence is transforming the [MASK] industry.'",
                    "tool_call": {
                        "name": "albert-base-v2",
                        "arguments": {
                            "text": "Artificial intelligence is transforming the [MASK] industry."
                        }
                    }
                },
                {
                    "user_intent": "Predict the missing word in this phrase: 'She enjoys reading books about [MASK] and history.'",
                    "tool_call": {
                        "name": "albert-base-v2",
                        "arguments": {
                            "text": "She enjoys reading books about [MASK] and history."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 1: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 2: A pretrained BERT large (cased) model for English that performs masked language modeling, allowing users to predict missing words in text. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters, making it suitable for tasks like text completion, language understanding, and NLP evaluation.\nTool 3: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.\nTool 4: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 5: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 6: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 7: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 9: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 10: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 11: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 12: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 13: The `bert-base-multilingual-cased` model is a case-sensitive BERT model pretrained on Wikipedia content from 104 languages using masked language modeling. It can predict missing words in text and serve as a foundation for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 14: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 15: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 16: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 17: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 18: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 19: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"albert-base-v2\\\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \\\"Hello I'm a [MASK] model.\\\"\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text containing a [MASK] token where the model should predict the missing word or phrase.\", \"required\": true, \"example\": \"Hello I'm a [MASK] model.\"}}, \"response\": \"The tool returns a list of predictions for a masked token in the input text. Each element in the response represents a single candidate for the masked position and typically contains the following components:  \\n\\n1. **`sequence`** \\u2013 The full input text with the `[MASK]` token replaced by the predicted word or token. This shows how the input would read if this candidate were inserted.  \\n2. **`score`** \\u2013 A numerical probability or confidence value representing the model\\u2019s likelihood that this token correctly fills the masked position. Higher scores indicate higher confidence.  \\n3. **`token`** \\u2013 The internal token ID corresponding to the predicted word or subword from the model\\u2019s vocabulary.  \\n4. **`token_str`** \\u2013 The textual form of the predicted token, i.e., the actual word or subword string that fills the mask.  \\n\\nThe response is typically ordered by descending confidence, so the first element is the model\\u2019s most likely prediction for the masked position. Multiple predictions allow exploration of alternative plausible words in context.\", \"examples\": [{\"user_intent\": \"I want to guess the missing word in this sentence: 'The quick brown fox jumps over the [MASK] dog.'\", \"tool_call\": {\"name\": \"albert-base-v2\", \"arguments\": {\"text\": \"The quick brown fox jumps over the [MASK] dog.\"}}}, {\"user_intent\": \"Fill in the blank for: 'Artificial intelligence is transforming the [MASK] industry.'\", \"tool_call\": {\"name\": \"albert-base-v2\", \"arguments\": {\"text\": \"Artificial intelligence is transforming the [MASK] industry.\"}}}, {\"user_intent\": \"Predict the missing word in this phrase: 'She enjoys reading books about [MASK] and history.'\", \"tool_call\": {\"name\": \"albert-base-v2\", \"arguments\": {\"text\": \"She enjoys reading books about [MASK] and history.\"}}}]}\n\n# User Query\nThe publisher has sent us a draft of an article, but some of the words have been masked. We need to identify the masked words.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict the missing words in a sentence with masked tokens.",
                    "tool_description": "Use a masked language model to predict the missing words in a sentence with masked tokens, completing the draft by identifying the masked words.",
                    "expected_response": "A list of predictions for each masked token in the input text, including possible replacements for the masked words."
                }
            ],
            "extracted_arguments": {
                "text": "The input text containing masked tokens where the model should predict the missing words or phrases."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_151",
        "query": "I have a Catalan language text and I need to publish it as a blog article in Spanish, so I want to translate my Catalan text into Spanish.",
        "instruction": "Given a `language translation` task, retrieve tools that facilitate translating text from Catalan to Spanish by processing the source language input and producing a coherent target language output aligned with the requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ca-es') , MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ca-es')",
                    "api_arguments": [
                        "source_languages",
                        "target_languages",
                        "dataset",
                        "model",
                        "pre-processing",
                        "download_original_weights",
                        "test_set_translations",
                        "test_set_scores"
                    ],
                    "python_environment_requirements": [
                        "PyTorch",
                        "TensorFlow",
                        "Transformers"
                    ],
                    "example_code": "from transformers import MarianMTModel, MarianTokenizer",
                    "performance": {
                        "dataset": "Tatoeba.ca.es",
                        "accuracy": {
                            "BLEU": 74.9,
                            "chr-F": 0.863
                        }
                    },
                    "description": "A Hugging Face model for translation between Catalan (ca) and Spanish (es) languages, based on the OPUS dataset and using the transformer-align architecture. The model has been pre-processed with normalization and SentencePiece.",
                    "id": "huggingface_api_541",
                    "name": "opus-mt-ca-es"
                },
                "id": "gorilla_huggingface_tool_537",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 2: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 3: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 4: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 5: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 6: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 7: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 8: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.",
            "Tool 9: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 10: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 11: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 12: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.",
            "Tool 13: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 14: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 15: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 16: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.",
            "Tool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 19: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation."
        ],
        "tool_view": {
            "description": "A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "parameters": {
                "source_languages": {
                    "type": "array",
                    "description": "A list of source languages that the model will translate from. This specifies the language(s) of the input text.",
                    "required": true,
                    "example": [
                        "ca"
                    ]
                },
                "target_languages": {
                    "type": "array",
                    "description": "A list of target languages that the model will translate into. This specifies the language(s) of the output text.",
                    "required": true,
                    "example": [
                        "es"
                    ]
                },
                "dataset": {
                    "type": "string",
                    "description": "The dataset used for training the model. This provides context for the data the model was trained on.",
                    "required": true,
                    "example": "Tatoeba.ca.es"
                },
                "model": {
                    "type": "string",
                    "description": "The specific pre-trained model to use. This refers to the model architecture or identifier from the Hugging Face library.",
                    "required": true,
                    "example": "Helsinki-NLP/opus-mt-ca-es"
                },
                "pre-processing": {
                    "type": "string",
                    "description": "Any pre-processing steps applied to the data before feeding it into the model, such as normalization or tokenization.",
                    "required": false,
                    "example": "Normalization, SentencePiece"
                },
                "download_original_weights": {
                    "type": "boolean",
                    "description": "A flag indicating whether to download the original weights of the pre-trained model. This might be useful for model fine-tuning or inspecting the original weights.",
                    "required": false,
                    "example": true
                },
                "test_set_translations": {
                    "type": "array",
                    "description": "A list of translations from a test set that the model can be evaluated on. This helps in assessing the model's performance on unseen data.",
                    "required": false,
                    "example": [
                        "Els gats són simpàtics.",
                        "Los gatos son simpáticos."
                    ]
                },
                "test_set_scores": {
                    "type": "object",
                    "description": "A set of performance metrics such as BLEU, chr-F, or other relevant scores to evaluate the accuracy of the model's translations on the test set.",
                    "required": false,
                    "example": {
                        "BLEU": 74.9,
                        "chr-F": 0.863
                    }
                }
            },
            "response": "The tool produces a translation output for a given input text from Catalan to Spanish. The response structure typically includes the following components:\n\n1. **Translated Text**:  \n   - The primary component of the response.  \n   - Contains the input text translated from the source language (Catalan) to the target language (Spanish).  \n   - Represented as a string.\n\n2. **Optional Metadata** (if implemented or returned by the framework):  \n   - **Source Text**: The original input text submitted for translation.  \n   - **Source Language**: The language code of the input text (Catalan, `ca`).  \n   - **Target Language**: The language code of the translation output (Spanish, `es`).  \n   - **Token-Level Information**: Some translation pipelines may optionally include tokenized forms or alignment information between source and target tokens.  \n   - **Scores / Confidence Metrics**: While not required by default, the model or downstream evaluation may provide metrics such as likelihood scores, BLEU, or chr-F estimates for the translation quality.\n\n3. **Batch / List Handling**:  \n   - If multiple texts are translated at once, the response is typically a list of translated text strings, each corresponding to the respective input in the same order.\n\nIn essence, the canonical response is a translated text (or a list of translations) from Catalan to Spanish, optionally accompanied by metadata about the input, tokenization, or quality metrics, depending on the implementation of the translation pipeline.",
            "examples": [
                {
                    "user_intent": "Translate this Catalan sentence into Spanish: 'Bon dia, com estàs?'",
                    "tool_call": {
                        "name": "opus-mt-ca-es",
                        "arguments": {
                            "source_languages": [
                                "ca"
                            ],
                            "target_languages": [
                                "es"
                            ],
                            "dataset": "Tatoeba.ca.es",
                            "model": "Helsinki-NLP/opus-mt-ca-es",
                            "pre-processing": "normalization + SentencePiece",
                            "download_original_weights": true,
                            "test_set_translations": [
                                "Bon dia, com estàs? -> Buenos días, ¿cómo estás?"
                            ],
                            "test_set_scores": {
                                "BLEU": 74.9,
                                "chr-F": 0.863
                            }
                        }
                    }
                },
                {
                    "user_intent": "I have a batch of Catalan sentences and I want to translate them all to Spanish using a pretrained model.",
                    "tool_call": {
                        "name": "opus-mt-ca-es",
                        "arguments": {
                            "source_languages": [
                                "ca"
                            ],
                            "target_languages": [
                                "es"
                            ],
                            "dataset": "Tatoeba.ca.es",
                            "model": "Helsinki-NLP/opus-mt-ca-es",
                            "pre-processing": "normalization + SentencePiece",
                            "download_original_weights": true,
                            "test_set_translations": [],
                            "test_set_scores": {}
                        }
                    }
                },
                {
                    "user_intent": "Check the translation quality of the Catalan to Spanish model using the Tatoeba test set.",
                    "tool_call": {
                        "name": "opus-mt-ca-es",
                        "arguments": {
                            "source_languages": [
                                "ca"
                            ],
                            "target_languages": [
                                "es"
                            ],
                            "dataset": "Tatoeba.ca.es",
                            "model": "Helsinki-NLP/opus-mt-ca-es",
                            "pre-processing": "normalization + SentencePiece",
                            "download_original_weights": false,
                            "test_set_translations": [
                                "El gat dorm -> El gato duerme",
                                "M'agrada la música -> Me gusta la música"
                            ],
                            "test_set_scores": {
                                "BLEU": 74.9,
                                "chr-F": 0.863
                            }
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 2: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 3: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 4: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 5: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 6: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 7: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 8: A Spanish text summarization tool based on BERT2BERT (BETO) fine-tuned on the MLSUM dataset, which generates concise summaries of input text using a Hugging Face Transformers sequence-to-sequence model.\nTool 9: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 10: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 11: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 12: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.\nTool 13: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 14: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 15: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 16: This tool provides a text-to-text translation capability using the T5 encoder-decoder model, allowing users to translate text from one language to another by leveraging a pre-trained model optimized for multiple natural language tasks. It is designed for easy integration in Python through the Hugging Face Transformers pipeline and is suitable for applications requiring automated language translation.\nTool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 19: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\", \"parameters\": {\"source_languages\": {\"type\": \"array\", \"description\": \"A list of source languages that the model will translate from. This specifies the language(s) of the input text.\", \"required\": true, \"example\": [\"ca\"]}, \"target_languages\": {\"type\": \"array\", \"description\": \"A list of target languages that the model will translate into. This specifies the language(s) of the output text.\", \"required\": true, \"example\": [\"es\"]}, \"dataset\": {\"type\": \"string\", \"description\": \"The dataset used for training the model. This provides context for the data the model was trained on.\", \"required\": true, \"example\": \"Tatoeba.ca.es\"}, \"model\": {\"type\": \"string\", \"description\": \"The specific pre-trained model to use. This refers to the model architecture or identifier from the Hugging Face library.\", \"required\": true, \"example\": \"Helsinki-NLP/opus-mt-ca-es\"}, \"pre-processing\": {\"type\": \"string\", \"description\": \"Any pre-processing steps applied to the data before feeding it into the model, such as normalization or tokenization.\", \"required\": false, \"example\": \"Normalization, SentencePiece\"}, \"download_original_weights\": {\"type\": \"boolean\", \"description\": \"A flag indicating whether to download the original weights of the pre-trained model. This might be useful for model fine-tuning or inspecting the original weights.\", \"required\": false, \"example\": true}, \"test_set_translations\": {\"type\": \"array\", \"description\": \"A list of translations from a test set that the model can be evaluated on. This helps in assessing the model's performance on unseen data.\", \"required\": false, \"example\": [\"Els gats s\\u00f3n simp\\u00e0tics.\", \"Los gatos son simp\\u00e1ticos.\"]}, \"test_set_scores\": {\"type\": \"object\", \"description\": \"A set of performance metrics such as BLEU, chr-F, or other relevant scores to evaluate the accuracy of the model's translations on the test set.\", \"required\": false, \"example\": {\"BLEU\": 74.9, \"chr-F\": 0.863}}}, \"response\": \"The tool produces a translation output for a given input text from Catalan to Spanish. The response structure typically includes the following components:\\n\\n1. **Translated Text**:  \\n   - The primary component of the response.  \\n   - Contains the input text translated from the source language (Catalan) to the target language (Spanish).  \\n   - Represented as a string.\\n\\n2. **Optional Metadata** (if implemented or returned by the framework):  \\n   - **Source Text**: The original input text submitted for translation.  \\n   - **Source Language**: The language code of the input text (Catalan, `ca`).  \\n   - **Target Language**: The language code of the translation output (Spanish, `es`).  \\n   - **Token-Level Information**: Some translation pipelines may optionally include tokenized forms or alignment information between source and target tokens.  \\n   - **Scores / Confidence Metrics**: While not required by default, the model or downstream evaluation may provide metrics such as likelihood scores, BLEU, or chr-F estimates for the translation quality.\\n\\n3. **Batch / List Handling**:  \\n   - If multiple texts are translated at once, the response is typically a list of translated text strings, each corresponding to the respective input in the same order.\\n\\nIn essence, the canonical response is a translated text (or a list of translations) from Catalan to Spanish, optionally accompanied by metadata about the input, tokenization, or quality metrics, depending on the implementation of the translation pipeline.\", \"examples\": [{\"user_intent\": \"Translate this Catalan sentence into Spanish: 'Bon dia, com est\\u00e0s?'\", \"tool_call\": {\"name\": \"opus-mt-ca-es\", \"arguments\": {\"source_languages\": [\"ca\"], \"target_languages\": [\"es\"], \"dataset\": \"Tatoeba.ca.es\", \"model\": \"Helsinki-NLP/opus-mt-ca-es\", \"pre-processing\": \"normalization + SentencePiece\", \"download_original_weights\": true, \"test_set_translations\": [\"Bon dia, com est\\u00e0s? -> Buenos d\\u00edas, \\u00bfc\\u00f3mo est\\u00e1s?\"], \"test_set_scores\": {\"BLEU\": 74.9, \"chr-F\": 0.863}}}}, {\"user_intent\": \"I have a batch of Catalan sentences and I want to translate them all to Spanish using a pretrained model.\", \"tool_call\": {\"name\": \"opus-mt-ca-es\", \"arguments\": {\"source_languages\": [\"ca\"], \"target_languages\": [\"es\"], \"dataset\": \"Tatoeba.ca.es\", \"model\": \"Helsinki-NLP/opus-mt-ca-es\", \"pre-processing\": \"normalization + SentencePiece\", \"download_original_weights\": true, \"test_set_translations\": [], \"test_set_scores\": {}}}}, {\"user_intent\": \"Check the translation quality of the Catalan to Spanish model using the Tatoeba test set.\", \"tool_call\": {\"name\": \"opus-mt-ca-es\", \"arguments\": {\"source_languages\": [\"ca\"], \"target_languages\": [\"es\"], \"dataset\": \"Tatoeba.ca.es\", \"model\": \"Helsinki-NLP/opus-mt-ca-es\", \"pre-processing\": \"normalization + SentencePiece\", \"download_original_weights\": false, \"test_set_translations\": [\"El gat dorm -> El gato duerme\", \"M'agrada la m\\u00fasica -> Me gusta la m\\u00fasica\"], \"test_set_scores\": {\"BLEU\": 74.9, \"chr-F\": 0.863}}}}]}\n\n# User Query\nI have a Catalan language text and I need to publish it as a blog article in Spanish, so I want to translate my Catalan text into Spanish.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate my Catalan text into Spanish for publication as a blog article.",
                    "tool_description": "Use a Hugging Face transformer model trained on Catalan-to-Spanish translation to convert the input text from Catalan to Spanish, applying preprocessing such as normalization and SentencePiece tokenization.",
                    "expected_response": "A translated Spanish text string corresponding to the input Catalan text, optionally including metadata such as source and target languages and translation quality scores."
                }
            ],
            "extracted_arguments": {
                "source_text": "The text written in Catalan that needs to be translated.",
                "source_languages": "The language code(s) representing the source language (Catalan).",
                "target_languages": "The language code(s) representing the target language (Spanish).",
                "dataset": "The dataset on which the translation model was trained, providing context for its performance.",
                "model": "The specific pre-trained model identifier or architecture to use for translation.",
                "pre_processing": "Any text preprocessing steps applied before translation, such as normalization or tokenization.",
                "download_original_weights": "A boolean flag indicating whether to download the original pre-trained model weights.",
                "test_set_translations": "Optional sample translations used to evaluate model performance.",
                "test_set_scores": "Optional evaluation metrics such as BLEU or chr-F for assessing translation quality."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_314",
        "query": "We are developing an AI chatbot for a Russian company. I want to extract features from the clients' text messages in Russian language.",
        "instruction": "Given a `feature extraction` task, retrieve tools that specialize in processing Russian text to extract and analyze linguistic features, specifically catering to language understanding and natural language processing requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Russian part of Wikipedia and news data",
                        "accuracy": ""
                    },
                    "description": "RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].",
                    "id": "huggingface_api_15",
                    "name": "DeepPavlov/rubert-base-cased"
                },
                "id": "gorilla_huggingface_tool_15",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 1: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 2: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 3: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 4: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 5: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.",
            "Tool 6: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 7: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 8: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 9: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 10: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 12: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.",
            "Tool 13: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.",
            "Tool 14: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 15: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 17: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 18: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 19: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis."
        ],
        "tool_view": {
            "description": "A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "parameters": {
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of different text sequences the model should generate for a single input prompt.",
                    "required": true,
                    "default": 1,
                    "example": 3
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "default": 512,
                    "example": 256
                },
                "no_repeat_ngram_size": {
                    "type": "number",
                    "description": "Prevents the model from repeating any n-grams of this size in the generated text.",
                    "required": true,
                    "default": 3,
                    "example": 2
                },
                "do_sample": {
                    "type": "boolean",
                    "description": "Whether to use sampling; if false, the model will use greedy decoding.",
                    "required": true,
                    "default": true,
                    "example": false
                },
                "top_k": {
                    "type": "number",
                    "description": "The number of highest probability vocabulary tokens to keep for top-k-filtering during sampling.",
                    "required": true,
                    "default": 50,
                    "example": 30
                },
                "top_p": {
                    "type": "number",
                    "description": "The cumulative probability threshold for nucleus (top-p) sampling. Tokens are considered until the cumulative probability exceeds this value.",
                    "required": true,
                    "default": 0.9,
                    "example": 0.8
                },
                "temperature": {
                    "type": "number",
                    "description": "Controls the randomness of predictions by scaling the logits before applying softmax. Higher values produce more random outputs.",
                    "required": true,
                    "default": 0.6,
                    "example": 1.0
                },
                "mask_token_id": {
                    "type": "string",
                    "description": "The token ID used to represent masked tokens in the input text, typically provided by the tokenizer.",
                    "required": true,
                    "default": "tokenizer.mask_token_id",
                    "example": "tokenizer.mask_token_id"
                },
                "eos_token_id": {
                    "type": "string",
                    "description": "The token ID representing the end-of-sequence marker, as defined by the tokenizer.",
                    "required": true,
                    "default": "tokenizer.eos_token_id",
                    "example": "tokenizer.eos_token_id"
                },
                "unk_token_id": {
                    "type": "string",
                    "description": "The token ID used for unknown tokens that the model cannot recognize.",
                    "required": true,
                    "default": "tokenizer.unk_token_id",
                    "example": "tokenizer.unk_token_id"
                },
                "pad_token_id": {
                    "type": "string",
                    "description": "The token ID used for padding sequences to a fixed length.",
                    "required": true,
                    "default": "tokenizer.pad_token_id",
                    "example": "tokenizer.pad_token_id"
                },
                "device": {
                    "type": "string",
                    "description": "Specifies the hardware device to run the model on, such as 'cpu' or 'cuda'.",
                    "required": true,
                    "default": "cpu",
                    "example": "cuda"
                }
            },
            "response": "The tool described performs text generation using a pre-trained and fine-tuned conversational model, specifically a version of DialoGPT for the Russian language, trained on Russian forums and Telegram chat data.\n\n### Expected Response Structure\n\nThe response from this tool is the result of text generation by the model. The key elements of the response would include:\n\n1. **Generated Text (or Sequences):**\n   - The tool will return one or more text sequences generated by the model. This output is based on the model’s inference using the input provided to it (e.g., a prompt or conversation history).\n   - The number of returned sequences is specified by the `num_return_sequences` parameter (default is 1). This field will contain the generated text(s) in string format.\n\n2. **Completion Quality Considerations:**\n   - The model uses parameters such as `max_length`, `top_k`, `top_p`, `temperature`, and `no_repeat_ngram_size` to influence the style, coherence, and variability of the output. These parameters control aspects like the length of the response, randomness, and how much repetition is allowed.\n   - Based on the provided settings, the generated text will reflect:\n     - A reasonable maximum length (`max_length`).\n     - Sampling behavior (influenced by `temperature`, `top_k`, and `top_p` settings).\n     - Avoidance of repetition at the n-gram level (`no_repeat_ngram_size`).\n\n3. **Tokenization Information:**\n   - The response may include tokenized representations if the underlying framework or function generates any internal data related to tokenization. However, typically, this is abstracted and only the generated output (in string form) is returned.\n\n4. **Metadata:**\n   - If any metadata or additional information about the generation process is included, it might consist of:\n     - **Model Details:** The specific version of the model used, which in this case is the `ruDialoGpt3-medium-finetuned-telegram` variant fine-tuned on Telegram data.\n     - **Parameters Used:** Details of the specific configuration parameters like `device` and model settings might be referenced, though this is more likely to be internal unless explicitly requested.\n\n### Summary:\nThe tool's response is primarily a string of text (or multiple strings) representing the model's generated output, based on the conversational or prompt context provided. This text will adhere to the length, randomness, and coherence characteristics specified in the parameters. Additional metadata about the model or process is typically not included unless explicitly requested, and any token-level data is abstracted for general use cases.",
            "examples": [
                {
                    "user_intent": "I want the bot to continue my Telegram conversation in Russian after I send 'Привет, как дела?'",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "Привет, как дела?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a friendly Russian response to someone complaining about a long day at work.",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "У меня был ужасно долгий день на работе."
                        }
                    }
                },
                {
                    "user_intent": "Simulate a casual chat in Russian about weekend plans with the Telegram bot.",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "Какие у тебя планы на выходные?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 1: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 2: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 3: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 4: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 5: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.\nTool 6: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 7: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 8: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 9: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 10: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 12: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.\nTool 13: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.\nTool 14: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 15: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 17: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 18: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 19: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\", \"parameters\": {\"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of different text sequences the model should generate for a single input prompt.\", \"required\": true, \"default\": 1, \"example\": 3}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"default\": 512, \"example\": 256}, \"no_repeat_ngram_size\": {\"type\": \"number\", \"description\": \"Prevents the model from repeating any n-grams of this size in the generated text.\", \"required\": true, \"default\": 3, \"example\": 2}, \"do_sample\": {\"type\": \"boolean\", \"description\": \"Whether to use sampling; if false, the model will use greedy decoding.\", \"required\": true, \"default\": true, \"example\": false}, \"top_k\": {\"type\": \"number\", \"description\": \"The number of highest probability vocabulary tokens to keep for top-k-filtering during sampling.\", \"required\": true, \"default\": 50, \"example\": 30}, \"top_p\": {\"type\": \"number\", \"description\": \"The cumulative probability threshold for nucleus (top-p) sampling. Tokens are considered until the cumulative probability exceeds this value.\", \"required\": true, \"default\": 0.9, \"example\": 0.8}, \"temperature\": {\"type\": \"number\", \"description\": \"Controls the randomness of predictions by scaling the logits before applying softmax. Higher values produce more random outputs.\", \"required\": true, \"default\": 0.6, \"example\": 1.0}, \"mask_token_id\": {\"type\": \"string\", \"description\": \"The token ID used to represent masked tokens in the input text, typically provided by the tokenizer.\", \"required\": true, \"default\": \"tokenizer.mask_token_id\", \"example\": \"tokenizer.mask_token_id\"}, \"eos_token_id\": {\"type\": \"string\", \"description\": \"The token ID representing the end-of-sequence marker, as defined by the tokenizer.\", \"required\": true, \"default\": \"tokenizer.eos_token_id\", \"example\": \"tokenizer.eos_token_id\"}, \"unk_token_id\": {\"type\": \"string\", \"description\": \"The token ID used for unknown tokens that the model cannot recognize.\", \"required\": true, \"default\": \"tokenizer.unk_token_id\", \"example\": \"tokenizer.unk_token_id\"}, \"pad_token_id\": {\"type\": \"string\", \"description\": \"The token ID used for padding sequences to a fixed length.\", \"required\": true, \"default\": \"tokenizer.pad_token_id\", \"example\": \"tokenizer.pad_token_id\"}, \"device\": {\"type\": \"string\", \"description\": \"Specifies the hardware device to run the model on, such as 'cpu' or 'cuda'.\", \"required\": true, \"default\": \"cpu\", \"example\": \"cuda\"}}, \"response\": \"The tool described performs text generation using a pre-trained and fine-tuned conversational model, specifically a version of DialoGPT for the Russian language, trained on Russian forums and Telegram chat data.\\n\\n### Expected Response Structure\\n\\nThe response from this tool is the result of text generation by the model. The key elements of the response would include:\\n\\n1. **Generated Text (or Sequences):**\\n   - The tool will return one or more text sequences generated by the model. This output is based on the model\\u2019s inference using the input provided to it (e.g., a prompt or conversation history).\\n   - The number of returned sequences is specified by the `num_return_sequences` parameter (default is 1). This field will contain the generated text(s) in string format.\\n\\n2. **Completion Quality Considerations:**\\n   - The model uses parameters such as `max_length`, `top_k`, `top_p`, `temperature`, and `no_repeat_ngram_size` to influence the style, coherence, and variability of the output. These parameters control aspects like the length of the response, randomness, and how much repetition is allowed.\\n   - Based on the provided settings, the generated text will reflect:\\n     - A reasonable maximum length (`max_length`).\\n     - Sampling behavior (influenced by `temperature`, `top_k`, and `top_p` settings).\\n     - Avoidance of repetition at the n-gram level (`no_repeat_ngram_size`).\\n\\n3. **Tokenization Information:**\\n   - The response may include tokenized representations if the underlying framework or function generates any internal data related to tokenization. However, typically, this is abstracted and only the generated output (in string form) is returned.\\n\\n4. **Metadata:**\\n   - If any metadata or additional information about the generation process is included, it might consist of:\\n     - **Model Details:** The specific version of the model used, which in this case is the `ruDialoGpt3-medium-finetuned-telegram` variant fine-tuned on Telegram data.\\n     - **Parameters Used:** Details of the specific configuration parameters like `device` and model settings might be referenced, though this is more likely to be internal unless explicitly requested.\\n\\n### Summary:\\nThe tool's response is primarily a string of text (or multiple strings) representing the model's generated output, based on the conversational or prompt context provided. This text will adhere to the length, randomness, and coherence characteristics specified in the parameters. Additional metadata about the model or process is typically not included unless explicitly requested, and any token-level data is abstracted for general use cases.\", \"examples\": [{\"user_intent\": \"I want the bot to continue my Telegram conversation in Russian after I send '\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?'\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?\"}}}, {\"user_intent\": \"Generate a friendly Russian response to someone complaining about a long day at work.\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u0423 \\u043c\\u0435\\u043d\\u044f \\u0431\\u044b\\u043b \\u0443\\u0436\\u0430\\u0441\\u043d\\u043e \\u0434\\u043e\\u043b\\u0433\\u0438\\u0439 \\u0434\\u0435\\u043d\\u044c \\u043d\\u0430 \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435.\"}}}, {\"user_intent\": \"Simulate a casual chat in Russian about weekend plans with the Telegram bot.\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u041a\\u0430\\u043a\\u0438\\u0435 \\u0443 \\u0442\\u0435\\u0431\\u044f \\u043f\\u043b\\u0430\\u043d\\u044b \\u043d\\u0430 \\u0432\\u044b\\u0445\\u043e\\u0434\\u043d\\u044b\\u0435?\"}}}]}\n\n# User Query\nWe are developing an AI chatbot for a Russian company. I want to extract features from the clients' text messages in Russian language.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract meaningful features from Russian text messages.",
                    "tool_description": "Use a pre-trained model to extract contextual features or embeddings from Russian text for natural language processing tasks like semantic analysis or feature extraction.",
                    "expected_response": "A set of feature vectors or embeddings representing the input text."
                }
            ],
            "extracted_arguments": {
                "text": "The Russian text message to extract features from."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_79",
        "query": "Our robotics team needs a way to stabilize a two-wheeled self-balancing robot. Would you suggest and apply any RL algorithm for this purpose?",
        "instruction": "Given a `robot stabilization` task, retrieve tools that apply reinforcement learning algorithms to balance robotic systems by utilizing frameworks like Stable-Baselines3 for effective self-balancing control and support continuous learning and adaptation.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_901",
                    "name": "sb3/ppo-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_894",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 1: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.",
            "Tool 2: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 3: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 6: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 7: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 9: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 10: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 11: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 12: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 13: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 14: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 15: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 17: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 18: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 19: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch."
        ],
        "tool_view": {
            "description": "This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "parameters": {},
            "response": "The tool provides a question-answering functionality using the `deepset/bert-base-cased-squad2` model, which is based on BERT and trained on the SQuAD v2 dataset. When given a `context` and a `question`, the model returns an answer that best matches the provided context.\n\n**Response Structure:**\n\n- **answer**: This field contains the text of the model's predicted answer to the question, derived from the provided context.\n- **score**: This is a numerical value representing the confidence score of the model for the predicted answer. It indicates how likely the model believes the answer is correct, based on its internal mechanisms and the provided context.\n- **start**: The character position in the `context` where the answer starts. This can be used to identify the exact location of the answer within the context text.\n- **end**: The character position in the `context` where the answer ends. This is also useful for extracting the specific span of text corresponding to the answer.\n\nIn summary, the response provides the predicted answer along with the confidence score and the positions (start and end) of the answer within the original context.",
            "examples": [
                {
                    "user_intent": "I want to know where this BERT model can be used for on-demand inference.",
                    "tool_call": {
                        "name": "deepset/bert-base-cased-squad2",
                        "arguments": {
                            "context": "This model can be loaded on the Inference API on-demand.",
                            "question": "Where can this model be loaded?"
                        }
                    }
                },
                {
                    "user_intent": "Find out which dataset was used to train this model.",
                    "tool_call": {
                        "name": "deepset/bert-base-cased-squad2",
                        "arguments": {
                            "context": "This is a BERT base cased model trained on SQuAD v2.",
                            "question": "Which dataset was this model trained on?"
                        }
                    }
                },
                {
                    "user_intent": "Check the exact match and F1 performance metrics of this model.",
                    "tool_call": {
                        "name": "deepset/bert-base-cased-squad2",
                        "arguments": {
                            "context": "The model achieves 71.152 exact match and 74.671 F1 score on the SQuAD v2 dataset.",
                            "question": "What are the exact match and F1 scores of this model?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 1: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.\nTool 2: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 3: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 6: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 7: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 8: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 9: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 10: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 11: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 12: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 13: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 14: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 15: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 17: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 18: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 19: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\", \"parameters\": {}, \"response\": \"The tool provides a question-answering functionality using the `deepset/bert-base-cased-squad2` model, which is based on BERT and trained on the SQuAD v2 dataset. When given a `context` and a `question`, the model returns an answer that best matches the provided context.\\n\\n**Response Structure:**\\n\\n- **answer**: This field contains the text of the model's predicted answer to the question, derived from the provided context.\\n- **score**: This is a numerical value representing the confidence score of the model for the predicted answer. It indicates how likely the model believes the answer is correct, based on its internal mechanisms and the provided context.\\n- **start**: The character position in the `context` where the answer starts. This can be used to identify the exact location of the answer within the context text.\\n- **end**: The character position in the `context` where the answer ends. This is also useful for extracting the specific span of text corresponding to the answer.\\n\\nIn summary, the response provides the predicted answer along with the confidence score and the positions (start and end) of the answer within the original context.\", \"examples\": [{\"user_intent\": \"I want to know where this BERT model can be used for on-demand inference.\", \"tool_call\": {\"name\": \"deepset/bert-base-cased-squad2\", \"arguments\": {\"context\": \"This model can be loaded on the Inference API on-demand.\", \"question\": \"Where can this model be loaded?\"}}}, {\"user_intent\": \"Find out which dataset was used to train this model.\", \"tool_call\": {\"name\": \"deepset/bert-base-cased-squad2\", \"arguments\": {\"context\": \"This is a BERT base cased model trained on SQuAD v2.\", \"question\": \"Which dataset was this model trained on?\"}}}, {\"user_intent\": \"Check the exact match and F1 performance metrics of this model.\", \"tool_call\": {\"name\": \"deepset/bert-base-cased-squad2\", \"arguments\": {\"context\": \"The model achieves 71.152 exact match and 74.671 F1 score on the SQuAD v2 dataset.\", \"question\": \"What are the exact match and F1 scores of this model?\"}}}]}\n\n# User Query\nOur robotics team needs a way to stabilize a two-wheeled self-balancing robot. Would you suggest and apply any RL algorithm for this purpose?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find a pre-trained reinforcement learning agent suitable for stabilizing a two-wheeled self-balancing robot.",
                    "tool_description": "Retrieve a pre-trained reinforcement learning agent capable of controlling and stabilizing a two-wheeled or inverted pendulum environment, such as CartPole, using established RL algorithms like PPO or DQN.",
                    "expected_response": "A ready-to-use RL agent object with policy and environment interaction methods that can be directly applied for stabilization tasks."
                },
                {
                    "user_intent": "Apply the selected RL agent to stabilize the two-wheeled robot in simulation or real-world testing.",
                    "tool_description": "Deploy the chosen RL agent on a simulated or physical two-wheeled robot environment to perform control actions and maintain balance based on learned policies.",
                    "expected_response": "Performance metrics showing the robot's stability over time, such as episode length, balance success rate, or action sequences executed by the agent."
                }
            ],
            "extracted_arguments": {
                "robot_type": "The type of robot to be stabilized, e.g., two-wheeled self-balancing robot.",
                "rl_algorithm": "The reinforcement learning algorithm or pre-trained agent to be applied, e.g., PPO or DQN.",
                "environment": "The simulation or physical setup representing the robot's dynamics for applying the RL agent."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_249",
        "query": "Our e-commerce platform needs automatic classification of product images without retraining. Develop a classifier for the e-commerce platform.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize zero-shot learning to classify product images based on their content without requiring retraining, leveraging pre-trained models to ensure adaptability across diverse categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')",
                    "api_arguments": {
                        "model_name": "OFA-Sys/chinese-clip-vit-large-patch14"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = []\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]",
                    "performance": {
                        "dataset": "MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC",
                        "accuracy": "Varies depending on the dataset"
                    },
                    "description": "Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.",
                    "id": "huggingface_api_375",
                    "name": "chinese-clip-vit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_371",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.",
            "Tool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 2: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 3: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 7: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 8: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 9: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 10: A CLIP ViT L/14 model trained on the LAION-2B English subset, designed for zero-shot image classification. It enables classification of images into arbitrary categories without task-specific training, making it useful for research and interdisciplinary studies of image understanding and model impact.",
            "Tool 11: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.",
            "Tool 12: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.",
            "Tool 13: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.",
            "Tool 14: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.",
            "Tool 15: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 16: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 17: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.",
            "Tool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 19: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path to the image that will be classified by the zero-shot image classification model.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "class_names": {
                    "type": "array",
                    "description": "A list of class names that the image can be classified into. Each class name is provided as a string in the list.",
                    "required": true,
                    "example": [
                        "cat",
                        "dog",
                        "car"
                    ]
                }
            },
            "response": "The tool produces a list of classification results for a given image evaluated against a set of candidate class names. Each element in the response typically represents one class label and contains the following components:\n\n1. **`label`** – A string representing the name of the predicted class from the provided list of class names. This indicates which class the model associates with the image.  \n\n2. **`score`** – A numerical value (usually a float between 0 and 1) representing the model’s confidence or probability that the image belongs to the corresponding class. Higher scores indicate greater confidence.  \n\nThe list is usually ordered in descending order of confidence, with the most likely class appearing first. The response allows downstream applications to select the top prediction or consider multiple likely classes along with their associated confidence scores.",
            "examples": [
                {
                    "user_intent": "I have a photo of my dog and I want to check if it matches 'dog', 'cat', or 'rabbit'.",
                    "tool_call": {
                        "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft",
                        "arguments": {
                            "image_path": "images/my_dog.jpg",
                            "class_names": [
                                "dog",
                                "cat",
                                "rabbit"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Classify this nature image to see if it contains 'forest', 'mountain', or 'lake'.",
                    "tool_call": {
                        "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft",
                        "arguments": {
                            "image_path": "images/nature_scene.png",
                            "class_names": [
                                "forest",
                                "mountain",
                                "lake"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I want to identify what type of fruit is in this picture: 'apple', 'banana', 'orange', or 'grape'.",
                    "tool_call": {
                        "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft",
                        "arguments": {
                            "image_path": "images/fruit_basket.jpg",
                            "class_names": [
                                "apple",
                                "banana",
                                "orange",
                                "grape"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.\nTool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 2: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 3: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 7: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 8: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 9: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 10: A CLIP ViT L/14 model trained on the LAION-2B English subset, designed for zero-shot image classification. It enables classification of images into arbitrary categories without task-specific training, making it useful for research and interdisciplinary studies of image understanding and model impact.\nTool 11: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.\nTool 12: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.\nTool 13: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\nTool 14: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.\nTool 15: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 16: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 17: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.\nTool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 19: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face\\u2019s pipeline API, making it easy to integrate into Python applications.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path to the image that will be classified by the zero-shot image classification model.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"class_names\": {\"type\": \"array\", \"description\": \"A list of class names that the image can be classified into. Each class name is provided as a string in the list.\", \"required\": true, \"example\": [\"cat\", \"dog\", \"car\"]}}, \"response\": \"The tool produces a list of classification results for a given image evaluated against a set of candidate class names. Each element in the response typically represents one class label and contains the following components:\\n\\n1. **`label`** \\u2013 A string representing the name of the predicted class from the provided list of class names. This indicates which class the model associates with the image.  \\n\\n2. **`score`** \\u2013 A numerical value (usually a float between 0 and 1) representing the model\\u2019s confidence or probability that the image belongs to the corresponding class. Higher scores indicate greater confidence.  \\n\\nThe list is usually ordered in descending order of confidence, with the most likely class appearing first. The response allows downstream applications to select the top prediction or consider multiple likely classes along with their associated confidence scores.\", \"examples\": [{\"user_intent\": \"I have a photo of my dog and I want to check if it matches 'dog', 'cat', or 'rabbit'.\", \"tool_call\": {\"name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\", \"arguments\": {\"image_path\": \"images/my_dog.jpg\", \"class_names\": [\"dog\", \"cat\", \"rabbit\"]}}}, {\"user_intent\": \"Classify this nature image to see if it contains 'forest', 'mountain', or 'lake'.\", \"tool_call\": {\"name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\", \"arguments\": {\"image_path\": \"images/nature_scene.png\", \"class_names\": [\"forest\", \"mountain\", \"lake\"]}}}, {\"user_intent\": \"I want to identify what type of fruit is in this picture: 'apple', 'banana', 'orange', or 'grape'.\", \"tool_call\": {\"name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\", \"arguments\": {\"image_path\": \"images/fruit_basket.jpg\", \"class_names\": [\"apple\", \"banana\", \"orange\", \"grape\"]}}}]}\n\n# User Query\nOur e-commerce platform needs automatic classification of product images without retraining. Develop a classifier for the e-commerce platform.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically classify product images for the e-commerce platform without retraining.",
                    "tool_description": "Perform zero-shot image classification using a CLIP-based model that can categorize images into arbitrary classes without task-specific training.",
                    "expected_response": "A list of classification results for each product image, including predicted class labels and confidence scores for each candidate category."
                }
            ],
            "extracted_arguments": {
                "image_path": "The file path to the product image to be classified.",
                "class_names": "A list of possible product category names that the image can be classified into."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_379",
        "query": "A company developing an application for transcribing customer service calls requires a model that can understand spoken language.",
        "instruction": "Given an `audio transcription` task, retrieve tools that excel in understanding and transcribing spoken language by accurately processing audio inputs to produce textual outputs, suitable for applications like customer service call transcription, and ensure compatibility with relevant machine learning frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transcription and Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')",
                    "api_arguments": [
                        "sample",
                        "sampling_rate",
                        "language",
                        "task",
                        "skip_special_tokens"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": "from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\n\nmodel.config.forced_decoder_ids = None\n\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\nsample = ds[0][audio]\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\n\npredicted_ids = model.generate(input_features)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "LibriSpeech (clean)",
                                "accuracy": 2.9
                            },
                            {
                                "name": "LibriSpeech (other)",
                                "accuracy": 5.9
                            },
                            {
                                "name": "Common Voice 11.0",
                                "accuracy": 53.87
                            }
                        ]
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.",
                    "id": "huggingface_api_767",
                    "name": "openai/whisper-medium"
                },
                "id": "gorilla_huggingface_tool_762",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.",
            "Tool 1: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 2: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 3: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 4: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 5: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 6: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 7: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 9: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 10: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 11: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 12: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 13: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 14: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 15: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 16: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 17: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 18: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 19: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages."
        ],
        "tool_view": {
            "description": "Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.",
            "parameters": {
                "forced_decoder_ids": {
                    "type": "string",
                    "description": "A string representing the decoder prompt IDs used to force the Whisper model to generate output in a specific language and task. Typically obtained via `WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')`.",
                    "required": true,
                    "example": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                }
            },
            "response": "The tool returns a transcription or translation result generated from an audio input. The typical response structure would include the following components:\n\n1. **Predicted IDs**  \n   - A sequence or tensor of token IDs produced by the model’s decoder after processing the audio input.  \n   - These IDs represent the model’s predicted text in a tokenized form.\n\n2. **Decoded Text (Transcription/Translation)**  \n   - A string or list of strings obtained by decoding the predicted token IDs using the processor.  \n   - This text represents the spoken content of the audio in the target language (e.g., English for transcription, or another language if performing translation).  \n   - Special tokens used during decoding (such as start-of-sequence or padding tokens) are typically removed or skipped.\n\n3. **Optional Metadata (depending on implementation)**  \n   - Information about processing, such as the sampling rate, length of the audio, or decoder configuration.  \n   - Could include confidence scores or timestamps if additional decoding steps or features are enabled, though not explicitly required by the default pipeline.\n\nIn essence, the primary semantic content of the response is the textual representation of the audio input, either as a direct transcription or as a translated version, with the model-generated token IDs serving as the intermediate representation.",
            "examples": [
                {
                    "user_intent": "Transcribe a short English audio clip of a podcast episode into text.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "openai/whisper-tiny",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                        }
                    }
                },
                {
                    "user_intent": "Convert a French interview recording into English text using automatic translation.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "openai/whisper-tiny",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='translate')"
                        }
                    }
                },
                {
                    "user_intent": "Transcribe a noisy audiobook excerpt from the LibriSpeech dataset for testing ASR performance.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "openai/whisper-tiny",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.\nTool 1: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 2: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 3: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 4: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 5: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 6: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 7: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 9: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 10: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 11: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 12: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 13: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 14: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 15: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 16: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 17: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 18: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 19: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.\", \"parameters\": {\"forced_decoder_ids\": {\"type\": \"string\", \"description\": \"A string representing the decoder prompt IDs used to force the Whisper model to generate output in a specific language and task. Typically obtained via `WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')`.\", \"required\": true, \"example\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}, \"response\": \"The tool returns a transcription or translation result generated from an audio input. The typical response structure would include the following components:\\n\\n1. **Predicted IDs**  \\n   - A sequence or tensor of token IDs produced by the model\\u2019s decoder after processing the audio input.  \\n   - These IDs represent the model\\u2019s predicted text in a tokenized form.\\n\\n2. **Decoded Text (Transcription/Translation)**  \\n   - A string or list of strings obtained by decoding the predicted token IDs using the processor.  \\n   - This text represents the spoken content of the audio in the target language (e.g., English for transcription, or another language if performing translation).  \\n   - Special tokens used during decoding (such as start-of-sequence or padding tokens) are typically removed or skipped.\\n\\n3. **Optional Metadata (depending on implementation)**  \\n   - Information about processing, such as the sampling rate, length of the audio, or decoder configuration.  \\n   - Could include confidence scores or timestamps if additional decoding steps or features are enabled, though not explicitly required by the default pipeline.\\n\\nIn essence, the primary semantic content of the response is the textual representation of the audio input, either as a direct transcription or as a translated version, with the model-generated token IDs serving as the intermediate representation.\", \"examples\": [{\"user_intent\": \"Transcribe a short English audio clip of a podcast episode into text.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"openai/whisper-tiny\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}}, {\"user_intent\": \"Convert a French interview recording into English text using automatic translation.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"openai/whisper-tiny\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='translate')\"}}}, {\"user_intent\": \"Transcribe a noisy audiobook excerpt from the LibriSpeech dataset for testing ASR performance.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"openai/whisper-tiny\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}}]}\n\n# User Query\nA company developing an application for transcribing customer service calls requires a model that can understand spoken language.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transcribe spoken customer service calls into text for analysis.",
                    "tool_description": "Use a pre-trained automatic speech recognition model to convert audio recordings of spoken language into written text without requiring fine-tuning.",
                    "expected_response": "A transcription of the audio input, including the decoded text representing the spoken content and optional metadata such as token IDs or timestamps."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The audio recording of customer service calls in a supported format for speech recognition."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_146",
        "query": "A famous writer is working on a novel. He needs your help to predict the punctuation marks needed in his written draft.",
        "instruction": "Given a `punctuation prediction` task, retrieve tools that are designed to assist in natural language processing by analyzing text inputs and accurately determining the necessary punctuation marks to enhance the coherence and readability of the written content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('token-classification')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "multilingual",
                        "accuracy": 0.98
                    },
                    "description": "A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.",
                    "id": "huggingface_api_934",
                    "name": "kredor/punctuate-all"
                },
                "id": "gorilla_huggingface_tool_415",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 2: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 3: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 5: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 6: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 9: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 11: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 12: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 14: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 15: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 16: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 17: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.",
            "Tool 18: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 19: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed."
        ],
        "tool_view": {
            "description": "This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.",
                    "required": true,
                    "example": "My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller"
                }
            },
            "response": "The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\n\n### Expected Response Description:\n\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\n- Periods (.)\n- Commas (,)\n- Question marks (?)\n- Dashes (-)\n- Colons (:)\n\n### Key Components of the Response:\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\n  \n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\n\n",
            "examples": [
                {
                    "user_intent": "I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps"
                        }
                    }
                },
                {
                    "user_intent": "I need proper punctuation for a German interview transcript I just transcribed.",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview"
                        }
                    }
                },
                {
                    "user_intent": "Can you add punctuation to this Italian speech transcription so I can publish it as an article?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 2: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 3: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 5: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 6: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 9: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 11: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 12: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 14: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 15: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 16: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 17: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\nTool 18: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 19: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.\", \"required\": true, \"example\": \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\\u00fcller\"}}, \"response\": \"The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\\n\\n### Expected Response Description:\\n\\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\\n- Periods (.)\\n- Commas (,)\\n- Question marks (?)\\n- Dashes (-)\\n- Colons (:)\\n\\n### Key Components of the Response:\\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\\n  \\n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\\n\\n\", \"examples\": [{\"user_intent\": \"I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps\"}}}, {\"user_intent\": \"I need proper punctuation for a German interview transcript I just transcribed.\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview\"}}}, {\"user_intent\": \"Can you add punctuation to this Italian speech transcription so I can publish it as an article?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo\"}}}]}\n\n# User Query\nA famous writer is working on a novel. He needs your help to predict the punctuation marks needed in his written draft.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to add punctuation to a draft of a novel to make it readable and correctly formatted.",
                    "tool_description": "Predict and restore punctuation in text for languages such as English, German, French, and Italian, inserting periods, commas, question marks, dashes, and colons to improve readability.",
                    "expected_response": "A text string with predicted punctuation marks inserted at appropriate locations throughout the draft."
                }
            ],
            "extracted_arguments": {
                "text": "The unpunctuated draft text of the novel that requires punctuation prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_429",
        "query": "We have a large text dataset and want to extract some important features from it for our data analysis.",
        "instruction": "Given a `text feature extraction` task, retrieve tools that can process large text datasets to identify and extract important features, facilitating data analysis by leveraging natural language processing capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('YituTech/conv-bert-base')",
                    "api_arguments": "N/A",
                    "python_environment_requirements": "transformers",
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.",
                    "id": "huggingface_api_0",
                    "name": "YituTech/conv-bert-base"
                },
                "id": "gorilla_huggingface_tool_0",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "Tool 1: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.",
            "Tool 2: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 3: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 5: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "Tool 6: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.",
            "Tool 7: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 8: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 10: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 14: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 15: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 16: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 17: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 18: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 19: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications."
        ],
        "tool_view": {
            "description": "The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.",
            "parameters": {},
            "response": "The tool `audio-spectrogram-transformer` is designed to generate audio feature representations using a pretrained Audio Spectrogram Transformer (AST) model from Hugging Face. While the JSON does not explicitly define the response format, the typical output of such a model can be described as follows:\n\n- **Response Type:** A numerical tensor or array representing audio features. This is usually a multi-dimensional array where each dimension encodes different aspects of the audio input.  \n\n- **Components and Meaning:**\n  1. **Batch Dimension (optional):** If multiple audio inputs are processed simultaneously, the first dimension indexes each input in the batch.  \n  2. **Time Frames / Sequence Dimension:** Encodes temporal segments of the input audio, reflecting how the model splits the audio into frames or patches for processing.  \n  3. **Feature Dimension / Embeddings:** Contains learned feature representations for each time frame, capturing spectral, temporal, and potentially semantic characteristics of the audio.  \n  4. **Optional Attention or Metadata (if returned):** Some AST models may provide additional outputs such as attention maps, logits, or class probabilities when fine-tuned for specific tasks, though this tool description focuses on feature extraction.\n\nIn summary, the primary response is a tensor of audio embeddings derived from the input waveform, structured to represent the temporal and spectral properties of the audio for downstream tasks such as classification, similarity measurement, or further modeling.",
            "examples": [
                {
                    "user_intent": "I want to extract audio features using a pre-trained audio spectrogram model.",
                    "tool_call": {
                        "name": "audio-spectrogram-transformer",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Load the MIT pre-trained audio spectrogram transformer model for testing.",
                    "tool_call": {
                        "name": "audio-spectrogram-transformer",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Can I use Hugging Face's AST model to analyze audio data for my project?",
                    "tool_call": {
                        "name": "audio-spectrogram-transformer",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"audio-spectrogram-transformer\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\nTool 1: DePlot is a model that converts images of plots or charts into structured, linearized tables by first translating the visual content into text and then reasoning over that text. This enables users to extract underlying data from visualizations, which can be directly used to prompt large language models for further analysis or reasoning, making it useful for tasks that combine visual understanding with text-based data interpretation.\nTool 2: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 3: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 5: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\nTool 6: A pre-trained ConvBERT model from YituTech, available via Hugging Face Transformers, designed to extract rich text features for natural language processing tasks.\nTool 7: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 8: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 10: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 14: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 15: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 16: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 17: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 18: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 19: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"audio-spectrogram-transformer\\\" tool is used to extract audio spectrogram features using a custom AST (Audio Spectrogram Transformer) model from Hugging Face Transformers. It is designed for testing purposes with Hugging Face repositories, enabling the analysis and processing of audio data.\", \"parameters\": {}, \"response\": \"The tool `audio-spectrogram-transformer` is designed to generate audio feature representations using a pretrained Audio Spectrogram Transformer (AST) model from Hugging Face. While the JSON does not explicitly define the response format, the typical output of such a model can be described as follows:\\n\\n- **Response Type:** A numerical tensor or array representing audio features. This is usually a multi-dimensional array where each dimension encodes different aspects of the audio input.  \\n\\n- **Components and Meaning:**\\n  1. **Batch Dimension (optional):** If multiple audio inputs are processed simultaneously, the first dimension indexes each input in the batch.  \\n  2. **Time Frames / Sequence Dimension:** Encodes temporal segments of the input audio, reflecting how the model splits the audio into frames or patches for processing.  \\n  3. **Feature Dimension / Embeddings:** Contains learned feature representations for each time frame, capturing spectral, temporal, and potentially semantic characteristics of the audio.  \\n  4. **Optional Attention or Metadata (if returned):** Some AST models may provide additional outputs such as attention maps, logits, or class probabilities when fine-tuned for specific tasks, though this tool description focuses on feature extraction.\\n\\nIn summary, the primary response is a tensor of audio embeddings derived from the input waveform, structured to represent the temporal and spectral properties of the audio for downstream tasks such as classification, similarity measurement, or further modeling.\", \"examples\": [{\"user_intent\": \"I want to extract audio features using a pre-trained audio spectrogram model.\", \"tool_call\": {\"name\": \"audio-spectrogram-transformer\", \"arguments\": {}}}, {\"user_intent\": \"Load the MIT pre-trained audio spectrogram transformer model for testing.\", \"tool_call\": {\"name\": \"audio-spectrogram-transformer\", \"arguments\": {}}}, {\"user_intent\": \"Can I use Hugging Face's AST model to analyze audio data for my project?\", \"tool_call\": {\"name\": \"audio-spectrogram-transformer\", \"arguments\": {}}}]}\n\n# User Query\nWe have a large text dataset and want to extract some important features from it for our data analysis.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract meaningful features from my text dataset for analysis.",
                    "tool_description": "Use a pre-trained text feature extraction model to convert raw text into numerical embeddings or feature representations suitable for downstream analysis or modeling.",
                    "expected_response": "A tensor or array where each entry represents the feature vector for a corresponding text input, capturing semantic and contextual characteristics."
                }
            ],
            "extracted_arguments": {
                "text_dataset": "The collection of raw textual data to be processed, provided as a list of strings or a structured text format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_480",
        "query": "In a kitchen, as a robot chef, we need to identify different objects such as fruits and dishes.",
        "instruction": "Given an `object detection` task in a robotic kitchen environment, retrieve tools that perform zero-shot object detection by processing visual inputs for identifying and distinguishing objects such as fruits and dishes, using a pre-trained model.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')",
                    "api_arguments": {
                        "model_name": "google/owlvit-large-patch14"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": [
                        "import requests",
                        "from PIL import Image",
                        "import torch",
                        "from transformers import OwlViTProcessor, OwlViTForObjectDetection",
                        "processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)",
                        "model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)",
                        "url = http://images.cocodataset.org/val2017/000000039769.jpg",
                        "image = Image.open(requests.get(url, stream=True).raw)",
                        "texts = [[a photo of a cat, a photo of a dog]",
                        "inputs = processor(text=texts, images=image, return_tensors=pt)",
                        "outputs = model(**inputs)",
                        "target_sizes = torch.Tensor([image.size[::-1]])",
                        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                        "i = 0",
                        "text = texts[i]",
                        "boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]",
                        "score_threshold = 0.1",
                        "for box, score, label in zip(boxes, scores, labels):",
                        " box = [round(i, 2) for i in box.tolist()]",
                        " if score >= score_threshold:",
                        " print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})"
                    ],
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.",
                    "id": "huggingface_api_218",
                    "name": "google/owlvit-large-patch14"
                },
                "id": "gorilla_huggingface_tool_214",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 1: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 2: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 3: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 6: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 7: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 8: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 9: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.",
            "Tool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 11: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 12: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 13: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.",
            "Tool 14: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 15: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 17: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.",
            "Tool 18: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 19: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\n\n1. **Detections / Results Array**  \n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \n   - Each detection usually contains information such as:\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \"forklift\" or \"person\".\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model’s certainty about the detection.\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\n\n2. **Image-Level Metadata**  \n   - Some responses may include metadata related to the processed image, such as:\n     - Image dimensions (width, height)\n     - The path or URL of the input image\n     - Number of detections in the image\n\n3. **Post-Processing / Rendering Support**  \n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\n\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.",
            "examples": [
                {
                    "user_intent": "I want to detect forklifts in an image from a URL.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect forklifts in a local image file.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "/path/to/local/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run forklift detection with custom confidence and IoU thresholds.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 1: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 2: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 3: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 5: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 6: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 7: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 8: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 9: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\nTool 10: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 11: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 12: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 13: Extract multilingual sentence embeddings using the LaBSE (Language-agnostic BERT Sentence Embedding) model, enabling consistent semantic representation of text across different languages.\nTool 14: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 15: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 17: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.\nTool 18: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 19: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\\n\\n1. **Detections / Results Array**  \\n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \\n   - Each detection usually contains information such as:\\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \\\"forklift\\\" or \\\"person\\\".\\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model\\u2019s certainty about the detection.\\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\\n\\n2. **Image-Level Metadata**  \\n   - Some responses may include metadata related to the processed image, such as:\\n     - Image dimensions (width, height)\\n     - The path or URL of the input image\\n     - Number of detections in the image\\n\\n3. **Post-Processing / Rendering Support**  \\n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \\n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\\n\\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.\", \"examples\": [{\"user_intent\": \"I want to detect forklifts in an image from a URL.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}, {\"user_intent\": \"Detect forklifts in a local image file.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"/path/to/local/forklift_image.jpg\"}}}, {\"user_intent\": \"Run forklift detection with custom confidence and IoU thresholds.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}]}\n\n# User Query\nIn a kitchen, as a robot chef, we need to identify different objects such as fruits and dishes.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect various objects such as fruits and dishes in an image.",
                    "tool_description": "Use an object detection model to identify and locate objects like fruits and dishes in a given image based on descriptive text or pre-defined categories.",
                    "expected_response": "A structured object containing the detected objects, bounding boxes, confidence scores, and object categories for fruits, dishes, and any other relevant items."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to run object detection on. Can be a URL or a local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_199",
        "query": "I have this app for sharing cooking recipes. Users upload photos and ask questions about the showcased recipe. I need to automatically answer their questions based on the recipe image provided.",
        "instruction": "Given a `visual question answering` task, retrieve tools that utilize multimodal models to interpret and respond to questions based on visual input, such as images, by integrating both visual and textual data to generate accurate answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A VisualBERT model for Visual Question Answering.",
                    "id": "huggingface_api_484",
                    "name": "uclanlp/visualbert-vqa"
                },
                "id": "gorilla_huggingface_tool_480",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 1: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 9: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 10: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 13: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 14: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 15: A pre-trained text-to-speech model from ESPnet, using the Amadeus recipe, that converts text into natural-sounding speech and can be used directly without additional training.",
            "Tool 16: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 18: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 19: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA."
        ],
        "tool_view": {
            "description": "A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "parameters": {
                "table": {
                    "type": "object",
                    "description": "A structured table represented as a dictionary where each key is a column name and the corresponding value is a list of column values. This table is the context used to answer the query.",
                    "required": true,
                    "example": {
                        "Repository": [
                            "Transformers",
                            "Datasets",
                            "Tokenizers"
                        ],
                        "Stars": [
                            "36542",
                            "4512",
                            "3934"
                        ],
                        "Contributors": [
                            "651",
                            "77",
                            "34"
                        ],
                        "Programming language": [
                            "Python",
                            "Python",
                            "Rust, Python and NodeJS"
                        ]
                    }
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question about the contents of the table. The model will generate an answer based on this query and the provided table.",
                    "required": true,
                    "example": "How many stars does the transformers repository have?"
                }
            },
            "response": "The tool’s response is a structured object representing the answer to a natural language question about a table. The response typically contains the following components:\n\n1. **answer** – A string or number representing the model’s predicted answer to the user query based on the input table. This is the main result of the table question answering process.\n\n2. **coordinates** (optional) – A list of table cell locations (row and column indices) that the model used to derive the answer. This indicates the exact cell(s) the answer was extracted from, which can be useful for validation or explanation.\n\n3. **aggregation** (optional) – A descriptor of any aggregation operation the model applied to produce the answer, such as `sum`, `average`, `count`, or `none`. This is relevant for questions requiring numerical computation over multiple cells.\n\n4. **cells** (optional) – A list of table cell values that contributed to the answer. This provides a concrete reference to the specific data points the model relied on.\n\nOverall, the response conveys both the answer itself and, optionally, the context within the table that supports that answer. It is structured to allow programmatic use and interpretation of both the result and the reasoning behind it.",
            "examples": [
                {
                    "user_intent": "I want to know how many stars the 'Transformers' repository has.",
                    "tool_call": {
                        "name": "navteca/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Repository": [
                                    "Transformers",
                                    "Datasets",
                                    "Tokenizers"
                                ],
                                "Stars": [
                                    "36542",
                                    "4512",
                                    "3934"
                                ],
                                "Contributors": [
                                    "651",
                                    "77",
                                    "34"
                                ],
                                "Programming language": [
                                    "Python",
                                    "Python",
                                    "Rust, Python and NodeJS"
                                ]
                            },
                            "query": "How many stars does the transformers repository have?"
                        }
                    }
                },
                {
                    "user_intent": "Find out which repository has the most contributors.",
                    "tool_call": {
                        "name": "navteca/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Repository": [
                                    "Transformers",
                                    "Datasets",
                                    "Tokenizers"
                                ],
                                "Stars": [
                                    "36542",
                                    "4512",
                                    "3934"
                                ],
                                "Contributors": [
                                    "651",
                                    "77",
                                    "34"
                                ],
                                "Programming language": [
                                    "Python",
                                    "Python",
                                    "Rust, Python and NodeJS"
                                ]
                            },
                            "query": "Which repository has the most contributors?"
                        }
                    }
                },
                {
                    "user_intent": "Tell me the programming language used for the 'Tokenizers' repository.",
                    "tool_call": {
                        "name": "navteca/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "Repository": [
                                    "Transformers",
                                    "Datasets",
                                    "Tokenizers"
                                ],
                                "Stars": [
                                    "36542",
                                    "4512",
                                    "3934"
                                ],
                                "Contributors": [
                                    "651",
                                    "77",
                                    "34"
                                ],
                                "Programming language": [
                                    "Python",
                                    "Python",
                                    "Rust, Python and NodeJS"
                                ]
                            },
                            "query": "What programming language is used for the tokenizers repository?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 1: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 5: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 7: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 8: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 9: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 10: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 12: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 13: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 14: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 15: A pre-trained text-to-speech model from ESPnet, using the Amadeus recipe, that converts text into natural-sounding speech and can be used directly without additional training.\nTool 16: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 18: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 19: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\", \"parameters\": {\"table\": {\"type\": \"object\", \"description\": \"A structured table represented as a dictionary where each key is a column name and the corresponding value is a list of column values. This table is the context used to answer the query.\", \"required\": true, \"example\": {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"], \"Contributors\": [\"651\", \"77\", \"34\"], \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]}}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question about the contents of the table. The model will generate an answer based on this query and the provided table.\", \"required\": true, \"example\": \"How many stars does the transformers repository have?\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the answer to a natural language question about a table. The response typically contains the following components:\\n\\n1. **answer** \\u2013 A string or number representing the model\\u2019s predicted answer to the user query based on the input table. This is the main result of the table question answering process.\\n\\n2. **coordinates** (optional) \\u2013 A list of table cell locations (row and column indices) that the model used to derive the answer. This indicates the exact cell(s) the answer was extracted from, which can be useful for validation or explanation.\\n\\n3. **aggregation** (optional) \\u2013 A descriptor of any aggregation operation the model applied to produce the answer, such as `sum`, `average`, `count`, or `none`. This is relevant for questions requiring numerical computation over multiple cells.\\n\\n4. **cells** (optional) \\u2013 A list of table cell values that contributed to the answer. This provides a concrete reference to the specific data points the model relied on.\\n\\nOverall, the response conveys both the answer itself and, optionally, the context within the table that supports that answer. It is structured to allow programmatic use and interpretation of both the result and the reasoning behind it.\", \"examples\": [{\"user_intent\": \"I want to know how many stars the 'Transformers' repository has.\", \"tool_call\": {\"name\": \"navteca/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"], \"Contributors\": [\"651\", \"77\", \"34\"], \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]}, \"query\": \"How many stars does the transformers repository have?\"}}}, {\"user_intent\": \"Find out which repository has the most contributors.\", \"tool_call\": {\"name\": \"navteca/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"], \"Contributors\": [\"651\", \"77\", \"34\"], \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]}, \"query\": \"Which repository has the most contributors?\"}}}, {\"user_intent\": \"Tell me the programming language used for the 'Tokenizers' repository.\", \"tool_call\": {\"name\": \"navteca/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"], \"Contributors\": [\"651\", \"77\", \"34\"], \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]}, \"query\": \"What programming language is used for the tokenizers repository?\"}}}]}\n\n# User Query\nI have this app for sharing cooking recipes. Users upload photos and ask questions about the showcased recipe. I need to automatically answer their questions based on the recipe image provided.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to answer questions about a recipe using an image of the dish.",
                    "tool_description": "A visual question answering model that processes an image of a recipe and a natural language question to generate an accurate answer based on the visual content.",
                    "expected_response": "A text string containing the answer to the user's question about the recipe shown in the image, potentially including details such as ingredients, cooking methods, or presentation."
                }
            ],
            "extracted_arguments": {
                "image": "The image file or object representing the recipe dish.",
                "question": "The natural language question about the recipe depicted in the image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_428",
        "query": "I run a call center and I need a system that can identify the person on the other end of the line by analyzing their voice.",
        "instruction": "Given a `voice identification` task, retrieve tools that analyze audio inputs to identify and verify speakers by utilizing models specifically trained for speaker recognition tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForAudioXVector.from_pretrained('anton-l/wav2vec2-base-superb-sv')",
                    "api_arguments": "anton-l/wav2vec2-base-superb-sv",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import AutoProcessor, AutoModelForAudioXVector\nprocessor = AutoProcessor.from_pretrained(anton-l/wav2vec2-base-superb-sv)\nmodel = AutoModelForAudioXVector.from_pretrained(anton-l/wav2vec2-base-superb-sv)",
                    "performance": {
                        "dataset": "superb",
                        "accuracy": "More information needed"
                    },
                    "description": "This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.",
                    "id": "huggingface_api_827",
                    "name": "wav2vec2-base-superb-sv"
                },
                "id": "gorilla_huggingface_tool_822",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 1: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.",
            "Tool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 3: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 4: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 9: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 10: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 11: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 12: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 13: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.",
            "Tool 14: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.",
            "Tool 15: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 16: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 17: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.",
            "Tool 18: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 19: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output."
        ],
        "tool_view": {
            "description": "A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "parameters": {},
            "response": "The tool returns a list of named entities extracted from the input text. Each item in the list represents a single entity and typically includes the following components:\n\n1. **`entity` / `label`**: A string indicating the type of the recognized entity, such as a person, location, organization, or other predefined category used by the model.\n\n2. **`score`**: A numeric confidence score representing the model’s certainty that the entity has been correctly identified and classified. Higher scores indicate higher confidence.\n\n3. **`start`**: An integer specifying the index of the first character of the entity in the original input text.\n\n4. **`end`**: An integer specifying the index of the character immediately after the last character of the entity in the original input text.\n\n5. **`word` / `text`**: The substring from the input text that corresponds to the recognized entity.\n\nThe response is structured as an array of such entity objects, allowing downstream processing to identify, highlight, or categorize entities within the input text.",
            "examples": [
                {
                    "user_intent": "Extract all the names of people and locations from this sentence: 'Alice moved to Paris last year.'",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "Alice moved to Paris last year."
                        }
                    }
                },
                {
                    "user_intent": "Identify organizations and locations mentioned in the news headline: 'Google announced a new office in Berlin.'",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "Google announced a new office in Berlin."
                        }
                    }
                },
                {
                    "user_intent": "Find all named entities in a paragraph describing recent events in New York involving John and Microsoft.",
                    "tool_call": {
                        "name": "dslim/bert-base-NER-uncased",
                        "arguments": {
                            "text": "John attended a conference hosted by Microsoft in New York to discuss AI developments."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 1: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\nTool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 3: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 4: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 6: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 9: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 10: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 11: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 12: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 13: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.\nTool 14: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.\nTool 15: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 16: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 17: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.\nTool 18: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 19: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\", \"parameters\": {}, \"response\": \"The tool returns a list of named entities extracted from the input text. Each item in the list represents a single entity and typically includes the following components:\\n\\n1. **`entity` / `label`**: A string indicating the type of the recognized entity, such as a person, location, organization, or other predefined category used by the model.\\n\\n2. **`score`**: A numeric confidence score representing the model\\u2019s certainty that the entity has been correctly identified and classified. Higher scores indicate higher confidence.\\n\\n3. **`start`**: An integer specifying the index of the first character of the entity in the original input text.\\n\\n4. **`end`**: An integer specifying the index of the character immediately after the last character of the entity in the original input text.\\n\\n5. **`word` / `text`**: The substring from the input text that corresponds to the recognized entity.\\n\\nThe response is structured as an array of such entity objects, allowing downstream processing to identify, highlight, or categorize entities within the input text.\", \"examples\": [{\"user_intent\": \"Extract all the names of people and locations from this sentence: 'Alice moved to Paris last year.'\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"Alice moved to Paris last year.\"}}}, {\"user_intent\": \"Identify organizations and locations mentioned in the news headline: 'Google announced a new office in Berlin.'\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"Google announced a new office in Berlin.\"}}}, {\"user_intent\": \"Find all named entities in a paragraph describing recent events in New York involving John and Microsoft.\", \"tool_call\": {\"name\": \"dslim/bert-base-NER-uncased\", \"arguments\": {\"text\": \"John attended a conference hosted by Microsoft in New York to discuss AI developments.\"}}}]}\n\n# User Query\nI run a call center and I need a system that can identify the person on the other end of the line by analyzing their voice.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect when a person is speaking in an audio recording from a call center conversation.",
                    "tool_description": "Analyze an audio recording to identify segments where voice activity occurs, distinguishing speech from silence or background noise.",
                    "expected_response": "A time-stamped segmentation indicating periods of speech and non-speech within the audio file."
                },
                {
                    "user_intent": "Identify and recognize the speaker based on the detected voice segments.",
                    "tool_description": "Perform speaker identification or speaker diarization on detected speech segments to determine the identity of the person speaking, using voice characteristics.",
                    "expected_response": "A labeled timeline of the audio with speaker identities assigned to each speech segment, including speaker change points and confidence scores."
                }
            ],
            "extracted_arguments": {
                "audio_recording": "The digital audio data of the phone call to be analyzed, in a standard audio file format.",
                "speaker_database": "A collection of known speaker voice profiles used for matching and identification purposes."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_194",
        "query": "I want to have a personal assistant app that can answer questions from a given text.",
        "instruction": "Given a `question answering` task, retrieve tools that process text inputs to find and extract relevant answers from a provided context, enabling the querying of information with high accuracy and speed.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\ncontext = r\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\nprint(\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\n...)",
                    "performance": {
                        "dataset": "SQuAD v1.1",
                        "accuracy": {
                            "Exact Match": 79.6,
                            "F1": 86.996
                        }
                    },
                    "description": "DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.",
                    "id": "huggingface_api_468",
                    "name": "distilbert-base-cased-distilled-squad"
                },
                "id": "gorilla_huggingface_tool_464",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 5: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 6: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 10: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 11: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 13: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 18: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 19: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information."
        ],
        "tool_view": {
            "description": "A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pretrained question answering model to load from the Transformers library.",
                    "required": true,
                    "example": "deepset/xlm-roberta-large-squad2"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or identifier of the tokenizer corresponding to the pretrained model, used to preprocess text inputs for question answering.",
                    "required": true,
                    "example": "deepset/xlm-roberta-large-squad2"
                }
            },
            "response": "The tool provides a model for extractive question answering based on the XLM-RoBERTa architecture. The response from this tool is likely to consist of the following components:\n\n1. **Answer Text**: The tool returns a text response that provides the most relevant answer to the question based on the provided context. This answer is extracted from the context paragraph using the model's understanding.\n\n2. **Start Position**: The tool may return the start position (as an index) of the answer within the context text. This represents where the answer begins in the context, useful for locating the specific segment.\n\n3. **End Position**: The tool may also return the end position (as an index) of the answer within the context text, indicating where the answer ends.\n\n4. **Confidence Scores** (optional): The tool might include confidence scores (e.g., a probability or a score indicating the model's certainty about the extracted answer). These scores provide insight into how confident the model is in its response.\n\nIn summary, the tool's response contains the most relevant text answer to the question, possibly along with its positions in the context and confidence metrics. These components help identify both the extracted information and the level of certainty regarding that information.",
            "examples": [
                {
                    "user_intent": "I want to find the answer to a question from a paragraph about AI advancements.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "What are the main benefits of AI in healthcare?",
                            "context": "AI is increasingly used in healthcare to improve diagnostics, personalize treatment plans, and optimize hospital workflows, leading to better patient outcomes and reduced costs."
                        }
                    }
                },
                {
                    "user_intent": "Answer a question in German using a multilingual model.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "Welche Vorteile bietet erneuerbare Energie?",
                            "context": "Erneuerbare Energien wie Wind- und Solarenergie helfen, den CO2-Ausstoß zu reduzieren und die Abhängigkeit von fossilen Brennstoffen zu verringern."
                        }
                    }
                },
                {
                    "user_intent": "Check if the model can handle tricky questions that might not have an answer in the text.",
                    "tool_call": {
                        "name": "deepset/xlm-roberta-large-squad2",
                        "arguments": {
                            "model_name": "deepset/xlm-roberta-large-squad2",
                            "tokenizer": "deepset/xlm-roberta-large-squad2",
                            "question": "What caused the extinction of dinosaurs in the 1800s?",
                            "context": "Dinosaurs became extinct around 66 million years ago, likely due to an asteroid impact and volcanic activity."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 5: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 6: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 9: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 10: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 11: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 12: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 13: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 15: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 18: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 19: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pretrained question answering model to load from the Transformers library.\", \"required\": true, \"example\": \"deepset/xlm-roberta-large-squad2\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or identifier of the tokenizer corresponding to the pretrained model, used to preprocess text inputs for question answering.\", \"required\": true, \"example\": \"deepset/xlm-roberta-large-squad2\"}}, \"response\": \"The tool provides a model for extractive question answering based on the XLM-RoBERTa architecture. The response from this tool is likely to consist of the following components:\\n\\n1. **Answer Text**: The tool returns a text response that provides the most relevant answer to the question based on the provided context. This answer is extracted from the context paragraph using the model's understanding.\\n\\n2. **Start Position**: The tool may return the start position (as an index) of the answer within the context text. This represents where the answer begins in the context, useful for locating the specific segment.\\n\\n3. **End Position**: The tool may also return the end position (as an index) of the answer within the context text, indicating where the answer ends.\\n\\n4. **Confidence Scores** (optional): The tool might include confidence scores (e.g., a probability or a score indicating the model's certainty about the extracted answer). These scores provide insight into how confident the model is in its response.\\n\\nIn summary, the tool's response contains the most relevant text answer to the question, possibly along with its positions in the context and confidence metrics. These components help identify both the extracted information and the level of certainty regarding that information.\", \"examples\": [{\"user_intent\": \"I want to find the answer to a question from a paragraph about AI advancements.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"What are the main benefits of AI in healthcare?\", \"context\": \"AI is increasingly used in healthcare to improve diagnostics, personalize treatment plans, and optimize hospital workflows, leading to better patient outcomes and reduced costs.\"}}}, {\"user_intent\": \"Answer a question in German using a multilingual model.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"Welche Vorteile bietet erneuerbare Energie?\", \"context\": \"Erneuerbare Energien wie Wind- und Solarenergie helfen, den CO2-Aussto\\u00df zu reduzieren und die Abh\\u00e4ngigkeit von fossilen Brennstoffen zu verringern.\"}}}, {\"user_intent\": \"Check if the model can handle tricky questions that might not have an answer in the text.\", \"tool_call\": {\"name\": \"deepset/xlm-roberta-large-squad2\", \"arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\", \"question\": \"What caused the extinction of dinosaurs in the 1800s?\", \"context\": \"Dinosaurs became extinct around 66 million years ago, likely due to an asteroid impact and volcanic activity.\"}}}]}\n\n# User Query\nI want to have a personal assistant app that can answer questions from a given text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find the answer to a question from a given text.",
                    "tool_description": "Use a multilingual or monolingual extractive question-answering model to identify and extract answers from a provided text context.",
                    "expected_response": "A text answer extracted from the given context, possibly including the start and end positions in the text and confidence scores."
                }
            ],
            "extracted_arguments": {
                "model_name": "The identifier of the pretrained question answering model to use.",
                "tokenizer": "The identifier of the tokenizer corresponding to the selected model for preprocessing text inputs.",
                "context": "The body of text or passage from which the user wants to extract answers.",
                "question": "The natural language question that needs to be answered based on the provided context."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_476",
        "query": "I'm working on creating images of various scenes based on their textual descriptions. The models should also consider the actual positions and poses of the objects in the scenes.",
        "instruction": "Given an `image generation from textual descriptions` task, retrieve tools that create images based on descriptive text inputs and consider specific spatial arrangements and object poses, using advanced techniques like text-to-image diffusion models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Diffusers",
                    "functionality": "Text-to-Image Diffusion Models",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_openpose')",
                    "api_arguments": {
                        "checkpoint": "lllyasviel/control_v11p_sd15_openpose",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux==0.3.0"
                    ],
                    "example_code": {
                        "import_libraries": [
                            "import torch",
                            "import os",
                            "from huggingface_hub import HfApi",
                            "from pathlib import Path",
                            "from diffusers.utils import load_image",
                            "from PIL import Image",
                            "import numpy as np",
                            "from controlnet_aux import OpenposeDetector",
                            "from diffusers import (",
                            " ControlNetModel,",
                            " StableDiffusionControlNetPipeline,",
                            " UniPCMultistepScheduler,",
                            ")"
                        ],
                        "load_model": [
                            "checkpoint = lllyasviel/control_v11p_sd15_openpose",
                            "controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)"
                        ],
                        "example_usage": [
                            "image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)",
                            "prompt = chef in the kitchen",
                            "processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')",
                            "control_image = processor(image, hand_and_face=True)",
                            "control_image.save(./images/control.png)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(",
                            " runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16",
                            ")",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_model_cpu_offload()",
                            "generator = torch.manual_seed(0)",
                            "image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]",
                            "image.save('images/image_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.",
                    "id": "huggingface_api_272",
                    "name": "lllyasviel/control_v11p_sd15_openpose"
                },
                "id": "gorilla_huggingface_tool_268",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 2: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.",
            "Tool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 4: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.",
            "Tool 5: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 7: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 8: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 9: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 10: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 12: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 13: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 14: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 15: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 16: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 17: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.",
            "Tool 18: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 19: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 2: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.\nTool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 4: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.\nTool 5: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 7: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 8: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 9: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 10: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 12: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 13: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 14: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 15: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 16: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 17: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.\nTool 18: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 19: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nI'm working on creating images of various scenes based on their textual descriptions. The models should also consider the actual positions and poses of the objects in the scenes.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate images from textual scene descriptions.",
                    "tool_description": "Create high-quality images based on English text prompts using a text-to-image diffusion model.",
                    "expected_response": "Generated images representing the scene described in the text prompt."
                },
                {
                    "user_intent": "Control the positioning and poses of objects in the generated images.",
                    "tool_description": "Apply a ControlNet model to guide text-to-image generation using reference pose or layout images, ensuring objects follow specific positions and poses while maintaining the style and content dictated by the text prompt.",
                    "expected_response": "Generated images that respect the specified poses and positions of objects within the scene."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "A textual description of the scene to be generated.",
                "pose_control_image": "An image or schematic indicating the desired positions and poses of objects within the scene."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_445",
        "query": "Create a character-like artwork image based on the phrase 'anime-style girl with a guitar'.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that create character-like artwork based on textual descriptions by leveraging multimodal models capable of translating text prompts into detailed and stylistically appropriate images.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')",
                    "api_arguments": {
                        "model_id": "andite/anything-v4.0",
                        "torch_dtype": "torch.float16",
                        "device": "cuda",
                        "prompt": "hatsune_miku"
                    },
                    "python_environment_requirements": {
                        "diffusers": "StableDiffusionPipeline",
                        "torch": "torch"
                    },
                    "example_code": {
                        "from diffusers import StableDiffusionPipeline": "",
                        "import torch": "",
                        "model_id = andite/anything-v4.0": "",
                        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)": "",
                        "pipe = pipe.to(cuda)": "",
                        "prompt = hatsune_miku": "",
                        "image = pipe(prompt).images[0]": "",
                        "image.save(./hatsune_miku.png)": ""
                    },
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.",
                    "id": "huggingface_api_41",
                    "name": "andite/anything-v4.0"
                },
                "id": "gorilla_huggingface_tool_41",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 1: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 2: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 3: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 4: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 5: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 6: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 7: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 8: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 9: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 10: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 12: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 13: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 14: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 15: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 16: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 19: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese."
        ],
        "tool_view": {
            "description": "Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "The main text prompt describing the scene, character, or style to generate in the anime-style image.",
                    "required": true,
                    "example": "1 girl, long hair, smiling, standing in a meadow at sunset"
                },
                "negative_prompt": {
                    "type": "string",
                    "description": "A text prompt specifying elements to avoid in the generated image, such as undesired styles, artifacts, or errors.",
                    "required": true,
                    "example": "low quality, blurry, bad anatomy, watermark, extra digits"
                }
            },
            "response": "The tool produces a single generated image in response to a text prompt. The response structure can be described as follows:\n\n- **images**: A collection (typically a list) of generated images. In most typical usage, only one image is returned, so the list contains a single image object. Each image object represents the visual output generated by the model based on the input prompt and optional negative prompt. The image object is usually in a standard image format compatible with Python image libraries (e.g., PIL Image).\n\nThe main components of the input that influence the response are:\n\n- **prompt**: A text string describing the desired content, style, and attributes of the generated image. This guides the model in producing the output.\n- **negative_prompt** (optional): A text string specifying elements or qualities to avoid in the generated image, helping refine the output and prevent unwanted artifacts.\n\nAdditional semantic notes:\n\n- The returned image is typically a 768x768 pixel RGB image, consistent with the model’s training resolution.\n- The response is structured as an object with an `images` field, where the first element of the list (`images[0]`) is the final generated image that can be directly saved or displayed.  \n\nNo other metadata (e.g., confidence scores or intermediate latent representations) is explicitly returned by this tool in standard usage.",
            "examples": [
                {
                    "user_intent": "Create a high-quality anime illustration of a girl with silver hair in a magical forest, looking surprised, with glowing fireflies around her.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-anime-1.0",
                        "arguments": {
                            "prompt": "anime, masterpiece, high quality, 1girl, silver hair, surprised expression, glowing fireflies, magical forest, detailed background, fantasy, full body",
                            "negative_prompt": "low quality, bad anatomy, blurry, watermark, text, duplicate, worst quality, jpeg artifacts, simple background"
                        }
                    }
                },
                {
                    "user_intent": "Generate a colorful anime-style image of a futuristic cityscape at night with flying cars and neon lights.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-anime-1.0",
                        "arguments": {
                            "prompt": "anime, masterpiece, high quality, futuristic cityscape, night, neon lights, flying cars, detailed architecture, vibrant colors, sci-fi",
                            "negative_prompt": "low quality, blurry, bad proportions, duplicate, simple background, watermark, text"
                        }
                    }
                },
                {
                    "user_intent": "Illustrate a cozy anime-style scene of a girl reading a book in a sunlit cottage with flowers outside.",
                    "tool_call": {
                        "name": "dreamlike-art/dreamlike-anime-1.0",
                        "arguments": {
                            "prompt": "anime, masterpiece, high quality, 1girl, reading a book, cozy cottage interior, sunlight streaming in, flowers outside, serene, soft lighting, detailed",
                            "negative_prompt": "low quality, blurry, bad anatomy, watermark, text, extra digits, worst quality, simple background, duplicate"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 1: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 2: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 3: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 4: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 5: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 6: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 7: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 8: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 9: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 10: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 12: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 13: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 14: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 15: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 16: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 17: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 18: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 19: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like \\u201cmasterpiece\\u201d and \\u201chigh quality,\\u201d allowing users to produce polished, creative anime artwork efficiently.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"The main text prompt describing the scene, character, or style to generate in the anime-style image.\", \"required\": true, \"example\": \"1 girl, long hair, smiling, standing in a meadow at sunset\"}, \"negative_prompt\": {\"type\": \"string\", \"description\": \"A text prompt specifying elements to avoid in the generated image, such as undesired styles, artifacts, or errors.\", \"required\": true, \"example\": \"low quality, blurry, bad anatomy, watermark, extra digits\"}}, \"response\": \"The tool produces a single generated image in response to a text prompt. The response structure can be described as follows:\\n\\n- **images**: A collection (typically a list) of generated images. In most typical usage, only one image is returned, so the list contains a single image object. Each image object represents the visual output generated by the model based on the input prompt and optional negative prompt. The image object is usually in a standard image format compatible with Python image libraries (e.g., PIL Image).\\n\\nThe main components of the input that influence the response are:\\n\\n- **prompt**: A text string describing the desired content, style, and attributes of the generated image. This guides the model in producing the output.\\n- **negative_prompt** (optional): A text string specifying elements or qualities to avoid in the generated image, helping refine the output and prevent unwanted artifacts.\\n\\nAdditional semantic notes:\\n\\n- The returned image is typically a 768x768 pixel RGB image, consistent with the model\\u2019s training resolution.\\n- The response is structured as an object with an `images` field, where the first element of the list (`images[0]`) is the final generated image that can be directly saved or displayed.  \\n\\nNo other metadata (e.g., confidence scores or intermediate latent representations) is explicitly returned by this tool in standard usage.\", \"examples\": [{\"user_intent\": \"Create a high-quality anime illustration of a girl with silver hair in a magical forest, looking surprised, with glowing fireflies around her.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-anime-1.0\", \"arguments\": {\"prompt\": \"anime, masterpiece, high quality, 1girl, silver hair, surprised expression, glowing fireflies, magical forest, detailed background, fantasy, full body\", \"negative_prompt\": \"low quality, bad anatomy, blurry, watermark, text, duplicate, worst quality, jpeg artifacts, simple background\"}}}, {\"user_intent\": \"Generate a colorful anime-style image of a futuristic cityscape at night with flying cars and neon lights.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-anime-1.0\", \"arguments\": {\"prompt\": \"anime, masterpiece, high quality, futuristic cityscape, night, neon lights, flying cars, detailed architecture, vibrant colors, sci-fi\", \"negative_prompt\": \"low quality, blurry, bad proportions, duplicate, simple background, watermark, text\"}}}, {\"user_intent\": \"Illustrate a cozy anime-style scene of a girl reading a book in a sunlit cottage with flowers outside.\", \"tool_call\": {\"name\": \"dreamlike-art/dreamlike-anime-1.0\", \"arguments\": {\"prompt\": \"anime, masterpiece, high quality, 1girl, reading a book, cozy cottage interior, sunlight streaming in, flowers outside, serene, soft lighting, detailed\", \"negative_prompt\": \"low quality, blurry, bad anatomy, watermark, text, extra digits, worst quality, simple background, duplicate\"}}}]}\n\n# User Query\nCreate a character-like artwork image based on the phrase 'anime-style girl with a guitar'.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an anime-style image of a girl holding a guitar based on a descriptive phrase.",
                    "tool_description": "Create a high-quality anime-style illustration from a text prompt, focusing on character design and accessories specified in the prompt.",
                    "expected_response": "A single generated anime-style image object representing the described character with a guitar."
                }
            ],
            "extracted_arguments": {
                "prompt": "A descriptive text string specifying the anime-style character, pose, objects, and visual details to include in the image.",
                "negative_prompt": "A descriptive text string specifying visual elements, artifacts, or styles to avoid in the generated image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_205",
        "query": "We have generated user reviews for movies. We need to check user opinions about the movie 'Inception' based on their reviews.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that process user reviews to determine opinions about specific movies, using natural language processing to classify sentiment as positive or negative based on the content of the reviews.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')",
                    "api_arguments": "text, candidate_labels",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\nresult = nlp('The movie was great!', ['positive', 'negative'])\nprint(result)",
                    "performance": {
                        "dataset": "MNLI",
                        "accuracy": {
                            "matched_acc": "89.19",
                            "mismatched_acc": "89.01"
                        }
                    },
                    "description": "distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.",
                    "id": "huggingface_api_511",
                    "name": "valhalla/distilbart-mnli-12-6"
                },
                "id": "gorilla_huggingface_tool_507",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 4: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 7: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 9: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 10: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 11: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 12: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 13: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.",
            "Tool 14: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 16: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 17: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 18: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 19: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "parameters": {},
            "response": "The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \n\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \n- **score**: A numeric value representing the model’s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \n\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers’ sentiment-analysis pipeline.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "The plot was captivating and the acting was superb."
                        }
                    }
                },
                {
                    "user_intent": "I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "I couldn't stand the boring dialogue and slow pacing."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a short review: 'It was an average movie, nothing special.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "It was an average movie, nothing special."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 4: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 7: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 8: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 9: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 10: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 11: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 12: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 13: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.\nTool 14: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 16: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 17: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 18: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 19: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\", \"parameters\": {}, \"response\": \"The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \\n\\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \\n- **score**: A numeric value representing the model\\u2019s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \\n\\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers\\u2019 sentiment-analysis pipeline.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"The plot was captivating and the acting was superb.\"}}}, {\"user_intent\": \"I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"I couldn't stand the boring dialogue and slow pacing.\"}}}, {\"user_intent\": \"Check the sentiment of a short review: 'It was an average movie, nothing special.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"It was an average movie, nothing special.\"}}}]}\n\n# User Query\nWe have generated user reviews for movies. We need to check user opinions about the movie 'Inception' based on their reviews.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of user reviews for the movie 'Inception'.",
                    "tool_description": "Perform sentiment analysis on English text reviews using a fine-tuned DistilBERT model to classify each review as positive or negative.",
                    "expected_response": "A structured list of classification results for each review, including a sentiment label (positive or negative) and a confidence score for the prediction."
                }
            ],
            "extracted_arguments": {
                "reviews": "A collection of textual user reviews for a specific movie, each represented as a string.",
                "movie_title": "The title of the movie for which the reviews were written."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_339",
        "query": "An IoT device collects images from different locations. Create a model to detect objects in these images to analyze the surroundings.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize computer vision models to analyze images by detecting and identifying objects, processing visual inputs to provide the necessary contextual insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')",
                    "api_arguments": {
                        "image": "Image.open(requests.get(url, stream=True).raw)",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import DetrFeatureExtractor, DetrForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\nbboxes = outputs.pred_boxes",
                    "performance": {
                        "dataset": "COCO 2017 validation",
                        "accuracy": "AP 44.9"
                    },
                    "description": "DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.",
                    "id": "huggingface_api_211",
                    "name": "facebook/detr-resnet-101-dc5"
                },
                "id": "gorilla_huggingface_tool_207",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 1: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 3: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 5: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 6: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 7: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 10: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 12: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 14: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 15: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 16: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 17: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\n\n1. **Detections / Results Array**  \n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \n   - Each detection usually contains information such as:\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \"forklift\" or \"person\".\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model’s certainty about the detection.\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\n\n2. **Image-Level Metadata**  \n   - Some responses may include metadata related to the processed image, such as:\n     - Image dimensions (width, height)\n     - The path or URL of the input image\n     - Number of detections in the image\n\n3. **Post-Processing / Rendering Support**  \n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\n\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.",
            "examples": [
                {
                    "user_intent": "I want to detect forklifts in an image from a URL.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect forklifts in a local image file.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "/path/to/local/forklift_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Run forklift detection with custom confidence and IoU thresholds.",
                    "tool_call": {
                        "name": "keremberke/yolov8m-forklift-detection",
                        "arguments": {
                            "image": "https://example.com/forklift_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 1: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 3: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 5: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 6: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 7: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 10: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 12: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 14: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 15: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 16: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 17: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image to run object detection on. Can be a URL pointing to an online image or a local file path on the system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured object representing the results of object detection performed on the input image. The response typically includes the following components:\\n\\n1. **Detections / Results Array**  \\n   - The main content is a list of detection results, where each element corresponds to one detected object in the image.  \\n   - Each detection usually contains information such as:\\n     - **Bounding Box (`boxes`)**: The coordinates of the detected object, often represented as `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.\\n     - **Class / Label (`class` or `label`)**: The category of the detected object, e.g., \\\"forklift\\\" or \\\"person\\\".\\n     - **Confidence Score (`confidence` or `score`)**: A numeric value indicating the model\\u2019s certainty about the detection.\\n     - **Optional Additional Attributes**: Depending on the implementation, this may include class probabilities, object tracking IDs, or mask segmentation information.\\n\\n2. **Image-Level Metadata**  \\n   - Some responses may include metadata related to the processed image, such as:\\n     - Image dimensions (width, height)\\n     - The path or URL of the input image\\n     - Number of detections in the image\\n\\n3. **Post-Processing / Rendering Support**  \\n   - The response is compatible with rendering utilities like `render_result`, which can visually overlay the detected bounding boxes and labels onto the image.  \\n   - This implies the bounding boxes and labels are structured in a way that can be directly interpreted for visualization.\\n\\nOverall, the response is a comprehensive object that provides both raw detection data and structured information suitable for visualization, further analysis, or filtering based on confidence or class.\", \"examples\": [{\"user_intent\": \"I want to detect forklifts in an image from a URL.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}, {\"user_intent\": \"Detect forklifts in a local image file.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"/path/to/local/forklift_image.jpg\"}}}, {\"user_intent\": \"Run forklift detection with custom confidence and IoU thresholds.\", \"tool_call\": {\"name\": \"keremberke/yolov8m-forklift-detection\", \"arguments\": {\"image\": \"https://example.com/forklift_image.jpg\"}}}]}\n\n# User Query\nAn IoT device collects images from different locations. Create a model to detect objects in these images to analyze the surroundings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect objects in images captured by an IoT device.",
                    "tool_description": "Utilize an object detection model to analyze images and identify various objects in the captured surroundings.",
                    "expected_response": "A collection of detected objects including their types, bounding boxes, confidence scores, and image metadata."
                }
            ],
            "extracted_arguments": {
                "image": "The image or set of images to analyze, which could be provided as URLs or local file paths from the IoT device."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_246",
        "query": "A forum moderator wants a tool to assess user-generated comments for toxic content. How does this model help?",
        "instruction": "Given a `content moderation` task, retrieve tools that can process text inputs to detect and classify toxic content, aiding moderation efforts by providing reliable assessments of user-generated comments.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline(model='martin-ha/toxic-comment-model')",
                    "api_arguments": {
                        "model_path": "martin-ha/toxic-comment-model"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = martin-ha/toxic-comment-model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))",
                    "performance": {
                        "dataset": "held-out test set",
                        "accuracy": 0.94,
                        "f1-score": 0.59
                    },
                    "description": "This model is a fine-tuned version of the DistilBERT model to classify toxic comments.",
                    "id": "huggingface_api_392",
                    "name": "martin-ha/toxic-comment-model"
                },
                "id": "gorilla_huggingface_tool_388",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 1: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 4: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 5: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 8: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 9: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 10: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 11: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 12: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 13: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 15: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 16: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 17: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.",
            "Tool 18: Pix2Struct is a multimodal model that encodes images and generates corresponding text, enabling tasks such as image captioning and visual question answering. It is pretrained on image-text pairs, including masked web page screenshots parsed into simplified HTML, and is designed to perform across diverse domains like documents, illustrations, user interfaces, and natural images. This model is useful for applications that require converting visual content into structured or descriptive text.",
            "Tool 19: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "parameters": {
                "model_path": {
                    "type": "string",
                    "description": "The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.",
                    "required": true,
                    "example": "martin-ha/toxic-comment-model"
                }
            },
            "response": "The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\n\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\n\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \"toxic,\" \"severe_toxic,\" \"obscene,\" or other similar categories depending on the model’s fine-tuning.\n\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model’s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\n\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.",
            "examples": [
                {
                    "user_intent": "Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "You're so stupid, I can't believe you wrote that."
                        }
                    }
                },
                {
                    "user_intent": "Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I hate this product, it's the worst thing I've ever bought!"
                        }
                    }
                },
                {
                    "user_intent": "I want to scan multiple forum posts for toxicity and flag any harmful comments.",
                    "tool_call": {
                        "name": "martin-ha/toxic-comment-model",
                        "arguments": {
                            "text": "I can't stand anyone who disagrees with me. They're all idiots."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 1: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 4: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 5: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 6: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 7: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 8: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 9: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 10: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 11: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 12: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 13: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 14: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 15: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 16: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 17: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.\nTool 18: Pix2Struct is a multimodal model that encodes images and generates corresponding text, enabling tasks such as image captioning and visual question answering. It is pretrained on image-text pairs, including masked web page screenshots parsed into simplified HTML, and is designed to perform across diverse domains like documents, illustrations, user interfaces, and natural images. This model is useful for applications that require converting visual content into structured or descriptive text.\nTool 19: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\", \"parameters\": {\"model_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pre-trained model to load for toxic comment classification, typically from Hugging Face model hub.\", \"required\": true, \"example\": \"martin-ha/toxic-comment-model\"}}, \"response\": \"The tool returns a structured classification of input text with respect to toxicity. The typical response is a list of objects, each representing a predicted label along with an associated confidence score. The structure and meaning of each component are as follows:\\n\\n- **List of classification results**: The response is an array where each element corresponds to one possible label predicted by the model for the given input text. The array may contain one or multiple elements depending on whether the pipeline is configured to return the top label only or multiple possible labels.\\n\\n- **`label`**: A string indicating the category assigned to the input text. In this context, the labels represent toxicity classes, such as \\\"toxic,\\\" \\\"severe_toxic,\\\" \\\"obscene,\\\" or other similar categories depending on the model\\u2019s fine-tuning.\\n\\n- **`score`**: A numeric value (typically a float between 0 and 1) representing the model\\u2019s confidence in the prediction of that label. Higher values indicate greater confidence that the input text belongs to the corresponding category.\\n\\nOverall, the response communicates which toxicity categories the model associates with the input text and how confident it is in each classification. The information is suitable for downstream tasks such as filtering, moderation, or further text analysis.\", \"examples\": [{\"user_intent\": \"Check if this comment is toxic: 'You're so stupid, I can't believe you wrote that.'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"You're so stupid, I can't believe you wrote that.\"}}}, {\"user_intent\": \"Analyze this customer review and see if it contains any toxic language: 'I hate this product, it's the worst thing I've ever bought!'\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I hate this product, it's the worst thing I've ever bought!\"}}}, {\"user_intent\": \"I want to scan multiple forum posts for toxicity and flag any harmful comments.\", \"tool_call\": {\"name\": \"martin-ha/toxic-comment-model\", \"arguments\": {\"text\": \"I can't stand anyone who disagrees with me. They're all idiots.\"}}}]}\n\n# User Query\nA forum moderator wants a tool to assess user-generated comments for toxic content. How does this model help?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to check if a user comment contains toxic content.",
                    "tool_description": "A fine-tuned DistilBERT model classifies input text to detect harmful or offensive language, providing labels and confidence scores for toxicity categories.",
                    "expected_response": "A structured list of classification results, each containing a toxicity label and a confidence score indicating the likelihood that the input text belongs to that category."
                }
            ],
            "extracted_arguments": {
                "text": "The user-generated comment text to be evaluated for toxicity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_340",
        "query": "I want to classify the images of houseplants to find out their type, like whether it's a cactus, fern, or succulent.",
        "instruction": "Given an `image classification` task, retrieve tools that accurately classify images by processing visual inputs to identify the type of houseplant, such as cactus, fern, or succulent, and return the predicted category.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/mobilenet_v1_0.75_192"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.",
                    "id": "huggingface_api_186",
                    "name": "google/mobilenet_v1_0.75_192"
                },
                "id": "gorilla_huggingface_tool_182",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 2: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 5: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 8: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 9: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "Tool 10: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 11: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.",
            "Tool 12: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 13: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 14: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 15: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 17: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 18: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications."
        ],
        "tool_view": {
            "description": "A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the image to be classified by the anime AI detector.",
                    "required": true,
                    "example": "path/to/anime_image.jpg"
                }
            },
            "response": "The tool returns a classification result for the input image, indicating whether the anime art was created by an AI or a human. The response would likely include the following components:\n\n1. **Class Label**: A textual or categorical label identifying the classification result, which could be \"AI\" or \"Human\" based on the model's determination.\n   \n2. **Confidence Score**: A numeric value representing the model's confidence in its classification, typically a probability or likelihood score, indicating how certain it is about the prediction.\n\nThe exact format and structure of the response are not explicitly defined, but based on the functionality and common usage of image classification models, the response will typically contain the class label and its associated confidence score.",
            "examples": [
                {
                    "user_intent": "I want to check if this anime illustration was created by AI or a human.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "/home/user/images/anime_art1.png"
                        }
                    }
                },
                {
                    "user_intent": "Classify a batch of anime character images to see which ones are AI-generated.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "/home/user/images/ai_batch/character_05.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I found this fan art online and I want to know if it’s human-made or AI-generated.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "https://example.com/fanart_image.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 2: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 3: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 5: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 8: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 9: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.\nTool 10: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 11: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\nTool 12: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 13: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 14: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 15: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 17: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 18: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the image to be classified by the anime AI detector.\", \"required\": true, \"example\": \"path/to/anime_image.jpg\"}}, \"response\": \"The tool returns a classification result for the input image, indicating whether the anime art was created by an AI or a human. The response would likely include the following components:\\n\\n1. **Class Label**: A textual or categorical label identifying the classification result, which could be \\\"AI\\\" or \\\"Human\\\" based on the model's determination.\\n   \\n2. **Confidence Score**: A numeric value representing the model's confidence in its classification, typically a probability or likelihood score, indicating how certain it is about the prediction.\\n\\nThe exact format and structure of the response are not explicitly defined, but based on the functionality and common usage of image classification models, the response will typically contain the class label and its associated confidence score.\", \"examples\": [{\"user_intent\": \"I want to check if this anime illustration was created by AI or a human.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"/home/user/images/anime_art1.png\"}}}, {\"user_intent\": \"Classify a batch of anime character images to see which ones are AI-generated.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"/home/user/images/ai_batch/character_05.jpg\"}}}, {\"user_intent\": \"I found this fan art online and I want to know if it\\u2019s human-made or AI-generated.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"https://example.com/fanart_image.png\"}}}]}\n\n# User Query\nI want to classify the images of houseplants to find out their type, like whether it's a cactus, fern, or succulent.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify images of houseplants to identify their type, such as cactus, fern, or succulent.",
                    "tool_description": "Perform zero-shot image classification on input images of houseplants, allowing the model to assign each image to one of the specified plant categories without requiring task-specific training.",
                    "expected_response": "For each input image, a classification result including the predicted plant type and a confidence score indicating the model's certainty."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the houseplant image to be classified.",
                "categories": "A list of possible plant types or labels for classification, such as cactus, fern, or succulent."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_331",
        "query": "We want to analyze a conference call recording to identify the speakers and the segments of the conversation they participated in.",
        "instruction": "Given a `speaker identification` task, retrieve tools that perform speaker diarization by processing audio inputs to segment and identify different speakers, aligning with the query's needs to analyze conversation dynamics in a conference call recording.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker Diarization",
                    "api_call": "Pipeline.from_pretrained('philschmid/pyannote-speaker-diarization-endpoint')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": [
                        "from pyannote.audio import Pipeline",
                        "pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "diarization = pipeline(audio.wav)",
                        "with open(audio.rttm, w) as rttm:",
                        "  diarization.write_rttm(rttm)"
                    ],
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "A speaker diarization pipeline that uses pyannote.audio to perform voice activity detection, speaker change detection, and overlapped speech detection. It can handle fully automatic processing with no manual intervention and can be fine-tuned with various hyperparameters.",
                    "id": "huggingface_api_841",
                    "name": "philschmid/pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_836",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 1: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 7: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.",
            "Tool 8: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 10: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 11: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 12: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 13: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 16: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 17: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 18: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 19: The \"mio/tokiwa_midori\" is a Text-to-Speech model developed using the ESPnet framework, trained on the Amadeus dataset. It enables converting text into speech, utilizing pre-trained models from the Hugging Face Hub. This tool is useful for speech synthesis applications and is easily invoked through a simple API call with specific configuration and model paths."
        ],
        "tool_view": {
            "description": "This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "parameters": {
                "num_speakers": {
                    "type": "number",
                    "description": "The exact number of speakers to detect in the audio. This overrides the min_speakers and max_speakers settings if provided.",
                    "required": true,
                    "example": 2
                },
                "min_speakers": {
                    "type": "number",
                    "description": "The minimum number of speakers expected in the audio. Used when the exact number of speakers is unknown.",
                    "required": true,
                    "example": 1
                },
                "max_speakers": {
                    "type": "number",
                    "description": "The maximum number of speakers expected in the audio. Helps constrain the diarization process when the exact number is unknown.",
                    "required": true,
                    "example": 5
                },
                "segmentation_onset": {
                    "type": "number",
                    "description": "A threshold parameter that adjusts the sensitivity for detecting the start of speech segments. Lower values make the system more sensitive to short speech segments.",
                    "required": true,
                    "example": 0.5
                }
            },
            "response": "The tool response would likely contain the following components:\n\n1. **Diarization Results**: This part of the response would include the speaker diarization output for the provided audio file. The diarization output typically includes:\n   - **Segments**: Time intervals where different speakers are active, with each segment linked to a specific speaker.\n   - **Speaker Labels**: Identifiers assigned to each speaker in the audio, such as \"Speaker 1,\" \"Speaker 2,\" etc., or possibly user-defined labels depending on the tool configuration.\n   - **Speech Time**: The total time a particular speaker is active during the audio, often associated with each speaker label.\n\n2. **RTTM File Format**: The diarization results will be structured in **RTTM (Rich Transcription Time Marked)** format, which typically includes:\n   - **File Name**: The audio file being processed.\n   - **Start Time**: The timestamp when the speaker begins speaking.\n   - **Duration**: The length of time the speaker was speaking.\n   - **Speaker ID**: A unique identifier for the speaker (e.g., \"Speaker 1\").\n   - **Speech/Non-Speech Label**: Indicating whether the segment contains speech or non-speech (e.g., \"SPEECH\" or \"NON_SPEECH\").\n\n3. **Performance Metrics**: The response might include performance metrics based on the evaluation on different datasets:\n   - **Dataset Name**: Identifiers for datasets used for evaluating the model.\n   - **Accuracy Metrics**: For each dataset, the performance metrics such as:\n     - **DER% (Diarization Error Rate)**: A measure of the overall error in the diarization.\n     - **FA% (False Alarm Rate)**: The rate of incorrectly detected speech from a non-speech segment.\n     - **Miss% (Missed Detection Rate)**: The rate of missed speech segments.\n     - **Conf% (Confusion Rate)**: The rate of confusion in speaker identification.\n\n4. **Error Handling**: If an error occurs during processing (e.g., invalid audio file, failure in the diarization process), the response may include an error message indicating the nature of the problem.\n\nThe output would summarize these results to provide insight into the audio's speaker segmentation and performance of the diarization model. It will also indicate the model's accuracy based on various evaluation datasets, helping the user assess its reliability.",
            "examples": [
                {
                    "user_intent": "I need to transcribe a conversation with 3 speakers from an audio file.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "num_speakers": 3
                        }
                    }
                },
                {
                    "user_intent": "Process an audio file with an unknown number of speakers, but I expect there to be between 2 and 4 speakers.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "min_speakers": 2,
                            "max_speakers": 4
                        }
                    }
                },
                {
                    "user_intent": "Diarize an audio recording with a low segmentation onset threshold for more precise speaker segmentation.",
                    "tool_call": {
                        "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint",
                        "arguments": {
                            "segmentation_onset": 0.3
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 1: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 5: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 6: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 7: This is a depth estimation model fine-tuned on the DIODE dataset, designed for use with Hugging Face Transformers. It can be loaded using the `AutoModel.from_pretrained` API call and requires the `transformers` and `torch` libraries.\nTool 8: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 9: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 10: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 11: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 12: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 13: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 14: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 16: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 17: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 18: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 19: The \"mio/tokiwa_midori\" is a Text-to-Speech model developed using the ESPnet framework, trained on the Amadeus dataset. It enables converting text into speech, utilizing pre-trained models from the Hugging Face Hub. This tool is useful for speech synthesis applications and is easily invoked through a simple API call with specific configuration and model paths.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\", \"parameters\": {\"num_speakers\": {\"type\": \"number\", \"description\": \"The exact number of speakers to detect in the audio. This overrides the min_speakers and max_speakers settings if provided.\", \"required\": true, \"example\": 2}, \"min_speakers\": {\"type\": \"number\", \"description\": \"The minimum number of speakers expected in the audio. Used when the exact number of speakers is unknown.\", \"required\": true, \"example\": 1}, \"max_speakers\": {\"type\": \"number\", \"description\": \"The maximum number of speakers expected in the audio. Helps constrain the diarization process when the exact number is unknown.\", \"required\": true, \"example\": 5}, \"segmentation_onset\": {\"type\": \"number\", \"description\": \"A threshold parameter that adjusts the sensitivity for detecting the start of speech segments. Lower values make the system more sensitive to short speech segments.\", \"required\": true, \"example\": 0.5}}, \"response\": \"The tool response would likely contain the following components:\\n\\n1. **Diarization Results**: This part of the response would include the speaker diarization output for the provided audio file. The diarization output typically includes:\\n   - **Segments**: Time intervals where different speakers are active, with each segment linked to a specific speaker.\\n   - **Speaker Labels**: Identifiers assigned to each speaker in the audio, such as \\\"Speaker 1,\\\" \\\"Speaker 2,\\\" etc., or possibly user-defined labels depending on the tool configuration.\\n   - **Speech Time**: The total time a particular speaker is active during the audio, often associated with each speaker label.\\n\\n2. **RTTM File Format**: The diarization results will be structured in **RTTM (Rich Transcription Time Marked)** format, which typically includes:\\n   - **File Name**: The audio file being processed.\\n   - **Start Time**: The timestamp when the speaker begins speaking.\\n   - **Duration**: The length of time the speaker was speaking.\\n   - **Speaker ID**: A unique identifier for the speaker (e.g., \\\"Speaker 1\\\").\\n   - **Speech/Non-Speech Label**: Indicating whether the segment contains speech or non-speech (e.g., \\\"SPEECH\\\" or \\\"NON_SPEECH\\\").\\n\\n3. **Performance Metrics**: The response might include performance metrics based on the evaluation on different datasets:\\n   - **Dataset Name**: Identifiers for datasets used for evaluating the model.\\n   - **Accuracy Metrics**: For each dataset, the performance metrics such as:\\n     - **DER% (Diarization Error Rate)**: A measure of the overall error in the diarization.\\n     - **FA% (False Alarm Rate)**: The rate of incorrectly detected speech from a non-speech segment.\\n     - **Miss% (Missed Detection Rate)**: The rate of missed speech segments.\\n     - **Conf% (Confusion Rate)**: The rate of confusion in speaker identification.\\n\\n4. **Error Handling**: If an error occurs during processing (e.g., invalid audio file, failure in the diarization process), the response may include an error message indicating the nature of the problem.\\n\\nThe output would summarize these results to provide insight into the audio's speaker segmentation and performance of the diarization model. It will also indicate the model's accuracy based on various evaluation datasets, helping the user assess its reliability.\", \"examples\": [{\"user_intent\": \"I need to transcribe a conversation with 3 speakers from an audio file.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"num_speakers\": 3}}}, {\"user_intent\": \"Process an audio file with an unknown number of speakers, but I expect there to be between 2 and 4 speakers.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"min_speakers\": 2, \"max_speakers\": 4}}}, {\"user_intent\": \"Diarize an audio recording with a low segmentation onset threshold for more precise speaker segmentation.\", \"tool_call\": {\"name\": \"johnislarry/cloned-pyannote-speaker-diarization-endpoint\", \"arguments\": {\"segmentation_onset\": 0.3}}}]}\n\n# User Query\nWe want to analyze a conference call recording to identify the speakers and the segments of the conversation they participated in.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze a conference call recording to identify all speakers and segment their speech.",
                    "tool_description": "Perform speaker diarization on an audio file using the pyannote.audio framework, detecting individual speakers and segmenting their speech throughout the recording. Supports specifying the number of speakers or a range, and allows adjustment of segmentation sensitivity.",
                    "expected_response": "A set of speaker-labeled segments in RTTM format, including start and end times of speech, speaker identifiers, and overall diarization metrics such as DER, FA, Miss, and Conf."
                }
            ],
            "extracted_arguments": {
                "num_speakers": "Optional. The exact number of speakers expected in the audio, if known.",
                "min_speakers": "Optional. The minimum number of speakers expected in the audio when the exact number is unknown.",
                "max_speakers": "Optional. The maximum number of speakers expected in the audio when the exact number is unknown.",
                "segmentation_onset": "Optional. A numeric threshold controlling sensitivity for detecting the start of speech segments."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_85",
        "query": "I want you to create a function that generates captions for a list of images.",
        "instruction": "Given an `image captioning` task, retrieve tools that process image inputs to generate descriptive textual captions, leveraging advanced models designed for translating visual content into coherent text.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Captioning",
                    "api_call": "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')",
                    "api_arguments": {
                        "model": "nlpconnect/vit-gpt2-image-captioning"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "PIL"
                    ],
                    "example_code": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']",
                    "performance": {
                        "dataset": "Not provided",
                        "accuracy": "Not provided"
                    },
                    "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.",
                    "id": "huggingface_api_58",
                    "name": "nlpconnect/vit-gpt2-image-captioning"
                },
                "id": "gorilla_huggingface_tool_58",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 2: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 3: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 4: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 5: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 6: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 7: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 8: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 9: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.",
            "Tool 10: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 11: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 12: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 13: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.",
            "Tool 14: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 15: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 17: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 18: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 19: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions."
        ],
        "tool_view": {
            "description": "This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "parameters": {
                "image_paths": {
                    "type": "array",
                    "description": "A list of file paths to the images that will be processed by the model for caption generation.",
                    "required": true,
                    "example": [
                        "image1.jpg",
                        "image2.png"
                    ]
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens allowed in the generated caption for each image.",
                    "required": true,
                    "default": 20,
                    "example": 20
                },
                "num_beams": {
                    "type": "number",
                    "description": "The number of beams to use in beam search for generating captions, controlling the trade-off between speed and quality.",
                    "required": true,
                    "default": 8,
                    "example": 8
                }
            },
            "response": "The tool produces textual captions corresponding to input images. The response is typically a list of strings, where each string represents the generated caption for one image provided in the input. Each caption summarizes the content or context of the associated image in natural language.  \n\nKey components of the response:\n\n- **List structure**: The response is an ordered collection of captions, matching the order of the input images.\n- **Caption strings**: Each element in the list is a text string that describes the content of the corresponding image.\n- **Length constraint**: Each caption is generated with a maximum length defined by the `max_length` parameter.\n- **Beam search influence**: The quality and diversity of the captions are influenced by the `num_beams` parameter, which affects the model’s search during generation.  \n\nIn summary, the tool maps each input image path to a single textual caption, producing a list of captions aligned with the input images.",
            "examples": [
                {
                    "user_intent": "Generate a short caption for a photo of a sunset over the mountains.",
                    "tool_call": {
                        "name": "AICVTG_What_if_a_machine_could_create_captions_automatically",
                        "arguments": {
                            "image_paths": [
                                "sunset_mountains.jpg"
                            ],
                            "max_length": 20,
                            "num_beams": 8
                        }
                    }
                },
                {
                    "user_intent": "Create captions for a set of product images to use in an online catalog.",
                    "tool_call": {
                        "name": "AICVTG_What_if_a_machine_could_create_captions_automatically",
                        "arguments": {
                            "image_paths": [
                                "laptop_front.jpg",
                                "laptop_side.jpg",
                                "laptop_back.jpg"
                            ],
                            "max_length": 20,
                            "num_beams": 8
                        }
                    }
                },
                {
                    "user_intent": "Caption a funny meme image automatically for social media posting.",
                    "tool_call": {
                        "name": "AICVTG_What_if_a_machine_could_create_captions_automatically",
                        "arguments": {
                            "image_paths": [
                                "funny_meme.png"
                            ],
                            "max_length": 20,
                            "num_beams": 8
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 2: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 3: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 4: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 5: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 6: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 7: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 8: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 9: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.\nTool 10: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 11: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 12: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 13: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.\nTool 14: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 15: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 17: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 18: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 19: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\", \"parameters\": {\"image_paths\": {\"type\": \"array\", \"description\": \"A list of file paths to the images that will be processed by the model for caption generation.\", \"required\": true, \"example\": [\"image1.jpg\", \"image2.png\"]}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens allowed in the generated caption for each image.\", \"required\": true, \"default\": 20, \"example\": 20}, \"num_beams\": {\"type\": \"number\", \"description\": \"The number of beams to use in beam search for generating captions, controlling the trade-off between speed and quality.\", \"required\": true, \"default\": 8, \"example\": 8}}, \"response\": \"The tool produces textual captions corresponding to input images. The response is typically a list of strings, where each string represents the generated caption for one image provided in the input. Each caption summarizes the content or context of the associated image in natural language.  \\n\\nKey components of the response:\\n\\n- **List structure**: The response is an ordered collection of captions, matching the order of the input images.\\n- **Caption strings**: Each element in the list is a text string that describes the content of the corresponding image.\\n- **Length constraint**: Each caption is generated with a maximum length defined by the `max_length` parameter.\\n- **Beam search influence**: The quality and diversity of the captions are influenced by the `num_beams` parameter, which affects the model\\u2019s search during generation.  \\n\\nIn summary, the tool maps each input image path to a single textual caption, producing a list of captions aligned with the input images.\", \"examples\": [{\"user_intent\": \"Generate a short caption for a photo of a sunset over the mountains.\", \"tool_call\": {\"name\": \"AICVTG_What_if_a_machine_could_create_captions_automatically\", \"arguments\": {\"image_paths\": [\"sunset_mountains.jpg\"], \"max_length\": 20, \"num_beams\": 8}}}, {\"user_intent\": \"Create captions for a set of product images to use in an online catalog.\", \"tool_call\": {\"name\": \"AICVTG_What_if_a_machine_could_create_captions_automatically\", \"arguments\": {\"image_paths\": [\"laptop_front.jpg\", \"laptop_side.jpg\", \"laptop_back.jpg\"], \"max_length\": 20, \"num_beams\": 8}}}, {\"user_intent\": \"Caption a funny meme image automatically for social media posting.\", \"tool_call\": {\"name\": \"AICVTG_What_if_a_machine_could_create_captions_automatically\", \"arguments\": {\"image_paths\": [\"funny_meme.png\"], \"max_length\": 20, \"num_beams\": 8}}}]}\n\n# User Query\nI want you to create a function that generates captions for a list of images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate captions automatically for a list of images.",
                    "tool_description": "Use a pretrained multimodal Vision-Encoder-Decoder model to process multiple images and generate concise textual descriptions for each image.",
                    "expected_response": "A list of strings, where each string is a generated caption corresponding to each input image, summarizing its content in natural language."
                }
            ],
            "extracted_arguments": {
                "image_paths": "An array containing file paths to all images that need captions.",
                "max_length": "Numeric value specifying the maximum number of tokens for each generated caption.",
                "num_beams": "Numeric value controlling the number of beams used in beam search for caption generation, affecting quality and diversity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_342",
        "query": "As a restaurant owner, I want to know if the total revenue for last week met our target revenue.",
        "instruction": "Given a `revenue evaluation` task, retrieve tools that analyze and compare financial data by processing numerical and table inputs to determine whether specific financial goals or targets have been met.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq'), TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')",
                    "api_arguments": "model_name_or_path, table, query",
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import TapasForQuestionAnswering, TapasTokenizer\n\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq')\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')\n\ninputs = tokenizer(table=table, queries=query, return_tensors='pt')\noutputs = model(**inputs)\n\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())",
                    "performance": {
                        "dataset": "wikitablequestions",
                        "accuracy": 0.3762
                    },
                    "description": "TAPAS small model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_434",
                    "name": "google/tapas-small-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_430",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 1: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.",
            "Tool 2: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 3: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 4: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 5: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 6: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 7: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.",
            "Tool 8: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 9: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.",
            "Tool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 11: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 13: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 14: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 16: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.",
            "Tool 17: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 18: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 19: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes."
        ],
        "tool_view": {
            "description": "A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('bert-base-uncased')"
                },
                "config": {
                    "type": "string",
                    "description": "The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.",
                    "required": true,
                    "example": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                }
            },
            "response": "The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\n\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\n\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \"positive\" or \"negative\"), which is the predicted sentiment of the input text.\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model’s certainty about its classification.\n  \n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\n\nThus, the expected response would primarily contain:\n- **Tokenized representations** of the input text.\n- **Predicted sentiment labels** along with any associated confidence or probability values.\n- Potential **error or diagnostic messages** if the process fails.",
            "examples": [
                {
                    "user_intent": "Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this review: 'Service was slow but the dessert was delightful.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 1: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.\nTool 2: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 3: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 4: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 5: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 6: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 7: mBART-50 is a multilingual sequence-to-sequence model designed for text generation and translation across multiple languages. It is pre-trained using a multilingual denoising objective, enabling it to convert text from a source language to a target language, such as English to Romanian, with high flexibility. This model is used in natural language processing tasks that require reliable cross-lingual text generation or translation.\nTool 8: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 9: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.\nTool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 11: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 13: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 14: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 16: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.\nTool 17: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 18: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 19: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\"}, \"config\": {\"type\": \"string\", \"description\": \"The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.\", \"required\": true, \"example\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}, \"response\": \"The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\\n\\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\\n\\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \\\"positive\\\" or \\\"negative\\\"), which is the predicted sentiment of the input text.\\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model\\u2019s certainty about its classification.\\n  \\n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\\n\\nThus, the expected response would primarily contain:\\n- **Tokenized representations** of the input text.\\n- **Predicted sentiment labels** along with any associated confidence or probability values.\\n- Potential **error or diagnostic messages** if the process fails.\", \"examples\": [{\"user_intent\": \"Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"Check the sentiment of this review: 'Service was slow but the dessert was delightful.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}]}\n\n# User Query\nAs a restaurant owner, I want to know if the total revenue for last week met our target revenue.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to calculate the total revenue for last week.",
                    "tool_description": "Retrieve and sum all revenue transactions for the past week to calculate total revenue.",
                    "expected_response": "A numerical value representing the total revenue for the last week."
                },
                {
                    "user_intent": "I want to compare the total revenue to our target revenue.",
                    "tool_description": "Compare the calculated total revenue to the target revenue to assess if the target was met.",
                    "expected_response": "A comparison result indicating whether the target revenue was met, along with a potential difference value."
                }
            ],
            "extracted_arguments": {
                "target_revenue": "The expected revenue target for comparison, represented as a numeric value.",
                "start_date": "The start date of the week for revenue calculation, represented as a date.",
                "end_date": "The end date of the week for revenue calculation, represented as a date."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_47",
        "query": "We are building a location recommendation system that identifies possible locations for new stores based on images from potential locations. Use the StreetCLIP model to generate probabilities for various cities.",
        "instruction": "Given a `location recommendation` task, retrieve tools that leverage image geolocalization capabilities to analyze images from potential store locations, using models like StreetCLIP, to generate location probabilities for various cities.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Geolocalization",
                    "api_call": "CLIPModel.from_pretrained('geolocal/StreetCLIP')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "geolocal/StreetCLIP"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "IM2GPS",
                                "accuracy": {
                                    "25km": 28.3,
                                    "200km": 45.1,
                                    "750km": 74.7,
                                    "2500km": 88.2
                                }
                            },
                            {
                                "name": "IM2GPS3K",
                                "accuracy": {
                                    "25km": 22.4,
                                    "200km": 37.4,
                                    "750km": 61.3,
                                    "2500km": 80.4
                                }
                            }
                        ]
                    },
                    "description": "StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.",
                    "id": "huggingface_api_374",
                    "name": "geolocal/StreetCLIP"
                },
                "id": "gorilla_huggingface_tool_370",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 1: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 3: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 6: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.",
            "Tool 7: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 8: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 9: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 10: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 11: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 12: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 13: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 14: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 15: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 16: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 17: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 18: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. This can be a URL pointing to an online image or a local file path on your system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool is designed for image segmentation, specifically using the YOLOv8 model to segment buildings in satellite images. When invoked with an image, the model performs segmentation and returns results that likely include the following components:\n\n1. **Predicted Bounding Boxes**: The response will contain information about the detected bounding boxes around the segmented building areas in the input image. Each bounding box will likely be represented by its coordinates (e.g., top-left and bottom-right corners), confidence score, and possibly class information (though the focus is on building segmentation).\n\n2. **Masks**: A segmentation mask will likely be included for each detected building. This mask indicates the pixel-level segmentation for the building in the image. Each mask corresponds to a specific building and would include binary (or multi-class) values showing which pixels belong to the building.\n\n3. **Confidence Scores**: The model might return the confidence scores associated with each detected building or segmentation mask. These scores indicate the likelihood that the model correctly identified a building in the segmented area.\n\n4. **Additional Metadata**: The response might also include additional metadata like:\n   - **Model settings/overrides** used for the prediction, such as confidence thresholds (`conf`), intersection-over-union (IoU) thresholds (`iou`), and other configuration options (e.g., `agnostic_nms`).\n   - **Performance metrics** from the model evaluation on a specific dataset, such as the mAP (mean average precision) at IoU threshold 0.5 for both bounding boxes and masks.\n\nIn summary, the response would include segmentation results for the buildings in the input image, with bounding box coordinates, masks, confidence scores, and potentially some performance evaluation metrics from the underlying model.",
            "examples": [
                {
                    "user_intent": "Segment all buildings in this satellite image of a city center.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "https://example.com/satellite_images/city_center.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I want to detect and outline buildings in a local aerial photo I took.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "/Users/alex/photos/aerial_building.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a satellite image from Google Earth to segment all visible buildings for urban planning.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-building-segmentation')",
                        "arguments": {
                            "image": "https://earth.google.com/satellite/buildings_area.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 1: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 3: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 6: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\nTool 7: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 8: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 9: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 10: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 11: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 12: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 13: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 14: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 15: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 16: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 17: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 18: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. This can be a URL pointing to an online image or a local file path on your system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool is designed for image segmentation, specifically using the YOLOv8 model to segment buildings in satellite images. When invoked with an image, the model performs segmentation and returns results that likely include the following components:\\n\\n1. **Predicted Bounding Boxes**: The response will contain information about the detected bounding boxes around the segmented building areas in the input image. Each bounding box will likely be represented by its coordinates (e.g., top-left and bottom-right corners), confidence score, and possibly class information (though the focus is on building segmentation).\\n\\n2. **Masks**: A segmentation mask will likely be included for each detected building. This mask indicates the pixel-level segmentation for the building in the image. Each mask corresponds to a specific building and would include binary (or multi-class) values showing which pixels belong to the building.\\n\\n3. **Confidence Scores**: The model might return the confidence scores associated with each detected building or segmentation mask. These scores indicate the likelihood that the model correctly identified a building in the segmented area.\\n\\n4. **Additional Metadata**: The response might also include additional metadata like:\\n   - **Model settings/overrides** used for the prediction, such as confidence thresholds (`conf`), intersection-over-union (IoU) thresholds (`iou`), and other configuration options (e.g., `agnostic_nms`).\\n   - **Performance metrics** from the model evaluation on a specific dataset, such as the mAP (mean average precision) at IoU threshold 0.5 for both bounding boxes and masks.\\n\\nIn summary, the response would include segmentation results for the buildings in the input image, with bounding box coordinates, masks, confidence scores, and potentially some performance evaluation metrics from the underlying model.\", \"examples\": [{\"user_intent\": \"Segment all buildings in this satellite image of a city center.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"https://example.com/satellite_images/city_center.jpg\"}}}, {\"user_intent\": \"I want to detect and outline buildings in a local aerial photo I took.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"/Users/alex/photos/aerial_building.jpg\"}}}, {\"user_intent\": \"Analyze a satellite image from Google Earth to segment all visible buildings for urban planning.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-building-segmentation')\", \"arguments\": {\"image\": \"https://earth.google.com/satellite/buildings_area.png\"}}}]}\n\n# User Query\nWe are building a location recommendation system that identifies possible locations for new stores based on images from potential locations. Use the StreetCLIP model to generate probabilities for various cities.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify the likely geographic locations of images of potential store sites.",
                    "tool_description": "Use the StreetCLIP model to perform zero-shot image geolocalization, predicting the probability distribution over possible cities for a given input image.",
                    "expected_response": "A probability map or list indicating the likelihood that each input image belongs to various cities, suitable for ranking or recommending potential store locations."
                }
            ],
            "extracted_arguments": {
                "image": "An image of a potential store location, provided as a URL or local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_7",
        "query": "We have been asked to predict future criminal re-offense from a given dataset. What model should we adopt and how do we proceed?",
        "instruction": "Given a `predictive modeling` task, retrieve tools that are designed for tabular classification to assess future criminal reoffense risks by leveraging appropriate datasets and machine learning frameworks to provide predictions and insights aligned with the query's objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('imodels/figs-compas-recidivism', 'sklearn_model.joblib')))",
                    "api_arguments": [
                        "REPO_ID",
                        "FILENAME"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "huggingface_hub",
                        "pandas",
                        "numpy",
                        "datasets",
                        "imodels",
                        "sklearn.model_selection"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = imodels/figs-compas-recidivism\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\npreds = model.predict(X_test)\nprint('accuracy', np.mean(preds==y_test))",
                    "performance": {
                        "dataset": "imodels/compas-recidivism",
                        "accuracy": 0.6759165485112416
                    },
                    "description": "A tabular classification model for predicting recidivism using the COMPAS dataset. The model is an imodels.FIGSClassifier trained with Scikit-learn and can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_865",
                    "name": "imodels/figs-compas-recidivism"
                },
                "id": "gorilla_huggingface_tool_858",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 1: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 2: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 3: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 4: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 5: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 6: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 7: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 8: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 9: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 10: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 13: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 14: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 15: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 16: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 17: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 18: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 19: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages."
        ],
        "tool_view": {
            "description": "This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "parameters": {
                "data": {
                    "type": "string",
                    "description": "The file path to the CSV file containing the input data for the model. The data should include all features required by the trained tabular regression model.",
                    "required": true,
                    "example": "data.csv"
                }
            },
            "response": "The response of this tool would provide the predicted carbon emissions for a given dataset, utilizing a trained tabular regression model. The structure and components of the response are as follows:\n\n1. **Predictions**:\n   - This field would contain the predicted carbon emissions values based on the input data provided to the model. Each entry corresponds to the predicted emission value for one observation in the dataset.\n\n2. **Performance Metrics**:\n   - The tool may include information about the model's performance based on the evaluation metrics provided in the `performance` section of the documentation:\n     - **Loss**: A measure of the error between predicted and actual values.\n     - **R2**: The coefficient of determination, indicating the proportion of variance explained by the model.\n     - **MSE**: Mean Squared Error, representing the average squared differences between predicted and actual values.\n     - **MAE**: Mean Absolute Error, representing the average absolute differences between predicted and actual values.\n     - **RMSLE**: Root Mean Squared Logarithmic Error, measuring the ratio between predicted and actual values in logarithmic terms.\n\n3. **Model Information**:\n   - **Name**: A string representing the name of the model, which in this case is \"Robertooo/autotrain-hmaet-2037366891.\"\n   - **Description**: A textual description of the model, specifying that it is a \"tabular regression model trained with AutoTrain to predict carbon emissions.\"\n\n4. **Input Data**:\n   - The response might also include an indication of the dataset or the features used for prediction, such as the names of features or the dataset itself (referencing the `features` from the configuration and the structure of the input data).\n\nThis structure enables the user to interpret the model’s outputs, evaluate its accuracy, and understand its performance in terms of relevant regression metrics.",
            "examples": [
                {
                    "user_intent": "Predict carbon emissions for a new dataset of vehicles using the pre-trained model.",
                    "tool_call": {
                        "name": "Robertooo/autotrain-hmaet-2037366891",
                        "arguments": {
                            "data": "new_vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Estimate carbon emissions for an industrial facility using the model with selected input features.",
                    "tool_call": {
                        "name": "Robertooo/autotrain-hmaet-2037366891",
                        "arguments": {
                            "data": "industrial_facility_features.csv"
                        }
                    }
                },
                {
                    "user_intent": "Run predictions on a batch of household energy consumption data to forecast carbon output.",
                    "tool_call": {
                        "name": "Robertooo/autotrain-hmaet-2037366891",
                        "arguments": {
                            "data": "household_energy_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 1: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 2: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 3: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 4: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 5: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 6: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 7: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 8: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 9: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 10: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 13: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 14: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 15: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 16: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 17: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 18: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 19: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\", \"parameters\": {\"data\": {\"type\": \"string\", \"description\": \"The file path to the CSV file containing the input data for the model. The data should include all features required by the trained tabular regression model.\", \"required\": true, \"example\": \"data.csv\"}}, \"response\": \"The response of this tool would provide the predicted carbon emissions for a given dataset, utilizing a trained tabular regression model. The structure and components of the response are as follows:\\n\\n1. **Predictions**:\\n   - This field would contain the predicted carbon emissions values based on the input data provided to the model. Each entry corresponds to the predicted emission value for one observation in the dataset.\\n\\n2. **Performance Metrics**:\\n   - The tool may include information about the model's performance based on the evaluation metrics provided in the `performance` section of the documentation:\\n     - **Loss**: A measure of the error between predicted and actual values.\\n     - **R2**: The coefficient of determination, indicating the proportion of variance explained by the model.\\n     - **MSE**: Mean Squared Error, representing the average squared differences between predicted and actual values.\\n     - **MAE**: Mean Absolute Error, representing the average absolute differences between predicted and actual values.\\n     - **RMSLE**: Root Mean Squared Logarithmic Error, measuring the ratio between predicted and actual values in logarithmic terms.\\n\\n3. **Model Information**:\\n   - **Name**: A string representing the name of the model, which in this case is \\\"Robertooo/autotrain-hmaet-2037366891.\\\"\\n   - **Description**: A textual description of the model, specifying that it is a \\\"tabular regression model trained with AutoTrain to predict carbon emissions.\\\"\\n\\n4. **Input Data**:\\n   - The response might also include an indication of the dataset or the features used for prediction, such as the names of features or the dataset itself (referencing the `features` from the configuration and the structure of the input data).\\n\\nThis structure enables the user to interpret the model\\u2019s outputs, evaluate its accuracy, and understand its performance in terms of relevant regression metrics.\", \"examples\": [{\"user_intent\": \"Predict carbon emissions for a new dataset of vehicles using the pre-trained model.\", \"tool_call\": {\"name\": \"Robertooo/autotrain-hmaet-2037366891\", \"arguments\": {\"data\": \"new_vehicle_data.csv\"}}}, {\"user_intent\": \"Estimate carbon emissions for an industrial facility using the model with selected input features.\", \"tool_call\": {\"name\": \"Robertooo/autotrain-hmaet-2037366891\", \"arguments\": {\"data\": \"industrial_facility_features.csv\"}}}, {\"user_intent\": \"Run predictions on a batch of household energy consumption data to forecast carbon output.\", \"tool_call\": {\"name\": \"Robertooo/autotrain-hmaet-2037366891\", \"arguments\": {\"data\": \"household_energy_data.csv\"}}}]}\n\n# User Query\nWe have been asked to predict future criminal re-offense from a given dataset. What model should we adopt and how do we proceed?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Identify a suitable predictive model for forecasting criminal re-offense from structured data.",
                    "tool_description": "Determine an appropriate tabular regression or classification model that can be trained to predict the likelihood of criminal re-offense based on structured input features.",
                    "expected_response": "A recommendation of model types suitable for criminal recidivism prediction, possibly including model architecture, training framework, and rationale."
                },
                {
                    "user_intent": "Train or apply the selected model to the provided dataset to generate predictions.",
                    "tool_description": "Load a tabular machine learning model, process the input dataset to match required features, and perform predictions to estimate the probability of re-offense for each individual record.",
                    "expected_response": "A structured output containing predicted probabilities or classification labels indicating potential criminal re-offense for each entry, along with relevant performance metrics if training is performed."
                }
            ],
            "extracted_arguments": {
                "dataset": "The file path or structured input containing individual records with features relevant for criminal re-offense prediction.",
                "target_variable": "The outcome variable representing whether re-offense occurs, used for training or validation if applicable.",
                "model_type": "A specification of the machine learning model suitable for tabular predictive tasks, such as regression or classification."
            }
        }
    }
]