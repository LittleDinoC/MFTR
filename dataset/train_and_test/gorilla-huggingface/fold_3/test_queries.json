[
    {
        "id": "gorilla_huggingface_query_338",
        "query": "A team is working on a video game where the player needs to land the spaceship on the lunar surface without crashing. They want to implement an AI module that can play the game and test it.",
        "instruction": "Given a `AI module implementation for gaming` task, retrieve tools that facilitate the integration and testing of AI models, specifically those capable of controlling game mechanics and improving gameplay strategies using reinforcement learning frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "LunarLander-v2",
                    "api_call": "DQN.load('araffin/dqn-LunarLander-v2')",
                    "api_arguments": {
                        "checkpoint": "araffin/dqn-LunarLander-v2",
                        "kwargs": {
                            "target_update_interval": 30
                        }
                    },
                    "python_environment_requirements": [
                        "huggingface_sb3",
                        "stable_baselines3"
                    ],
                    "example_code": {
                        "load_model": "from huggingface_sb3 import load_from_hub\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\n\nkwargs = dict(target_update_interval=30)\n\nmodel = DQN.load(checkpoint, **kwargs)\nenv = make_vec_env(LunarLander-v2, n_envs=1)",
                        "evaluate": "mean_reward, std_reward = evaluate_policy(\n model,\n env,\n n_eval_episodes=20,\n deterministic=True,\n)\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})"
                    },
                    "performance": {
                        "dataset": "LunarLander-v2",
                        "accuracy": "280.22 +/- 13.03"
                    },
                    "description": "This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.",
                    "id": "huggingface_api_902",
                    "name": "araffin/dqn-LunarLander-v2"
                },
                "id": "gorilla_huggingface_tool_895",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 1: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 3: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 4: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 6: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 7: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 8: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 10: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 11: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 12: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 13: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 14: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 15: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 17: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 18: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 19: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data."
        ],
        "tool_view": {
            "description": "A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "parameters": {
                "checkpoint": {
                    "type": "string",
                    "description": "The identifier or path of the pre-trained DQN model checkpoint to load from the Hugging Face Hub.",
                    "required": true,
                    "example": "araffin/dqn-LunarLander-v2"
                },
                "kwargs": {
                    "type": "object",
                    "description": "A dictionary of additional keyword arguments to pass when loading the DQN model, such as training hyperparameters.",
                    "required": true,
                    "example": {
                        "target_update_interval": 30
                    }
                }
            },
            "response": "The tool’s response would typically include a loaded reinforcement learning model object representing a Deep Q-Network (DQN) agent trained on the LunarLander-v2 environment using the Stable-Baselines3 framework. This model object encapsulates the learned policy, including neural network weights and relevant training parameters (such as the target update interval).\n\nWhen evaluating the model, the response would consist of numerical performance metrics derived from running the agent on the environment for a specified number of evaluation episodes. These metrics generally include:\n\n- **mean_reward**: The average cumulative reward achieved by the agent per episode during evaluation, reflecting overall policy effectiveness.\n- **std_reward**: The standard deviation of the rewards across evaluation episodes, indicating variability in the agent’s performance.\n\nThus, the typical response structure involves:\n\n1. **Model Object**: A fully instantiated DQN agent loaded with pretrained weights and configured with specified hyperparameters (e.g., target update interval).\n\n2. **Evaluation Results**: A tuple or dictionary containing the mean and standard deviation of episodic rewards obtained from policy evaluation runs, which quantify the agent’s performance on the LunarLander-v2 task.\n\nNo raw data like logs or step-wise actions are explicitly defined in the response; rather, the emphasis is on returning the trained model for use or further evaluation and the summarized statistical metrics that characterize its effectiveness.",
            "examples": [
                {
                    "user_intent": "Load the pre-trained DQN model for LunarLander-v2 and evaluate its performance over 20 episodes.",
                    "tool_call": {
                        "name": "DQN.load",
                        "arguments": {
                            "checkpoint": "araffin/dqn-LunarLander-v2",
                            "kwargs": {
                                "target_update_interval": 30
                            }
                        }
                    }
                },
                {
                    "user_intent": "I want to run the LunarLander-v2 environment with the pre-trained DQN agent and see the mean reward.",
                    "tool_call": {
                        "name": "DQN.load",
                        "arguments": {
                            "checkpoint": "araffin/dqn-LunarLander-v2",
                            "kwargs": {
                                "target_update_interval": 30
                            }
                        }
                    }
                },
                {
                    "user_intent": "Use the trained DQN model from HuggingFace to test its stability in LunarLander-v2 over multiple evaluation episodes.",
                    "tool_call": {
                        "name": "DQN.load",
                        "arguments": {
                            "checkpoint": "araffin/dqn-LunarLander-v2",
                            "kwargs": {
                                "target_update_interval": 30
                            }
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 1: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 3: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 4: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 6: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 7: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 8: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 10: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 11: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 12: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 13: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 14: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 15: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 17: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 18: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 19: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\", \"parameters\": {\"checkpoint\": {\"type\": \"string\", \"description\": \"The identifier or path of the pre-trained DQN model checkpoint to load from the Hugging Face Hub.\", \"required\": true, \"example\": \"araffin/dqn-LunarLander-v2\"}, \"kwargs\": {\"type\": \"object\", \"description\": \"A dictionary of additional keyword arguments to pass when loading the DQN model, such as training hyperparameters.\", \"required\": true, \"example\": {\"target_update_interval\": 30}}}, \"response\": \"The tool\\u2019s response would typically include a loaded reinforcement learning model object representing a Deep Q-Network (DQN) agent trained on the LunarLander-v2 environment using the Stable-Baselines3 framework. This model object encapsulates the learned policy, including neural network weights and relevant training parameters (such as the target update interval).\\n\\nWhen evaluating the model, the response would consist of numerical performance metrics derived from running the agent on the environment for a specified number of evaluation episodes. These metrics generally include:\\n\\n- **mean_reward**: The average cumulative reward achieved by the agent per episode during evaluation, reflecting overall policy effectiveness.\\n- **std_reward**: The standard deviation of the rewards across evaluation episodes, indicating variability in the agent\\u2019s performance.\\n\\nThus, the typical response structure involves:\\n\\n1. **Model Object**: A fully instantiated DQN agent loaded with pretrained weights and configured with specified hyperparameters (e.g., target update interval).\\n\\n2. **Evaluation Results**: A tuple or dictionary containing the mean and standard deviation of episodic rewards obtained from policy evaluation runs, which quantify the agent\\u2019s performance on the LunarLander-v2 task.\\n\\nNo raw data like logs or step-wise actions are explicitly defined in the response; rather, the emphasis is on returning the trained model for use or further evaluation and the summarized statistical metrics that characterize its effectiveness.\", \"examples\": [{\"user_intent\": \"Load the pre-trained DQN model for LunarLander-v2 and evaluate its performance over 20 episodes.\", \"tool_call\": {\"name\": \"DQN.load\", \"arguments\": {\"checkpoint\": \"araffin/dqn-LunarLander-v2\", \"kwargs\": {\"target_update_interval\": 30}}}}, {\"user_intent\": \"I want to run the LunarLander-v2 environment with the pre-trained DQN agent and see the mean reward.\", \"tool_call\": {\"name\": \"DQN.load\", \"arguments\": {\"checkpoint\": \"araffin/dqn-LunarLander-v2\", \"kwargs\": {\"target_update_interval\": 30}}}}, {\"user_intent\": \"Use the trained DQN model from HuggingFace to test its stability in LunarLander-v2 over multiple evaluation episodes.\", \"tool_call\": {\"name\": \"DQN.load\", \"arguments\": {\"checkpoint\": \"araffin/dqn-LunarLander-v2\", \"kwargs\": {\"target_update_interval\": 30}}}}]}\n\n# User Query\nA team is working on a video game where the player needs to land the spaceship on the lunar surface without crashing. They want to implement an AI module that can play the game and test it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Load a pre-trained AI agent capable of playing LunarLander-v2 to test the landing mechanics.",
                    "tool_description": "Load a pre-trained Deep Q-Network (DQN) model for the LunarLander-v2 environment using Stable-Baselines3, allowing the agent to interact with the game and execute landings based on learned policies.",
                    "expected_response": "A loaded DQN model object with trained neural network weights and policy parameters, ready for evaluation or gameplay simulation."
                },
                {
                    "user_intent": "Evaluate the AI agent's performance in landing the spaceship safely on the lunar surface.",
                    "tool_description": "Run the loaded DQN agent in the LunarLander-v2 environment for a set number of episodes, measuring its effectiveness in performing landings without crashes.",
                    "expected_response": "Numerical metrics including mean reward and standard deviation of rewards over the evaluation episodes, reflecting the agent's landing performance and stability."
                }
            ],
            "extracted_arguments": {
                "checkpoint": "The identifier or path of the pre-trained DQN model checkpoint to load.",
                "kwargs": "A dictionary of additional configuration parameters for loading and evaluating the DQN model, such as hyperparameters."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_25",
        "query": "I am making a keyword search engine that ranks text passages based on their importance regarding a given keyword.",
        "instruction": "Given a `keyword-based ranking` task, retrieve tools that can process text passages by evaluating their relevance to a specified keyword, and rank them accordingly based on importance and content relevance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')",
                    "api_arguments": {
                        "padding": "True",
                        "truncation": "True",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForSequenceClassification",
                        "torch": "import torch"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\nmodel.eval()\nwith torch.no_grad():\n scores = model(**features).logits\n print(scores)",
                    "performance": {
                        "dataset": {
                            "TREC Deep Learning 2019": {
                                "NDCG@10": 74.31
                            },
                            "MS Marco Passage Reranking": {
                                "MRR@10": 39.02,
                                "accuracy": "960 Docs / Sec"
                            }
                        }
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco",
                    "id": "huggingface_api_391",
                    "name": "cross-encoder/ms-marco-MiniLM-L-12-v2"
                },
                "id": "gorilla_huggingface_tool_387",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "Tool 1: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 2: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 3: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 4: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.",
            "Tool 5: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 6: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 7: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 8: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.",
            "Tool 9: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.",
            "Tool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 11: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 12: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 13: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 14: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 15: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 17: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 18: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 19: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair."
        ],
        "tool_view": {
            "description": "This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pretrained model to load for sequence classification from Hugging Face Transformers.",
                    "required": true,
                    "example": "cross-encoder/ms-marco-TinyBERT-L-2-v2"
                },
                "queries": {
                    "type": "array",
                    "description": "A list of query strings for which the model will retrieve and rank relevant passages.",
                    "required": true,
                    "example": [
                        "How many people live in Berlin?",
                        "What is the population of New York City?"
                    ]
                },
                "passages": {
                    "type": "array",
                    "description": "A list of candidate passage strings corresponding to each query. Each query will be compared against all passages to compute relevance scores.",
                    "required": true,
                    "example": [
                        "Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.",
                        "New York City is famous for the Metropolitan Museum of Art."
                    ]
                },
                "padding": {
                    "type": "boolean",
                    "description": "Whether to pad sequences to the same length when tokenizing the queries and passages.",
                    "required": false,
                    "default": true,
                    "example": true
                },
                "truncation": {
                    "type": "boolean",
                    "description": "Whether to truncate sequences that are longer than the model's maximum input length during tokenization.",
                    "required": false,
                    "default": true,
                    "example": true
                },
                "return_tensors": {
                    "type": "string",
                    "description": "The format of the returned tokenized features. Typically 'pt' for PyTorch tensors.",
                    "required": false,
                    "default": "pt",
                    "example": "pt"
                }
            },
            "response": "The tool produces a numerical output representing the relevance or similarity scores between input query-passage pairs. The response is structured as a tensor (or array) of logits, where each element corresponds to a single query-passage pair from the input.  \n\n**Components of the response:**\n\n1. **Tensor/Array of logits (`scores`)**  \n   - Each value indicates the model’s predicted relevance of a specific passage to a given query.  \n   - Higher values represent higher estimated relevance.  \n   - The shape of the tensor corresponds to the number of input query-passage pairs.\n\n2. **Association to input pairs**  \n   - Each score directly maps to the query and passage pair at the same index in the input features.  \n   - This allows downstream processing, such as ranking passages by relevance for a given query.\n\n3. **Optional post-processing**  \n   - While the raw response is logits, these can be converted to probabilities or normalized scores if needed.  \n   - Sorting or ranking operations are typically applied to these scores to produce a ranked list of passages per query.  \n\nOverall, the response provides the relevance estimation necessary for Information Retrieval tasks, enabling ranking of passages according to their predicted suitability to the input query.",
            "examples": [
                {
                    "user_intent": "I want to rank a set of product descriptions based on how relevant they are to the query 'best wireless headphones'.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['best wireless headphones'], ['These headphones have noise-canceling features.', 'This wired headset offers high fidelity sound.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                },
                {
                    "user_intent": "I need to find the most relevant passages about 'climate change effects on polar bears' from a list of articles.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['climate change effects on polar bears'], ['Polar bears are losing habitat due to melting ice.', 'Economic policies are changing worldwide.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                },
                {
                    "user_intent": "Compare which city description best answers 'population of Tokyo' from multiple passages.",
                    "tool_call": {
                        "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2",
                        "arguments": {
                            "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                            "features": "features = tokenizer(['population of Tokyo'], ['Tokyo has over 13 million residents.', 'Paris is the capital of France.'], padding=True, truncation=True, return_tensors='pt')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\nTool 1: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 2: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 3: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 4: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.\nTool 5: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 6: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 7: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 8: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\nTool 9: The `facebook/dpr-question_encoder-single-nq-base` is a Dense Passage Retrieval (DPR) question encoder that converts natural language questions into dense vector embeddings for open-domain question answering. Trained on the Natural Questions (NQ) dataset, it is designed to capture semantic meaning of queries, enabling efficient retrieval of relevant passages from large text corpora. This model is used in NLP systems to improve the accuracy and speed of information retrieval in applications like search engines and QA pipelines.\nTool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 11: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 12: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 13: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 14: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 15: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 16: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 17: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 18: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 19: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This model, trained on the MS Marco Passage Ranking task, is designed for information retrieval. It encodes a query along with multiple passages, which can be retrieved using tools like ElasticSearch, and ranks them by relevance. The model uses the Hugging Face Transformers framework and is based on the TinyBERT architecture. It provides a way to assess the relevance of passages to a given query, making it useful for ranking search results or improving question-answering systems.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pretrained model to load for sequence classification from Hugging Face Transformers.\", \"required\": true, \"example\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"}, \"queries\": {\"type\": \"array\", \"description\": \"A list of query strings for which the model will retrieve and rank relevant passages.\", \"required\": true, \"example\": [\"How many people live in Berlin?\", \"What is the population of New York City?\"]}, \"passages\": {\"type\": \"array\", \"description\": \"A list of candidate passage strings corresponding to each query. Each query will be compared against all passages to compute relevance scores.\", \"required\": true, \"example\": [\"Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\", \"New York City is famous for the Metropolitan Museum of Art.\"]}, \"padding\": {\"type\": \"boolean\", \"description\": \"Whether to pad sequences to the same length when tokenizing the queries and passages.\", \"required\": false, \"default\": true, \"example\": true}, \"truncation\": {\"type\": \"boolean\", \"description\": \"Whether to truncate sequences that are longer than the model's maximum input length during tokenization.\", \"required\": false, \"default\": true, \"example\": true}, \"return_tensors\": {\"type\": \"string\", \"description\": \"The format of the returned tokenized features. Typically 'pt' for PyTorch tensors.\", \"required\": false, \"default\": \"pt\", \"example\": \"pt\"}}, \"response\": \"The tool produces a numerical output representing the relevance or similarity scores between input query-passage pairs. The response is structured as a tensor (or array) of logits, where each element corresponds to a single query-passage pair from the input.  \\n\\n**Components of the response:**\\n\\n1. **Tensor/Array of logits (`scores`)**  \\n   - Each value indicates the model\\u2019s predicted relevance of a specific passage to a given query.  \\n   - Higher values represent higher estimated relevance.  \\n   - The shape of the tensor corresponds to the number of input query-passage pairs.\\n\\n2. **Association to input pairs**  \\n   - Each score directly maps to the query and passage pair at the same index in the input features.  \\n   - This allows downstream processing, such as ranking passages by relevance for a given query.\\n\\n3. **Optional post-processing**  \\n   - While the raw response is logits, these can be converted to probabilities or normalized scores if needed.  \\n   - Sorting or ranking operations are typically applied to these scores to produce a ranked list of passages per query.  \\n\\nOverall, the response provides the relevance estimation necessary for Information Retrieval tasks, enabling ranking of passages according to their predicted suitability to the input query.\", \"examples\": [{\"user_intent\": \"I want to rank a set of product descriptions based on how relevant they are to the query 'best wireless headphones'.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['best wireless headphones'], ['These headphones have noise-canceling features.', 'This wired headset offers high fidelity sound.'], padding=True, truncation=True, return_tensors='pt')\"}}}, {\"user_intent\": \"I need to find the most relevant passages about 'climate change effects on polar bears' from a list of articles.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['climate change effects on polar bears'], ['Polar bears are losing habitat due to melting ice.', 'Economic policies are changing worldwide.'], padding=True, truncation=True, return_tensors='pt')\"}}}, {\"user_intent\": \"Compare which city description best answers 'population of Tokyo' from multiple passages.\", \"tool_call\": {\"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\", \"features\": \"features = tokenizer(['population of Tokyo'], ['Tokyo has over 13 million residents.', 'Paris is the capital of France.'], padding=True, truncation=True, return_tensors='pt')\"}}}]}\n\n# User Query\nI am making a keyword search engine that ranks text passages based on their importance regarding a given keyword.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to rank text passages based on their relevance to a given keyword.",
                    "tool_description": "Retrieve and rank a list of candidate passages by relevance to a specified keyword using a pre-trained cross-encoder model.",
                    "expected_response": "A tensor or array of relevance scores for each passage, indicating how closely each passage matches the given keyword."
                }
            ],
            "extracted_arguments": {
                "keyword": "The specific keyword or query phrase for which relevance is being assessed.",
                "passages": "A list of candidate passage strings that will be evaluated for relevance to the keyword."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_473",
        "query": "Our client wants to analyze videos for their marketing website. They need a quick solution to categorize video content without worrying about accuracy.",
        "instruction": "Given a `video classification` task, retrieve tools that categorize video content by utilizing models capable of processing video inputs to deliver quick classification results without prioritizing high accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')",
                    "api_arguments": "model",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random VideoMAE model for video classification.",
                    "id": "huggingface_api_340",
                    "name": "tiny-random-VideoMAEForVideoClassification"
                },
                "id": "gorilla_huggingface_tool_336",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 1: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.",
            "Tool 2: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 3: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 4: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 5: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 6: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.",
            "Tool 7: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 8: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 9: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 10: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 13: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 14: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 19: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings."
        ],
        "tool_view": {
            "description": "A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "parameters": {
                "learning_rate": {
                    "type": "number",
                    "description": "The learning rate used during training. Controls the step size for weight updates in the optimizer.",
                    "required": true,
                    "example": 5e-05
                },
                "train_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward/backward pass during training.",
                    "required": true,
                    "example": 8
                },
                "eval_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward pass during evaluation.",
                    "required": true,
                    "example": 8
                },
                "seed": {
                    "type": "number",
                    "description": "A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.",
                    "required": true,
                    "example": 42
                },
                "optimizer": {
                    "type": "string",
                    "description": "The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.",
                    "required": true,
                    "example": "Adam with betas=(0.9,0.999) and epsilon=1e-08"
                },
                "lr_scheduler_type": {
                    "type": "string",
                    "description": "The type of learning rate scheduler used to adjust the learning rate during training.",
                    "required": true,
                    "example": "linear"
                },
                "lr_scheduler_warmup_ratio": {
                    "type": "number",
                    "description": "The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.",
                    "required": true,
                    "example": 0.1
                },
                "training_steps": {
                    "type": "number",
                    "description": "The total number of training steps to perform.",
                    "required": true,
                    "example": 111
                }
            },
            "response": "The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \n\n1. **Predicted Label(s)** – One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \n\n2. **Confidence Scores / Probabilities** – A numeric value or array indicating the model’s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \n\n3. **Optional Metadata** – Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \n\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.",
            "examples": [
                {
                    "user_intent": "I want to use a pre-trained video classification model for a project. Can you help me with setting it up?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                },
                {
                    "user_intent": "I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset",
                            "learning_rate": 5e-05,
                            "train_batch_size": 8,
                            "eval_batch_size": 8,
                            "seed": 42,
                            "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                            "lr_scheduler_type": "linear",
                            "lr_scheduler_warmup_ratio": 0.1,
                            "training_steps": 111
                        }
                    }
                },
                {
                    "user_intent": "Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 1: A Hugging Face Transformers model for video classification, fine-tuned from VideoMAE on Kinetics and RWF2000 datasets, designed to categorize videos into predefined classes with an accuracy of approximately 73%.\nTool 2: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 3: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 4: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 5: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 6: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.\nTool 7: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 8: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 9: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 10: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 13: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 14: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 16: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 19: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\", \"parameters\": {\"learning_rate\": {\"type\": \"number\", \"description\": \"The learning rate used during training. Controls the step size for weight updates in the optimizer.\", \"required\": true, \"example\": 5e-05}, \"train_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward/backward pass during training.\", \"required\": true, \"example\": 8}, \"eval_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward pass during evaluation.\", \"required\": true, \"example\": 8}, \"seed\": {\"type\": \"number\", \"description\": \"A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.\", \"required\": true, \"example\": 42}, \"optimizer\": {\"type\": \"string\", \"description\": \"The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.\", \"required\": true, \"example\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\"}, \"lr_scheduler_type\": {\"type\": \"string\", \"description\": \"The type of learning rate scheduler used to adjust the learning rate during training.\", \"required\": true, \"example\": \"linear\"}, \"lr_scheduler_warmup_ratio\": {\"type\": \"number\", \"description\": \"The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.\", \"required\": true, \"example\": 0.1}, \"training_steps\": {\"type\": \"number\", \"description\": \"The total number of training steps to perform.\", \"required\": true, \"example\": 111}}, \"response\": \"The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \\n\\n1. **Predicted Label(s)** \\u2013 One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \\n\\n2. **Confidence Scores / Probabilities** \\u2013 A numeric value or array indicating the model\\u2019s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \\n\\n3. **Optional Metadata** \\u2013 Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \\n\\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.\", \"examples\": [{\"user_intent\": \"I want to use a pre-trained video classification model for a project. Can you help me with setting it up?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}, {\"user_intent\": \"I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}}}, {\"user_intent\": \"Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}]}\n\n# User Query\nOur client wants to analyze videos for their marketing website. They need a quick solution to categorize video content without worrying about accuracy.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to quickly categorize video content for a marketing website without focusing on high accuracy.",
                    "tool_description": "Use a lightweight or pre-trained video classification model to automatically label videos into predefined categories, prioritizing speed over precision.",
                    "expected_response": "A list of predicted video categories for each input video, possibly with confidence scores, sufficient for general content organization."
                }
            ],
            "extracted_arguments": {
                "video_input": "The video files or streams that need to be analyzed and categorized.",
                "classification_priority": "A parameter indicating the importance of speed versus accuracy, set to favor speed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_383",
        "query": "We need to identify the relationship between two sentences whether they are contradictory, entail each other, or neutral.",
        "instruction": "Given a `sentence relationship identification` task, retrieve tools that can evaluate and classify relationships between sentence pairs as contradictory, entailment, or neutral by processing textual inputs and returning relevant classification scores.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "CrossEncoder('cross-encoder/nli-deberta-v3-small')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": {
                            "SNLI-test": "91.65",
                            "MNLI-mismatched": "87.55"
                        },
                        "accuracy": {
                            "SNLI-test": "91.65",
                            "MNLI-mismatched": "87.55"
                        }
                    },
                    "description": "Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_501",
                    "name": "cross-encoder/nli-deberta-v3-small"
                },
                "id": "gorilla_huggingface_tool_497",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 1: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 2: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.",
            "Tool 3: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.",
            "Tool 4: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 5: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 7: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 8: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 11: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 12: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 13: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 14: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 15: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 19: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences."
        ],
        "tool_view": {
            "description": "A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "parameters": {
                "sentence1": {
                    "type": "string",
                    "description": "The first sentence in the pair to be classified for natural language inference.",
                    "required": true,
                    "example": "A man is eating pizza"
                },
                "sentence2": {
                    "type": "string",
                    "description": "The second sentence in the pair to be classified for natural language inference.",
                    "required": true,
                    "example": "A man eats something"
                }
            },
            "response": "The tool is designed for performing **Natural Language Inference (NLI)** using a pretrained **cross-encoder model** based on the **MiniLM** architecture, specifically **cross-encoder/nli-MiniLM2-L6-H768**. \n\nThe response generated by this tool will typically contain **three scores** for each given pair of sentences. These scores represent the likelihood of three possible relations between the sentences:\n\n1. **Contradiction**: The score indicating how much the second sentence contradicts the first one.\n2. **Entailment**: The score indicating how much the second sentence entails (or supports) the first one.\n3. **Neutral**: The score indicating how much the second sentence is neutral in relation to the first one (i.e., neither supporting nor contradicting it).\n\nThese scores are used to classify the relationship between two sentences based on their semantic meaning. The response will include these three scores for each pair of input sentences provided to the model.\n\nThe response may also indicate a **confidence score** or normalized values for each label (Contradiction, Entailment, and Neutral), where higher values typically indicate stronger confidence in the respective classification.\n\n### Expected components of the response:\n\n1. **scores** (list): A list of three numeric values corresponding to the **Contradiction**, **Entailment**, and **Neutral** labels for each pair of sentences.\n2. **label** (optional, inferred): Based on the scores, a label indicating the most likely relationship (either \"contradiction\", \"entailment\", or \"neutral\").\n\nThe response could be structured as follows:\n- For each pair of input sentences, the tool will provide the corresponding three classification scores.\n",
            "examples": [
                {
                    "user_intent": "I want to check if 'The cat is sleeping on the sofa' implies 'A cat is resting indoors'.",
                    "tool_call": {
                        "name": "cross-encoder/nli-MiniLM2-L6-H768",
                        "arguments": {
                            "sentence1": "The cat is sleeping on the sofa",
                            "sentence2": "A cat is resting indoors"
                        }
                    }
                },
                {
                    "user_intent": "Determine whether 'A person is jogging in the park' contradicts 'Someone is sitting on a bench'.",
                    "tool_call": {
                        "name": "cross-encoder/nli-MiniLM2-L6-H768",
                        "arguments": {
                            "sentence1": "A person is jogging in the park",
                            "sentence2": "Someone is sitting on a bench"
                        }
                    }
                },
                {
                    "user_intent": "Check the relationship between 'A black race car starts up in front of a crowd of people.' and 'A man is driving down a lonely road.'",
                    "tool_call": {
                        "name": "cross-encoder/nli-MiniLM2-L6-H768",
                        "arguments": {
                            "sentence1": "A black race car starts up in front of a crowd of people.",
                            "sentence2": "A man is driving down a lonely road."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 1: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 2: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.\nTool 3: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.\nTool 4: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 5: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 7: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 8: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 11: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 12: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 13: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 14: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 15: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 17: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 19: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\", \"parameters\": {\"sentence1\": {\"type\": \"string\", \"description\": \"The first sentence in the pair to be classified for natural language inference.\", \"required\": true, \"example\": \"A man is eating pizza\"}, \"sentence2\": {\"type\": \"string\", \"description\": \"The second sentence in the pair to be classified for natural language inference.\", \"required\": true, \"example\": \"A man eats something\"}}, \"response\": \"The tool is designed for performing **Natural Language Inference (NLI)** using a pretrained **cross-encoder model** based on the **MiniLM** architecture, specifically **cross-encoder/nli-MiniLM2-L6-H768**. \\n\\nThe response generated by this tool will typically contain **three scores** for each given pair of sentences. These scores represent the likelihood of three possible relations between the sentences:\\n\\n1. **Contradiction**: The score indicating how much the second sentence contradicts the first one.\\n2. **Entailment**: The score indicating how much the second sentence entails (or supports) the first one.\\n3. **Neutral**: The score indicating how much the second sentence is neutral in relation to the first one (i.e., neither supporting nor contradicting it).\\n\\nThese scores are used to classify the relationship between two sentences based on their semantic meaning. The response will include these three scores for each pair of input sentences provided to the model.\\n\\nThe response may also indicate a **confidence score** or normalized values for each label (Contradiction, Entailment, and Neutral), where higher values typically indicate stronger confidence in the respective classification.\\n\\n### Expected components of the response:\\n\\n1. **scores** (list): A list of three numeric values corresponding to the **Contradiction**, **Entailment**, and **Neutral** labels for each pair of sentences.\\n2. **label** (optional, inferred): Based on the scores, a label indicating the most likely relationship (either \\\"contradiction\\\", \\\"entailment\\\", or \\\"neutral\\\").\\n\\nThe response could be structured as follows:\\n- For each pair of input sentences, the tool will provide the corresponding three classification scores.\\n\", \"examples\": [{\"user_intent\": \"I want to check if 'The cat is sleeping on the sofa' implies 'A cat is resting indoors'.\", \"tool_call\": {\"name\": \"cross-encoder/nli-MiniLM2-L6-H768\", \"arguments\": {\"sentence1\": \"The cat is sleeping on the sofa\", \"sentence2\": \"A cat is resting indoors\"}}}, {\"user_intent\": \"Determine whether 'A person is jogging in the park' contradicts 'Someone is sitting on a bench'.\", \"tool_call\": {\"name\": \"cross-encoder/nli-MiniLM2-L6-H768\", \"arguments\": {\"sentence1\": \"A person is jogging in the park\", \"sentence2\": \"Someone is sitting on a bench\"}}}, {\"user_intent\": \"Check the relationship between 'A black race car starts up in front of a crowd of people.' and 'A man is driving down a lonely road.'\", \"tool_call\": {\"name\": \"cross-encoder/nli-MiniLM2-L6-H768\", \"arguments\": {\"sentence1\": \"A black race car starts up in front of a crowd of people.\", \"sentence2\": \"A man is driving down a lonely road.\"}}}]}\n\n# User Query\nWe need to identify the relationship between two sentences whether they are contradictory, entail each other, or neutral.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Determine the semantic relationship between two sentences to see if they contradict, entail, or are neutral to each other.",
                    "tool_description": "Perform natural language inference (NLI) on a pair of sentences using a pre-trained cross-encoder model, outputting scores for contradiction, entailment, and neutral relationships to classify their semantic connection.",
                    "expected_response": "A set of three numeric scores indicating the likelihood of contradiction, entailment, and neutral relationships between the two sentences, optionally including the most probable label."
                }
            ],
            "extracted_arguments": {
                "sentence1": "The first sentence in the pair to analyze for semantic relationship.",
                "sentence2": "The second sentence in the pair to analyze for semantic relationship."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_226",
        "query": "We are making an AI copywriter for marketing content. Help me to provide content for a product relating to eco-friendly kitchenware.",
        "instruction": "Given a `marketing content generation` task, retrieve tools that specialize in producing creative and compelling text by processing thematic inputs related to products, such as eco-friendly kitchenware, to generate effective marketing copy.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='facebook/opt-125m')",
                    "api_arguments": {
                        "do_sample": "True"
                    },
                    "python_environment_requirements": "from transformers import pipeline, set_seed",
                    "example_code": "generator(Hello, I'm am conscious and)",
                    "performance": {
                        "dataset": "Various",
                        "accuracy": "Roughly matches GPT-3 performance"
                    },
                    "description": "OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.",
                    "id": "huggingface_api_617",
                    "name": "facebook/opt-125m"
                },
                "id": "gorilla_huggingface_tool_612",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 2: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 3: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 4: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.",
            "Tool 5: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 7: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 8: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.",
            "Tool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 10: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 11: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 12: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 13: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.",
            "Tool 14: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 15: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 16: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 18: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 19: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness."
        ],
        "tool_view": {
            "description": "The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "parameters": {},
            "response": "The tool’s response is a structured output indicating whether the input text is likely to have been generated by a GPT-2 model. The response is typically a list of dictionaries (or objects), where each dictionary corresponds to a single text input and contains the following components:\n\n1. **label** – A string representing the predicted class for the input text. For this detector, the possible values are typically along the lines of `\"GPT-2 generated\"` or `\"Human-written\"` (or similar phrasing depending on the implementation). This indicates the model’s categorical prediction.\n\n2. **score** – A numerical value representing the model’s confidence in its prediction. This is usually a probability between 0 and 1, quantifying how strongly the model believes the text belongs to the predicted class. Higher values indicate higher confidence.\n\n3. **additional metadata (optional)** – Some text-classification pipelines may include extra fields, such as the input text or model-specific information, but the core components are the `label` and `score`.\n\nIn summary, for each input text, the response provides a classification label and a confidence score that together describe whether the text is likely AI-generated or human-written. The list structure allows the tool to handle multiple texts in a single call.",
            "examples": [
                {
                    "user_intent": "I want to check if this paragraph was written by GPT-2: 'In the future, AI will revolutionize the way we work and live.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "In the future, AI will revolutionize the way we work and live."
                        }
                    }
                },
                {
                    "user_intent": "Detect whether the following email is AI-generated: 'Dear user, your account has been updated successfully.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "Dear user, your account has been updated successfully."
                        }
                    }
                },
                {
                    "user_intent": "Analyze this article snippet to see if it was produced by a GPT-2 model: 'Climate change impacts are becoming increasingly severe worldwide.'",
                    "tool_call": {
                        "name": "roberta-base-openai-detector",
                        "arguments": {
                            "text": "Climate change impacts are becoming increasingly severe worldwide."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 2: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 3: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 4: This tool uses the \"bhadresh-savani/distilbert-base-uncased-emotion\" model, fine-tuned on the Emotion dataset, to classify text based on emotional content. It leverages the Hugging Face Transformers framework to provide fast and efficient emotion detection, with high accuracy (93.8%) on the Twitter-Sentiment-Analysis dataset. The model is a distilled version of BERT, making it smaller and faster while retaining most of BERT's language understanding capabilities. It is designed for tasks that involve analyzing and classifying emotional tones in text.\nTool 5: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 6: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 7: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 8: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\nTool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 10: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 11: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 12: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 13: The GIT-large-COCO model is a generative image-to-text transformer that converts images into descriptive text, fine-tuned on the COCO dataset to produce accurate and detailed captions for visual content. It is designed for multimodal applications that require understanding and verbalizing the content of images.\nTool 14: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 15: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 16: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 18: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 19: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\", \"parameters\": {}, \"response\": \"The tool\\u2019s response is a structured output indicating whether the input text is likely to have been generated by a GPT-2 model. The response is typically a list of dictionaries (or objects), where each dictionary corresponds to a single text input and contains the following components:\\n\\n1. **label** \\u2013 A string representing the predicted class for the input text. For this detector, the possible values are typically along the lines of `\\\"GPT-2 generated\\\"` or `\\\"Human-written\\\"` (or similar phrasing depending on the implementation). This indicates the model\\u2019s categorical prediction.\\n\\n2. **score** \\u2013 A numerical value representing the model\\u2019s confidence in its prediction. This is usually a probability between 0 and 1, quantifying how strongly the model believes the text belongs to the predicted class. Higher values indicate higher confidence.\\n\\n3. **additional metadata (optional)** \\u2013 Some text-classification pipelines may include extra fields, such as the input text or model-specific information, but the core components are the `label` and `score`.\\n\\nIn summary, for each input text, the response provides a classification label and a confidence score that together describe whether the text is likely AI-generated or human-written. The list structure allows the tool to handle multiple texts in a single call.\", \"examples\": [{\"user_intent\": \"I want to check if this paragraph was written by GPT-2: 'In the future, AI will revolutionize the way we work and live.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"In the future, AI will revolutionize the way we work and live.\"}}}, {\"user_intent\": \"Detect whether the following email is AI-generated: 'Dear user, your account has been updated successfully.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"Dear user, your account has been updated successfully.\"}}}, {\"user_intent\": \"Analyze this article snippet to see if it was produced by a GPT-2 model: 'Climate change impacts are becoming increasingly severe worldwide.'\", \"tool_call\": {\"name\": \"roberta-base-openai-detector\", \"arguments\": {\"text\": \"Climate change impacts are becoming increasingly severe worldwide.\"}}}]}\n\n# User Query\nWe are making an AI copywriter for marketing content. Help me to provide content for a product relating to eco-friendly kitchenware.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate marketing content for an eco-friendly kitchenware product.",
                    "tool_description": "Use a pre-trained language model to create coherent and contextually relevant promotional text based on a product description and marketing context.",
                    "expected_response": "A text output containing multiple suggested marketing copy variations suitable for eco-friendly kitchenware, including headlines, slogans, or descriptive paragraphs."
                }
            ],
            "extracted_arguments": {
                "product_category": "The type of product to promote, e.g., eco-friendly kitchenware, as a textual description.",
                "marketing_context": "Optional additional context for the marketing content, such as target audience, tone, or style, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_38",
        "query": "We are developing a program to teach French to English speakers. The program should complete a sentence with a missing word in French.",
        "instruction": "Given a `language learning assistance` task, retrieve tools that utilize natural language processing to complete sentences in French by identifying and suggesting suitable words for fill-mask tasks, designed to enhance language learning for English speakers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')",
                    "api_arguments": [
                        "model",
                        "tokenizer"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')",
                    "performance": {
                        "dataset": "oscar",
                        "accuracy": "N/A"
                    },
                    "description": "CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.",
                    "id": "huggingface_api_669",
                    "name": "camembert-base"
                },
                "id": "gorilla_huggingface_tool_664",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 1: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 4: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 5: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 7: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 8: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 10: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 11: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 12: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 13: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 14: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 15: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 16: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 17: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 19: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text."
        ],
        "tool_view": {
            "description": "CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.",
                    "required": true,
                    "example": "camembert-base"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.",
                    "required": true,
                    "example": "camembert-base"
                }
            },
            "response": "The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \n\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \n- **score**: A numerical value representing the model’s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \n- **token**: The identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \n\nThe response is usually ordered by descending score, so the first entry is the model’s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.",
            "examples": [
                {
                    "user_intent": "Predict the missing word in the French sentence about cheese.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Le camembert est <mask> :)"
                        }
                    }
                },
                {
                    "user_intent": "Complete this French sentence describing the weather.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Aujourd'hui, il fait très <mask> dehors."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for a French sentence about happiness.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Je me sens vraiment <mask> aujourd'hui."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 1: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 3: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 4: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 5: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 6: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 7: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 8: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 10: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 11: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 12: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 13: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 14: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 15: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 16: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 17: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 19: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.\", \"required\": true, \"example\": \"camembert-base\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.\", \"required\": true, \"example\": \"camembert-base\"}}, \"response\": \"The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \\n\\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \\n- **score**: A numerical value representing the model\\u2019s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \\n- **token**: The identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \\n\\nThe response is usually ordered by descending score, so the first entry is the model\\u2019s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.\", \"examples\": [{\"user_intent\": \"Predict the missing word in the French sentence about cheese.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Le camembert est <mask> :)\"}}}, {\"user_intent\": \"Complete this French sentence describing the weather.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Aujourd'hui, il fait tr\\u00e8s <mask> dehors.\"}}}, {\"user_intent\": \"Fill in the blank for a French sentence about happiness.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Je me sens vraiment <mask> aujourd'hui.\"}}}]}\n\n# User Query\nWe are developing a program to teach French to English speakers. The program should complete a sentence with a missing word in French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Complete a French sentence with a missing word for language learning purposes.",
                    "tool_description": "Use a French masked language model to predict the missing word in a sentence, providing contextual word suggestions to aid in language education.",
                    "expected_response": "A ranked list of candidate words to fill the masked position, each with the completed sentence and a confidence score."
                }
            ],
            "extracted_arguments": {
                "model": "The pretrained French language model to use for fill-mask predictions.",
                "tokenizer": "The tokenizer corresponding to the selected French language model.",
                "text": "The French sentence containing a masked position representing the missing word."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_316",
        "query": "As a news agency, we need a summarized version of a recent article about YouTube's new policy on vaccine misinformation.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise and coherent summaries that capture the main points of the original content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 1: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 2: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 3: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 4: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 5: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 8: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 9: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 11: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 12: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 13: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 17: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 18: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables."
        ],
        "tool_view": {
            "description": "A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in Italian that you want the model to summarize. This should be a coherent piece of text such as a news article or a paragraph.",
                    "required": true,
                    "example": "Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di una sorta di Netflix della cultura."
                }
            },
            "response": "The tool produces a concise summary of the input Italian text provided in the `text` argument. The response is a string representing the condensed version of the original content, preserving the main points and essential information while omitting less critical details.  \n\nSpecifically:  \n\n- **Type**: Text string.  \n- **Content**: A shortened version of the input text, maintaining semantic coherence, grammatical correctness, and the key information or narrative of the original.  \n- **Purpose**: To enable quick understanding of long or detailed Italian news articles by distilling them into a few sentences or a brief paragraph.  \n- **Constraints**: The summary is generated in Italian, reflecting the source language, and is typically aligned with the style and factual content of the original input.  \n\nNo additional metadata or structured fields are returned; the output is solely the generated summarized text.",
            "examples": [
                {
                    "user_intent": "Can you provide a concise summary of this Italian news article about the launch of ITsART?",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."
                        }
                    }
                },
                {
                    "user_intent": "Summarize this Italian article about recent cultural events for a newsletter.",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Il ministero della Cultura ha annunciato una nuova serie di eventi culturali in tutta Italia, inclusi spettacoli teatrali e mostre d’arte contemporanea. L’iniziativa mira a promuovere la partecipazione dei giovani e a valorizzare le produzioni locali, con particolare attenzione alle regioni meno servite dai grandi eventi. Gli organizzatori sperano che la rassegna stimoli un aumento del turismo culturale durante i prossimi mesi."
                        }
                    }
                },
                {
                    "user_intent": "Create a brief summary of this Italian news text for a mobile app notification.",
                    "tool_call": {
                        "name": "it5-base-news-summarization",
                        "arguments": {
                            "text": "Secondo le ultime statistiche, la piattaforma ITsART ha superato i 100.000 abbonati nel primo mese di attività. Il catalogo comprende film, documentari e spettacoli teatrali, molti dei quali accessibili solo a pagamento, mentre alcune produzioni sono disponibili gratuitamente su altre piattaforme. L’iniziativa vuole offrire una vetrina internazionale per la cultura italiana."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 1: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 2: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 3: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 4: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 5: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 8: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 9: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 10: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 11: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 12: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 13: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 17: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 18: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in Italian that you want the model to summarize. This should be a coherent piece of text such as a news article or a paragraph.\", \"required\": true, \"example\": \"Dal 31 maggio \\u00e8 infine partita la piattaforma ITsART, a pi\\u00f9 di un anno da quando \\u2013 durante il primo lockdown \\u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di una sorta di Netflix della cultura.\"}}, \"response\": \"The tool produces a concise summary of the input Italian text provided in the `text` argument. The response is a string representing the condensed version of the original content, preserving the main points and essential information while omitting less critical details.  \\n\\nSpecifically:  \\n\\n- **Type**: Text string.  \\n- **Content**: A shortened version of the input text, maintaining semantic coherence, grammatical correctness, and the key information or narrative of the original.  \\n- **Purpose**: To enable quick understanding of long or detailed Italian news articles by distilling them into a few sentences or a brief paragraph.  \\n- **Constraints**: The summary is generated in Italian, reflecting the source language, and is typically aligned with the style and factual content of the original input.  \\n\\nNo additional metadata or structured fields are returned; the output is solely the generated summarized text.\", \"examples\": [{\"user_intent\": \"Can you provide a concise summary of this Italian news article about the launch of ITsART?\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Dal 31 maggio \\u00e8 infine partita la piattaforma ITsART, a pi\\u00f9 di un anno da quando \\u2013 durante il primo lockdown \\u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \\u00abuna sorta di Netflix della cultura\\u00bb, pensata per \\u00aboffrire a tutto il mondo la cultura italiana a pagamento\\u00bb. \\u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\\u00e0 difficile farlo anche pi\\u00f9 avanti senza numeri precisi. Al momento, l\\u2019unica cosa che si pu\\u00f2 fare \\u00e8 guardare com\\u2019\\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \\u201ctitoli\\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\\u00e0. Intanto, una cosa notata da pi\\u00f9 parti \\u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.\"}}}, {\"user_intent\": \"Summarize this Italian article about recent cultural events for a newsletter.\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Il ministero della Cultura ha annunciato una nuova serie di eventi culturali in tutta Italia, inclusi spettacoli teatrali e mostre d\\u2019arte contemporanea. L\\u2019iniziativa mira a promuovere la partecipazione dei giovani e a valorizzare le produzioni locali, con particolare attenzione alle regioni meno servite dai grandi eventi. Gli organizzatori sperano che la rassegna stimoli un aumento del turismo culturale durante i prossimi mesi.\"}}}, {\"user_intent\": \"Create a brief summary of this Italian news text for a mobile app notification.\", \"tool_call\": {\"name\": \"it5-base-news-summarization\", \"arguments\": {\"text\": \"Secondo le ultime statistiche, la piattaforma ITsART ha superato i 100.000 abbonati nel primo mese di attivit\\u00e0. Il catalogo comprende film, documentari e spettacoli teatrali, molti dei quali accessibili solo a pagamento, mentre alcune produzioni sono disponibili gratuitamente su altre piattaforme. L\\u2019iniziativa vuole offrire una vetrina internazionale per la cultura italiana.\"}}}]}\n\n# User Query\nAs a news agency, we need a summarized version of a recent article about YouTube's new policy on vaccine misinformation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Provide a concise summary of a recent Italian news article about YouTube's new policy on vaccine misinformation.",
                    "tool_description": "Generate a condensed summary of an Italian-language news article, capturing the main points and essential information about YouTube's updated policy on vaccine misinformation.",
                    "expected_response": "A text string containing a brief, coherent summary of the original article in Italian, preserving the key facts and narrative while omitting less critical details."
                }
            ],
            "extracted_arguments": {
                "text": "The full Italian news article discussing YouTube's new policy on vaccine misinformation that needs to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_236",
        "query": "Create a program that predicts carbon emissions for new data using the given model.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that utilize pre-trained regression models to predict emissions by processing new data inputs and applying the specified model configurations to generate predictions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "model.joblib",
                        "config.json",
                        "data.csv"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-dragino-7-7-max_300m",
                        "accuracy": {
                            "Loss": 50.918,
                            "R2": 0.304,
                            "MSE": 2592.667,
                            "MAE": 39.693,
                            "RMSLE": 0.429
                        }
                    },
                    "description": "A tabular regression model for predicting carbon emissions using the pcoloc/autotrain-dragino-7-7-max_300m-1861063640 dataset. Trained with AutoTrain.",
                    "id": "huggingface_api_881",
                    "name": "pcoloc/autotrain-dragino-7-7-max_300m-1861063640"
                },
                "id": "gorilla_huggingface_tool_874",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 1: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 3: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 5: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 7: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 9: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 10: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 11: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 13: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 14: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 15: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 16: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 18: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 19: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset."
        ],
        "tool_view": {
            "description": "A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "parameters": {},
            "response": "The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \n\n1. **Predictions**:  \n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \n   - Each element corresponds to a single input record, representing the model’s estimated carbon emissions category.  \n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\n\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\n\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.",
            "examples": [
                {
                    "user_intent": "I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "machines.csv"
                        }
                    }
                },
                {
                    "user_intent": "I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "delivery_trucks.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 1: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 3: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 5: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 7: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 9: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 10: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 11: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 13: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 14: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 15: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 16: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 18: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 19: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\", \"parameters\": {}, \"response\": \"The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \\n\\n1. **Predictions**:  \\n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \\n   - Each element corresponds to a single input record, representing the model\\u2019s estimated carbon emissions category.  \\n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\\n\\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \\n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \\n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\\n\\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.\", \"examples\": [{\"user_intent\": \"I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"machines.csv\"}}}, {\"user_intent\": \"I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"delivery_trucks.csv\"}}}]}\n\n# User Query\nCreate a program that predicts carbon emissions for new data using the given model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict carbon emissions for a new dataset using the provided model.",
                    "tool_description": "Use a pre-trained tabular classification or regression model to generate carbon emission predictions for input data based on specified features.",
                    "expected_response": "A structured output containing predicted carbon emission values or class labels for each row in the input dataset."
                }
            ],
            "extracted_arguments": {
                "input_data": "Structured tabular dataset for which carbon emissions need to be predicted.",
                "model": "Pre-trained model object or file used to generate predictions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_341",
        "query": "Our customer wants to analyze the sentiment of their customers' feedback. The feedback is in Spanish.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that perform language processing by analyzing textual feedback in Spanish to determine and classify the sentiment as positive, negative, or neutral, based on the query's language and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')",
                    "api_arguments": "text",
                    "python_environment_requirements": "Hugging Face Transformers library",
                    "example_code": "",
                    "performance": {
                        "dataset": "TASS 2020 corpus",
                        "accuracy": ""
                    },
                    "description": "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.",
                    "id": "huggingface_api_387",
                    "name": "finiteautomata/beto-sentiment-analysis"
                },
                "id": "gorilla_huggingface_tool_383",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 3: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 5: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 6: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 7: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 8: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 9: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 10: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 11: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 12: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 13: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 15: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 16: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 17: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 18: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 19: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 3: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 4: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 5: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 6: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 7: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 8: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 9: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 10: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 11: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 12: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 13: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 15: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 16: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 17: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 18: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 19: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nOur customer wants to analyze the sentiment of their customers' feedback. The feedback is in Spanish.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the sentiment of customer feedback in Spanish.",
                    "tool_description": "Perform sentiment analysis on a Spanish-language text to classify it as positive, negative, or neutral based on the feedback provided.",
                    "expected_response": "A sentiment classification result with a label (positive, negative, or neutral) and a confidence score for the given Spanish feedback."
                }
            ],
            "extracted_arguments": {
                "text": "The Spanish-language customer feedback or text snippet for sentiment analysis."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_93",
        "query": "Build a simple application to predict the survival status of passengers on the Titanic based on their age, gender, and passenger class.",
        "instruction": "Given a `predictive modeling` task, retrieve tools that facilitate the creation of a binary classification application using tabular data, specifically designed to predict outcomes based on characteristics like age, gender, and class for the Titanic survival dataset.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face",
                    "functionality": "Binary Classification",
                    "api_call": "AutoModel.from_pretrained('harithapliyal/autotrain-tatanic-survival-51030121311')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "harithapliyal/autotrain-data-tatanic-survival",
                        "accuracy": 0.872
                    },
                    "description": "A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.",
                    "id": "huggingface_api_852",
                    "name": "harithapliyal/autotrain-tatanic-survival-51030121311"
                },
                "id": "gorilla_huggingface_tool_846",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 1: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 2: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 3: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 4: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 5: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.",
            "Tool 6: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 8: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 9: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.",
            "Tool 10: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 11: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 12: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 13: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 14: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 15: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 16: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 17: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.",
            "Tool 18: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 19: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features."
        ],
        "tool_view": {
            "description": "A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "parameters": {
                "data": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the input features for the Titanic survival prediction model. Columns should match the features specified in the model's configuration.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_age': [22, 38], 'feat_gender': [0, 1], 'feat_pclass': [3, 1]})"
                }
            },
            "response": "The tool produces a **binary classification output** for tabular data, indicating whether each input instance belongs to one of two classes (e.g., survived vs. did not survive). The response typically has the following structure and components:  \n\n1. **Predictions Array**:  \n   - A sequence (e.g., list, array, or Series) where each element corresponds to one row of the input data.  \n   - Each element represents the predicted class label for that row, encoded in a binary format (e.g., 0 or 1, False or True, depending on implementation).  \n\n2. **Optional Probabilities / Scores (if supported)**:  \n   - Some implementations may provide an array of probabilities alongside the predicted labels, representing the model’s confidence that the instance belongs to the positive class.  \n   - This would be structured as either a single probability per instance or a two-element vector for [negative_class_prob, positive_class_prob].  \n\n3. **Index Alignment**:  \n   - The predictions are aligned with the input data rows, preserving the original ordering of the dataset.  \n\nIn summary, the response is a structured output mapping each input row of tabular features to a predicted binary class, optionally including confidence scores for each prediction. This enables downstream tasks such as evaluation, thresholding, or further analysis.",
            "examples": [
                {
                    "user_intent": "Predict whether passengers survived the Titanic using a CSV file with passenger details.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "titanic_passengers.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a dataset with age, sex, and ticket class of passengers. Check who survived.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "passenger_info.csv"
                        }
                    }
                },
                {
                    "user_intent": "Classify a batch of Titanic passengers from an Excel export to see survival predictions.",
                    "tool_call": {
                        "name": "harithapliyal/autotrain-tatanic-survival-51030121311",
                        "arguments": {
                            "data": "batch_passengers.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 1: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 2: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 3: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 4: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 5: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.\nTool 6: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 8: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 9: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\nTool 10: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 11: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 12: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 13: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 14: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 15: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 16: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 17: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.\nTool 18: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 19: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\", \"parameters\": {\"data\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the input features for the Titanic survival prediction model. Columns should match the features specified in the model's configuration.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_age': [22, 38], 'feat_gender': [0, 1], 'feat_pclass': [3, 1]})\"}}, \"response\": \"The tool produces a **binary classification output** for tabular data, indicating whether each input instance belongs to one of two classes (e.g., survived vs. did not survive). The response typically has the following structure and components:  \\n\\n1. **Predictions Array**:  \\n   - A sequence (e.g., list, array, or Series) where each element corresponds to one row of the input data.  \\n   - Each element represents the predicted class label for that row, encoded in a binary format (e.g., 0 or 1, False or True, depending on implementation).  \\n\\n2. **Optional Probabilities / Scores (if supported)**:  \\n   - Some implementations may provide an array of probabilities alongside the predicted labels, representing the model\\u2019s confidence that the instance belongs to the positive class.  \\n   - This would be structured as either a single probability per instance or a two-element vector for [negative_class_prob, positive_class_prob].  \\n\\n3. **Index Alignment**:  \\n   - The predictions are aligned with the input data rows, preserving the original ordering of the dataset.  \\n\\nIn summary, the response is a structured output mapping each input row of tabular features to a predicted binary class, optionally including confidence scores for each prediction. This enables downstream tasks such as evaluation, thresholding, or further analysis.\", \"examples\": [{\"user_intent\": \"Predict whether passengers survived the Titanic using a CSV file with passenger details.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"titanic_passengers.csv\"}}}, {\"user_intent\": \"I have a dataset with age, sex, and ticket class of passengers. Check who survived.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"passenger_info.csv\"}}}, {\"user_intent\": \"Classify a batch of Titanic passengers from an Excel export to see survival predictions.\", \"tool_call\": {\"name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"arguments\": {\"data\": \"batch_passengers.csv\"}}}]}\n\n# User Query\nBuild a simple application to predict the survival status of passengers on the Titanic based on their age, gender, and passenger class.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict whether passengers survived the Titanic using their age, gender, and passenger class.",
                    "tool_description": "Apply a binary tabular classification model trained on the Titanic survival dataset to predict passenger survival based on input features such as age, gender, and passenger class.",
                    "expected_response": "A structured binary classification output indicating survival status for each passenger, optionally including prediction probabilities."
                }
            ],
            "extracted_arguments": {
                "data": "A pandas DataFrame containing passenger features including age, gender, and passenger class for survival prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_325",
        "query": "You want to create a bot that can play the Pong No Frameskip-v4 game with exceptional skill.",
        "instruction": "Given a `game-playing AI` task, retrieve tools or models capable of leveraging deep reinforcement learning frameworks to train or deploy agents that exhibit advanced skill levels in designated environments, such as playing the Pong No Frameskip-v4 game.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-PongNoFrameskip-v4')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "RL Zoo",
                        "SB3",
                        "SB3 Contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env PongNoFrameskip-v4 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "PongNoFrameskip-v4",
                        "accuracy": "21.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_898",
                    "name": "ppo-PongNoFrameskip-v4"
                },
                "id": "gorilla_huggingface_tool_891",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 4: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 5: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 6: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 7: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 8: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 9: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 10: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 11: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 12: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 13: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 14: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 15: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 16: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 17: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts."
        ],
        "tool_view": {
            "description": "Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The text description that will be used to generate an image. This is the main input that guides the image generation process.",
                    "required": true,
                    "example": "A futuristic city skyline at sunset"
                }
            },
            "response": "The tool is designed to generate images from text descriptions using a model called \"Openjourney v4,\" which has been trained on over 124,000 Midjourney v4 images. When the tool is called, it accepts a textual input and returns an image based on that input.\n\n### Expected Response Description:\n\nThe response from the tool is expected to be an image or a reference to an image generated based on the given text prompt. The format and structure of the response will likely depend on how the tool is integrated into a specific environment, but the core components will generally include:\n\n1. **Generated Image**: The main output will be the image itself, created based on the input text. This image may be provided directly in the form of a visual output or as a URL linking to the generated image.\n   \n2. **Metadata (Optional)**: There may be metadata accompanying the image, such as:\n   - A unique identifier for the generated image.\n   - Any information about the processing or rendering parameters used.\n   - Possible error messages if the input was not processed correctly or if there was an issue with the model execution.\n\nIn a typical implementation, the system would not provide detailed accuracy measures or performance stats in the response, as those are typically part of the model documentation or overall system metrics.\n\nThe response is expected to be structured primarily around delivering the generated image or the link to it.",
            "examples": [
                {
                    "user_intent": "Create a fantasy landscape with floating islands and a sunset sky.",
                    "tool_call": {
                        "name": "prompthero/openjourney-v4",
                        "arguments": {
                            "text": "fantasy landscape with floating islands, dramatic sunset sky, highly detailed"
                        }
                    }
                },
                {
                    "user_intent": "Generate a realistic portrait of a cyberpunk robot in a neon city at night.",
                    "tool_call": {
                        "name": "prompthero/openjourney-v4",
                        "arguments": {
                            "text": "cyberpunk robot portrait, neon city background, night, photorealistic, intricate details"
                        }
                    }
                },
                {
                    "user_intent": "Make an illustration of a cozy cottage in the middle of a snowy forest.",
                    "tool_call": {
                        "name": "prompthero/openjourney-v4",
                        "arguments": {
                            "text": "cozy cottage, snowy forest, winter morning, warm lighting, storybook style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 4: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 5: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 6: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 7: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 8: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 9: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 10: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 11: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 12: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 13: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 14: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 15: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 16: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 17: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The text description that will be used to generate an image. This is the main input that guides the image generation process.\", \"required\": true, \"example\": \"A futuristic city skyline at sunset\"}}, \"response\": \"The tool is designed to generate images from text descriptions using a model called \\\"Openjourney v4,\\\" which has been trained on over 124,000 Midjourney v4 images. When the tool is called, it accepts a textual input and returns an image based on that input.\\n\\n### Expected Response Description:\\n\\nThe response from the tool is expected to be an image or a reference to an image generated based on the given text prompt. The format and structure of the response will likely depend on how the tool is integrated into a specific environment, but the core components will generally include:\\n\\n1. **Generated Image**: The main output will be the image itself, created based on the input text. This image may be provided directly in the form of a visual output or as a URL linking to the generated image.\\n   \\n2. **Metadata (Optional)**: There may be metadata accompanying the image, such as:\\n   - A unique identifier for the generated image.\\n   - Any information about the processing or rendering parameters used.\\n   - Possible error messages if the input was not processed correctly or if there was an issue with the model execution.\\n\\nIn a typical implementation, the system would not provide detailed accuracy measures or performance stats in the response, as those are typically part of the model documentation or overall system metrics.\\n\\nThe response is expected to be structured primarily around delivering the generated image or the link to it.\", \"examples\": [{\"user_intent\": \"Create a fantasy landscape with floating islands and a sunset sky.\", \"tool_call\": {\"name\": \"prompthero/openjourney-v4\", \"arguments\": {\"text\": \"fantasy landscape with floating islands, dramatic sunset sky, highly detailed\"}}}, {\"user_intent\": \"Generate a realistic portrait of a cyberpunk robot in a neon city at night.\", \"tool_call\": {\"name\": \"prompthero/openjourney-v4\", \"arguments\": {\"text\": \"cyberpunk robot portrait, neon city background, night, photorealistic, intricate details\"}}}, {\"user_intent\": \"Make an illustration of a cozy cottage in the middle of a snowy forest.\", \"tool_call\": {\"name\": \"prompthero/openjourney-v4\", \"arguments\": {\"text\": \"cozy cottage, snowy forest, winter morning, warm lighting, storybook style\"}}}]}\n\n# User Query\nYou want to create a bot that can play the Pong No Frameskip-v4 game with exceptional skill.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained reinforcement learning agent for the PongNoFrameskip-v4 environment.",
                    "tool_description": "Load a pre-trained PPO reinforcement learning agent designed for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo.",
                    "expected_response": "A confirmation object indicating that the model has been successfully loaded and is ready to interact with the environment."
                },
                {
                    "user_intent": "I want to evaluate the performance of the loaded agent in the PongNoFrameskip-v4 environment.",
                    "tool_description": "Evaluate the performance of the pre-trained PPO reinforcement learning agent in the PongNoFrameskip-v4 environment, simulating gameplay and assessing its performance.",
                    "expected_response": "A report or feedback on the agent's performance during the evaluation, potentially including scores or metrics like win rate."
                }
            ],
            "extracted_arguments": {
                "environment": "The name of the environment (PongNoFrameskip-v4) for which the reinforcement learning model is trained.",
                "model_type": "The specific type of model (PPO) to be used for reinforcement learning.",
                "evaluation_task": "The task of evaluating the performance of the pre-trained agent in the given environment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_497",
        "query": "We have a dataset related to coffee and tea prices. We need to answer a question on who sells hot chocolate and their prices.",
        "instruction": "Given a `table-based question answering` task, retrieve tools that can process tabular data inputs to answer specific queries about the dataset, focusing on entities and their corresponding attributes or prices.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')",
                    "api_arguments": [
                        "model_name",
                        "table",
                        "queries"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "N/A",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.5148
                    },
                    "description": "TAPAS mini model fine-tuned on Sequential Question Answering (SQA)",
                    "id": "huggingface_api_457",
                    "name": "google/tapas-mini-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_453",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 1: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 2: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 3: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 5: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 8: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 9: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 11: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 15: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 16: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 17: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 18: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 19: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021."
        ],
        "tool_view": {
            "description": "This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "parameters": {
                "data": {
                    "type": "pandas.DataFrame",
                    "description": "A pandas DataFrame containing the input data for the model. This data will be used for making predictions.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_1': [1, 2, 3], 'feat_2': [4, 5, 6]})"
                }
            },
            "response": "The response from this tool would contain the following key components:\n\n1. **Model Predictions**:\n   - **predictions**: This would be the primary output, containing the predicted values for the input data. It represents the model’s predicted values based on the input features from the provided data (typically in the form of a pandas DataFrame).\n   \n2. **Model Information**:\n   - **name**: The identifier or name of the model, in this case, \"jwan2021/autotrain-us-housing-prices-1771761511\".\n   - **description**: A textual description of the model, explaining that it was trained using AutoTrain on the US housing prices dataset, with the dataset ID and the model being a single column regression model.\n   - **framework**: Specifies the framework used to train the model, which is \"Joblib\" in this case.\n   \n3. **Performance Metrics**:\n   - **accuracy**: A dictionary containing various performance metrics that evaluate the model's predictive accuracy. These metrics might include:\n     - **Loss**: The loss value (such as Mean Squared Error, or MSE).\n     - **R2**: The R-squared value, which measures the proportion of variance in the dependent variable explained by the model.\n     - **MSE**: The Mean Squared Error, representing the average squared difference between actual and predicted values.\n     - **MAE**: The Mean Absolute Error, indicating the average absolute differences between predicted and actual values.\n     - **RMSLE**: The Root Mean Squared Logarithmic Error, which is a measure of prediction error for regression tasks where scale is important.\n\n4. **Python Environment Requirements**:\n   - **joblib**: Specifies the version of the joblib library required (the latest version).\n   - **pandas**: Specifies the version of the pandas library required (the latest version).\n\n5. **Example Code**:\n   - An example of how to use the tool, including:\n     - Loading the model using `joblib.load('model.joblib')`.\n     - Loading the configuration file (`config.json`) and selecting features.\n     - Reading input data from a CSV file using pandas and applying the necessary feature transformations (renaming columns).\n     - Using the model to make predictions on the transformed data.\n\nThe response is structured around providing not only the predictions but also key details about the model's configuration, performance metrics, and the environment needed to run it effectively.",
            "examples": [
                {
                    "user_intent": "Predict the prices of houses in my CSV file using the pre-trained US housing model.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "pd.read_csv('my_housing_data.csv')"
                        }
                    }
                },
                {
                    "user_intent": "I have a dataset with selected features and I want to get the model's predicted housing prices.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "data[['num_bedrooms','num_bathrooms','square_feet','zipcode']]"
                        }
                    }
                },
                {
                    "user_intent": "Run the single column regression model on a small DataFrame to estimate housing prices for testing.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "pd.DataFrame({'num_bedrooms':[3,4], 'num_bathrooms':[2,3], 'square_feet':[1500, 2200], 'zipcode':[94103, 94016]})"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 1: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 2: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 3: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 5: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 8: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 9: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 11: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 12: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 13: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 14: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 15: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 16: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 17: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 18: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 19: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\", \"parameters\": {\"data\": {\"type\": \"pandas.DataFrame\", \"description\": \"A pandas DataFrame containing the input data for the model. This data will be used for making predictions.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_1': [1, 2, 3], 'feat_2': [4, 5, 6]})\"}}, \"response\": \"The response from this tool would contain the following key components:\\n\\n1. **Model Predictions**:\\n   - **predictions**: This would be the primary output, containing the predicted values for the input data. It represents the model\\u2019s predicted values based on the input features from the provided data (typically in the form of a pandas DataFrame).\\n   \\n2. **Model Information**:\\n   - **name**: The identifier or name of the model, in this case, \\\"jwan2021/autotrain-us-housing-prices-1771761511\\\".\\n   - **description**: A textual description of the model, explaining that it was trained using AutoTrain on the US housing prices dataset, with the dataset ID and the model being a single column regression model.\\n   - **framework**: Specifies the framework used to train the model, which is \\\"Joblib\\\" in this case.\\n   \\n3. **Performance Metrics**:\\n   - **accuracy**: A dictionary containing various performance metrics that evaluate the model's predictive accuracy. These metrics might include:\\n     - **Loss**: The loss value (such as Mean Squared Error, or MSE).\\n     - **R2**: The R-squared value, which measures the proportion of variance in the dependent variable explained by the model.\\n     - **MSE**: The Mean Squared Error, representing the average squared difference between actual and predicted values.\\n     - **MAE**: The Mean Absolute Error, indicating the average absolute differences between predicted and actual values.\\n     - **RMSLE**: The Root Mean Squared Logarithmic Error, which is a measure of prediction error for regression tasks where scale is important.\\n\\n4. **Python Environment Requirements**:\\n   - **joblib**: Specifies the version of the joblib library required (the latest version).\\n   - **pandas**: Specifies the version of the pandas library required (the latest version).\\n\\n5. **Example Code**:\\n   - An example of how to use the tool, including:\\n     - Loading the model using `joblib.load('model.joblib')`.\\n     - Loading the configuration file (`config.json`) and selecting features.\\n     - Reading input data from a CSV file using pandas and applying the necessary feature transformations (renaming columns).\\n     - Using the model to make predictions on the transformed data.\\n\\nThe response is structured around providing not only the predictions but also key details about the model's configuration, performance metrics, and the environment needed to run it effectively.\", \"examples\": [{\"user_intent\": \"Predict the prices of houses in my CSV file using the pre-trained US housing model.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"pd.read_csv('my_housing_data.csv')\"}}}, {\"user_intent\": \"I have a dataset with selected features and I want to get the model's predicted housing prices.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"data[['num_bedrooms','num_bathrooms','square_feet','zipcode']]\"}}}, {\"user_intent\": \"Run the single column regression model on a small DataFrame to estimate housing prices for testing.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"pd.DataFrame({'num_bedrooms':[3,4], 'num_bathrooms':[2,3], 'square_feet':[1500, 2200], 'zipcode':[94103, 94016]})\"}}}]}\n\n# User Query\nWe have a dataset related to coffee and tea prices. We need to answer a question on who sells hot chocolate and their prices.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to answer a question based on the coffee and tea price dataset.",
                    "tool_description": "Use a table question answering model to process a tabular dataset and answer natural language questions related to the data.",
                    "expected_response": "A relevant answer based on the provided tabular data, such as who sells hot chocolate and their corresponding prices."
                }
            ],
            "extracted_arguments": {
                "table_data": "A pandas DataFrame containing the coffee and tea price dataset to answer questions from.",
                "question": "The natural language question asking for specific information from the dataset, such as who sells hot chocolate and their prices."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_229",
        "query": "We are building a robot for hopping in a controlled environment. Train it to perform a hop using Decision Transformers.",
        "instruction": "Given a `robot training` task, retrieve tools that utilize Reinforcement Learning frameworks, specifically Decision Transformers, to train a robot for performing actions such as hopping by processing environment-specific inputs and configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')",
                    "api_arguments": {
                        "mean": [
                            1.311279,
                            -0.08469521,
                            -0.5382719,
                            -0.07201576,
                            0.04932366,
                            2.1066856,
                            -0.15017354,
                            0.00878345,
                            -0.2848186,
                            -0.18540096,
                            -0.28461286
                        ],
                        "std": [
                            0.17790751,
                            0.05444621,
                            0.21297139,
                            0.14530419,
                            0.6124444,
                            0.85174465,
                            1.4515252,
                            0.6751696,
                            1.536239,
                            1.6160746,
                            5.6072536
                        ]
                    },
                    "python_environment_requirements": {
                        "transformers": "latest"
                    },
                    "example_code": "See our Blog Post, Colab notebook or Example Script for usage.",
                    "performance": {
                        "dataset": "Gym Hopper environment",
                        "accuracy": "Not provided"
                    },
                    "description": "Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.",
                    "id": "huggingface_api_896",
                    "name": "decision-transformer-gym-hopper-medium"
                },
                "id": "gorilla_huggingface_tool_889",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.",
            "Tool 1: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 2: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 3: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 4: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 5: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 6: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 8: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 9: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 10: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 11: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 13: This tool provides a ControlNet-based neural network model designed to guide diffusion-based text-to-image generation by conditioning on instruct pix2pix images, enabling more controlled and precise image synthesis within the Stable Diffusion framework.",
            "Tool 14: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.",
            "Tool 15: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 16: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 17: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 18: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.",
            "Tool 19: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms."
        ],
        "tool_view": {
            "description": "This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.",
            "parameters": {
                "mean": {
                    "type": "array of numbers",
                    "description": "A list of numerical values representing the mean used for normalizing input features for the Decision Transformer model.",
                    "required": true,
                    "example": [
                        1.35,
                        -0.11,
                        -0.55,
                        -0.13,
                        -0.004,
                        2.61,
                        0.02,
                        -0.016,
                        -0.068,
                        -0.052,
                        0.043
                    ]
                },
                "std": {
                    "type": "array of numbers",
                    "description": "A list of numerical values representing the standard deviation used for normalizing input features for the Decision Transformer model.",
                    "required": true,
                    "example": [
                        0.16,
                        0.045,
                        0.14,
                        0.18,
                        0.59,
                        0.59,
                        1.54,
                        0.82,
                        2.02,
                        2.41,
                        5.84
                    ]
                }
            },
            "response": "The tool loads a pretrained Decision Transformer model from the Hugging Face Transformers library. The response produced by the `AutoModel.from_pretrained` call is a **model object** representing this transformer. The structure and meaning of its components are as follows:\n\n- **Model Object**: This is the primary response. It encapsulates the neural network architecture and pretrained weights of the Decision Transformer trained on expert trajectories from the Gym Hopper environment. This object is used for inference or further fine-tuning in reinforcement learning tasks.  \n\n- **Internal Components of the Model Object** (typical for Hugging Face Transformers):\n  - **Configuration**: Contains hyperparameters defining the architecture (e.g., number of layers, hidden size, attention heads) and any training-specific settings.\n  - **Parameters / Weights**: Numerical tensors representing the learned weights of the transformer model. These include embeddings, attention layers, feedforward layers, and output layers.\n  - **Forward Method**: The callable interface used to pass input sequences (observations, actions, returns-to-go) through the model to obtain outputs (e.g., predicted actions or trajectories).\n  - **Device and dtype Information**: Specifies where the model is loaded (CPU/GPU) and the data type of its tensors.\n\n- **Purpose in RL Context**: The model object is designed to generate or predict actions conditioned on sequences of past states, actions, and returns, following the Decision Transformer paradigm. The provided `mean` and `std` arrays are typically used for normalizing input observations before passing them to the model.\n\n- **Additional Metadata**: While not returned explicitly by the API call, the model object may expose methods to access its configuration, tokenizer (if applicable), and pretrained weights.\n\nIn summary, the response is a fully initialized transformer model object ready for inference in reinforcement learning tasks, with all learned weights and architectural details encapsulated.",
            "examples": [
                {
                    "user_intent": "Load a pre-trained Decision Transformer to test expert-level control on the Gym Hopper environment.",
                    "tool_call": {
                        "name": "AutoModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "edbeeching/decision-transformer-gym-hopper-expert"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze the mean and standard deviation of the expert trajectories used in the Hopper model.",
                    "tool_call": {
                        "name": "AutoModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "edbeeching/decision-transformer-gym-hopper-expert",
                            "mean": [
                                1.3490015,
                                -0.11208222,
                                -0.5506444,
                                -0.13188992,
                                -0.00378754,
                                2.6071432,
                                0.02322114,
                                -0.01626922,
                                -0.06840388,
                                -0.05183131,
                                0.04272673
                            ],
                            "std": [
                                0.15980862,
                                0.0446214,
                                0.14307782,
                                0.17629202,
                                0.5912333,
                                0.5899924,
                                1.5405099,
                                0.8152689,
                                2.0173461,
                                2.4107876,
                                5.8440027
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Set up the Decision Transformer model in a Colab notebook with the required Python libraries to run Hopper environment simulations.",
                    "tool_call": {
                        "name": "AutoModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "edbeeching/decision-transformer-gym-hopper-expert"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.\nTool 1: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 2: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 3: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 4: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 5: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 6: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 7: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 8: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 9: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 10: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 11: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 13: This tool provides a ControlNet-based neural network model designed to guide diffusion-based text-to-image generation by conditioning on instruct pix2pix images, enabling more controlled and precise image synthesis within the Stable Diffusion framework.\nTool 14: The \"mio/Artoria\" is a text-to-speech model trained using the ESPnet framework and the Fate recipe, capable of converting text input into speech output. It can be accessed through the ESPnet pipeline API, specifically for text-to-speech tasks, and requires the Transformers library in a Python environment.\nTool 15: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 16: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 17: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 18: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.\nTool 19: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.\", \"parameters\": {\"mean\": {\"type\": \"array of numbers\", \"description\": \"A list of numerical values representing the mean used for normalizing input features for the Decision Transformer model.\", \"required\": true, \"example\": [1.35, -0.11, -0.55, -0.13, -0.004, 2.61, 0.02, -0.016, -0.068, -0.052, 0.043]}, \"std\": {\"type\": \"array of numbers\", \"description\": \"A list of numerical values representing the standard deviation used for normalizing input features for the Decision Transformer model.\", \"required\": true, \"example\": [0.16, 0.045, 0.14, 0.18, 0.59, 0.59, 1.54, 0.82, 2.02, 2.41, 5.84]}}, \"response\": \"The tool loads a pretrained Decision Transformer model from the Hugging Face Transformers library. The response produced by the `AutoModel.from_pretrained` call is a **model object** representing this transformer. The structure and meaning of its components are as follows:\\n\\n- **Model Object**: This is the primary response. It encapsulates the neural network architecture and pretrained weights of the Decision Transformer trained on expert trajectories from the Gym Hopper environment. This object is used for inference or further fine-tuning in reinforcement learning tasks.  \\n\\n- **Internal Components of the Model Object** (typical for Hugging Face Transformers):\\n  - **Configuration**: Contains hyperparameters defining the architecture (e.g., number of layers, hidden size, attention heads) and any training-specific settings.\\n  - **Parameters / Weights**: Numerical tensors representing the learned weights of the transformer model. These include embeddings, attention layers, feedforward layers, and output layers.\\n  - **Forward Method**: The callable interface used to pass input sequences (observations, actions, returns-to-go) through the model to obtain outputs (e.g., predicted actions or trajectories).\\n  - **Device and dtype Information**: Specifies where the model is loaded (CPU/GPU) and the data type of its tensors.\\n\\n- **Purpose in RL Context**: The model object is designed to generate or predict actions conditioned on sequences of past states, actions, and returns, following the Decision Transformer paradigm. The provided `mean` and `std` arrays are typically used for normalizing input observations before passing them to the model.\\n\\n- **Additional Metadata**: While not returned explicitly by the API call, the model object may expose methods to access its configuration, tokenizer (if applicable), and pretrained weights.\\n\\nIn summary, the response is a fully initialized transformer model object ready for inference in reinforcement learning tasks, with all learned weights and architectural details encapsulated.\", \"examples\": [{\"user_intent\": \"Load a pre-trained Decision Transformer to test expert-level control on the Gym Hopper environment.\", \"tool_call\": {\"name\": \"AutoModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"edbeeching/decision-transformer-gym-hopper-expert\"}}}, {\"user_intent\": \"I want to analyze the mean and standard deviation of the expert trajectories used in the Hopper model.\", \"tool_call\": {\"name\": \"AutoModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"edbeeching/decision-transformer-gym-hopper-expert\", \"mean\": [1.3490015, -0.11208222, -0.5506444, -0.13188992, -0.00378754, 2.6071432, 0.02322114, -0.01626922, -0.06840388, -0.05183131, 0.04272673], \"std\": [0.15980862, 0.0446214, 0.14307782, 0.17629202, 0.5912333, 0.5899924, 1.5405099, 0.8152689, 2.0173461, 2.4107876, 5.8440027]}}}, {\"user_intent\": \"Set up the Decision Transformer model in a Colab notebook with the required Python libraries to run Hopper environment simulations.\", \"tool_call\": {\"name\": \"AutoModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"edbeeching/decision-transformer-gym-hopper-expert\"}}}]}\n\n# User Query\nWe are building a robot for hopping in a controlled environment. Train it to perform a hop using Decision Transformers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Load a pre-trained Decision Transformer for robotic hopping in a simulated environment.",
                    "tool_description": "Initialize a Decision Transformer model trained on expert trajectories from the Gym Hopper environment using Hugging Face Transformers, enabling reinforcement learning for action prediction and control.",
                    "expected_response": "A model object representing the Decision Transformer, including its configuration, learned weights, and methods for predicting actions in the Hopper environment."
                },
                {
                    "user_intent": "Use the loaded Decision Transformer to train the robot to perform hopping actions.",
                    "tool_description": "Apply the Decision Transformer model to generate and refine action sequences for the robot, using reinforcement learning to optimize hopping performance in the controlled simulation.",
                    "expected_response": "Trained action sequences or policy outputs that allow the robot to perform hops effectively in the simulated environment."
                }
            ],
            "extracted_arguments": {
                "environment": "The simulation environment in which the robot will perform hopping tasks, typically a Gym-like controlled setup.",
                "mean": "Array of numerical values representing the mean for normalizing input features for the Decision Transformer model.",
                "std": "Array of numerical values representing the standard deviation for normalizing input features for the Decision Transformer model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_439",
        "query": "I want to estimate the price of a house based on its features using this API. Please provide the code.",
        "instruction": "Given a `house price estimation` task using features, retrieve tools that perform regression analysis by processing the input features and estimating property prices based on a trained model, emphasizing requirements for integration with APIs and consideration of necessary frameworks or libraries.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 100581.032,
                            "R2": 0.922,
                            "MSE": 10116543945.03,
                            "MAE": 81586.656,
                            "RMSLE": 0.101
                        }
                    },
                    "description": "A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.",
                    "id": "huggingface_api_868",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761513"
                },
                "id": "gorilla_huggingface_tool_861",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 1: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 3: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 5: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 6: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 7: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 8: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 9: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 10: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 11: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 12: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 13: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 14: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.",
            "Tool 15: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 16: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "Tool 17: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 18: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 19: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests."
        ],
        "tool_view": {
            "description": "A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "parameters": {},
            "response": "The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \n\n1. **Predictions**:  \n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \n   - Each element corresponds to a single input record, representing the model’s estimated carbon emissions category.  \n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\n\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\n\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.",
            "examples": [
                {
                    "user_intent": "I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "machines.csv"
                        }
                    }
                },
                {
                    "user_intent": "I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.",
                    "tool_call": {
                        "name": "tejas23/autotrain-amx2-1702259728",
                        "arguments": {
                            "data": "delivery_trucks.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 1: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 3: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 4: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 5: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 6: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 7: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 8: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 9: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 10: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 11: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 12: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 13: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 14: This tool provides a model for audio separation, specifically trained on the sep_clean task of the Libri2Mix dataset using the Asteroid framework. The model, named \"ConvTasNet_Libri2Mix_sepclean_8k,\" can be accessed through the Hugging Face API and is designed to improve audio separation accuracy, as demonstrated by its high performance on metrics like SI-SDR and SDR. It requires the `huggingface_hub` package and is typically used for tasks involving separation of mixed audio sources.\nTool 15: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 16: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\nTool 17: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 18: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 19: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\", \"parameters\": {}, \"response\": \"The tool produces a **tabular classification prediction** output based on input data. Specifically, its response structure would typically consist of the following components:  \\n\\n1. **Predictions**:  \\n   - A list, array, or Series containing the predicted class labels for each row in the input dataset.  \\n   - Each element corresponds to a single input record, representing the model\\u2019s estimated carbon emissions category.  \\n   - The predicted labels are likely categorical (multi-class), reflecting the classes used during model training.\\n\\n2. **Optional Additional Outputs (not explicitly defined but plausible)**:  \\n   - **Probabilities or confidence scores**: Some classification models can provide a probability distribution over all possible classes for each input record.  \\n   - **Metadata about input-output mapping**: For example, row identifiers or indexes corresponding to the input data.\\n\\nThe core expectation is that after passing a preprocessed tabular dataset to the model, the tool returns a structured set of predictions indicating the predicted CO2 emission class for each input row. No additional formatting, aggregation, or postprocessing is explicitly defined in the documentation.\", \"examples\": [{\"user_intent\": \"I have a CSV file with vehicle specs and I want to predict their CO2 emissions using the trained model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"Predict carbon emissions for a dataset of industrial machines stored in 'machines.csv'.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"machines.csv\"}}}, {\"user_intent\": \"I want to estimate CO2 emissions for my fleet of delivery trucks using the provided model.\", \"tool_call\": {\"name\": \"tejas23/autotrain-amx2-1702259728\", \"arguments\": {\"data\": \"delivery_trucks.csv\"}}}]}\n\n# User Query\nI want to estimate the price of a house based on its features using this API. Please provide the code.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the price of a house based on its features using a regression model.",
                    "tool_description": "Use a pre-trained regression model to predict the price of a house based on its features, typically represented in a structured tabular format.",
                    "expected_response": "A predicted price value for the house based on the input features."
                }
            ],
            "extracted_arguments": {
                "data": "The structured dataset containing house features for prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_330",
        "query": "Create a program to calculate sentence similarity scores between a list of sentences.",
        "instruction": "Given a `sentence similarity calculation` task, retrieve tools that evaluate the semantic similarity between sentences by generating embeddings for each sentence and calculating their similarity scores using models optimized for natural language processing tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Embeddings",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Automated evaluation"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_696",
                    "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                },
                "id": "gorilla_huggingface_tool_691",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 1: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 2: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 3: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 4: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 5: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 6: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 7: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.",
            "Tool 8: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 9: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 11: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 12: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 13: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 14: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 15: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 16: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.",
            "Tool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 18: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity."
        ],
        "tool_view": {
            "description": "This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "parameters": {
                "query": {
                    "type": "string",
                    "description": "The input sentence or question for which you want to find semantically similar documents.",
                    "required": true,
                    "example": "How many people live in London?"
                },
                "docs": {
                    "type": "array",
                    "description": "A list of sentences or paragraphs to compare against the query for semantic similarity.",
                    "required": true,
                    "example": [
                        "Around 9 million people live in London",
                        "London is known for its financial district"
                    ]
                }
            },
            "response": "The tool produces a response that consists of similarity scores between a query sentence and a list of candidate sentences or documents. The structure and meaning of each component are as follows:\n\n1. **Scores List**:  \n   - The primary output is a list of numerical values representing the semantic similarity between the query and each document in the input list.  \n   - Each score is typically a floating-point number indicating the closeness in meaning, often computed as a cosine similarity between the embeddings of the query and the document.  \n   - Higher scores indicate greater semantic similarity.\n\n2. **Document-Score Pairing (Optional/Typical)**:  \n   - For practical usage, the scores are often paired with the corresponding documents, forming tuples or objects like `(document, score)`.  \n   - This allows the user to easily identify which documents are most relevant to the query.\n\n3. **Sorted Results (Optional/Typical)**:  \n   - The document-score pairs are frequently sorted in descending order of similarity, so the most semantically relevant documents appear first.\n\n4. **Embedding Vectors (Internal/Optional)**:  \n   - The model internally produces dense 768-dimensional vector embeddings for both the query and the documents.  \n   - While these embeddings may be returned if explicitly requested, the typical end-user response focuses on the similarity scores rather than the raw vectors.\n\nIn summary, the response provides a quantitative measure of semantic similarity between the query and each document, often structured as a sorted list of document-score pairs for easy interpretation and downstream usage.",
            "examples": [
                {
                    "user_intent": "I want to find which news headlines are most relevant to the question 'What are the latest advancements in AI research?'",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "What are the latest advancements in AI research?",
                            "docs": [
                                "Google announces new AI language model that surpasses previous benchmarks",
                                "Stock markets are volatile today due to economic uncertainty",
                                "Breakthrough in quantum computing could accelerate AI simulations"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to match customer questions to the most relevant FAQ entries on our website.",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "How can I reset my account password?",
                            "docs": [
                                "To reset your password, go to account settings and click 'Reset Password'",
                                "Shipping options vary depending on your location",
                                "You can track your order using the tracking number sent to your email"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Compare user reviews to see which product description they most closely relate to.",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "This smartphone has an excellent camera and battery life.",
                            "docs": [
                                "The phone's camera takes stunning low-light photos and battery lasts all day",
                                "This tablet is perfect for reading and light browsing",
                                "Our headphones provide immersive sound but limited battery life"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 1: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 2: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 3: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 4: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 5: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 6: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 7: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.\nTool 8: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 9: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 11: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 12: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 13: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 14: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 15: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 16: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.\nTool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 18: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 19: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question\\u2013answer pairs to capture rich semantic relationships between sentences.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"The input sentence or question for which you want to find semantically similar documents.\", \"required\": true, \"example\": \"How many people live in London?\"}, \"docs\": {\"type\": \"array\", \"description\": \"A list of sentences or paragraphs to compare against the query for semantic similarity.\", \"required\": true, \"example\": [\"Around 9 million people live in London\", \"London is known for its financial district\"]}}, \"response\": \"The tool produces a response that consists of similarity scores between a query sentence and a list of candidate sentences or documents. The structure and meaning of each component are as follows:\\n\\n1. **Scores List**:  \\n   - The primary output is a list of numerical values representing the semantic similarity between the query and each document in the input list.  \\n   - Each score is typically a floating-point number indicating the closeness in meaning, often computed as a cosine similarity between the embeddings of the query and the document.  \\n   - Higher scores indicate greater semantic similarity.\\n\\n2. **Document-Score Pairing (Optional/Typical)**:  \\n   - For practical usage, the scores are often paired with the corresponding documents, forming tuples or objects like `(document, score)`.  \\n   - This allows the user to easily identify which documents are most relevant to the query.\\n\\n3. **Sorted Results (Optional/Typical)**:  \\n   - The document-score pairs are frequently sorted in descending order of similarity, so the most semantically relevant documents appear first.\\n\\n4. **Embedding Vectors (Internal/Optional)**:  \\n   - The model internally produces dense 768-dimensional vector embeddings for both the query and the documents.  \\n   - While these embeddings may be returned if explicitly requested, the typical end-user response focuses on the similarity scores rather than the raw vectors.\\n\\nIn summary, the response provides a quantitative measure of semantic similarity between the query and each document, often structured as a sorted list of document-score pairs for easy interpretation and downstream usage.\", \"examples\": [{\"user_intent\": \"I want to find which news headlines are most relevant to the question 'What are the latest advancements in AI research?'\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"What are the latest advancements in AI research?\", \"docs\": [\"Google announces new AI language model that surpasses previous benchmarks\", \"Stock markets are volatile today due to economic uncertainty\", \"Breakthrough in quantum computing could accelerate AI simulations\"]}}}, {\"user_intent\": \"I need to match customer questions to the most relevant FAQ entries on our website.\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"How can I reset my account password?\", \"docs\": [\"To reset your password, go to account settings and click 'Reset Password'\", \"Shipping options vary depending on your location\", \"You can track your order using the tracking number sent to your email\"]}}}, {\"user_intent\": \"Compare user reviews to see which product description they most closely relate to.\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"This smartphone has an excellent camera and battery life.\", \"docs\": [\"The phone's camera takes stunning low-light photos and battery lasts all day\", \"This tablet is perfect for reading and light browsing\", \"Our headphones provide immersive sound but limited battery life\"]}}}]}\n\n# User Query\nCreate a program to calculate sentence similarity scores between a list of sentences.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to calculate similarity scores between multiple sentences.",
                    "tool_description": "Use a sentence-transformers model to encode a list of sentences into dense vector embeddings and compute pairwise semantic similarity scores between them.",
                    "expected_response": "A matrix or list of numerical similarity scores representing the semantic closeness between each pair of sentences."
                }
            ],
            "extracted_arguments": {
                "sentences_list": "An array of sentences or paragraphs for which semantic similarity scores are to be calculated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_98",
        "query": "We are creating an online video conference service, and we need to detect when two or more speakers are speaking at the same time in the audio.",
        "instruction": "Given an `overlapped speech detection` task, retrieve tools that analyze audio inputs to identify instances where multiple speakers are speaking simultaneously, ensuring accurate and efficient detection tailored to the query's specific requirements and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "pyannote.audio",
                    "functionality": "overlapped-speech-detection",
                    "api_call": "pipeline.from_pretrained('pyannote/overlapped-speech-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # two or more speakers are active between speech.start and speech.end\n  ...",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": null
                    },
                    "description": "Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.",
                    "id": "huggingface_api_750",
                    "name": "pyannote/overlapped-speech-detection"
                },
                "id": "gorilla_huggingface_tool_745",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 1: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 2: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 3: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 4: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 5: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 6: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 7: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 9: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 10: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 11: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 12: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 13: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 14: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 15: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 16: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 17: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 18: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 19: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images."
        ],
        "tool_view": {
            "description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "parameters": {
                "audio.wav": {
                    "type": "string",
                    "description": "Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\n\n1. **Timeline object**:  \n   - Represents the sequence of time intervals in the audio.  \n   - Each interval corresponds to a period where overlapped speech occurs.\n\n2. **Speech segments** (within the timeline):  \n   - Each segment has at least two key attributes:  \n     - **start**: The timestamp indicating when the overlapped speech begins.  \n     - **end**: The timestamp indicating when the overlapped speech ends.  \n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\n\n3. **Support or confidence information** (optional, depending on the output object):  \n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\n\n4. **Methods for iteration and access**:  \n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\n\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.",
            "examples": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recording.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "conference_call.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a meeting recording to identify parts with overlapping speech.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "team_meeting.wav"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any overlapping speech segments in this podcast episode.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_3.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 1: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 2: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 3: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 4: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 5: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 6: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 7: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 9: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 10: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 11: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 12: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 13: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 14: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 15: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 16: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 17: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 18: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 19: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\", \"parameters\": {\"audio.wav\": {\"type\": \"string\", \"description\": \"Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\\n\\n1. **Timeline object**:  \\n   - Represents the sequence of time intervals in the audio.  \\n   - Each interval corresponds to a period where overlapped speech occurs.\\n\\n2. **Speech segments** (within the timeline):  \\n   - Each segment has at least two key attributes:  \\n     - **start**: The timestamp indicating when the overlapped speech begins.  \\n     - **end**: The timestamp indicating when the overlapped speech ends.  \\n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\\n\\n3. **Support or confidence information** (optional, depending on the output object):  \\n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\\n\\n4. **Methods for iteration and access**:  \\n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\\n\\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.\", \"examples\": [{\"user_intent\": \"Detect when multiple speakers are talking simultaneously in a recording.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"conference_call.wav\"}}}, {\"user_intent\": \"I need to analyze a meeting recording to identify parts with overlapping speech.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"team_meeting.wav\"}}}, {\"user_intent\": \"Check if there are any overlapping speech segments in this podcast episode.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_3.wav\"}}}]}\n\n# User Query\nWe are creating an online video conference service, and we need to detect when two or more speakers are speaking at the same time in the audio.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in an audio recording.",
                    "tool_description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
                    "expected_response": "A structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, including start and end timestamps for each overlapped speech segment."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the audio file in .wav format to be analyzed for overlapping speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_127",
        "query": "As a city planner, I need to measure the depth of spaces in a series of images taken from streets.",
        "instruction": "Given a `depth measurement` task, retrieve tools that handle computer vision capabilities, processing image inputs to estimate and measure the depth of spaces accurately and efficiently based on the characteristics of the images and relevant requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "Transformers 4.24.0",
                        "Pytorch 1.12.1+cu116",
                        "Datasets 2.8.0",
                        "Tokenizers 0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.4222,
                            "Mae": 0.411,
                            "Rmse": 0.6292,
                            "Abs Rel": 0.3778,
                            "Log Mae": 0.1636,
                            "Log Rmse": 0.224,
                            "Delta1": 0.432,
                            "Delta2": 0.6806,
                            "Delta3": 0.8068
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_170",
                    "name": "glpn-nyu-finetuned-diode-221221-102136"
                },
                "id": "gorilla_huggingface_tool_166",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 2: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 3: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 4: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 5: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 6: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 7: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 8: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 9: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 12: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 13: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 14: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 15: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 16: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.",
            "Tool 17: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 18: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 19: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure."
        ],
        "tool_view": {
            "description": "A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "parameters": {},
            "response": "The tool is a depth estimation model implemented using the Hugging Face Transformers framework. Its response, when invoked on an input image, would typically consist of a dense depth map corresponding to the scene depicted in the image. The expected structure and components of the response are as follows:\n\n- **Depth Map Tensor/Array**: The core output is a multi-dimensional numerical array (tensor) representing per-pixel depth values. Each element corresponds to the estimated distance of the scene point from the camera.  \n- **Shape/Dimensions**: The depth map usually has the same spatial dimensions as the input image or a fixed resolution determined by the model. The structure is generally `[height, width]` for a single-channel depth map, or `[batch_size, height, width]` if multiple images are processed at once.  \n- **Data Type**: The array contains continuous numeric values, typically floating-point numbers, representing relative or absolute depth measurements.  \n- **Optional Metadata**: Depending on the model implementation, the response may include auxiliary information such as preprocessing parameters, scaling factors, or confidence scores, though this is not guaranteed.  \n\nOverall, the response provides a pixel-wise representation of scene depth that can be used for tasks such as 3D reconstruction, augmented reality, or robotics perception.",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth map of an indoor scene using a pre-trained model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_path": "images/living_room.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Generate a depth map for an outdoor landscape photo with a pre-trained DIODE model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_path": "images/mountain_landscape.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to process multiple images and get their depth estimations using the Hugging Face GLPN model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-112116",
                        "arguments": {
                            "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')",
                            "input_image_paths": [
                                "images/office.jpg",
                                "images/kitchen.png",
                                "images/park.jpg"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 2: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 3: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 4: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 5: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 6: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 7: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 8: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 9: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 12: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 13: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 14: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 15: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 16: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\nTool 17: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 18: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 19: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\", \"parameters\": {}, \"response\": \"The tool is a depth estimation model implemented using the Hugging Face Transformers framework. Its response, when invoked on an input image, would typically consist of a dense depth map corresponding to the scene depicted in the image. The expected structure and components of the response are as follows:\\n\\n- **Depth Map Tensor/Array**: The core output is a multi-dimensional numerical array (tensor) representing per-pixel depth values. Each element corresponds to the estimated distance of the scene point from the camera.  \\n- **Shape/Dimensions**: The depth map usually has the same spatial dimensions as the input image or a fixed resolution determined by the model. The structure is generally `[height, width]` for a single-channel depth map, or `[batch_size, height, width]` if multiple images are processed at once.  \\n- **Data Type**: The array contains continuous numeric values, typically floating-point numbers, representing relative or absolute depth measurements.  \\n- **Optional Metadata**: Depending on the model implementation, the response may include auxiliary information such as preprocessing parameters, scaling factors, or confidence scores, though this is not guaranteed.  \\n\\nOverall, the response provides a pixel-wise representation of scene depth that can be used for tasks such as 3D reconstruction, augmented reality, or robotics perception.\", \"examples\": [{\"user_intent\": \"I want to estimate the depth map of an indoor scene using a pre-trained model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_path\": \"images/living_room.jpg\"}}}, {\"user_intent\": \"Generate a depth map for an outdoor landscape photo with a pre-trained DIODE model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_path\": \"images/mountain_landscape.png\"}}}, {\"user_intent\": \"I want to process multiple images and get their depth estimations using the Hugging Face GLPN model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"arguments\": {\"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"input_image_paths\": [\"images/office.jpg\", \"images/kitchen.png\", \"images/park.jpg\"]}}}]}\n\n# User Query\nAs a city planner, I need to measure the depth of spaces in a series of images taken from streets.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate depth maps for a series of street images to analyze spatial dimensions.",
                    "tool_description": "Use a Hugging Face Transformers-based depth estimation model, fine-tuned on the DIODE dataset, to predict per-pixel depth from input images.",
                    "expected_response": "A collection of dense depth maps corresponding to each input street image, represented as multi-dimensional arrays with per-pixel depth values."
                }
            ],
            "extracted_arguments": {
                "input_images": "A list of image files representing street scenes for which depth estimation is required."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_55",
        "query": "Design an AI algorithm to answer questions from scanned documents.",
        "instruction": "Given a `document question answering` task, retrieve tools that leverage AI algorithms to process multimodal inputs, specifically scanned documents, to generate accurate responses to the questions related to the content of the documents.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": "4.12.2",
                        "pytorch": "1.8.0+cu101",
                        "datasets": "1.14.0",
                        "tokenizers": "0.10.3"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": {
                            "Loss": 2.087
                        }
                    },
                    "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.",
                    "id": "huggingface_api_131",
                    "name": "layoutlmv2-base-uncased-finetuned-infovqa"
                },
                "id": "gorilla_huggingface_tool_130",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 3: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 5: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 6: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 8: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 9: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 10: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 11: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.",
            "Tool 12: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 15: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 17: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images."
        ],
        "tool_view": {
            "description": "A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want the model to answer based on the given document context.",
                    "required": true,
                    "example": "What is the total amount on the invoice?"
                },
                "context": {
                    "type": "string",
                    "description": "The textual content of the document that provides the information needed to answer the question.",
                    "required": true,
                    "example": "Invoice #12345 dated 2025-01-15 shows a total amount of $1,250 for office supplies."
                }
            },
            "response": "The tool performs document question answering using the LayoutLM architecture, specifically designed for visual question answering tasks involving documents. The model takes in two key inputs:\n\n1. **Question**: A text query that asks about a specific detail or aspect of the document.\n2. **Context**: The text or content of the document in which the answer is to be found.\n\nThe response is expected to be structured as follows:\n\n- **Answer**: A string containing the model's response to the given question, derived from the provided document context.\n- **Start**: The starting character index of the answer within the context (if applicable), indicating the position of the answer in the original document.\n- **End**: The ending character index of the answer within the context (if applicable), marking the conclusion of the answer in the document.\n\nAdditionally, the response may also include confidence or score-related information (depending on the exact model implementation), which quantifies the likelihood or certainty of the answer being correct, but this is not explicitly mentioned in the documentation.\n\nThus, the response will likely include:\n\n- **Answer**: The model-generated answer to the question.\n- **Start**: The index in the context text where the answer starts.\n- **End**: The index in the context text where the answer ends. \n\nIf the model's performance includes any accuracy or relevance metrics, they may also be part of the response but are not explicitly defined in this case.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount billed.",
                    "tool_call": {
                        "name": "layoutlm-vqa",
                        "arguments": {
                            "question": "What is the total amount on this invoice?",
                            "context": "Invoice image text: Invoice #12345, Date: 2025-12-01, Item: Laptop, Price: $1200, Tax: $100, Total: $1300"
                        }
                    }
                },
                {
                    "user_intent": "I need to find the expiration date on a passport document.",
                    "tool_call": {
                        "name": "layoutlm-vqa",
                        "arguments": {
                            "question": "What is the expiration date of the passport?",
                            "context": "Passport scan text: Name: John Doe, Passport No: X1234567, Nationality: USA, Date of Birth: 1990-05-12, Expiration Date: 2030-05-11"
                        }
                    }
                },
                {
                    "user_intent": "Check the due date on a utility bill PDF.",
                    "tool_call": {
                        "name": "layoutlm-vqa",
                        "arguments": {
                            "question": "When is the payment due for this utility bill?",
                            "context": "Utility Bill text: Customer: Jane Smith, Account No: 987654, Billing Date: 2025-12-01, Due Date: 2025-12-20, Amount Due: $85.75"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 1: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 2: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 3: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 4: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 5: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 6: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 8: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 9: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 10: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 11: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.\nTool 12: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 13: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 14: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 15: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 17: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 18: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 19: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want the model to answer based on the given document context.\", \"required\": true, \"example\": \"What is the total amount on the invoice?\"}, \"context\": {\"type\": \"string\", \"description\": \"The textual content of the document that provides the information needed to answer the question.\", \"required\": true, \"example\": \"Invoice #12345 dated 2025-01-15 shows a total amount of $1,250 for office supplies.\"}}, \"response\": \"The tool performs document question answering using the LayoutLM architecture, specifically designed for visual question answering tasks involving documents. The model takes in two key inputs:\\n\\n1. **Question**: A text query that asks about a specific detail or aspect of the document.\\n2. **Context**: The text or content of the document in which the answer is to be found.\\n\\nThe response is expected to be structured as follows:\\n\\n- **Answer**: A string containing the model's response to the given question, derived from the provided document context.\\n- **Start**: The starting character index of the answer within the context (if applicable), indicating the position of the answer in the original document.\\n- **End**: The ending character index of the answer within the context (if applicable), marking the conclusion of the answer in the document.\\n\\nAdditionally, the response may also include confidence or score-related information (depending on the exact model implementation), which quantifies the likelihood or certainty of the answer being correct, but this is not explicitly mentioned in the documentation.\\n\\nThus, the response will likely include:\\n\\n- **Answer**: The model-generated answer to the question.\\n- **Start**: The index in the context text where the answer starts.\\n- **End**: The index in the context text where the answer ends. \\n\\nIf the model's performance includes any accuracy or relevance metrics, they may also be part of the response but are not explicitly defined in this case.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount billed.\", \"tool_call\": {\"name\": \"layoutlm-vqa\", \"arguments\": {\"question\": \"What is the total amount on this invoice?\", \"context\": \"Invoice image text: Invoice #12345, Date: 2025-12-01, Item: Laptop, Price: $1200, Tax: $100, Total: $1300\"}}}, {\"user_intent\": \"I need to find the expiration date on a passport document.\", \"tool_call\": {\"name\": \"layoutlm-vqa\", \"arguments\": {\"question\": \"What is the expiration date of the passport?\", \"context\": \"Passport scan text: Name: John Doe, Passport No: X1234567, Nationality: USA, Date of Birth: 1990-05-12, Expiration Date: 2030-05-11\"}}}, {\"user_intent\": \"Check the due date on a utility bill PDF.\", \"tool_call\": {\"name\": \"layoutlm-vqa\", \"arguments\": {\"question\": \"When is the payment due for this utility bill?\", \"context\": \"Utility Bill text: Customer: Jane Smith, Account No: 987654, Billing Date: 2025-12-01, Due Date: 2025-12-20, Amount Due: $85.75\"}}}]}\n\n# User Query\nDesign an AI algorithm to answer questions from scanned documents.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract answers from scanned or photographed documents using AI.",
                    "tool_description": "Use a multimodal document question-answering model to process scanned document images or PDFs and answer questions based on their content and layout.",
                    "expected_response": "A text answer extracted from the document, optionally including the position indices of the answer within the document and confidence metrics."
                }
            ],
            "extracted_arguments": {
                "document_input": "The scanned document image or PDF from which information is to be extracted.",
                "question": "The specific natural language query about the document content that needs to be answered."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_359",
        "query": "One of our clients is facing noise issues on their audio recordings. Can you help them to remove the noise from the audio?",
        "instruction": "Given an `audio enhancement` task, retrieve tools that handle audio inputs by reducing or removing noise from recordings to enhance the sound quality, aligning with the query's needs and specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "speech-enhancement",
                    "api_call": "SpectralMaskEnhancement.from_hparams(source='speechbrain/metricgan-plus-voicebank', savedir='pretrained_models/metricgan-plus-voicebank')",
                    "api_arguments": {
                        "source": "speechbrain/metricgan-plus-voicebank",
                        "savedir": "pretrained_models/metricgan-plus-voicebank"
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "import torch\nimport torchaudio\nfrom speechbrain.pretrained import SpectralMaskEnhancement\nenhance_model = SpectralMaskEnhancement.from_hparams(\n source='speechbrain/metricgan-plus-voicebank',\n savedir='pretrained_models/metricgan-plus-voicebank',\n)\nnoisy = enhance_model.load_audio(\n 'speechbrain/metricgan-plus-voicebank/example.wav'\n).unsqueeze(0)\nenhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\ntorchaudio.save('enhanced.wav', enhanced.cpu(), 16000)",
                    "performance": {
                        "dataset": "Voicebank",
                        "accuracy": {
                            "Test PESQ": "3.15",
                            "Test STOI": "93.0"
                        }
                    },
                    "description": "MetricGAN-trained model for Enhancement",
                    "id": "huggingface_api_778",
                    "name": "speechbrain/metricgan-plus-voicebank"
                },
                "id": "gorilla_huggingface_tool_773",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 4: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 5: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 6: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 7: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 8: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 9: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 11: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 12: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 13: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 14: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 15: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "Tool 16: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 17: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 18: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.",
            "Tool 19: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data."
        ],
        "tool_view": {
            "description": "This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that will be processed for speech enhancement using the SepFormer model.",
                    "required": true,
                    "example": "example_wham.wav"
                }
            },
            "response": "The tool’s response is an enhanced (denoised) version of the input audio, produced by the SepFormer speech enhancement model. Structurally, the response is typically an audio tensor or array representing one or more separated/enhanced audio sources. The components of the response can be described as follows:  \n\n- **Audio Tensor/Array**: A multi-dimensional numeric array representing the waveform(s) of the enhanced audio.  \n  - **Dimensions**: Usually structured as `[channels, samples]` or `[sources, channels, samples]` depending on whether multiple sources are returned.  \n  - **Values**: Floating-point numbers corresponding to the amplitude of the audio signal at each sample point.  \n\n- **Source Separation Component (if multiple outputs)**: When the model separates overlapping speech, the response includes multiple tensors, each corresponding to an estimated source.  \n\n- **Sampling Rate Context**: While not embedded in the tensor itself, the model assumes a fixed sampling rate (8 kHz for this pretrained model), which is necessary when saving or playing back the audio.  \n\nIn summary, the response provides the denoised or enhanced waveform data, optionally separated into multiple sources, ready to be saved to an audio file or further processed.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of a meeting and want to enhance the speech clarity.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "recordings/meeting_noisy.wav"
                        }
                    }
                },
                {
                    "user_intent": "Enhance the audio quality of a podcast episode that has background noise.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "podcasts/episode3_raw.wav"
                        }
                    }
                },
                {
                    "user_intent": "Clean up an old voicemail recording for transcription.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "voicemails/old_message.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 4: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 5: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 6: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 7: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 8: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 9: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 11: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 12: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 13: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 14: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 15: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\nTool 16: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 17: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 18: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.\nTool 19: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8\\u202fkHz sampling frequency.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that will be processed for speech enhancement using the SepFormer model.\", \"required\": true, \"example\": \"example_wham.wav\"}}, \"response\": \"The tool\\u2019s response is an enhanced (denoised) version of the input audio, produced by the SepFormer speech enhancement model. Structurally, the response is typically an audio tensor or array representing one or more separated/enhanced audio sources. The components of the response can be described as follows:  \\n\\n- **Audio Tensor/Array**: A multi-dimensional numeric array representing the waveform(s) of the enhanced audio.  \\n  - **Dimensions**: Usually structured as `[channels, samples]` or `[sources, channels, samples]` depending on whether multiple sources are returned.  \\n  - **Values**: Floating-point numbers corresponding to the amplitude of the audio signal at each sample point.  \\n\\n- **Source Separation Component (if multiple outputs)**: When the model separates overlapping speech, the response includes multiple tensors, each corresponding to an estimated source.  \\n\\n- **Sampling Rate Context**: While not embedded in the tensor itself, the model assumes a fixed sampling rate (8 kHz for this pretrained model), which is necessary when saving or playing back the audio.  \\n\\nIn summary, the response provides the denoised or enhanced waveform data, optionally separated into multiple sources, ready to be saved to an audio file or further processed.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of a meeting and want to enhance the speech clarity.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"recordings/meeting_noisy.wav\"}}}, {\"user_intent\": \"Enhance the audio quality of a podcast episode that has background noise.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"podcasts/episode3_raw.wav\"}}}, {\"user_intent\": \"Clean up an old voicemail recording for transcription.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"voicemails/old_message.wav\"}}}]}\n\n# User Query\nOne of our clients is facing noise issues on their audio recordings. Can you help them to remove the noise from the audio?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to remove noise from an audio recording to improve speech clarity.",
                    "tool_description": "Use a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from the audio recording.",
                    "expected_response": "An enhanced (denoised) version of the input audio, produced by the SepFormer speech enhancement model."
                }
            ],
            "extracted_arguments": {
                "path": "The file path to the input audio file that will be processed for speech enhancement using the SepFormer model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_206",
        "query": "My company wants to develop an application that will analyze images in relation to food and answer questions about them. We want it to handle questions like \"what is in the dish\" and \"how many calories does it have\".",
        "instruction": "Given an `image analysis and question-answering` task, retrieve tools that process visual inputs and answer related questions by integrating image data with food-specific queries to provide information like dish composition and calorie counts.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')",
                    "api_arguments": {
                        "image": "path_to_image",
                        "question": "question_text"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Visual Question Answering model fine-tuned on the Polish language.",
                    "id": "huggingface_api_108",
                    "name": "azwierzc/vilt-b32-finetuned-vqa-pl"
                },
                "id": "gorilla_huggingface_tool_107",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 2: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 3: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 5: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 7: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 10: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 12: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 13: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 14: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 15: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 16: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 18: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 19: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks."
        ],
        "tool_view": {
            "description": "A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "parameters": {},
            "response": "The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \n\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \n\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.",
            "examples": [
                {
                    "user_intent": "I have an image of a street scene and I want to know how many cars are visible in it.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/street_scene.jpg",
                            "question": "How many cars are there in this image?"
                        }
                    }
                },
                {
                    "user_intent": "Look at this photo of a kitchen and tell me what color the refrigerator is.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/kitchen.jpg",
                            "question": "What color is the refrigerator?"
                        }
                    }
                },
                {
                    "user_intent": "I want to analyze a picture of a park and find out if there are any people sitting on benches.",
                    "tool_call": {
                        "name": "uclanlp/visualbert-vqa",
                        "arguments": {
                            "image_path": "images/park.jpg",
                            "question": "Are there people sitting on benches?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 1: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 2: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 3: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 4: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 5: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 6: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 7: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 10: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 11: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 12: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 13: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 14: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 15: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 16: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 17: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 18: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 19: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\", \"parameters\": {}, \"response\": \"The tool is a VisualBERT model designed for multimodal Visual Question Answering (VQA). Its response is not explicitly defined in the JSON, but based on its functionality, a typical response would have the following structure:  \\n\\n- **Answer Text**: The primary output is a textual answer to the input question, derived from analyzing both the image and the question. This is usually a string representing the predicted answer.  \\n- **Confidence Scores (optional)**: Some VQA models may include a confidence score or probability distribution over possible answers, indicating the model's certainty.  \\n- **Metadata (optional)**: The response may include additional information such as the question input, image reference, or model logits used internally for prediction.  \\n\\nIn essence, the main component of the response is the predicted answer to the user-provided question about the image, potentially accompanied by confidence or auxiliary metadata.\", \"examples\": [{\"user_intent\": \"I have an image of a street scene and I want to know how many cars are visible in it.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/street_scene.jpg\", \"question\": \"How many cars are there in this image?\"}}}, {\"user_intent\": \"Look at this photo of a kitchen and tell me what color the refrigerator is.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/kitchen.jpg\", \"question\": \"What color is the refrigerator?\"}}}, {\"user_intent\": \"I want to analyze a picture of a park and find out if there are any people sitting on benches.\", \"tool_call\": {\"name\": \"uclanlp/visualbert-vqa\", \"arguments\": {\"image_path\": \"images/park.jpg\", \"question\": \"Are there people sitting on benches?\"}}}]}\n\n# User Query\nMy company wants to develop an application that will analyze images in relation to food and answer questions about them. We want it to handle questions like \"what is in the dish\" and \"how many calories does it have\".\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze an image containing food and identify the items present in the dish.",
                    "tool_description": "Use a visual question answering (VQA) model specialized in food images to identify and classify food items in an image based on a natural language question.",
                    "expected_response": "A textual list or description of food items detected in the image, possibly with confidence scores for each item."
                },
                {
                    "user_intent": "Estimate or provide nutritional information such as calories for the identified food items.",
                    "tool_description": "Leverage a VQA model combined with a nutritional knowledge base or model to answer questions about calories or other nutritional content of the food items in the image.",
                    "expected_response": "A numeric or textual estimation of calories or nutritional information corresponding to the identified food items."
                }
            ],
            "extracted_arguments": {
                "image_input": "The image file containing food to be analyzed.",
                "question_text": "The natural language question regarding the food in the image, such as identification or nutritional content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_31",
        "query": "Write an overview for astronomers about how Jupiter became the largest planet in our solar system.",
        "instruction": "Given an `overview writing` task, retrieve tools that assist in generating comprehensive and informative overviews by utilizing natural language processing capabilities to synthesize information aligned with the topic's focus and the query's needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')",
                    "api_arguments": {
                        "input_ids": "encoding['input_ids']",
                        "attention_mask": "encoding['attention_mask']"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\ntokenizer = AutoTokenizer.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\nmodel = AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\ntext = 'Huggingface has democratized NLP. Huge thanks to Huggingface for this.'\nquestion = 'What has Huggingface done ?'\nencoding = tokenizer(question, text, return_tensors='pt')\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\nstart_scores, end_scores = model(input_ids, attention_mask=attention_mask)\nall_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\nanswer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))",
                    "performance": {
                        "dataset": "squad_v1",
                        "accuracy": {
                            "Exact Match": 85.1466,
                            "F1": 91.5415
                        }
                    },
                    "description": "This is longformer-base-4096 model fine-tuned on SQuAD v1 dataset for question answering task. Longformer model created by Iz Beltagy, Matthew E. Peters, Arman Coha from AllenAI. As the paper explains it, Longformer is a BERT-like model for long documents. The pre-trained model can handle sequences with up to 4096 tokens.",
                    "id": "huggingface_api_473",
                    "name": "valhalla/longformer-base-4096-finetuned-squadv1"
                },
                "id": "gorilla_huggingface_tool_469",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 1: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 2: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 3: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 11: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 15: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 16: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 19: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values."
        ],
        "tool_view": {
            "description": "This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "parameters": {
                "source": {
                    "type": "string",
                    "description": "The source of the pre-trained model, specified as a string. This indicates where the model can be loaded from, typically a repository or URL.",
                    "required": true,
                    "example": "padmalcom/tts-hifigan-german"
                },
                "mel_output": {
                    "type": "any",
                    "description": "The mel spectrogram output generated by the Text-to-Speech model. This is the input used by the vocoder to generate the final waveform.",
                    "required": true,
                    "example": "[array of mel spectrogram values]"
                }
            },
            "response": "The tool produces an audio waveform corresponding to a given input spectrogram. The response is structured as follows:\n\n- **Waveform tensor**: The primary output is a multi-dimensional array (tensor) representing the generated audio signal. Each element of the tensor corresponds to a sampled amplitude value in the time domain. The tensor’s dimensions typically represent `[batch_size, channels, time_steps]`, where `channels` is usually 1 for mono audio.  \n\n- **Batch support**: The tool can process multiple spectrogram inputs at once, returning a batch of corresponding waveforms, each aligned with the input spectrogram in order.  \n\n- **Sampling rate context**: While not explicitly part of the output, the generated waveform is intended to be used with a known sampling rate (e.g., 22050 Hz) when saving or playing the audio.  \n\n- **Alignment with input**: Each waveform in the output corresponds directly to the input spectrogram provided to the vocoder. The output is ready for audio playback or further post-processing.  \n\nIn summary, the response is a waveform tensor representing synthesized audio, generated from the input spectrograms, suitable for conversion to standard audio file formats.",
            "examples": [
                {
                    "user_intent": "Convert this German sentence into speech and save it as an audio file: 'Guten Morgen, wie geht es dir?'",
                    "tool_call": {
                        "name": "tts-hifigan-german",
                        "arguments": {
                            "mel_output": "mel_output_from_tacotron2_for_Guten_Morgen_wie_geht_es_dir"
                        }
                    }
                },
                {
                    "user_intent": "I want to generate a waveform from the spectrogram produced by my German TTS system for the sentence 'Das Wetter heute ist sonnig'.",
                    "tool_call": {
                        "name": "tts-hifigan-german",
                        "arguments": {
                            "mel_output": "mel_output_from_tacotron2_for_Das_Wetter_heute_ist_sonnig"
                        }
                    }
                },
                {
                    "user_intent": "Produce speech audio from the spectrogram output of my Tacotron2 TTS model for the German phrase 'Bitte sprechen Sie langsamer'.",
                    "tool_call": {
                        "name": "tts-hifigan-german",
                        "arguments": {
                            "mel_output": "mel_output_from_tacotron2_for_Bitte_sprechen_Sie_langsamer"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 1: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 2: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 3: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 6: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 9: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 11: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 12: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 14: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 15: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 16: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 17: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 18: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 19: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\", \"parameters\": {\"source\": {\"type\": \"string\", \"description\": \"The source of the pre-trained model, specified as a string. This indicates where the model can be loaded from, typically a repository or URL.\", \"required\": true, \"example\": \"padmalcom/tts-hifigan-german\"}, \"mel_output\": {\"type\": \"any\", \"description\": \"The mel spectrogram output generated by the Text-to-Speech model. This is the input used by the vocoder to generate the final waveform.\", \"required\": true, \"example\": \"[array of mel spectrogram values]\"}}, \"response\": \"The tool produces an audio waveform corresponding to a given input spectrogram. The response is structured as follows:\\n\\n- **Waveform tensor**: The primary output is a multi-dimensional array (tensor) representing the generated audio signal. Each element of the tensor corresponds to a sampled amplitude value in the time domain. The tensor\\u2019s dimensions typically represent `[batch_size, channels, time_steps]`, where `channels` is usually 1 for mono audio.  \\n\\n- **Batch support**: The tool can process multiple spectrogram inputs at once, returning a batch of corresponding waveforms, each aligned with the input spectrogram in order.  \\n\\n- **Sampling rate context**: While not explicitly part of the output, the generated waveform is intended to be used with a known sampling rate (e.g., 22050 Hz) when saving or playing the audio.  \\n\\n- **Alignment with input**: Each waveform in the output corresponds directly to the input spectrogram provided to the vocoder. The output is ready for audio playback or further post-processing.  \\n\\nIn summary, the response is a waveform tensor representing synthesized audio, generated from the input spectrograms, suitable for conversion to standard audio file formats.\", \"examples\": [{\"user_intent\": \"Convert this German sentence into speech and save it as an audio file: 'Guten Morgen, wie geht es dir?'\", \"tool_call\": {\"name\": \"tts-hifigan-german\", \"arguments\": {\"mel_output\": \"mel_output_from_tacotron2_for_Guten_Morgen_wie_geht_es_dir\"}}}, {\"user_intent\": \"I want to generate a waveform from the spectrogram produced by my German TTS system for the sentence 'Das Wetter heute ist sonnig'.\", \"tool_call\": {\"name\": \"tts-hifigan-german\", \"arguments\": {\"mel_output\": \"mel_output_from_tacotron2_for_Das_Wetter_heute_ist_sonnig\"}}}, {\"user_intent\": \"Produce speech audio from the spectrogram output of my Tacotron2 TTS model for the German phrase 'Bitte sprechen Sie langsamer'.\", \"tool_call\": {\"name\": \"tts-hifigan-german\", \"arguments\": {\"mel_output\": \"mel_output_from_tacotron2_for_Bitte_sprechen_Sie_langsamer\"}}}]}\n\n# User Query\nWrite an overview for astronomers about how Jupiter became the largest planet in our solar system.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Create a detailed overview explaining how Jupiter grew to become the largest planet in the solar system for an audience of astronomers.",
                    "tool_description": "Generate a scientifically accurate and coherent explanatory text discussing the formation and growth of Jupiter, including relevant astronomical concepts, theories of planetary accretion, and comparisons to other solar system planets.",
                    "expected_response": "A structured, well-written overview text suitable for astronomers, summarizing Jupiter's formation history, growth mechanisms, and factors contributing to its size."
                }
            ],
            "extracted_arguments": {
                "audience": "The target reader group, specified as a professional or specialist category, e.g., astronomers.",
                "topic": "The scientific subject to be explained, in this case the formation and size evolution of Jupiter."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_50",
        "query": "Explain how to use GPT-3 to create a slogan for an e-commerce website that sells eco-friendly products.",
        "instruction": "Given a `slogan creation` task, retrieve tools that assist in generating creative text content by leveraging language models like GPT-3 to produce slogans tailored for specific themes or industries, such as eco-friendly e-commerce websites.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Synthesis",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').",
                    "api_arguments": "None",
                    "python_environment_requirements": "diffusers",
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-cifar10-32\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception_score": 9.46,
                            "FID_score": 3.17
                        }
                    },
                    "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.",
                    "id": "huggingface_api_286",
                    "name": "google/ddpm-cifar10-32"
                },
                "id": "gorilla_huggingface_tool_282",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 1: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 2: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.",
            "Tool 3: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 4: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 5: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 6: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 7: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 11: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 15: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 16: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 18: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 19: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts."
        ],
        "tool_view": {
            "description": "This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "parameters": {
                "checkpoint": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained ControlNet model checkpoint to load.",
                    "required": true,
                    "example": "lllyasviel/control_v11p_sd15_inpaint"
                },
                "torch_dtype": {
                    "type": "string",
                    "description": "The PyTorch data type to use for model tensors. Commonly used to reduce memory usage or improve performance.",
                    "required": true,
                    "example": "torch.float16"
                }
            },
            "response": "The tool produces a single inpainted image as its primary response. The response can be described as follows:\n\n- **Type:** Image object  \n  The output is a generated image resulting from the image inpainting process, where masked regions in the input image are filled or modified according to the specified prompt and model conditions.\n\n- **Content:**  \n  - **Image pixels:** The pixel data of the generated image, typically in RGB format.  \n  - **Dimensions:** The width and height match the expected output size of the pipeline, which is usually consistent with the input image dimensions or model default resolution.  \n  - **File format or representation:** Can be returned as a PIL Image object, NumPy array, or saved directly to a file, depending on the user’s handling of the pipeline output.\n\n- **Optional metadata (if exposed by the pipeline):**  \n  - **Prompt used:** The text prompt that guided the generation.  \n  - **Negative prompt:** Text specifying undesired features to avoid in the generated image.  \n  - **Inference settings:** Number of steps, random seed, and other scheduler or pipeline parameters.  \n  - **Control conditions:** The processed image and mask inputs used for inpainting, which influenced the generation.\n\nIn summary, the response is a generated image reflecting the model’s inpainting of masked regions in the input image, optionally accompanied by generation parameters and metadata describing the conditioning and prompts used.",
            "examples": [
                {
                    "user_intent": "I want to inpaint an image where certain parts are missing or damaged.",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "How can I use ControlNet to fix specific parts of an image using a mask?",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "I need to repair a damaged image with inpainting and use a mask to guide the restoration process.",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 1: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 2: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.\nTool 3: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 4: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 5: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 6: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 7: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 8: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 9: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 10: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 11: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 13: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 15: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 16: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 18: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 19: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\", \"parameters\": {\"checkpoint\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained ControlNet model checkpoint to load.\", \"required\": true, \"example\": \"lllyasviel/control_v11p_sd15_inpaint\"}, \"torch_dtype\": {\"type\": \"string\", \"description\": \"The PyTorch data type to use for model tensors. Commonly used to reduce memory usage or improve performance.\", \"required\": true, \"example\": \"torch.float16\"}}, \"response\": \"The tool produces a single inpainted image as its primary response. The response can be described as follows:\\n\\n- **Type:** Image object  \\n  The output is a generated image resulting from the image inpainting process, where masked regions in the input image are filled or modified according to the specified prompt and model conditions.\\n\\n- **Content:**  \\n  - **Image pixels:** The pixel data of the generated image, typically in RGB format.  \\n  - **Dimensions:** The width and height match the expected output size of the pipeline, which is usually consistent with the input image dimensions or model default resolution.  \\n  - **File format or representation:** Can be returned as a PIL Image object, NumPy array, or saved directly to a file, depending on the user\\u2019s handling of the pipeline output.\\n\\n- **Optional metadata (if exposed by the pipeline):**  \\n  - **Prompt used:** The text prompt that guided the generation.  \\n  - **Negative prompt:** Text specifying undesired features to avoid in the generated image.  \\n  - **Inference settings:** Number of steps, random seed, and other scheduler or pipeline parameters.  \\n  - **Control conditions:** The processed image and mask inputs used for inpainting, which influenced the generation.\\n\\nIn summary, the response is a generated image reflecting the model\\u2019s inpainting of masked regions in the input image, optionally accompanied by generation parameters and metadata describing the conditioning and prompts used.\", \"examples\": [{\"user_intent\": \"I want to inpaint an image where certain parts are missing or damaged.\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"How can I use ControlNet to fix specific parts of an image using a mask?\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"I need to repair a damaged image with inpainting and use a mask to guide the restoration process.\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}]}\n\n# User Query\nExplain how to use GPT-3 to create a slogan for an e-commerce website that sells eco-friendly products.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a slogan for an e-commerce website using GPT-3.",
                    "tool_description": "Generate a creative slogan based on a given prompt using a large language model like GPT-3.",
                    "expected_response": "A text string containing a catchy and relevant slogan for an e-commerce website selling eco-friendly products."
                }
            ],
            "extracted_arguments": {
                "product_type": "The description of the products being sold by the website, in this case eco-friendly products.",
                "target_audience": "The target demographic or market segment for the slogan, such as eco-conscious consumers."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_27",
        "query": "We are developing an app to classify food images. We have a set of images and want to use a pre-trained model for classification.",
        "instruction": "Given an `image classification` task, retrieve tools that leverage pre-trained models for classifying images by processing image inputs and returning classification labels, ensuring the appropriate application of zero-shot or standard classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')",
                    "api_arguments": [
                        "image",
                        "possible_class_names"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "80.1"
                    },
                    "description": "A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.",
                    "id": "huggingface_api_350",
                    "name": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
                },
                "id": "gorilla_huggingface_tool_346",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 1: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 2: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 3: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 4: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 5: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 7: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 8: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.",
            "Tool 9: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.",
            "Tool 10: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 11: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 13: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 14: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 15: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.",
            "Tool 16: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.",
            "Tool 17: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 18: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 19: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks."
        ],
        "tool_view": {
            "description": "A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "parameters": {
                "image_path": {
                    "type": "string",
                    "description": "The file path or URL of the image to be classified by the model.",
                    "required": true,
                    "example": "images/dog.jpg"
                }
            },
            "response": "The tool returns a structured classification result for a given input image. Each response typically includes the following components:\n\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \"dog\" or \"food\"). This represents the model’s best guess for the image class.\n\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\n\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\n\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.",
            "examples": [
                {
                    "user_intent": "I have a photo of my pet and I want to check if it's a dog or something else.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/pet1.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Classify this picture to see if it's a meal or a dog.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/images/dinner_plate.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically sort my images folder into dogs and food photos.",
                    "tool_call": {
                        "name": "abhishek/autotrain-dog-vs-food",
                        "arguments": {
                            "image_path": "/home/user/photos/vacation/photo_2025.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 1: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 2: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 3: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 4: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 5: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 7: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 8: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.\nTool 9: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.\nTool 10: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 11: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 13: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 14: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 15: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\nTool 16: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.\nTool 17: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 18: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 19: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\", \"parameters\": {\"image_path\": {\"type\": \"string\", \"description\": \"The file path or URL of the image to be classified by the model.\", \"required\": true, \"example\": \"images/dog.jpg\"}}, \"response\": \"The tool returns a structured classification result for a given input image. Each response typically includes the following components:\\n\\n1. **Label / Class Name**: A string indicating the predicted category of the image (e.g., \\\"dog\\\" or \\\"food\\\"). This represents the model\\u2019s best guess for the image class.\\n\\n2. **Score / Confidence**: A numerical value, usually between 0 and 1, representing the model's confidence in the predicted label. Higher values indicate greater certainty in the classification.\\n\\n3. **Optional Additional Metadata**: Depending on the Hugging Face pipeline implementation, the response may include multiple predictions sorted by confidence, allowing inspection of secondary likely classes along with their respective scores.\\n\\nOverall, the response is a list of one or more objects, each containing a label and its associated confidence score, providing both the classification result and the model's certainty about that result.\", \"examples\": [{\"user_intent\": \"I have a photo of my pet and I want to check if it's a dog or something else.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/pet1.jpg\"}}}, {\"user_intent\": \"Classify this picture to see if it's a meal or a dog.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/images/dinner_plate.png\"}}}, {\"user_intent\": \"I want to automatically sort my images folder into dogs and food photos.\", \"tool_call\": {\"name\": \"abhishek/autotrain-dog-vs-food\", \"arguments\": {\"image_path\": \"/home/user/photos/vacation/photo_2025.jpg\"}}}]}\n\n# User Query\nWe are developing an app to classify food images. We have a set of images and want to use a pre-trained model for classification.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify a set of images to identify food items in them.",
                    "tool_description": "Use a pre-trained Hugging Face Transformers model to classify images as food or non-food, providing structured predictions with confidence scores for each image.",
                    "expected_response": "A list of classification results for each image, including predicted labels (e.g., 'food' or 'dog') and associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "image_paths": "A list of file paths or URLs of the images to be classified by the model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_449",
        "query": "I am designing an e-commerce website, and I want a program to generate images based on written descriptions to be used as mock product images.",
        "instruction": "Given an `image generation` task, retrieve tools that can create visual representations by processing textual descriptions to generate images suitable for use as product mockups in an e-commerce context.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))",
                    "api_arguments": {
                        "model": "CompVis/stable-diffusion-v1-4",
                        "vae": "AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)"
                    },
                    "python_environment_requirements": {
                        "diffusers": "diffusers library"
                    },
                    "example_code": "from diffusers.models import AutoencoderKL\nfrom diffusers import StableDiffusionPipeline\nmodel = CompVis/stable-diffusion-v1-4\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)",
                    "performance": {
                        "dataset": {
                            "COCO 2017 (256x256, val, 5000 images)": {
                                "accuracy": {
                                    "rFID": 4.42,
                                    "PSNR": "23.8 +/- 3.9",
                                    "SSIM": "0.69 +/- 0.13",
                                    "PSIM": "0.96 +/- 0.27"
                                }
                            },
                            "LAION-Aesthetics 5+ (256x256, subset, 10000 images)": {
                                "accuracy": {
                                    "rFID": 1.77,
                                    "PSNR": "26.7 +/- 4.8",
                                    "SSIM": "0.82 +/- 0.12",
                                    "PSIM": "0.67 +/- 0.34"
                                }
                            }
                        }
                    },
                    "description": "This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.",
                    "id": "huggingface_api_43",
                    "name": "stabilityai/sd-vae-ft-ema"
                },
                "id": "gorilla_huggingface_tool_43",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 2: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 3: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 4: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 6: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 9: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 10: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 12: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.",
            "Tool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.",
            "Tool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 16: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 18: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content."
        ],
        "tool_view": {
            "description": "Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "parameters": {},
            "response": "The tool is a text-to-image model that generates images based on textual descriptions. The response from this tool would typically contain the generated image(s) as output.\n\n### Expected Response Description:\n\n- **Generated Image(s):** \n  - The primary component of the response would be one or more images, generated based on the text description provided to the model. The format and structure of the images would likely depend on the specific implementation of the API call, but they would typically be represented in standard image formats (e.g., PNG, JPEG). The response would contain image data in a format suitable for rendering or further processing.\n  \n  - If multiple images are generated (e.g., based on model settings or parameters), the response would include an array or list of images.\n  \n- **Metadata (optional):** \n  - In some cases, the response might include additional metadata about the generated image, such as:\n    - **Image dimensions (width, height):** The size of the generated image(s).\n    - **Generation time:** The time taken to generate the image(s).\n    - **Model-specific information:** Details about the model or any settings used during generation, though this is not always required or included.\n\nThis description assumes the tool follows typical text-to-image response patterns, where the generated content is the primary focus and any extra information (e.g., metadata) may be included for context or further processing.",
            "examples": [
                {
                    "user_intent": "Create an image of a futuristic city skyline at sunset with flying cars.",
                    "tool_call": {
                        "name": "Linaqruf/anything-v3.0",
                        "arguments": {
                            "prompt": "futuristic city skyline at sunset, flying cars, neon lights, cinematic view"
                        }
                    }
                },
                {
                    "user_intent": "Generate a fantasy illustration of a dragon perched on a cliff overlooking a village.",
                    "tool_call": {
                        "name": "Linaqruf/anything-v3.0",
                        "arguments": {
                            "prompt": "majestic dragon on cliff, overlooking a medieval village, dramatic lighting, fantasy art style"
                        }
                    }
                },
                {
                    "user_intent": "Produce a realistic image of a cozy coffee shop interior with people working on laptops.",
                    "tool_call": {
                        "name": "Linaqruf/anything-v3.0",
                        "arguments": {
                            "prompt": "cozy coffee shop interior, people working on laptops, warm lighting, realistic style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 1: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 2: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 3: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 4: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 6: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 9: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 10: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 12: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.\nTool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.\nTool 14: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 15: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 16: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 18: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\", \"parameters\": {}, \"response\": \"The tool is a text-to-image model that generates images based on textual descriptions. The response from this tool would typically contain the generated image(s) as output.\\n\\n### Expected Response Description:\\n\\n- **Generated Image(s):** \\n  - The primary component of the response would be one or more images, generated based on the text description provided to the model. The format and structure of the images would likely depend on the specific implementation of the API call, but they would typically be represented in standard image formats (e.g., PNG, JPEG). The response would contain image data in a format suitable for rendering or further processing.\\n  \\n  - If multiple images are generated (e.g., based on model settings or parameters), the response would include an array or list of images.\\n  \\n- **Metadata (optional):** \\n  - In some cases, the response might include additional metadata about the generated image, such as:\\n    - **Image dimensions (width, height):** The size of the generated image(s).\\n    - **Generation time:** The time taken to generate the image(s).\\n    - **Model-specific information:** Details about the model or any settings used during generation, though this is not always required or included.\\n\\nThis description assumes the tool follows typical text-to-image response patterns, where the generated content is the primary focus and any extra information (e.g., metadata) may be included for context or further processing.\", \"examples\": [{\"user_intent\": \"Create an image of a futuristic city skyline at sunset with flying cars.\", \"tool_call\": {\"name\": \"Linaqruf/anything-v3.0\", \"arguments\": {\"prompt\": \"futuristic city skyline at sunset, flying cars, neon lights, cinematic view\"}}}, {\"user_intent\": \"Generate a fantasy illustration of a dragon perched on a cliff overlooking a village.\", \"tool_call\": {\"name\": \"Linaqruf/anything-v3.0\", \"arguments\": {\"prompt\": \"majestic dragon on cliff, overlooking a medieval village, dramatic lighting, fantasy art style\"}}}, {\"user_intent\": \"Produce a realistic image of a cozy coffee shop interior with people working on laptops.\", \"tool_call\": {\"name\": \"Linaqruf/anything-v3.0\", \"arguments\": {\"prompt\": \"cozy coffee shop interior, people working on laptops, warm lighting, realistic style\"}}}]}\n\n# User Query\nI am designing an e-commerce website, and I want a program to generate images based on written descriptions to be used as mock product images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate images from textual descriptions to create mock product visuals for an e-commerce website.",
                    "tool_description": "Create visual content by converting user-provided text prompts into images using a text-to-image generation model.",
                    "expected_response": "One or more images representing the visual interpretation of the input text descriptions, suitable for use as product mockups."
                }
            ],
            "extracted_arguments": {
                "prompt": "A textual description of the product or scene to be visually generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_210",
        "query": "In this Star Wars movie scene, I want to create a depth estimation for the stormtroopers.",
        "instruction": "Given a `depth estimation` task, retrieve tools that perform depth estimation in images by applying computer vision techniques to estimate and provide depth-related insights from the visual scene, aligning with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Depth Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "PIL",
                        "numpy",
                        "torch"
                    ],
                    "example_code": {
                        "install_packages": "pip install diffusers transformers accelerate",
                        "code": [
                            "from transformers import pipeline",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "from PIL import Image",
                            "import numpy as np",
                            "import torch",
                            "from diffusers.utils import load_image",
                            "depth_estimator = pipeline('depth-estimation')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)",
                            "image = depth_estimator(image)['depth']",
                            "image = np.array(image)",
                            "image = image[:, :, None]",
                            "image = np.concatenate([image, image, image], axis=2)",
                            "image = Image.fromarray(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]",
                            "image.save('./images/stormtrooper_depth_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "3M depth-image, caption pairs",
                        "accuracy": "500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_265",
                    "name": "lllyasviel/sd-controlnet-depth"
                },
                "id": "gorilla_huggingface_tool_261",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 1: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 2: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 3: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 4: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 5: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 7: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 8: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.",
            "Tool 9: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.",
            "Tool 10: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.",
            "Tool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 12: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 13: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 14: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 15: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 17: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 18: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.",
            "Tool 19: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "parameters": {},
            "response": "The tool loads a depth estimation model based on the GLPN architecture that has been fine-tuned on the DIODE dataset. The model, once instantiated via the `AutoModel.from_pretrained` call, returns an object representing the pre-trained transformer model, which can be used for performing depth estimation on input images.  \n\nThe response (i.e., the model object) typically includes the following components:\n\n- **Model architecture and configuration**: Metadata about the GLPN model, including layer definitions, input/output shapes, and hyperparameters. This defines how input images are processed to generate depth predictions.  \n- **Pre-trained weights**: The learned parameters obtained from fine-tuning on the DIODE dataset, enabling the model to predict depth maps accurately.  \n- **Forward method**: A callable interface for passing image tensors through the model to compute outputs. For depth estimation, this produces depth maps corresponding to the input images.  \n- **Utility methods**: Functions for model inspection, saving/loading, and possibly device management (CPU/GPU).  \n- **Configuration object (`config`)**: Contains settings related to the pre-trained model, such as normalization parameters, expected input resolution, and any model-specific options.  \n\nThe model object itself does not return depth predictions until an input image tensor is passed through it; therefore, the immediate response is a configured, ready-to-use model instance rather than actual depth maps.",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth of objects in a single image using a pre-trained model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_path": "living_room.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Generate a depth map for an outdoor scene photo I took yesterday.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_path": "park_scene.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to create a depth visualization for multiple images in my dataset to analyze scene geometry.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_paths": [
                                "kitchen.png",
                                "office.jpg",
                                "street_view.jpeg"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 1: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 2: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 3: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 4: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 5: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 7: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 8: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.\nTool 9: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.\nTool 10: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.\nTool 11: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 12: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 13: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 14: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 15: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 17: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 18: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.\nTool 19: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\", \"parameters\": {}, \"response\": \"The tool loads a depth estimation model based on the GLPN architecture that has been fine-tuned on the DIODE dataset. The model, once instantiated via the `AutoModel.from_pretrained` call, returns an object representing the pre-trained transformer model, which can be used for performing depth estimation on input images.  \\n\\nThe response (i.e., the model object) typically includes the following components:\\n\\n- **Model architecture and configuration**: Metadata about the GLPN model, including layer definitions, input/output shapes, and hyperparameters. This defines how input images are processed to generate depth predictions.  \\n- **Pre-trained weights**: The learned parameters obtained from fine-tuning on the DIODE dataset, enabling the model to predict depth maps accurately.  \\n- **Forward method**: A callable interface for passing image tensors through the model to compute outputs. For depth estimation, this produces depth maps corresponding to the input images.  \\n- **Utility methods**: Functions for model inspection, saving/loading, and possibly device management (CPU/GPU).  \\n- **Configuration object (`config`)**: Contains settings related to the pre-trained model, such as normalization parameters, expected input resolution, and any model-specific options.  \\n\\nThe model object itself does not return depth predictions until an input image tensor is passed through it; therefore, the immediate response is a configured, ready-to-use model instance rather than actual depth maps.\", \"examples\": [{\"user_intent\": \"I want to estimate the depth of objects in a single image using a pre-trained model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_path\": \"living_room.jpg\"}}}, {\"user_intent\": \"Generate a depth map for an outdoor scene photo I took yesterday.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_path\": \"park_scene.png\"}}}, {\"user_intent\": \"I want to create a depth visualization for multiple images in my dataset to analyze scene geometry.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_paths\": [\"kitchen.png\", \"office.jpg\", \"street_view.jpeg\"]}}}]}\n\n# User Query\nIn this Star Wars movie scene, I want to create a depth estimation for the stormtroopers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth of stormtroopers in a Star Wars movie scene using a pre-trained model.",
                    "tool_description": "Use a GLPN-based depth estimation model fine-tuned on the DIODE dataset to predict per-pixel depth from a single image frame extracted from the movie scene.",
                    "expected_response": "A depth map representing per-pixel distances for the objects in the input image, highlighting the spatial arrangement of stormtroopers and other elements in the scene."
                }
            ],
            "extracted_arguments": {
                "image_frame": "A single image frame or snapshot extracted from the Star Wars movie scene."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_378",
        "query": "A media company needs to summarize a news article in order to make it easy for their audience to understand the main points quickly.",
        "instruction": "Given a `text summarization` task, retrieve tools that process text inputs to generate concise summaries, ensuring that the main points are clearly articulated in a manner that is easily digestible by the target audience.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('csebuetnlp/mT5_multilingual_XLSum')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.11.0.dev0"
                    ],
                    "example_code": "import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ninput_ids = tokenizer(\n [WHITESPACE_HANDLER(article_text)],\n return_tensors=pt,\n padding=max_length,\n truncation=True,\n max_length=512\n)[input_ids]\noutput_ids = model.generate(\n input_ids=input_ids,\n max_length=84,\n no_repeat_ngram_size=2,\n num_beams=4\n)[0]\nsummary = tokenizer.decode(\n output_ids,\n skip_special_tokens=True,\n clean_up_tokenization_spaces=False\n)\nprint(summary)",
                    "performance": {
                        "dataset": "xsum",
                        "accuracy": {
                            "ROUGE-1": 36.5,
                            "ROUGE-2": 13.934,
                            "ROUGE-L": 28.988,
                            "ROUGE-LSUM": 28.996,
                            "loss": 2.067,
                            "gen_len": 26.973
                        }
                    },
                    "description": "This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.",
                    "id": "huggingface_api_562",
                    "name": "csebuetnlp/mT5_multilingual_XLSum"
                },
                "id": "gorilla_huggingface_tool_558",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 1: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 5: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 6: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 10: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 12: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 13: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 14: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 15: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 16: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 17: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 18: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 19: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis."
        ],
        "tool_view": {
            "description": "A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "parameters": {
                "input_text": {
                    "type": "string",
                    "description": "The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.",
                    "required": true,
                    "example": "India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England."
                }
            },
            "response": "The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \n\nSpecifically:  \n\n- **Type:** String  \n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \n- **Length:** Typically shorter than the input text, with the exact length depending on the model’s internal generation constraints and the input complexity.  \n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \n\nThere are no additional structured fields; the output is purely the generated summary string.",
            "examples": "[\n    {\n        \"user_intent\": \"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\",\n        \"tool_call\": {\n            \"name\": \"tuner007/pegasus_summarizer\",\n            \"arguments\": {\n                \"input_text\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day"
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 1: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 2: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 5: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 6: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 7: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 10: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 12: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 13: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 14: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 15: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 16: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 17: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 18: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 19: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\", \"parameters\": {\"input_text\": {\"type\": \"string\", \"description\": \"The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.\", \"required\": true, \"example\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England.\"}}, \"response\": \"The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \\n\\nSpecifically:  \\n\\n- **Type:** String  \\n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \\n- **Length:** Typically shorter than the input text, with the exact length depending on the model\\u2019s internal generation constraints and the input complexity.  \\n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \\n\\nThere are no additional structured fields; the output is purely the generated summary string.\", \"examples\": \"[\\n    {\\n        \\\"user_intent\\\": \\\"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\\\",\\n        \\\"tool_call\\\": {\\n            \\\"name\\\": \\\"tuner007/pegasus_summarizer\\\",\\n            \\\"arguments\\\": {\\n                \\\"input_text\\\": \\\"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day\"}\n\n# User Query\nA media company needs to summarize a news article in order to make it easy for their audience to understand the main points quickly.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize a news article to make the main points easy to understand for the audience.",
                    "tool_description": "Use a PEGASUS-based text summarization model to condense a full-length news article into a concise summary, preserving key information and removing less relevant details.",
                    "expected_response": "A coherent, fluent textual summary of the input news article highlighting the main points, key events, and essential information."
                }
            ],
            "extracted_arguments": {
                "input_text": "The full content of the news article that needs to be summarized, provided as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_181",
        "query": "I am working in a bank, I want to estimate the mortgage for a given housing using the housing's features.",
        "instruction": "Given a `mortgage estimation` task, retrieve tools that utilize housing features and predictive models to estimate the mortgage value by processing tabular data inputs and leveraging regression algorithms to provide accurate financial assessments.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": {
                        "data": "pandas.DataFrame"
                    },
                    "python_environment_requirements": {
                        "joblib": "latest",
                        "pandas": "latest"
                    },
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 134406.507,
                            "R2": 0.861,
                            "MSE": 18065109105.27,
                            "MAE": 103271.843,
                            "RMSLE": 0.139
                        }
                    },
                    "description": "A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.",
                    "id": "huggingface_api_871",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761511"
                },
                "id": "gorilla_huggingface_tool_864",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 1: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 2: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 3: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 4: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 7: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 9: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 10: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 11: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 12: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 14: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 15: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 16: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 17: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 18: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.",
            "Tool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently."
        ],
        "tool_view": {
            "description": "A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "parameters": {
                "bootstrap": {
                    "type": "boolean",
                    "description": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
                    "required": true,
                    "default": "True",
                    "example": true
                },
                "ccp_alpha": {
                    "type": "number",
                    "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the trees.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "criterion": {
                    "type": "string",
                    "description": "The function to measure the quality of a split. Supported options include 'squared_error', 'absolute_error', etc.",
                    "required": true,
                    "default": "squared_error",
                    "example": "squared_error"
                },
                "max_depth": {
                    "type": "number",
                    "description": "The maximum depth of the tree. If not set, nodes are expanded until all leaves are pure or contain fewer samples than min_samples_split.",
                    "required": true,
                    "example": 10
                },
                "max_features": {
                    "type": "string",
                    "description": "The number of features to consider when looking for the best split. Can be a number, a fraction, 'sqrt', or 'log2'.",
                    "required": true,
                    "default": "1.0",
                    "example": "1.0"
                },
                "max_leaf_nodes": {
                    "type": "number",
                    "description": "Grow trees with max_leaf_nodes in best-first fashion. If None, unlimited number of leaf nodes.",
                    "required": true,
                    "example": 50
                },
                "max_samples": {
                    "type": "number",
                    "description": "If bootstrap is True, the number of samples to draw from X to train each base estimator. If None, draw X.shape[0] samples.",
                    "required": true,
                    "example": 500
                },
                "min_impurity_decrease": {
                    "type": "number",
                    "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "min_samples_leaf": {
                    "type": "number",
                    "description": "The minimum number of samples required to be at a leaf node.",
                    "required": true,
                    "default": "1",
                    "example": 2
                },
                "min_samples_split": {
                    "type": "number",
                    "description": "The minimum number of samples required to split an internal node.",
                    "required": true,
                    "default": "2",
                    "example": 5
                },
                "min_weight_fraction_leaf": {
                    "type": "number",
                    "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "n_estimators": {
                    "type": "number",
                    "description": "The number of trees in the forest.",
                    "required": true,
                    "default": "100",
                    "example": 200
                },
                "n_jobs": {
                    "type": "number",
                    "description": "The number of jobs to run in parallel for both fit and predict. None means 1. -1 means using all processors.",
                    "required": true,
                    "example": -1
                },
                "oob_score": {
                    "type": "boolean",
                    "description": "Whether to use out-of-bag samples to estimate the R^2 on unseen data.",
                    "required": true,
                    "default": "False",
                    "example": true
                },
                "random_state": {
                    "type": "number",
                    "description": "Controls both the randomness of the bootstrapping of the samples and the sampling of the features when looking for the best split.",
                    "required": true,
                    "example": 42
                },
                "verbose": {
                    "type": "number",
                    "description": "Controls the verbosity when fitting and predicting. Higher values give more messages.",
                    "required": true,
                    "default": "0",
                    "example": 1
                },
                "warm_start": {
                    "type": "boolean",
                    "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.",
                    "required": true,
                    "default": "False",
                    "example": true
                }
            },
            "response": "The tool is a wrapper around Scikit-learn’s `RandomForestRegressor` for tabular regression tasks. Its response is typically the output of a trained regression model applied to input tabular data. The response structure and the meaning of each component are as follows:\n\n1. **Predictions**  \n   - **Type:** Array or list of numeric values (floats or integers).  \n   - **Description:** Each element represents the predicted target value for a corresponding input sample. The order of predictions matches the order of the input samples.\n\n2. **Optional Model Metadata (if exposed by the wrapper)**  \n   - **Feature Importances**  \n     - **Type:** Array of numeric values.  \n     - **Description:** Relative importance of each input feature in the model’s predictions. Useful for interpretability.  \n   - **Number of Trees**  \n     - **Type:** Integer.  \n     - **Description:** Total number of decision trees used in the ensemble (`n_estimators`).  \n   - **Model Parameters**  \n     - **Type:** Dictionary.  \n     - **Description:** Configuration of hyperparameters used for training (e.g., `max_depth`, `min_samples_split`, `criterion`).  \n   - **Training Information** (optional)  \n     - **Type:** May include training metrics or dataset statistics.  \n     - **Description:** Could provide performance metrics such as mean squared error on training data, but this is not explicitly defined in the JSON.\n\nIn practice, the primary response is the predicted numeric target values for the input tabular data. Additional metadata may be available depending on how the tool exposes model attributes.",
            "examples": [
                {
                    "user_intent": "I want to train a Random Forest model to predict California housing prices with default settings.",
                    "tool_call": {
                        "name": "rajistics/california_housing",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": "",
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 100,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": "",
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "Set up a Random Forest to predict housing prices but limit the depth to 10 and use 200 trees.",
                    "tool_call": {
                        "name": "rajistics/california_housing",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 200,
                            "n_jobs": -1,
                            "oob_score": false,
                            "random_state": 42,
                            "verbose": 1,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "I need a Random Forest model for California housing, using out-of-bag estimates and a smaller minimum samples per leaf of 5.",
                    "tool_call": {
                        "name": "rajistics/california_housing",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": "",
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 5,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 100,
                            "n_jobs": -1,
                            "oob_score": true,
                            "random_state": 123,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 1: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 2: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 3: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 4: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 7: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 8: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 9: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 10: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 11: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 12: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 14: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 15: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 16: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 17: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 18: A Keras-based TabTransformer model for supervised and semi-supervised learning on tabular data, capable of handling both numerical and categorical features. Categorical features are embedded and processed through Transformer blocks to capture contextual relationships, then combined with numerical features and passed through a final feed-forward network for prediction, with a SoftMax output for classification. This model is designed to leverage self-attention mechanisms to improve learning from structured datasets.\nTool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\", \"parameters\": {\"bootstrap\": {\"type\": \"boolean\", \"description\": \"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\", \"required\": true, \"default\": \"True\", \"example\": true}, \"ccp_alpha\": {\"type\": \"number\", \"description\": \"Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the trees.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"criterion\": {\"type\": \"string\", \"description\": \"The function to measure the quality of a split. Supported options include 'squared_error', 'absolute_error', etc.\", \"required\": true, \"default\": \"squared_error\", \"example\": \"squared_error\"}, \"max_depth\": {\"type\": \"number\", \"description\": \"The maximum depth of the tree. If not set, nodes are expanded until all leaves are pure or contain fewer samples than min_samples_split.\", \"required\": true, \"example\": 10}, \"max_features\": {\"type\": \"string\", \"description\": \"The number of features to consider when looking for the best split. Can be a number, a fraction, 'sqrt', or 'log2'.\", \"required\": true, \"default\": \"1.0\", \"example\": \"1.0\"}, \"max_leaf_nodes\": {\"type\": \"number\", \"description\": \"Grow trees with max_leaf_nodes in best-first fashion. If None, unlimited number of leaf nodes.\", \"required\": true, \"example\": 50}, \"max_samples\": {\"type\": \"number\", \"description\": \"If bootstrap is True, the number of samples to draw from X to train each base estimator. If None, draw X.shape[0] samples.\", \"required\": true, \"example\": 500}, \"min_impurity_decrease\": {\"type\": \"number\", \"description\": \"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"min_samples_leaf\": {\"type\": \"number\", \"description\": \"The minimum number of samples required to be at a leaf node.\", \"required\": true, \"default\": \"1\", \"example\": 2}, \"min_samples_split\": {\"type\": \"number\", \"description\": \"The minimum number of samples required to split an internal node.\", \"required\": true, \"default\": \"2\", \"example\": 5}, \"min_weight_fraction_leaf\": {\"type\": \"number\", \"description\": \"The minimum weighted fraction of the sum total of weights required to be at a leaf node.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"n_estimators\": {\"type\": \"number\", \"description\": \"The number of trees in the forest.\", \"required\": true, \"default\": \"100\", \"example\": 200}, \"n_jobs\": {\"type\": \"number\", \"description\": \"The number of jobs to run in parallel for both fit and predict. None means 1. -1 means using all processors.\", \"required\": true, \"example\": -1}, \"oob_score\": {\"type\": \"boolean\", \"description\": \"Whether to use out-of-bag samples to estimate the R^2 on unseen data.\", \"required\": true, \"default\": \"False\", \"example\": true}, \"random_state\": {\"type\": \"number\", \"description\": \"Controls both the randomness of the bootstrapping of the samples and the sampling of the features when looking for the best split.\", \"required\": true, \"example\": 42}, \"verbose\": {\"type\": \"number\", \"description\": \"Controls the verbosity when fitting and predicting. Higher values give more messages.\", \"required\": true, \"default\": \"0\", \"example\": 1}, \"warm_start\": {\"type\": \"boolean\", \"description\": \"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.\", \"required\": true, \"default\": \"False\", \"example\": true}}, \"response\": \"The tool is a wrapper around Scikit-learn\\u2019s `RandomForestRegressor` for tabular regression tasks. Its response is typically the output of a trained regression model applied to input tabular data. The response structure and the meaning of each component are as follows:\\n\\n1. **Predictions**  \\n   - **Type:** Array or list of numeric values (floats or integers).  \\n   - **Description:** Each element represents the predicted target value for a corresponding input sample. The order of predictions matches the order of the input samples.\\n\\n2. **Optional Model Metadata (if exposed by the wrapper)**  \\n   - **Feature Importances**  \\n     - **Type:** Array of numeric values.  \\n     - **Description:** Relative importance of each input feature in the model\\u2019s predictions. Useful for interpretability.  \\n   - **Number of Trees**  \\n     - **Type:** Integer.  \\n     - **Description:** Total number of decision trees used in the ensemble (`n_estimators`).  \\n   - **Model Parameters**  \\n     - **Type:** Dictionary.  \\n     - **Description:** Configuration of hyperparameters used for training (e.g., `max_depth`, `min_samples_split`, `criterion`).  \\n   - **Training Information** (optional)  \\n     - **Type:** May include training metrics or dataset statistics.  \\n     - **Description:** Could provide performance metrics such as mean squared error on training data, but this is not explicitly defined in the JSON.\\n\\nIn practice, the primary response is the predicted numeric target values for the input tabular data. Additional metadata may be available depending on how the tool exposes model attributes.\", \"examples\": [{\"user_intent\": \"I want to train a Random Forest model to predict California housing prices with default settings.\", \"tool_call\": {\"name\": \"rajistics/california_housing\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": \"\", \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": \"\", \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"Set up a Random Forest to predict housing prices but limit the depth to 10 and use 200 trees.\", \"tool_call\": {\"name\": \"rajistics/california_housing\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 200, \"n_jobs\": -1, \"oob_score\": false, \"random_state\": 42, \"verbose\": 1, \"warm_start\": false}}}, {\"user_intent\": \"I need a Random Forest model for California housing, using out-of-bag estimates and a smaller minimum samples per leaf of 5.\", \"tool_call\": {\"name\": \"rajistics/california_housing\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": \"\", \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 5, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": -1, \"oob_score\": true, \"random_state\": 123, \"verbose\": 0, \"warm_start\": false}}}]}\n\n# User Query\nI am working in a bank, I want to estimate the mortgage for a given housing using the housing's features.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict housing prices based on the features of a property.",
                    "tool_description": "Use a pre-trained regression model to estimate housing prices from input tabular features representing property characteristics.",
                    "expected_response": "A numeric value representing the predicted housing price for each input sample, optionally including model metadata such as feature importances."
                }
            ],
            "extracted_arguments": {
                "housing_features": "A structured set of numerical and categorical features describing the property, provided in tabular format suitable for regression input."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_220",
        "query": "My coworker sent me a voice message in Spanish. Translate this audio message to English, so that I can understand it.",
        "instruction": "Given a `speech translation` task, retrieve tools that process spoken language inputs, specifically audio in Spanish, to accurately produce textual translations in English aligned with the query's requirements and context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "textless_sm_sl_es()",
                    "api_arguments": null,
                    "python_environment_requirements": "fairseq",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "A Fairseq model for audio-to-audio speech-to-speech translation.",
                    "id": "huggingface_api_800",
                    "name": "textless_sm_sl_es"
                },
                "id": "gorilla_huggingface_tool_795",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 2: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 3: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 4: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 6: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 7: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 8: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 9: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 11: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 13: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 14: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 15: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 16: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 17: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 19: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication."
        ],
        "tool_view": {
            "description": "Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "parameters": {
                "input_prompt": {
                    "type": "string",
                    "description": "The full text prompt provided to the model, including the character's persona, dialogue history, and the user's input message. This prompt guides the model in generating an appropriate conversational response.",
                    "required": true,
                    "example": "Alice's Persona: Alice is a curious and cheerful adventurer.\n[DIALOGUE HISTORY]\nYou: Hello Alice, how was your day?\nAlice:"
                }
            },
            "response": "The tool performs text generation within a conversational context, using the Pygmalion 1.3B model from Hugging Face Transformers. The response format is implicitly defined through its functionality, which is to generate a conversational reply based on the provided input prompt.\n\n### Expected Response Description:\nThe response is a generated piece of text, specifically a conversational reply from the model. It is the model's continuation of the dialogue given the input message, with the following key aspects:\n\n1. **Generated Dialogue Output**: The core component of the response is the text generated by the model. This text represents the next part of the conversation based on the provided \"input_prompt\", which includes the persona of a character, any preceding dialogue history, and the user’s latest message.\n   \n2. **Context Sensitivity**: The response will be contextually aware of the preceding dialogue and any character persona specified in the input. This means the output will attempt to match the tone, style, and character of the persona outlined in the input prompt.\n   \n3. **No Explicit Metadata**: The response does not contain any additional metadata or performance indicators in its structure. It is a plain text output, focused solely on providing a continuation of the conversation as per the model's designed behavior.\n\nIn summary, the response will be a natural language text that serves as the model’s reply to the latest user input, influenced by the character persona and dialogue history provided in the prompt.",
            "examples": [
                {
                    "user_intent": "I want to have a conversation with an enthusiastic sci-fi storyteller named Alex who loves space adventures.",
                    "tool_call": {
                        "name": "pygmalion-1.3b",
                        "arguments": {
                            "input_prompt": "Alex's Persona: Alex is an enthusiastic sci-fi storyteller who loves space adventures and futuristic technology.\n[DIALOGUE HISTORY]\nYou: Hi Alex! Can you tell me a story about a space mission?\nAlex:"
                        }
                    }
                },
                {
                    "user_intent": "Chat with a sarcastic detective character to solve a mysterious case.",
                    "tool_call": {
                        "name": "pygmalion-1.3b",
                        "arguments": {
                            "input_prompt": "Detective Sam's Persona: Sam is a sarcastic, quick-witted detective who enjoys solving complex mysteries and teasing suspects.\n[DIALOGUE HISTORY]\nYou: Detective Sam, what do you think happened at the crime scene?\nSam:"
                        }
                    }
                },
                {
                    "user_intent": "Roleplay a friendly medieval bard who tells humorous stories to entertain the tavern guests.",
                    "tool_call": {
                        "name": "pygmalion-1.3b",
                        "arguments": {
                            "input_prompt": "Bard Lyria's Persona: Lyria is a jovial medieval bard who loves telling humorous tales and entertaining tavern guests with songs.\n[DIALOGUE HISTORY]\nYou: Lyria, tell us a funny story about your travels!\nLyria:"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 2: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 3: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 4: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 6: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 7: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 8: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 9: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 10: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 11: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 12: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 13: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 14: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 15: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 16: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 17: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 18: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 19: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character\\u2019s persona, dialogue history, and the user\\u2019s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\", \"parameters\": {\"input_prompt\": {\"type\": \"string\", \"description\": \"The full text prompt provided to the model, including the character's persona, dialogue history, and the user's input message. This prompt guides the model in generating an appropriate conversational response.\", \"required\": true, \"example\": \"Alice's Persona: Alice is a curious and cheerful adventurer.\\n[DIALOGUE HISTORY]\\nYou: Hello Alice, how was your day?\\nAlice:\"}}, \"response\": \"The tool performs text generation within a conversational context, using the Pygmalion 1.3B model from Hugging Face Transformers. The response format is implicitly defined through its functionality, which is to generate a conversational reply based on the provided input prompt.\\n\\n### Expected Response Description:\\nThe response is a generated piece of text, specifically a conversational reply from the model. It is the model's continuation of the dialogue given the input message, with the following key aspects:\\n\\n1. **Generated Dialogue Output**: The core component of the response is the text generated by the model. This text represents the next part of the conversation based on the provided \\\"input_prompt\\\", which includes the persona of a character, any preceding dialogue history, and the user\\u2019s latest message.\\n   \\n2. **Context Sensitivity**: The response will be contextually aware of the preceding dialogue and any character persona specified in the input. This means the output will attempt to match the tone, style, and character of the persona outlined in the input prompt.\\n   \\n3. **No Explicit Metadata**: The response does not contain any additional metadata or performance indicators in its structure. It is a plain text output, focused solely on providing a continuation of the conversation as per the model's designed behavior.\\n\\nIn summary, the response will be a natural language text that serves as the model\\u2019s reply to the latest user input, influenced by the character persona and dialogue history provided in the prompt.\", \"examples\": [{\"user_intent\": \"I want to have a conversation with an enthusiastic sci-fi storyteller named Alex who loves space adventures.\", \"tool_call\": {\"name\": \"pygmalion-1.3b\", \"arguments\": {\"input_prompt\": \"Alex's Persona: Alex is an enthusiastic sci-fi storyteller who loves space adventures and futuristic technology.\\n[DIALOGUE HISTORY]\\nYou: Hi Alex! Can you tell me a story about a space mission?\\nAlex:\"}}}, {\"user_intent\": \"Chat with a sarcastic detective character to solve a mysterious case.\", \"tool_call\": {\"name\": \"pygmalion-1.3b\", \"arguments\": {\"input_prompt\": \"Detective Sam's Persona: Sam is a sarcastic, quick-witted detective who enjoys solving complex mysteries and teasing suspects.\\n[DIALOGUE HISTORY]\\nYou: Detective Sam, what do you think happened at the crime scene?\\nSam:\"}}}, {\"user_intent\": \"Roleplay a friendly medieval bard who tells humorous stories to entertain the tavern guests.\", \"tool_call\": {\"name\": \"pygmalion-1.3b\", \"arguments\": {\"input_prompt\": \"Bard Lyria's Persona: Lyria is a jovial medieval bard who loves telling humorous tales and entertaining tavern guests with songs.\\n[DIALOGUE HISTORY]\\nYou: Lyria, tell us a funny story about your travels!\\nLyria:\"}}}]}\n\n# User Query\nMy coworker sent me a voice message in Spanish. Translate this audio message to English, so that I can understand it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transcribe the Spanish voice message into text.",
                    "tool_description": "Convert spoken Spanish audio from a voice message into written Spanish text using a speech recognition model.",
                    "expected_response": "A textual transcription of the Spanish audio message."
                },
                {
                    "user_intent": "Translate the transcribed Spanish text into English.",
                    "tool_description": "Translate Spanish text into English using a machine translation model trained for Spanish-to-English conversion.",
                    "expected_response": "The English translation of the transcribed Spanish text."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The Spanish-language audio file containing the spoken message.",
                "source_language": "The language of the original audio content, in this case Spanish.",
                "target_language": "The language into which the text should be translated, in this case English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_49",
        "query": "I am developing a news analysis platform. I need to predict the named entities from the articles.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that perform entity extraction by processing text inputs to identify and label named entities within the content of news articles.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "pipeline('ner')",
                    "api_arguments": {},
                    "python_environment_requirements": {
                        "transformers": ">=4.0.0"
                    },
                    "example_code": "nlp('My name is John and I live in New York.')",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.",
                    "id": "huggingface_api_928",
                    "name": "dslim/bert-base-NER-uncased"
                },
                "id": "gorilla_huggingface_tool_405",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 3: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 4: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 6: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 7: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 8: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 9: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 10: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 11: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 12: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 13: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 14: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 16: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 17: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.",
            "Tool 18: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 3: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 4: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 5: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 6: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 7: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 8: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 9: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 10: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 11: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 12: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 13: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 14: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 15: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 16: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 17: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.\nTool 18: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 19: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nI am developing a news analysis platform. I need to predict the named entities from the articles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract named entities from a news article.",
                    "tool_description": "Perform named entity recognition (NER) to identify and extract entities such as person names, organizations, locations, and other relevant terms from the input article.",
                    "expected_response": "A structured list of recognized named entities categorized into types like persons, locations, organizations, and miscellaneous entities."
                }
            ],
            "extracted_arguments": {
                "text": "The article or text content from which named entities need to be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_48",
        "query": "We are developing an app that can help mediate virtual therapy sessions. We need a feature that can detect emotions in the voice of the user.",
        "instruction": "Given an `emotion detection` task, retrieve tools that analyze voice inputs to identify and classify emotions, specifically leveraging advanced audio processing capabilities to assist in virtual therapy sessions.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')",
                    "api_arguments": {
                        "audio_file": "string"
                    },
                    "python_environment_requirements": [
                        "audio_models",
                        "transformers",
                        "torch",
                        "numpy",
                        "pydub"
                    ],
                    "example_code": "def predict_emotion_hubert(audio_file):\n from audio_models import HubertForSpeechClassification\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\n import torch.nn.functional as F\n import torch\n import numpy as np\n from pydub import AudioSegment\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\nsampling_rate=16000\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\ndef speech_file_to_array(path, sampling_rate):\n sound = AudioSegment.from_file(path)\n sound = sound.set_frame_rate(sampling_rate)\n sound_array = np.array(sound.get_array_of_samples())\n return sound_array\nsound_array = speech_file_to_array(audio_file, sampling_rate)\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\nwith torch.no_grad():\n logits = model(**inputs).logits\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\noutputs = [{\n emo: config.id2label[i],\n score: round(score * 100, 1)}\n for i, score in enumerate(scores)\n]\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\nresult = predict_emotion_hubert(male-crying.mp3)\nresult",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "A pretrained model for predicting emotion in local audio files using Hubert.",
                    "id": "huggingface_api_811",
                    "name": "Rajaram1996/Hubert_emotion"
                },
                "id": "gorilla_huggingface_tool_806",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 1: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 3: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 5: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 6: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 7: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 8: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 9: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 12: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.",
            "Tool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 14: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 15: facebook/bart-large is a pre-trained transformer encoder-decoder model with a bidirectional encoder and an autoregressive decoder, designed for feature extraction and text understanding. It can process and encode text inputs to generate contextualized representations, making it useful for tasks such as text summarization, translation, classification, and question answering.",
            "Tool 16: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 18: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 19: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions."
        ],
        "tool_view": {
            "description": "FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "parameters": {},
            "response": "The tool, FSMN-VAD, is designed for Voice Activity Detection (VAD). The response from this tool will likely contain information about the presence or absence of speech (voice activity) within a given audio input. \n\nIn general, the response from the tool is expected to include:\n\n1. **Speech Segments**: The main output will be a list or a set of labeled time intervals that indicate where speech occurs in the audio input. Each segment corresponds to a portion of the audio where voice activity was detected. This might include:\n   - **Start Time**: The beginning of the segment where voice activity is detected.\n   - **End Time**: The end of the segment where voice activity is detected.\n   \n2. **Confidence Scores** (optional): Depending on the implementation, the response could include a confidence score or probability associated with each detected segment, reflecting the model's certainty that the segment is speech-related.\n\n3. **Non-Speech Periods** (optional): The response might also highlight periods where no speech activity is detected, though this is typically inferred from the absence of speech segments.\n\nOverall, the structure is expected to focus on marking the regions of the audio that contain speech, potentially including start and end times for these regions, and possibly including confidence scores or other metadata related to the detected speech activity.",
            "examples": [
                {
                    "user_intent": "I want to detect when someone is speaking in this meeting recording.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "team_meeting_2025_12_30.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this podcast audio and identify all segments with voice activity.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "tech_podcast_episode12.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I have surveillance audio from last night and I want to find all periods of speech.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "security_cam_2025_12_29.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 1: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 3: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 5: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 6: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 7: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 8: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 9: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 10: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 12: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.\nTool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 14: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 15: facebook/bart-large is a pre-trained transformer encoder-decoder model with a bidirectional encoder and an autoregressive decoder, designed for feature extraction and text understanding. It can process and encode text inputs to generate contextualized representations, making it useful for tasks such as text summarization, translation, classification, and question answering.\nTool 16: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 18: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 19: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\", \"parameters\": {}, \"response\": \"The tool, FSMN-VAD, is designed for Voice Activity Detection (VAD). The response from this tool will likely contain information about the presence or absence of speech (voice activity) within a given audio input. \\n\\nIn general, the response from the tool is expected to include:\\n\\n1. **Speech Segments**: The main output will be a list or a set of labeled time intervals that indicate where speech occurs in the audio input. Each segment corresponds to a portion of the audio where voice activity was detected. This might include:\\n   - **Start Time**: The beginning of the segment where voice activity is detected.\\n   - **End Time**: The end of the segment where voice activity is detected.\\n   \\n2. **Confidence Scores** (optional): Depending on the implementation, the response could include a confidence score or probability associated with each detected segment, reflecting the model's certainty that the segment is speech-related.\\n\\n3. **Non-Speech Periods** (optional): The response might also highlight periods where no speech activity is detected, though this is typically inferred from the absence of speech segments.\\n\\nOverall, the structure is expected to focus on marking the regions of the audio that contain speech, potentially including start and end times for these regions, and possibly including confidence scores or other metadata related to the detected speech activity.\", \"examples\": [{\"user_intent\": \"I want to detect when someone is speaking in this meeting recording.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"team_meeting_2025_12_30.wav\"}}}, {\"user_intent\": \"Analyze this podcast audio and identify all segments with voice activity.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"tech_podcast_episode12.mp3\"}}}, {\"user_intent\": \"I have surveillance audio from last night and I want to find all periods of speech.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"security_cam_2025_12_29.wav\"}}}]}\n\n# User Query\nWe are developing an app that can help mediate virtual therapy sessions. We need a feature that can detect emotions in the voice of the user.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect emotions in the user's voice during virtual therapy sessions.",
                    "tool_description": "Analyze audio recordings of speech to identify and classify emotions such as anger, disgust, fear, happiness, and sadness using a pretrained speech emotion recognition model.",
                    "expected_response": "A set of emotion labels with associated confidence scores for each detected speech segment in the audio."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The audio recording of the user's voice in a digital format suitable for processing by a speech emotion recognition model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_97",
        "query": "I am organizing a virtual party and want to create a short, autogenerated video based on a text description (e.g., \"cats playing with laser pointer\"). Can you accomplish this with a text-to-video generation API?",
        "instruction": "Given a `text-to-video generation` task, retrieve tools that transform descriptive text inputs into autogenerated videos by processing text prompts and generating video content that aligns with the input description and query requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-video-synthesis",
                    "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')",
                    "api_arguments": [
                        "prompt",
                        "num_inference_steps",
                        "num_frames"
                    ],
                    "python_environment_requirements": [
                        "pip install git+https://github.com/huggingface/diffusers transformers accelerate"
                    ],
                    "example_code": "pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)",
                    "performance": {
                        "dataset": "Webvid",
                        "accuracy": "Not specified"
                    },
                    "description": "A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.",
                    "id": "huggingface_api_95",
                    "name": "damo-vilab/text-to-video-ms-1.7b"
                },
                "id": "gorilla_huggingface_tool_90",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 2: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 3: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 4: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 6: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 7: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 8: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 9: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 10: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 11: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 14: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 15: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 16: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 17: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 18: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.",
            "Tool 19: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content."
        ],
        "tool_view": {
            "description": "A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A natural language text description that specifies the content and actions to appear in the generated video.",
                    "required": true,
                    "example": "A superhero flying over a city at sunset"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.",
                    "required": true,
                    "example": 25
                },
                "num_frames": {
                    "type": "number",
                    "description": "The total number of video frames to generate for the output video.",
                    "required": true,
                    "example": 16
                }
            },
            "response": "The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\n\n### Expected Response Structure:\n\n1. **Video Frames**:\n   - **Type**: Array/Sequence of Image Frames\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\n\n2. **Video File (Optional)**:\n   - **Type**: Path or URL\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\n   - **Meaning**: This component encapsulates the entire video in a playable format.\n\n3. **Inference Details (Optional)**:\n   - **Type**: Metadata (optional)\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\n     - `num_frames`: Total number of frames in the generated video.\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\n\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.",
            "examples": [
                {
                    "user_intent": "I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A dog playing in the park",
                            "num_inference_steps": 30,
                            "num_frames": 60
                        }
                    }
                },
                {
                    "user_intent": "Can you make a video from the description 'A futuristic city at night with flying cars'?",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A futuristic city at night with flying cars",
                            "num_inference_steps": 40,
                            "num_frames": 90
                        }
                    }
                },
                {
                    "user_intent": "I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b",
                        "arguments": {
                            "prompt": "A cat jumping over a fence",
                            "num_inference_steps": 15,
                            "num_frames": 30
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 2: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 3: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 4: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 6: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 7: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 8: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 9: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 10: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 11: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 12: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 14: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 15: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 16: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 17: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 18: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.\nTool 19: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A natural language text description that specifies the content and actions to appear in the generated video.\", \"required\": true, \"example\": \"A superhero flying over a city at sunset\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of diffusion steps to run during video generation. Higher values typically improve quality but increase computation time.\", \"required\": true, \"example\": 25}, \"num_frames\": {\"type\": \"number\", \"description\": \"The total number of video frames to generate for the output video.\", \"required\": true, \"example\": 16}}, \"response\": \"The tool performs text-to-video synthesis using a multimodal diffusion model, where the response is expected to be a video generated from the provided textual prompt.\\n\\n### Expected Response Structure:\\n\\n1. **Video Frames**:\\n   - **Type**: Array/Sequence of Image Frames\\n   - **Description**: The primary output consists of a sequence of video frames that correspond to the given text prompt. These frames represent the visual content generated based on the input description.\\n   - **Meaning**: Each frame is a snapshot of the video at a specific moment in time, forming part of the overall video when played sequentially.\\n\\n2. **Video File (Optional)**:\\n   - **Type**: Path or URL\\n   - **Description**: A reference to the final video file generated from the sequence of frames. This could be a local path or an online URL where the video can be accessed.\\n   - **Meaning**: This component encapsulates the entire video in a playable format.\\n\\n3. **Inference Details (Optional)**:\\n   - **Type**: Metadata (optional)\\n   - **Description**: Information regarding the number of inference steps or frames used to generate the video. This can include:\\n     - `num_inference_steps`: Number of inference steps taken during the model's process.\\n     - `num_frames`: Total number of frames in the generated video.\\n   - **Meaning**: Provides insights into the model's processing parameters and can be useful for debugging or understanding the quality/complexity of the generated video.\\n\\nIn summary, the response contains primarily the frames of the video and may include additional metadata about the inference process and a reference to the final video file if applicable.\", \"examples\": [{\"user_intent\": \"I want to create a video based on the text 'A dog playing in the park'. Can you generate that for me?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A dog playing in the park\", \"num_inference_steps\": 30, \"num_frames\": 60}}}, {\"user_intent\": \"Can you make a video from the description 'A futuristic city at night with flying cars'?\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A futuristic city at night with flying cars\", \"num_inference_steps\": 40, \"num_frames\": 90}}}, {\"user_intent\": \"I need a short video based on the phrase 'A cat jumping over a fence' with minimal steps.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b\", \"arguments\": {\"prompt\": \"A cat jumping over a fence\", \"num_inference_steps\": 15, \"num_frames\": 30}}}]}\n\n# User Query\nI am organizing a virtual party and want to create a short, autogenerated video based on a text description (e.g., \"cats playing with laser pointer\"). Can you accomplish this with a text-to-video generation API?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a video based on a text description for a virtual party.",
                    "tool_description": "Create a video from a textual description using a text-to-video generation model, generating a sequence of video frames that visually correspond to the provided description.",
                    "expected_response": "A collection of video frames or a video file representing the visual content based on the provided text description."
                }
            ],
            "extracted_arguments": {
                "prompt": "A natural language description specifying the visual content to appear in the video.",
                "num_inference_steps": "The number of steps the model takes to generate the video, determining the video quality and computational complexity.",
                "num_frames": "The total number of video frames to generate, affecting the length and detail of the final video."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_208",
        "query": "I want a language model that can fill in short blanks for example sentences, quizzes, or trivia questions.",
        "instruction": "Given a `fill-in-the-blank` task, retrieve tools that process textual inputs to generate or suggest suitable words or phrases to complete blank spaces in example sentences, quizzes, or trivia questions aligned with the query's content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaModel.from_pretrained('microsoft/deberta-base')",
                    "api_arguments": "text",
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\nfill_mask('The capital of France is [MASK].')",
                    "performance": {
                        "dataset": {
                            "SQuAD 1.1": "93.1/87.2",
                            "SQuAD 2.0": "86.2/83.1",
                            "MNLI-m": "88.8"
                        }
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.",
                    "id": "huggingface_api_668",
                    "name": "microsoft/deberta-base"
                },
                "id": "gorilla_huggingface_tool_663",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 1: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 3: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 4: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 5: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 6: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 7: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 8: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.",
            "Tool 9: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 10: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 11: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.",
            "Tool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 13: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 14: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 15: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 16: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 17: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 19: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.",
                    "required": true,
                    "example": "Hello I'm a <mask> model."
                }
            },
            "response": "The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\n\n1. **`sequence`** – The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \n\n2. **`score`** – A numerical value representing the model’s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \n\n3. **`token`** – The integer ID of the predicted token in the model’s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \n\n4. **`token_str`** – The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \n\nThe response is typically ordered by descending confidence, so the first entry is the model’s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \n\nThis response allows users to understand the model’s likely substitutions for a masked word in context, along with a measure of confidence for each option.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing word in this sentence: 'The weather today is <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "The weather today is <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "Artificial intelligence is transforming <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "I can't wait to eat some <mask> tonight!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 1: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 3: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 4: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 5: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 6: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 7: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 8: A minimal, untrained LayoutLM model designed for document-based question answering, intended as an example to demonstrate the LayoutLM architecture.\nTool 9: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 10: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 11: Bio_ClinicalBERT is a Transformer-based model built on BioBERT and trained on MIMIC clinical notes, designed for natural language processing tasks in the clinical domain, including fill-mask predictions, named entity recognition, and natural language inference.\nTool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 13: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 14: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 15: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 16: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 17: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 19: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.\", \"required\": true, \"example\": \"Hello I'm a <mask> model.\"}}, \"response\": \"The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\\n\\n1. **`sequence`** \\u2013 The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \\n\\n3. **`token`** \\u2013 The integer ID of the predicted token in the model\\u2019s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \\n\\n4. **`token_str`** \\u2013 The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \\n\\nThe response is typically ordered by descending confidence, so the first entry is the model\\u2019s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \\n\\nThis response allows users to understand the model\\u2019s likely substitutions for a masked word in context, along with a measure of confidence for each option.\", \"examples\": [{\"user_intent\": \"I want to predict the missing word in this sentence: 'The weather today is <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"The weather today is <mask>.\"}}}, {\"user_intent\": \"Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"Artificial intelligence is transforming <mask>.\"}}}, {\"user_intent\": \"Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"I can't wait to eat some <mask> tonight!\"}}}]}\n\n# User Query\nI want a language model that can fill in short blanks for example sentences, quizzes, or trivia questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to use a language model to predict missing words in sentences for quizzes or trivia.",
                    "tool_description": "Use a masked language model to predict missing words in input sentences, where the missing word is denoted by a special token (<mask>). This can be applied to generate responses for quizzes or trivia questions that require text completion.",
                    "expected_response": "A list of predicted words or phrases that can fill the blank, along with a confidence score for each prediction."
                }
            ],
            "extracted_arguments": {
                "text": "A sentence containing a masked word, denoted by <mask>, that needs to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_334",
        "query": "I want to write a story about a brave knight and a dragon but I'm unable to come up with a good start. Help me with that.",
        "instruction": "Given a `story generation` task, retrieve tools that facilitate text generation by using models capable of expanding a narrative or providing creative story prompts based on an initial idea or theme.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Text Generation",
                    "api_call": "TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')",
                    "api_arguments": {
                        "model": "sshleifer/tiny-gpt2"
                    },
                    "python_environment_requirements": {
                        "huggingface_transformers": ">=4.0.0"
                    },
                    "example_code": "from transformers import pipeline\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\nresult = nlp('Once upon a time')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.",
                    "id": "huggingface_api_622",
                    "name": "sshleifer/tiny-gpt2"
                },
                "id": "gorilla_huggingface_tool_617",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 2: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 4: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 5: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 6: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 7: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 8: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.",
            "Tool 9: This tool uses score-based generative modeling with stochastic differential equations (SDE) to perform unconditional image generation, producing high-fidelity images up to 1024×1024. Implemented with Hugging Face Transformers, it leverages the `google/ncsnpp-church-256` model and achieves state-of-the-art performance on CIFAR-10, making it suitable for generating diverse and realistic images without conditional inputs.",
            "Tool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 11: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.",
            "Tool 12: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 13: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 14: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 15: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 16: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 17: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words."
        ],
        "tool_view": {
            "description": "GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will continue or generate text from.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "example": 50
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of independent text sequences to generate for the given input prompt.",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\n\nHere’s a description of the response structure:\n\n1. **Generated Texts**: \n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \n   - Each sequence will represent a continuation or completion of the input text (e.g., \"Hello, I'm a language model,\").\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\n\n2. **Content of Each Text Generation**: \n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\n\n### Key Components:\n\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\n   - Each element in the array is a string, representing a complete generated text.\n\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\n\n### Example of Expected Output Content (Abstract):\n\n- A list of 5 strings, each one being a text sequence that continues from the input (\"Hello, I'm a language model,\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\n\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a distant galaxy,'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "Once upon a time in a distant galaxy,",
                            "max_length": 50,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Continue this sentence realistically: 'The stock market crashed yesterday because'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "The stock market crashed yesterday because",
                            "max_length": 40,
                            "num_return_sequences": 2
                        }
                    }
                },
                {
                    "user_intent": "Write multiple humorous completions for: 'I never expected my cat to'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "I never expected my cat to",
                            "max_length": 30,
                            "num_return_sequences": 5
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 2: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 3: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 4: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 5: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 6: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 7: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 8: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\nTool 9: This tool uses score-based generative modeling with stochastic differential equations (SDE) to perform unconditional image generation, producing high-fidelity images up to 1024×1024. Implemented with Hugging Face Transformers, it leverages the `google/ncsnpp-church-256` model and achieves state-of-the-art performance on CIFAR-10, making it suitable for generating diverse and realistic images without conditional inputs.\nTool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 11: The \"google/bigbird-pegasus-large-bigpatent\" model is a transformer-based tool optimized for summarizing long documents. Using a sparse-attention mechanism, it can process sequences up to 4096 tokens with lower computational costs compared to traditional models like BERT. This model is specifically designed for tasks such as long document summarization and question-answering with lengthy contexts, making it ideal for applications requiring the processing of large amounts of text.\nTool 12: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 13: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 14: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 15: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 16: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 17: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will continue or generate text from.\", \"required\": true, \"example\": \"Once upon a time\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"example\": 50}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of independent text sequences to generate for the given input prompt.\", \"required\": true, \"example\": 3}}, \"response\": \"The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\\n\\nHere\\u2019s a description of the response structure:\\n\\n1. **Generated Texts**: \\n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \\n   - Each sequence will represent a continuation or completion of the input text (e.g., \\\"Hello, I'm a language model,\\\").\\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\\n\\n2. **Content of Each Text Generation**: \\n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\\n\\n### Key Components:\\n\\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \\n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\\n   - Each element in the array is a string, representing a complete generated text.\\n\\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\\n\\n### Example of Expected Output Content (Abstract):\\n\\n- A list of 5 strings, each one being a text sequence that continues from the input (\\\"Hello, I'm a language model,\\\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\\n\\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a distant galaxy,'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"Once upon a time in a distant galaxy,\", \"max_length\": 50, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Continue this sentence realistically: 'The stock market crashed yesterday because'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"The stock market crashed yesterday because\", \"max_length\": 40, \"num_return_sequences\": 2}}}, {\"user_intent\": \"Write multiple humorous completions for: 'I never expected my cat to'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"I never expected my cat to\", \"max_length\": 30, \"num_return_sequences\": 5}}}]}\n\n# User Query\nI want to write a story about a brave knight and a dragon but I'm unable to come up with a good start. Help me with that.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a creative story starting with a prompt about a brave knight and a dragon.",
                    "tool_description": "Use the GPT-2 Large model to generate a continuation or beginning of a story based on a given prompt, focusing on creative and narrative text generation.",
                    "expected_response": "A set of creative story beginnings or continuations, focusing on a brave knight and a dragon."
                }
            ],
            "extracted_arguments": {
                "text": "The initial story prompt or starting point for the creative text generation.",
                "max_length": "The maximum number of tokens for the generated story beginning.",
                "num_return_sequences": "The number of different story beginnings to generate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_124",
        "query": "Help us create an AI solution to automatically label images taken by a security camera.",
        "instruction": "Given an `image labeling` task, retrieve tools that utilize computer vision techniques to automatically classify images by processing visual inputs and assigning appropriate labels based on the content detected within the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "zuppif/regnet-y-040"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor, RegNetForImageClassification",
                        "torch": "torch",
                        "datasets": "load_dataset"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.",
                    "id": "huggingface_api_199",
                    "name": "facebook/regnet-y-008"
                },
                "id": "gorilla_huggingface_tool_195",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 2: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 3: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 4: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 8: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 10: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 12: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 15: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 16: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 17: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 19: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart."
        ],
        "tool_view": {
            "description": "Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that specifies the content, style, and details of the image to be generated by the model.",
                    "required": true,
                    "example": "A futuristic cityscape at sunset with flying cars, in a cyberpunk art style"
                }
            },
            "response": "The tool is designed to generate images from text prompts using the \"StableDiffusionPipeline\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \n\nThe expected response format includes the following components:\n\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\n\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\n\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\n\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.",
            "examples": [
                {
                    "user_intent": "Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "futuristic city skyline at sunset, retro 80s art style"
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a medieval knight in full armor, riding a horse.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "medieval knight in full armor, riding a horse"
                        }
                    }
                },
                {
                    "user_intent": "I want a retro-style image of different cars with various colors and shapes.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "retro series of different cars with different colors and shapes, mdjrny-v4 style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 2: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 3: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 4: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 8: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 9: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 10: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 12: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 13: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 15: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 16: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 17: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 19: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that specifies the content, style, and details of the image to be generated by the model.\", \"required\": true, \"example\": \"A futuristic cityscape at sunset with flying cars, in a cyberpunk art style\"}}, \"response\": \"The tool is designed to generate images from text prompts using the \\\"StableDiffusionPipeline\\\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \\n\\nThe expected response format includes the following components:\\n\\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\\n\\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\\n\\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\\n\\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.\", \"examples\": [{\"user_intent\": \"Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"futuristic city skyline at sunset, retro 80s art style\"}}}, {\"user_intent\": \"Create an illustration of a medieval knight in full armor, riding a horse.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"medieval knight in full armor, riding a horse\"}}}, {\"user_intent\": \"I want a retro-style image of different cars with various colors and shapes.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"retro series of different cars with different colors and shapes, mdjrny-v4 style\"}}}]}\n\n# User Query\nHelp us create an AI solution to automatically label images taken by a security camera.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to automatically detect and classify objects in security camera images.",
                    "tool_description": "Perform zero-shot image classification using a model trained on a large dataset, classifying images into arbitrary categories without task-specific training.",
                    "expected_response": "A list of labels or categories that describe the objects detected in the security camera image."
                }
            ],
            "extracted_arguments": {
                "image_data": "The security camera image to be analyzed for object classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_242",
        "query": "Our company needs a versatile NLP model to build a social media manager to generate summaries of lengthy articles for sharing on social media.",
        "instruction": "Given a `NLP summarization` task, retrieve tools that utilize advanced natural language processing techniques to generate concise summaries from lengthy articles, suitable for social media sharing, by processing text inputs and leveraging model capabilities designed for multi-purpose NLP tasks such as summarization, translation, or sentiment analysis.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis, Regression",
                    "api_call": "T5Model.from_pretrained('t5-large')",
                    "api_arguments": {
                        "input_ids": "tokenizer(..., return_tensors='pt').input_ids",
                        "decoder_input_ids": "tokenizer(..., return_tensors='pt').input_ids"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import T5Tokenizer, T5Model"
                    },
                    "example_code": "tokenizer = T5Tokenizer.from_pretrained('t5-large')\nmodel = T5Model.from_pretrained('t5-large')\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.",
                    "id": "huggingface_api_526",
                    "name": "t5-large"
                },
                "id": "gorilla_huggingface_tool_522",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 1: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 2: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 4: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 5: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 7: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 8: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 9: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 10: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 11: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 12: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 13: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 15: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.",
            "Tool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 18: T5-Small is a versatile Text-to-Text Transfer Transformer (T5) model capable of handling a wide range of NLP tasks, including translation, summarization, question answering, and text classification or regression. Pre-trained on the Colossal Clean Crawled Corpus (C4), it can process input text and generate corresponding outputs, making it suitable for both general-purpose language understanding and task-specific fine-tuning.",
            "Tool 19: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset."
        ],
        "tool_view": {
            "description": "Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input Chinese text that you want the model to summarize. This should be a complete text string.",
                    "required": true,
                    "example": "今天天气很好，我们去公园散步吧。"
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum number of tokens to generate in the summary. This limits the length of the output summary.",
                    "required": true,
                    "example": 150
                }
            },
            "response": "The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\n\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\n\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\"summary_text\"`) pointing to the summarized string. The content of the summary reflects the model’s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \n\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.",
            "examples": [
                {
                    "user_intent": "我有一篇关于人工智能发展的中文文章，想生成一段简短摘要。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "人工智能近年来取得了显著的发展，从自然语言处理到计算机视觉，各个领域都有突破性进展。技术的进步推动了产业升级，同时也带来了伦理和隐私等挑战。",
                            "max_length": 60
                        }
                    }
                },
                {
                    "user_intent": "需要对一条微博内容生成中文摘要，方便快速浏览。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "今天在上海举办的科技大会上，多家AI公司展示了最新的智能产品，包括自动驾驶、语音助手和智能家居设备，引发了广泛关注。",
                            "max_length": 40
                        }
                    }
                },
                {
                    "user_intent": "我有一篇教育领域的长篇文章，希望生成概括性摘要用于报告。",
                    "tool_call": {
                        "name": "Randeng-Pegasus-238M-Summary-Chinese",
                        "arguments": {
                            "text": "近年来，中国教育领域进行了多项改革，尤其在中小学课程和教学方法上有所调整。在线教育和智能教育工具的应用提高了学习效率，同时也引发了教育公平性的讨论。",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 1: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 2: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 4: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 5: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 6: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 7: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 8: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 9: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 10: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 11: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 12: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 13: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 15: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\nTool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 18: T5-Small is a versatile Text-to-Text Transfer Transformer (T5) model capable of handling a wide range of NLP tasks, including translation, summarization, question answering, and text classification or regression. Pre-trained on the Colossal Clean Crawled Corpus (C4), it can process input text and generate corresponding outputs, making it suitable for both general-purpose language understanding and task-specific fine-tuning.\nTool 19: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input Chinese text that you want the model to summarize. This should be a complete text string.\", \"required\": true, \"example\": \"\\u4eca\\u5929\\u5929\\u6c14\\u5f88\\u597d\\uff0c\\u6211\\u4eec\\u53bb\\u516c\\u56ed\\u6563\\u6b65\\u5427\\u3002\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum number of tokens to generate in the summary. This limits the length of the output summary.\", \"required\": true, \"example\": 150}}, \"response\": \"The tool produces a text summarization output in the form of a single string representing the condensed version of the input Chinese text. The typical structure and components of the response are as follows:\\n\\n- **summary_text**: A string containing the generated summary of the input text. This summary is produced by the Pegasus model and is designed to capture the most salient points of the original text while reducing its length. It is expected to be coherent, grammatically correct, and contextually faithful to the input.\\n\\nThe response may be returned directly as a string or, in some implementations, as an object or dictionary with a key (e.g., `\\\"summary_text\\\"`) pointing to the summarized string. The content of the summary reflects the model\\u2019s understanding of the main ideas in the input and is constrained by parameters such as `max_length`. \\n\\nNo additional metadata, scores, or token-level details are included in the basic response unless explicitly requested or wrapped by additional code.\", \"examples\": [{\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u5173\\u4e8e\\u4eba\\u5de5\\u667a\\u80fd\\u53d1\\u5c55\\u7684\\u4e2d\\u6587\\u6587\\u7ae0\\uff0c\\u60f3\\u751f\\u6210\\u4e00\\u6bb5\\u7b80\\u77ed\\u6458\\u8981\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eba\\u5de5\\u667a\\u80fd\\u8fd1\\u5e74\\u6765\\u53d6\\u5f97\\u4e86\\u663e\\u8457\\u7684\\u53d1\\u5c55\\uff0c\\u4ece\\u81ea\\u7136\\u8bed\\u8a00\\u5904\\u7406\\u5230\\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\uff0c\\u5404\\u4e2a\\u9886\\u57df\\u90fd\\u6709\\u7a81\\u7834\\u6027\\u8fdb\\u5c55\\u3002\\u6280\\u672f\\u7684\\u8fdb\\u6b65\\u63a8\\u52a8\\u4e86\\u4ea7\\u4e1a\\u5347\\u7ea7\\uff0c\\u540c\\u65f6\\u4e5f\\u5e26\\u6765\\u4e86\\u4f26\\u7406\\u548c\\u9690\\u79c1\\u7b49\\u6311\\u6218\\u3002\", \"max_length\": 60}}}, {\"user_intent\": \"\\u9700\\u8981\\u5bf9\\u4e00\\u6761\\u5fae\\u535a\\u5185\\u5bb9\\u751f\\u6210\\u4e2d\\u6587\\u6458\\u8981\\uff0c\\u65b9\\u4fbf\\u5feb\\u901f\\u6d4f\\u89c8\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u4eca\\u5929\\u5728\\u4e0a\\u6d77\\u4e3e\\u529e\\u7684\\u79d1\\u6280\\u5927\\u4f1a\\u4e0a\\uff0c\\u591a\\u5bb6AI\\u516c\\u53f8\\u5c55\\u793a\\u4e86\\u6700\\u65b0\\u7684\\u667a\\u80fd\\u4ea7\\u54c1\\uff0c\\u5305\\u62ec\\u81ea\\u52a8\\u9a7e\\u9a76\\u3001\\u8bed\\u97f3\\u52a9\\u624b\\u548c\\u667a\\u80fd\\u5bb6\\u5c45\\u8bbe\\u5907\\uff0c\\u5f15\\u53d1\\u4e86\\u5e7f\\u6cdb\\u5173\\u6ce8\\u3002\", \"max_length\": 40}}}, {\"user_intent\": \"\\u6211\\u6709\\u4e00\\u7bc7\\u6559\\u80b2\\u9886\\u57df\\u7684\\u957f\\u7bc7\\u6587\\u7ae0\\uff0c\\u5e0c\\u671b\\u751f\\u6210\\u6982\\u62ec\\u6027\\u6458\\u8981\\u7528\\u4e8e\\u62a5\\u544a\\u3002\", \"tool_call\": {\"name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"arguments\": {\"text\": \"\\u8fd1\\u5e74\\u6765\\uff0c\\u4e2d\\u56fd\\u6559\\u80b2\\u9886\\u57df\\u8fdb\\u884c\\u4e86\\u591a\\u9879\\u6539\\u9769\\uff0c\\u5c24\\u5176\\u5728\\u4e2d\\u5c0f\\u5b66\\u8bfe\\u7a0b\\u548c\\u6559\\u5b66\\u65b9\\u6cd5\\u4e0a\\u6709\\u6240\\u8c03\\u6574\\u3002\\u5728\\u7ebf\\u6559\\u80b2\\u548c\\u667a\\u80fd\\u6559\\u80b2\\u5de5\\u5177\\u7684\\u5e94\\u7528\\u63d0\\u9ad8\\u4e86\\u5b66\\u4e60\\u6548\\u7387\\uff0c\\u540c\\u65f6\\u4e5f\\u5f15\\u53d1\\u4e86\\u6559\\u80b2\\u516c\\u5e73\\u6027\\u7684\\u8ba8\\u8bba\\u3002\", \"max_length\": 80}}}]}\n\n# User Query\nOur company needs a versatile NLP model to build a social media manager to generate summaries of lengthy articles for sharing on social media.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate concise summaries of long articles for social media sharing.",
                    "tool_description": "Use a text-to-text NLP model fine-tuned for abstractive summarization to condense lengthy articles into brief, coherent summaries suitable for social media posts.",
                    "expected_response": "A text string representing the summarized version of the input article, capturing key points while reducing length."
                }
            ],
            "extracted_arguments": {
                "text": "The full content of the article to be summarized, provided as a complete text string.",
                "max_length": "The maximum number of tokens or words for the generated summary, defining its length."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_295",
        "query": "The marketing department wants to use AI-generated images for their next social media campaign. They want a high-resolution image of a vintage sports car racing through a desert landscape during sunset.",
        "instruction": "Given an `AI-generated image creation` task, retrieve tools that produce high-resolution images from text-based descriptions, specifically focusing on generating visual art such as a vintage sports car in a desert landscape during sunset, by processing detailed prompts to achieve desired visual attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('prompthero/openjourney')",
                    "api_arguments": {
                        "prompt": "string"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "torch"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = prompthero/openjourney\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\nimage = pipe(prompt).images[0]\nimage.save(./retro_cars.png)",
                    "performance": {
                        "dataset": "Midjourney images",
                        "accuracy": "Not specified"
                    },
                    "description": "Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.",
                    "id": "huggingface_api_31",
                    "name": "prompthero/openjourney"
                },
                "id": "gorilla_huggingface_tool_31",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 1: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 2: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 3: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 4: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 5: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 6: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 7: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 8: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 9: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 10: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 11: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.",
            "Tool 12: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 14: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 15: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 16: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 17: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 18: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.",
            "Tool 19: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language."
        ],
        "tool_view": {
            "description": "The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that guides the upscaling process, typically describing the scene or subject matter to generate at a higher resolution.",
                    "required": true,
                    "example": "a photo of an astronaut high resolution, unreal engine, ultra realistic"
                },
                "image": {
                    "type": "array",
                    "description": "The low-resolution latents or image embeddings to be upscaled, typically generated from a previous diffusion model.",
                    "required": true,
                    "example": "low_res_latents"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps to be used during the upscaling process. Higher values generally result in better quality but take more time.",
                    "required": true,
                    "default": 20,
                    "example": 20
                },
                "guidance_scale": {
                    "type": "number",
                    "description": "A scale factor to control the strength of guidance during the upscaling. Higher values enforce more adherence to the prompt, while lower values make the model more flexible.",
                    "required": true,
                    "default": 0,
                    "example": 0
                },
                "generator": {
                    "type": "object",
                    "description": "A random generator object used for controlling the randomness of the image generation process. Typically, a torch generator for reproducibility.",
                    "required": true,
                    "example": "torch.manual_seed(33)"
                }
            },
            "response": "The tool's response describes the upscaled image generated by the **Stable Diffusion x2 latent upscaler**. Specifically, the response would include the following elements:\n\n1. **Upscaled Image**: The primary output is the **upscaled image**, which is the result of applying the upscaling model to a given low-resolution latent image. This image is generated based on the **text prompt** and the **low-resolution image** (in the form of latent space representation). The upscaled image typically has improved resolution and clarity compared to the input image.\n\n2. **Image Representation**: The upscaled image will be provided in a form that is suitable for saving or displaying, such as a **file or image object**. This image will be of higher resolution than the input and should visually reflect the details described in the **text prompt**.\n\n3. **Inference Settings**: The response may also implicitly reflect the settings used in the upscaling process, such as the **number of inference steps (num_inference_steps)** and the **guidance scale**, though these parameters typically influence the generation quality rather than being directly visible in the response itself.\n\nIn summary, the response contains the **upscaled image**, which is the result of the model processing the provided **low-resolution latents** using the given **text prompt** and upscaling parameters. This image will have enhanced resolution, with potential improvements in detail and clarity.",
            "examples": [
                {
                    "user_intent": "I want to upscale a low-resolution sketch of a futuristic city to high resolution for my concept art portfolio.",
                    "tool_call": {
                        "name": "StableDiffusionLatentUpscalePipeline.from_pretrained",
                        "arguments": {
                            "prompt": "futuristic cityscape, detailed concept art, ultra realistic",
                            "image": "low_res_city_latents",
                            "num_inference_steps": 25,
                            "guidance_scale": 7.5,
                            "generator": "torch.manual_seed(42)"
                        }
                    }
                },
                {
                    "user_intent": "Enhance a small anime-style character illustration to full HD without losing detail.",
                    "tool_call": {
                        "name": "StableDiffusionLatentUpscalePipeline.from_pretrained",
                        "arguments": {
                            "prompt": "anime character, vibrant colors, full body, high detail",
                            "image": "low_res_anime_latents",
                            "num_inference_steps": 30,
                            "guidance_scale": 8.0,
                            "generator": "torch.manual_seed(101)"
                        }
                    }
                },
                {
                    "user_intent": "Upscale a realistic nature photograph from my AI-generated low-res output for printing.",
                    "tool_call": {
                        "name": "StableDiffusionLatentUpscalePipeline.from_pretrained",
                        "arguments": {
                            "prompt": "high resolution forest landscape, morning light, ultra realistic",
                            "image": "low_res_forest_latents",
                            "num_inference_steps": 20,
                            "guidance_scale": 6.5,
                            "generator": "torch.manual_seed(2025)"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 1: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 2: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 3: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 4: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 5: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 6: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 7: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 8: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 9: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 10: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 11: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.\nTool 12: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 14: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 15: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 16: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 17: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 18: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.\nTool 19: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that guides the upscaling process, typically describing the scene or subject matter to generate at a higher resolution.\", \"required\": true, \"example\": \"a photo of an astronaut high resolution, unreal engine, ultra realistic\"}, \"image\": {\"type\": \"array\", \"description\": \"The low-resolution latents or image embeddings to be upscaled, typically generated from a previous diffusion model.\", \"required\": true, \"example\": \"low_res_latents\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps to be used during the upscaling process. Higher values generally result in better quality but take more time.\", \"required\": true, \"default\": 20, \"example\": 20}, \"guidance_scale\": {\"type\": \"number\", \"description\": \"A scale factor to control the strength of guidance during the upscaling. Higher values enforce more adherence to the prompt, while lower values make the model more flexible.\", \"required\": true, \"default\": 0, \"example\": 0}, \"generator\": {\"type\": \"object\", \"description\": \"A random generator object used for controlling the randomness of the image generation process. Typically, a torch generator for reproducibility.\", \"required\": true, \"example\": \"torch.manual_seed(33)\"}}, \"response\": \"The tool's response describes the upscaled image generated by the **Stable Diffusion x2 latent upscaler**. Specifically, the response would include the following elements:\\n\\n1. **Upscaled Image**: The primary output is the **upscaled image**, which is the result of applying the upscaling model to a given low-resolution latent image. This image is generated based on the **text prompt** and the **low-resolution image** (in the form of latent space representation). The upscaled image typically has improved resolution and clarity compared to the input image.\\n\\n2. **Image Representation**: The upscaled image will be provided in a form that is suitable for saving or displaying, such as a **file or image object**. This image will be of higher resolution than the input and should visually reflect the details described in the **text prompt**.\\n\\n3. **Inference Settings**: The response may also implicitly reflect the settings used in the upscaling process, such as the **number of inference steps (num_inference_steps)** and the **guidance scale**, though these parameters typically influence the generation quality rather than being directly visible in the response itself.\\n\\nIn summary, the response contains the **upscaled image**, which is the result of the model processing the provided **low-resolution latents** using the given **text prompt** and upscaling parameters. This image will have enhanced resolution, with potential improvements in detail and clarity.\", \"examples\": [{\"user_intent\": \"I want to upscale a low-resolution sketch of a futuristic city to high resolution for my concept art portfolio.\", \"tool_call\": {\"name\": \"StableDiffusionLatentUpscalePipeline.from_pretrained\", \"arguments\": {\"prompt\": \"futuristic cityscape, detailed concept art, ultra realistic\", \"image\": \"low_res_city_latents\", \"num_inference_steps\": 25, \"guidance_scale\": 7.5, \"generator\": \"torch.manual_seed(42)\"}}}, {\"user_intent\": \"Enhance a small anime-style character illustration to full HD without losing detail.\", \"tool_call\": {\"name\": \"StableDiffusionLatentUpscalePipeline.from_pretrained\", \"arguments\": {\"prompt\": \"anime character, vibrant colors, full body, high detail\", \"image\": \"low_res_anime_latents\", \"num_inference_steps\": 30, \"guidance_scale\": 8.0, \"generator\": \"torch.manual_seed(101)\"}}}, {\"user_intent\": \"Upscale a realistic nature photograph from my AI-generated low-res output for printing.\", \"tool_call\": {\"name\": \"StableDiffusionLatentUpscalePipeline.from_pretrained\", \"arguments\": {\"prompt\": \"high resolution forest landscape, morning light, ultra realistic\", \"image\": \"low_res_forest_latents\", \"num_inference_steps\": 20, \"guidance_scale\": 6.5, \"generator\": \"torch.manual_seed(2025)\"}}}]}\n\n# User Query\nThe marketing department wants to use AI-generated images for their next social media campaign. They want a high-resolution image of a vintage sports car racing through a desert landscape during sunset.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a vintage-style image of a sports car racing through a desert landscape at sunset.",
                    "tool_description": "Use a fine-tuned vintage-style diffusion model to generate an image based on a textual prompt describing the desired scene.",
                    "expected_response": "A high-quality, AI-generated image depicting the scene described in the prompt."
                },
                {
                    "user_intent": "Enhance the resolution of the generated vintage sports car image for social media use.",
                    "tool_description": "Apply a latent diffusion-based upscaler to increase the resolution of the generated image, producing a detailed high-resolution output suitable for social media campaigns.",
                    "expected_response": "An upscaled high-resolution image that retains the details and style of the original generated image."
                }
            ],
            "extracted_arguments": {
                "prompt_generation": "A textual description of the scene to generate, including object (sports car), style (vintage), environment (desert), and lighting conditions (sunset).",
                "prompt_upscale": "The same textual description used to guide the upscaling process, ensuring the upscaled image matches the intended scene.",
                "low_res_image": "The initially generated vintage-style image in low resolution, to be upscaled.",
                "num_inference_steps": "Numeric value controlling the number of inference steps during upscaling for quality enhancement.",
                "guidance_scale": "Numeric factor controlling adherence of the upscaled image to the textual prompt.",
                "generator": "Random generator object to ensure reproducibility of the upscaled image output."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_292",
        "query": "Our company specializes in providing information on similar topics. We want to find similar sentences in a text document.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that apply natural language processing techniques, particularly sentence transformers, to analyze and find similarities between sentences in a text document.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "1,170,060,424 training pairs",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_691",
                    "name": "sentence-transformers/all-MiniLM-L12-v2"
                },
                "id": "gorilla_huggingface_tool_686",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 1: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 2: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 4: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 6: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 7: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 11: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 12: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 13: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 14: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 18: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 19: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content."
        ],
        "tool_view": {
            "description": "This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text that will be classified by the emotion classification model. The model predicts the emotion expressed in the text based on Ekman's 6 basic emotions and a neutral class.",
                    "required": true,
                    "example": "I love this!"
                }
            },
            "response": "The tool returns a structured list of emotion predictions for a given input text. Each element in the response corresponds to one possible emotion label and includes the following components:\n\n1. **`label`** – A string representing the predicted emotion class. Valid labels include the six basic Ekman emotions plus neutral: `anger`, `disgust`, `fear`, `joy`, `neutral`, `sadness`, and `surprise`.\n\n2. **`score`** – A numerical value representing the model’s confidence or probability that the input text expresses the corresponding emotion. The score is typically a floating-point number between 0 and 1.\n\nBecause the tool is configured with `return_all_scores=True`, the response includes **all emotion labels** for the input text, each paired with its corresponding confidence score, rather than only the single highest-probability label. The overall structure is a list of dictionaries, with one dictionary per emotion. This allows downstream applications to interpret the full distribution of emotional predictions for the text.",
            "examples": [
                {
                    "user_intent": "I want to analyze the emotions in this tweet: 'Just got a promotion at work, feeling on top of the world!'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "Just got a promotion at work, feeling on top of the world!"
                        }
                    }
                },
                {
                    "user_intent": "Classify the emotion expressed in this customer review: 'The product stopped working after a week, really disappointed.'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "The product stopped working after a week, really disappointed."
                        }
                    }
                },
                {
                    "user_intent": "Detect the emotional tone of this sentence: 'Wow, I can't believe this surprise party, I'm speechless!'",
                    "tool_call": {
                        "name": "j-hartmann/emotion-english-distilroberta-base",
                        "arguments": {
                            "text": "Wow, I can't believe this surprise party, I'm speechless!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 1: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 2: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 4: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 5: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 6: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 7: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 8: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 10: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 11: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 12: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 13: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 14: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 17: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 18: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 19: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text that will be classified by the emotion classification model. The model predicts the emotion expressed in the text based on Ekman's 6 basic emotions and a neutral class.\", \"required\": true, \"example\": \"I love this!\"}}, \"response\": \"The tool returns a structured list of emotion predictions for a given input text. Each element in the response corresponds to one possible emotion label and includes the following components:\\n\\n1. **`label`** \\u2013 A string representing the predicted emotion class. Valid labels include the six basic Ekman emotions plus neutral: `anger`, `disgust`, `fear`, `joy`, `neutral`, `sadness`, and `surprise`.\\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability that the input text expresses the corresponding emotion. The score is typically a floating-point number between 0 and 1.\\n\\nBecause the tool is configured with `return_all_scores=True`, the response includes **all emotion labels** for the input text, each paired with its corresponding confidence score, rather than only the single highest-probability label. The overall structure is a list of dictionaries, with one dictionary per emotion. This allows downstream applications to interpret the full distribution of emotional predictions for the text.\", \"examples\": [{\"user_intent\": \"I want to analyze the emotions in this tweet: 'Just got a promotion at work, feeling on top of the world!'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"Just got a promotion at work, feeling on top of the world!\"}}}, {\"user_intent\": \"Classify the emotion expressed in this customer review: 'The product stopped working after a week, really disappointed.'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"The product stopped working after a week, really disappointed.\"}}}, {\"user_intent\": \"Detect the emotional tone of this sentence: 'Wow, I can't believe this surprise party, I'm speechless!'\", \"tool_call\": {\"name\": \"j-hartmann/emotion-english-distilroberta-base\", \"arguments\": {\"text\": \"Wow, I can't believe this surprise party, I'm speechless!\"}}}]}\n\n# User Query\nOur company specializes in providing information on similar topics. We want to find similar sentences in a text document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify and extract sentences from a document that are similar in meaning or topic.",
                    "tool_description": "Analyze the text document to detect semantic similarities between sentences, returning pairs or groups of sentences that share related content or topics.",
                    "expected_response": "A structured list of sentence pairs or clusters, each representing sentences with high semantic similarity, optionally including similarity scores."
                }
            ],
            "extracted_arguments": {
                "document_text": "The full text content of the document in which similar sentences need to be identified, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_67",
        "query": "Write a summary of a conference held by the World Health Organization discussing the impacts of climate change on human health.",
        "instruction": "Given a `text summarization` task, retrieve tools that can process detailed textual content to produce concise and coherent summaries, reflecting the core aspects and findings of the discussed topic, using models designed for abstractive summarization.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-xsum')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": {
                                    "ROUGE-1": 46.862,
                                    "ROUGE-2": 24.453,
                                    "ROUGE-L": 39.055,
                                    "ROUGE-LSUM": 39.099
                                }
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": {
                                    "ROUGE-1": 22.206,
                                    "ROUGE-2": 7.67,
                                    "ROUGE-L": 15.405,
                                    "ROUGE-LSUM": 19.218
                                }
                            },
                            {
                                "name": "samsum",
                                "accuracy": {
                                    "ROUGE-1": 21.81,
                                    "ROUGE-2": 4.253,
                                    "ROUGE-L": 17.447,
                                    "ROUGE-LSUM": 18.891
                                }
                            }
                        ]
                    },
                    "description": "PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.",
                    "id": "huggingface_api_554",
                    "name": "google/pegasus-xsum"
                },
                "id": "gorilla_huggingface_tool_550",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.",
            "Tool 4: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 5: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 6: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 7: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 8: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 9: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 10: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 11: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 12: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 13: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 14: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 15: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 16: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 17: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 18: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.",
            "Tool 19: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "parameters": {
                "sequence": {
                    "type": "string",
                    "description": "The input text sequence that you want to classify using the zero-shot model.",
                    "required": true,
                    "example": "El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo"
                },
                "candidate_labels": {
                    "type": "array",
                    "description": "A list of labels or categories that the model will consider when classifying the input sequence.",
                    "required": true,
                    "example": [
                        "cultura",
                        "sociedad",
                        "economia",
                        "salud",
                        "deportes"
                    ]
                },
                "hypothesis_template": {
                    "type": "string",
                    "description": "A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.",
                    "required": true,
                    "example": "Este ejemplo es {}."
                }
            },
            "response": "The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\n\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\n   \n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\n\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\n\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\n\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.",
            "examples": [
                {
                    "user_intent": "I want to classify this Spanish sentence into categories: 'El gobierno anunció nuevas medidas económicas para combatir la inflación.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El gobierno anunció nuevas medidas económicas para combatir la inflación.",
                            "candidate_labels": [
                                "economia",
                                "sociedad",
                                "salud",
                                "cultura",
                                "deportes"
                            ],
                            "hypothesis_template": "Este texto trata sobre {}."
                        }
                    }
                },
                {
                    "user_intent": "Determine whether this Spanish sentence is about health, culture, or sports: 'El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "El Maratón de la Ciudad atrajo a miles de corredores de todo el mundo.",
                            "candidate_labels": [
                                "deportes",
                                "salud",
                                "cultura",
                                "economia",
                                "sociedad"
                            ],
                            "hypothesis_template": "Este texto se refiere a {}."
                        }
                    }
                },
                {
                    "user_intent": "Classify this Spanish review about a museum visit: 'La exposición de arte contemporáneo fue fascinante y educativa.'",
                    "tool_call": {
                        "name": "Recognai/bert-base-spanish-wwm-cased-xnli",
                        "arguments": {
                            "sequence": "La exposición de arte contemporáneo fue fascinante y educativa.",
                            "candidate_labels": [
                                "cultura",
                                "sociedad",
                                "economia",
                                "deportes",
                                "salud"
                            ],
                            "hypothesis_template": "Este texto es sobre {}."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 3: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\nTool 4: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 5: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 6: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 7: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 8: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 9: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 10: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 11: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 12: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 13: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 14: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 15: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 16: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 17: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 18: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.\nTool 19: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\", \"parameters\": {\"sequence\": {\"type\": \"string\", \"description\": \"The input text sequence that you want to classify using the zero-shot model.\", \"required\": true, \"example\": \"El autor se perfila, a los 50 a\\u00f1os de su muerte, como uno de los grandes de su siglo\"}, \"candidate_labels\": {\"type\": \"array\", \"description\": \"A list of labels or categories that the model will consider when classifying the input sequence.\", \"required\": true, \"example\": [\"cultura\", \"sociedad\", \"economia\", \"salud\", \"deportes\"]}, \"hypothesis_template\": {\"type\": \"string\", \"description\": \"A template string used to frame the classification hypothesis, where '{}' will be replaced with each candidate label.\", \"required\": true, \"example\": \"Este ejemplo es {}.\"}}, \"response\": \"The tool performs zero-shot classification using a pre-trained Spanish BERT model fine-tuned on the XNLI dataset. The response from this tool typically includes the following components:\\n\\n1. **Labels**: A list of candidate labels that the model has evaluated as the most relevant or likely classifications for the input text.\\n   \\n2. **Scores**: A corresponding list of numeric scores indicating the model's confidence or probability for each label. These scores typically represent the model's prediction confidence in the range [0, 1] for each label.\\n\\n3. **Sequence**: The input sequence of text that was classified. This may be included to confirm the processed input.\\n\\n4. **Chosen Label(s)**: Depending on the configuration, the model might also return the label with the highest confidence as the primary classification.\\n\\nThe response will reflect the likelihood of each candidate label in relation to the given text and hypothesis template, allowing users to interpret the predicted classification for the sequence. The exact structure of the output depends on the specific implementation but will generally include these key components.\", \"examples\": [{\"user_intent\": \"I want to classify this Spanish sentence into categories: 'El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El gobierno anunci\\u00f3 nuevas medidas econ\\u00f3micas para combatir la inflaci\\u00f3n.\", \"candidate_labels\": [\"economia\", \"sociedad\", \"salud\", \"cultura\", \"deportes\"], \"hypothesis_template\": \"Este texto trata sobre {}.\"}}}, {\"user_intent\": \"Determine whether this Spanish sentence is about health, culture, or sports: 'El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"El Marat\\u00f3n de la Ciudad atrajo a miles de corredores de todo el mundo.\", \"candidate_labels\": [\"deportes\", \"salud\", \"cultura\", \"economia\", \"sociedad\"], \"hypothesis_template\": \"Este texto se refiere a {}.\"}}}, {\"user_intent\": \"Classify this Spanish review about a museum visit: 'La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.'\", \"tool_call\": {\"name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"arguments\": {\"sequence\": \"La exposici\\u00f3n de arte contempor\\u00e1neo fue fascinante y educativa.\", \"candidate_labels\": [\"cultura\", \"sociedad\", \"economia\", \"deportes\", \"salud\"], \"hypothesis_template\": \"Este texto es sobre {}.\"}}}]}\n\n# User Query\nWrite a summary of a conference held by the World Health Organization discussing the impacts of climate change on human health.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize the content of a conference discussing climate change and human health.",
                    "tool_description": "Generate a concise and coherent summary from a text input, capturing the main points and key information discussed in the source document.",
                    "expected_response": "A summarized text that highlights the key topics, conclusions, and insights from the conference discussion."
                }
            ],
            "extracted_arguments": {
                "source_text": "The full transcript or detailed report of the conference discussing climate change impacts on human health."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_376",
        "query": "We want to build a Twitter Bot that creates an image based on users' textual requests. Generate an image with an astronaut playing guitar in space using a model.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that transform textual prompts into visual outputs by leveraging models designed to create images based on descriptive text inputs, ensuring alignment with the query's creative requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')",
                    "api_arguments": {
                        "prompt": "photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens"
                    },
                    "python_environment_requirements": {
                        "torch": "torch.float16",
                        "diffusers": "StableDiffusionPipeline"
                    },
                    "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\nimage = pipe(prompt).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "Stable Diffusion 1.5",
                        "accuracy": "Not specified"
                    },
                    "description": "Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.",
                    "id": "huggingface_api_39",
                    "name": "dreamlike-art/dreamlike-photoreal-2.0"
                },
                "id": "gorilla_huggingface_tool_39",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.",
            "Tool 1: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 2: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 4: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 5: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 6: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.",
            "Tool 7: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 8: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 9: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 12: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 13: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 14: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 15: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 16: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.",
            "Tool 17: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.",
            "Tool 18: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 19: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing."
        ],
        "tool_view": {
            "description": "Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that the model uses to generate an image. This should clearly describe the scene, objects, style, or any other details you want in the generated image.",
                    "required": true,
                    "example": "a photo of an astronaut riding a horse on Mars"
                }
            },
            "response": "The tool produces a generated image based on a given text prompt. The response structure typically includes the following components:\n\n1. **images**:  \n   - A list or collection of image objects generated by the model.  \n   - Each element represents one output image corresponding to the input prompt.  \n   - The images are usually in a standard image format (e.g., PIL Image objects in Python) and can be saved, displayed, or further processed.\n\n2. **metadata (optional in some implementations)**:  \n   - Information about the generation process, which may include:  \n     - The text prompt used to generate the image.  \n     - Model parameters such as scheduler settings, guidance scale, or number of inference steps.  \n     - Device information (CPU or GPU) used for generation.  \n     - Any warnings or notes about content safety filtering if applied.\n\nIn essence, the response provides the generated visual output corresponding to the text prompt, along with optional auxiliary information describing the conditions under which the image was created. The primary component is the image itself, which is the direct result of the text-to-image generation process.",
            "examples": [
                {
                    "user_intent": "Generate a realistic image of a futuristic city skyline at sunset.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-base",
                        "arguments": {
                            "prompt": "a futuristic city skyline at sunset, highly detailed, cinematic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Create an artistic illustration of a dragon flying over a medieval village.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-base",
                        "arguments": {
                            "prompt": "an epic illustration of a dragon flying over a medieval village, fantasy style, vibrant colors"
                        }
                    }
                },
                {
                    "user_intent": "Produce a cute digital painting of a corgi wearing a wizard hat in a magical forest.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-base",
                        "arguments": {
                            "prompt": "a cute corgi wearing a wizard hat in a magical forest, digital painting, whimsical style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.\nTool 1: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 2: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 4: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 5: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 6: Stable Diffusion is a text-to-image generation model that creates high-quality, photo-realistic images from any text prompt, allowing users to turn descriptive language into visual content.\nTool 7: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 8: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 9: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 10: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 11: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 12: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 13: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 14: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 15: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 16: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.\nTool 17: BLIP-2 OPT-6.7B is a multimodal model that generates text based on images and optional textual prompts. It can be used for tasks such as image captioning, visual question answering, and interactive image-based conversations by predicting the next text token from image embeddings and previous text input.\nTool 18: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 19: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that the model uses to generate an image. This should clearly describe the scene, objects, style, or any other details you want in the generated image.\", \"required\": true, \"example\": \"a photo of an astronaut riding a horse on Mars\"}}, \"response\": \"The tool produces a generated image based on a given text prompt. The response structure typically includes the following components:\\n\\n1. **images**:  \\n   - A list or collection of image objects generated by the model.  \\n   - Each element represents one output image corresponding to the input prompt.  \\n   - The images are usually in a standard image format (e.g., PIL Image objects in Python) and can be saved, displayed, or further processed.\\n\\n2. **metadata (optional in some implementations)**:  \\n   - Information about the generation process, which may include:  \\n     - The text prompt used to generate the image.  \\n     - Model parameters such as scheduler settings, guidance scale, or number of inference steps.  \\n     - Device information (CPU or GPU) used for generation.  \\n     - Any warnings or notes about content safety filtering if applied.\\n\\nIn essence, the response provides the generated visual output corresponding to the text prompt, along with optional auxiliary information describing the conditions under which the image was created. The primary component is the image itself, which is the direct result of the text-to-image generation process.\", \"examples\": [{\"user_intent\": \"Generate a realistic image of a futuristic city skyline at sunset.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-base\", \"arguments\": {\"prompt\": \"a futuristic city skyline at sunset, highly detailed, cinematic lighting\"}}}, {\"user_intent\": \"Create an artistic illustration of a dragon flying over a medieval village.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-base\", \"arguments\": {\"prompt\": \"an epic illustration of a dragon flying over a medieval village, fantasy style, vibrant colors\"}}}, {\"user_intent\": \"Produce a cute digital painting of a corgi wearing a wizard hat in a magical forest.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-base\", \"arguments\": {\"prompt\": \"a cute corgi wearing a wizard hat in a magical forest, digital painting, whimsical style\"}}}]}\n\n# User Query\nWe want to build a Twitter Bot that creates an image based on users' textual requests. Generate an image with an astronaut playing guitar in space using a model.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an image based on a user's textual description.",
                    "tool_description": "Use a diffusion-based text-to-image generation model to create a high-quality image from a descriptive text prompt.",
                    "expected_response": "A generated image corresponding to the text prompt, along with optional metadata about the generation process."
                }
            ],
            "extracted_arguments": {
                "prompt": "A text description detailing the scene, objects, and style for the image to be generated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_109",
        "query": "I recently interviewed a person in Japanese. I need to transcribe the interview in order to find relevant quotes for my article.",
        "instruction": "Given an `audio transcription` task, retrieve tools specialized in transcribing audio content in Japanese by processing audio inputs to produce textual transcriptions that capture the interview dialogue accurately for use in an article.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Recognition",
                    "api_call": "SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')",
                    "api_arguments": [
                        "audio_paths"
                    ],
                    "python_environment_requirements": [
                        "huggingsound",
                        "torch",
                        "librosa",
                        "datasets",
                        "transformers"
                    ],
                    "example_code": "from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\ntranscriptions = model.transcribe(audio_paths)",
                    "performance": {
                        "dataset": "common_voice",
                        "accuracy": {
                            "WER": 81.8,
                            "CER": 20.16
                        }
                    },
                    "description": "Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.",
                    "id": "huggingface_api_761",
                    "name": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese"
                },
                "id": "gorilla_huggingface_tool_756",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 3: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.",
            "Tool 4: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 5: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 6: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.",
            "Tool 7: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 9: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 10: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 11: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 12: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 13: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 14: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents.",
            "Tool 15: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.",
            "Tool 16: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 17: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 19: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition."
        ],
        "tool_view": {
            "description": "A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name of the pretrained BERT model to be used. In this case, it specifies the model trained for Japanese text processing with word-level tokenization.",
                    "required": true,
                    "example": "cl-tohoku/bert-base-japanese-whole-word-masking"
                }
            },
            "response": "The tool is designed to perform the \"Fill-Mask\" task using a BERT model pretrained on Japanese text. The expected response from this tool would be a set of predictions for the masked token in a given input text.\n\n### Structure and Meaning of the Response:\n1. **Predictions**: \n   - The response will likely include a list or collection of possible words or phrases that can replace the masked token in the input text.\n   - Each prediction would typically have a score, representing the model's confidence or likelihood of that particular replacement.\n\n2. **Fields in the Response**:\n   - **sequence**: The completed text after the masked token has been filled.\n     - This represents the original input text with the masked word replaced by one of the model's predictions.\n   - **score**: The confidence score for the prediction, typically in the form of a probability.\n     - This score shows how likely the model thinks that the predicted word or phrase is the correct replacement for the masked token.\n   - **token**: The predicted token itself (i.e., the word or phrase that the model suggests to fill in the mask).\n     - This will be a text string representing the candidate word that the model considers a good fit for the masked position.\n   - **token_id**: A numerical ID corresponding to the predicted token in the model’s vocabulary.\n     - This ID refers to the index of the token within the model's vocabulary.\n\n3. **Optional Fields**:\n   - Depending on the implementation, additional metadata might be returned, such as:\n     - **probabilities**: A list of scores for all possible tokens considered by the model for filling the mask, ordered by likelihood.\n     - **top_k**: The number of top predictions returned, which could be limited based on the `k` parameter set in the request.\n\nThe tool will return a set of predictions with associated confidence scores, and possibly a ranked list of multiple candidates for the masked token.",
            "examples": [
                {
                    "user_intent": "I want to fill in a missing word in a Japanese sentence.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "[MASK] は日本で人気があります。"
                        }
                    }
                },
                {
                    "user_intent": "Can you help me predict the masked word in this Japanese sentence: '今日は[MASK]に行きます'?",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "今日は[MASK]に行きます"
                        }
                    }
                },
                {
                    "user_intent": "Fill in the missing word for a Japanese phrase using a pretrained model.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-whole-word-masking",
                        "arguments": {
                            "model": "cl-tohoku/bert-base-japanese-whole-word-masking",
                            "input": "[MASK]を食べるのが好きです。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 1: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 2: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 3: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.\nTool 4: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 5: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 6: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.\nTool 7: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 8: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 9: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 10: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 11: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 12: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 13: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 14: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents.\nTool 15: The VC-1 model is a vision transformer (ViT) pre-trained on thousands of hours of egocentric video and ImageNet data, designed for EmbodiedAI tasks such as object manipulation and indoor navigation. It can be loaded via Hugging Face Transformers to generate image embeddings, enabling applications that require understanding and interacting with visual environments from a first-person perspective.\nTool 16: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 17: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 19: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name of the pretrained BERT model to be used. In this case, it specifies the model trained for Japanese text processing with word-level tokenization.\", \"required\": true, \"example\": \"cl-tohoku/bert-base-japanese-whole-word-masking\"}}, \"response\": \"The tool is designed to perform the \\\"Fill-Mask\\\" task using a BERT model pretrained on Japanese text. The expected response from this tool would be a set of predictions for the masked token in a given input text.\\n\\n### Structure and Meaning of the Response:\\n1. **Predictions**: \\n   - The response will likely include a list or collection of possible words or phrases that can replace the masked token in the input text.\\n   - Each prediction would typically have a score, representing the model's confidence or likelihood of that particular replacement.\\n\\n2. **Fields in the Response**:\\n   - **sequence**: The completed text after the masked token has been filled.\\n     - This represents the original input text with the masked word replaced by one of the model's predictions.\\n   - **score**: The confidence score for the prediction, typically in the form of a probability.\\n     - This score shows how likely the model thinks that the predicted word or phrase is the correct replacement for the masked token.\\n   - **token**: The predicted token itself (i.e., the word or phrase that the model suggests to fill in the mask).\\n     - This will be a text string representing the candidate word that the model considers a good fit for the masked position.\\n   - **token_id**: A numerical ID corresponding to the predicted token in the model\\u2019s vocabulary.\\n     - This ID refers to the index of the token within the model's vocabulary.\\n\\n3. **Optional Fields**:\\n   - Depending on the implementation, additional metadata might be returned, such as:\\n     - **probabilities**: A list of scores for all possible tokens considered by the model for filling the mask, ordered by likelihood.\\n     - **top_k**: The number of top predictions returned, which could be limited based on the `k` parameter set in the request.\\n\\nThe tool will return a set of predictions with associated confidence scores, and possibly a ranked list of multiple candidates for the masked token.\", \"examples\": [{\"user_intent\": \"I want to fill in a missing word in a Japanese sentence.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"[MASK] \\u306f\\u65e5\\u672c\\u3067\\u4eba\\u6c17\\u304c\\u3042\\u308a\\u307e\\u3059\\u3002\"}}}, {\"user_intent\": \"Can you help me predict the masked word in this Japanese sentence: '\\u4eca\\u65e5\\u306f[MASK]\\u306b\\u884c\\u304d\\u307e\\u3059'?\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"\\u4eca\\u65e5\\u306f[MASK]\\u306b\\u884c\\u304d\\u307e\\u3059\"}}}, {\"user_intent\": \"Fill in the missing word for a Japanese phrase using a pretrained model.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"arguments\": {\"model\": \"cl-tohoku/bert-base-japanese-whole-word-masking\", \"input\": \"[MASK]\\u3092\\u98df\\u3079\\u308b\\u306e\\u304c\\u597d\\u304d\\u3067\\u3059\\u3002\"}}}]}\n\n# User Query\nI recently interviewed a person in Japanese. I need to transcribe the interview in order to find relevant quotes for my article.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to transcribe my Japanese audio interview into text.",
                    "tool_description": "Convert spoken Japanese audio into written text using a pre-trained automatic speech recognition (ASR) model.",
                    "expected_response": "A textual transcription of the Japanese audio, preserving the spoken content accurately."
                },
                {
                    "user_intent": "I want to identify and extract relevant quotes from the transcribed Japanese text.",
                    "tool_description": "Use a Japanese language processing model to detect and extract meaningful phrases or sentences from text, such as quotes or important statements.",
                    "expected_response": "A list of extracted quotes or key sentences with their positions in the transcript."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The audio recording of the Japanese interview to be transcribed.",
                "transcription_text": "The full text output of the transcribed interview from which quotes will be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_391",
        "query": "Create a machine learning-based image recognition tool that can identify whether an animal in an image is a cat or a dog.",
        "instruction": "Given an `image recognition` task, retrieve tools that facilitate machine learning-based image classification by processing image inputs and determining the presence of specific objects, such as distinguishing between cats and dogs, leveraging zero-shot classification techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')",
                    "api_arguments": {
                        "image_path": "Path to the image file",
                        "class_names": "List of comma-separated class names"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "75.9-76.9%"
                    },
                    "description": "A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.",
                    "id": "huggingface_api_366",
                    "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft"
                },
                "id": "gorilla_huggingface_tool_362",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.",
            "Tool 1: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 2: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 3: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents.",
            "Tool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 6: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.",
            "Tool 7: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 8: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 9: This tool uses the ConvNeXT large model from Hugging Face to perform image classification, allowing users to predict the category of an input image. Built as a modernized convolutional neural network inspired by Vision Transformers, it is designed for high-performance visual recognition tasks and can be applied to datasets like ImageNet or custom image collections.",
            "Tool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 11: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 12: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 13: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 15: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 16: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 17: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 19: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained TrOCR model to load. This can be a model name from the Hugging Face model hub or a local directory containing model weights.",
                    "required": true,
                    "example": "microsoft/trocr-base-handwritten"
                }
            },
            "response": "The tool produces a textual transcription of input images, specifically designed for handwritten text recognition. The response is structured as a single text string representing the content of the image after OCR (Optical Character Recognition).  \n\nKey components and their meanings:\n\n- **Text output**: A single string containing the recognized text from the provided image. This string is typically cleaned of special tokens and formatted as plain text.  \n- **Optional batch decoding**: If multiple images are processed at once, the tool may return a list of strings, with each string corresponding to the recognized text of one image.  \n- **Encoding/decoding process**: Internally, the model generates token IDs for the text, which are then converted to human-readable text using a processor. These token IDs are not usually exposed in the final response unless explicitly requested.  \n\nIn principle, the response conveys the semantic content of the handwritten input image as machine-readable text, suitable for downstream applications such as text analysis, document digitization, or further NLP tasks.",
            "examples": [
                {
                    "user_intent": "I have a scanned handwritten note and I want to extract the text from it.",
                    "tool_call": {
                        "name": "microsoft/trocr-base-handwritten",
                        "arguments": {
                            "image_url": "https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Convert this handwritten assignment page into digital text for editing.",
                    "tool_call": {
                        "name": "microsoft/trocr-base-handwritten",
                        "arguments": {
                            "image_url": "https://example.com/homework_page.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically read multiple images of handwritten receipts and get their text content.",
                    "tool_call": {
                        "name": "microsoft/trocr-base-handwritten",
                        "arguments": {
                            "image_url": "https://example.com/receipt_image_01.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.\nTool 1: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 2: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 3: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents.\nTool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 6: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image Transformer as an encoder and a text Transformer as a decoder, leveraging pre-trained weights from BEiT for the encoder and RoBERTa for the decoder. This model is suitable for extracting text from scanned documents or images, enabling automated text recognition and digitization of printed materials.\nTool 7: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 8: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 9: This tool uses the ConvNeXT large model from Hugging Face to perform image classification, allowing users to predict the category of an input image. Built as a modernized convolutional neural network inspired by Vision Transformers, it is designed for high-performance visual recognition tasks and can be applied to datasets like ImageNet or custom image collections.\nTool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 11: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 12: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 13: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 15: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 16: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 17: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 18: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 19: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained TrOCR model for converting images of handwritten text into machine-readable text. It uses a transformer-based encoder-decoder architecture, with an image Transformer as the encoder and a text Transformer as the decoder, allowing accurate transcription of handwritten documents. It is designed for optical character recognition tasks and can be applied to scanned or photographed handwritten material.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained TrOCR model to load. This can be a model name from the Hugging Face model hub or a local directory containing model weights.\", \"required\": true, \"example\": \"microsoft/trocr-base-handwritten\"}}, \"response\": \"The tool produces a textual transcription of input images, specifically designed for handwritten text recognition. The response is structured as a single text string representing the content of the image after OCR (Optical Character Recognition).  \\n\\nKey components and their meanings:\\n\\n- **Text output**: A single string containing the recognized text from the provided image. This string is typically cleaned of special tokens and formatted as plain text.  \\n- **Optional batch decoding**: If multiple images are processed at once, the tool may return a list of strings, with each string corresponding to the recognized text of one image.  \\n- **Encoding/decoding process**: Internally, the model generates token IDs for the text, which are then converted to human-readable text using a processor. These token IDs are not usually exposed in the final response unless explicitly requested.  \\n\\nIn principle, the response conveys the semantic content of the handwritten input image as machine-readable text, suitable for downstream applications such as text analysis, document digitization, or further NLP tasks.\", \"examples\": [{\"user_intent\": \"I have a scanned handwritten note and I want to extract the text from it.\", \"tool_call\": {\"name\": \"microsoft/trocr-base-handwritten\", \"arguments\": {\"image_url\": \"https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg\"}}}, {\"user_intent\": \"Convert this handwritten assignment page into digital text for editing.\", \"tool_call\": {\"name\": \"microsoft/trocr-base-handwritten\", \"arguments\": {\"image_url\": \"https://example.com/homework_page.jpg\"}}}, {\"user_intent\": \"I want to automatically read multiple images of handwritten receipts and get their text content.\", \"tool_call\": {\"name\": \"microsoft/trocr-base-handwritten\", \"arguments\": {\"image_url\": \"https://example.com/receipt_image_01.jpg\"}}}]}\n\n# User Query\nCreate a machine learning-based image recognition tool that can identify whether an animal in an image is a cat or a dog.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to develop a machine learning model to classify images of animals.",
                    "tool_description": "Create a machine learning-based image recognition model that can classify images of animals as either cats or dogs.",
                    "expected_response": "A trained model capable of predicting whether an input image contains a cat or a dog."
                }
            ],
            "extracted_arguments": {
                "animal_type": "The category of animals to classify, specifically 'cat' or 'dog'.",
                "image_input": "The image data that needs to be analyzed for classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_142",
        "query": "Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.",
        "instruction": "Given an `article recommendation` task, retrieve tools that utilize natural language processing to measure article similarity by analyzing the content of user interactions, thus enabling the recommendation of articles akin to previously liked content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.",
                    "id": "huggingface_api_2",
                    "name": "princeton-nlp/unsup-simcse-roberta-base"
                },
                "id": "gorilla_huggingface_tool_2",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 1: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 2: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 5: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 6: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 7: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 8: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 9: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 10: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 11: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 12: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 14: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 15: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 16: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 17: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 18: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning."
        ],
        "tool_view": {
            "description": "A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text or article that you want the model to summarize.",
                    "required": true,
                    "example": "The stock market saw significant gains today as tech companies reported higher than expected earnings."
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens for the generated summary. Controls the length of the output.",
                    "required": false,
                    "default": 150,
                    "example": 100
                }
            },
            "response": "The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \n\nKey aspects of the response:  \n\n- **Type:** String (text)  \n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \n\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.",
            "examples": [
                {
                    "user_intent": "Summarize a recent news article about the global climate summit in under 100 words.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.",
                            "max_length": 100
                        }
                    }
                },
                {
                    "user_intent": "Create a concise summary of a long article about advancements in AI and robotics.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.",
                            "max_length": 120
                        }
                    }
                },
                {
                    "user_intent": "Provide a short summary for a news article about a major sports event.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-summarize-news",
                        "arguments": {
                            "text": "In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.",
                            "max_length": 80
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 1: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 2: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 3: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 5: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 6: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 7: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 8: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 9: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 10: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 11: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 12: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 13: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 14: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 15: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 16: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 17: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 18: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 19: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text or article that you want the model to summarize.\", \"required\": true, \"example\": \"The stock market saw significant gains today as tech companies reported higher than expected earnings.\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens for the generated summary. Controls the length of the output.\", \"required\": false, \"default\": 150, \"example\": 100}}, \"response\": \"The tool produces a text-based summary of an input document or passage. The response is a single string representing the condensed version of the original text. Semantically, this string contains the most salient information from the input, preserving the main ideas while reducing length. \\n\\nKey aspects of the response:  \\n\\n- **Type:** String (text)  \\n- **Content:** A coherent, human-readable summary that captures the essential points of the input text.  \\n- **Length:** Controlled by the `max_length` parameter, which specifies the maximum number of tokens in the generated summary.  \\n- **Determinism/Variability:** Generation may vary slightly depending on model settings like beam search (`num_beams`), repetition penalty, length penalty, and early stopping, but the output remains a concise summarization of the input.  \\n\\nThe response does **not** include metadata, scoring, or multiple candidate summaries unless explicitly modified; by default, it returns the first generated summary from the model.\", \"examples\": [{\"user_intent\": \"Summarize a recent news article about the global climate summit in under 100 words.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"The Global Climate Summit held this week brought together world leaders to discuss urgent measures to reduce carbon emissions. Several countries pledged new targets for renewable energy investments and carbon neutrality. Environmental activists urged faster action, while economists highlighted the challenges of balancing growth and sustainability. The summit concluded with a joint statement emphasizing cooperation and accountability.\", \"max_length\": 100}}}, {\"user_intent\": \"Create a concise summary of a long article about advancements in AI and robotics.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"Recent advancements in AI and robotics have enabled machines to perform increasingly complex tasks. From self-driving cars to intelligent personal assistants, these technologies are transforming industries and daily life. Researchers are focusing on ethical considerations, safety, and collaboration between humans and machines. The integration of AI into manufacturing, healthcare, and logistics is expected to boost efficiency while also creating new challenges around employment and data privacy.\", \"max_length\": 120}}}, {\"user_intent\": \"Provide a short summary for a news article about a major sports event.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"arguments\": {\"text\": \"In a thrilling championship final, the underdog team clinched victory against the reigning champions in the last minutes of the game. Fans celebrated in the streets as the team lifted the trophy, marking their first major win in over a decade. Analysts praised the strategic gameplay and determination of the players throughout the tournament.\", \"max_length\": 80}}}]}\n\n# User Query\nDesign a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze articles based on user preferences and recommend similar content.",
                    "tool_description": "Use a model to generate vector embeddings for both previously liked articles and new content to measure similarity between them.",
                    "expected_response": "A similarity score or ranking of new articles based on how closely they match the user's previously liked content."
                },
                {
                    "user_intent": "I need to compare the semantic similarity between two or more articles to recommend similar ones.",
                    "tool_description": "Generate dense vector embeddings for multiple articles and compute their semantic similarity, which can be used for content recommendation.",
                    "expected_response": "A list of similarity scores indicating how close each new article is to the user's previous favorites."
                }
            ],
            "extracted_arguments": {
                "user_articles": "A list of articles previously liked by the user, represented as text.",
                "new_article": "The new article to be compared to the user’s previously liked articles for similarity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_121",
        "query": "I have few picture from my album, and I would like to find out whether it is a hotdog or not.",
        "instruction": "Given an `image classification` task, retrieve tools that can analyze images from the user's photo album to determine specific classifications, such as identifying whether an object in the image is a hotdog or not, and provide the classification results based on the visual content of the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "pipeline('image-classification', model='julien-c/hotdog-not-hotdog')",
                    "api_arguments": "image",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": 0.825
                    },
                    "description": "A model that classifies images as hotdog or not hotdog.",
                    "id": "huggingface_api_194",
                    "name": "julien-c/hotdog-not-hotdog"
                },
                "id": "gorilla_huggingface_tool_190",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "Tool 1: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 2: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 3: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.",
            "Tool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 5: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 6: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.",
            "Tool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 8: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 9: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.",
            "Tool 10: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 11: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 12: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 13: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 14: FLAN-T5 is a multilingual language model fine-tuned on over 1,000 diverse tasks, designed for text-to-text generation in zero-shot and few-shot NLP settings. It can perform tasks such as reasoning, translation, and question answering, achieving strong benchmark performance and supporting research in versatile natural language understanding and generation applications.",
            "Tool 15: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 16: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 17: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 18: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 19: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis."
        ],
        "tool_view": {
            "description": "A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The image to be classified by the model. This can be a URL, file path, or image data in a supported format.",
                    "required": true,
                    "example": "path/to/image.jpg"
                }
            },
            "response": "The tool returns a classification result for a given image, indicating whether the image depicts a hotdog or not. The response is typically structured as a list of one or more classification entries. Each entry contains the following components:\n\n- **label**: A string representing the predicted class for the image (e.g., \"hotdog\" or \"not hotdog\").  \n- **score**: A numerical value representing the model’s confidence in the prediction, usually as a probability between 0 and 1.  \n\nIn principle, if multiple predictions are returned, they are sorted by confidence score in descending order. The overall response provides both the predicted class and the confidence level, allowing downstream applications to interpret the likelihood of the classification.",
            "examples": [
                {
                    "user_intent": "I want to check if this picture I took is a hotdog or not.",
                    "tool_call": {
                        "name": "julien-c/hotdog-not-hotdog",
                        "arguments": {
                            "image": "/home/user/photos/lunch.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Classify an image from a URL to see if it contains a hotdog.",
                    "tool_call": {
                        "name": "julien-c/hotdog-not-hotdog",
                        "arguments": {
                            "image": "https://example.com/images/street_food.png"
                        }
                    }
                },
                {
                    "user_intent": "I have a batch of food images. Let's test one to determine if it's a hotdog.",
                    "tool_call": {
                        "name": "julien-c/hotdog-not-hotdog",
                        "arguments": {
                            "image": "/data/food_images/meal_001.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.\nTool 1: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 2: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 3: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\nTool 4: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 5: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 6: FLAN-T5 XXL is a large, fine-tuned T5 language model designed for text-to-text generation across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, translation, and question answering, and achieves strong performance on benchmarks like MMLU. This model is suited for research and applications requiring advanced natural language understanding and generation.\nTool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 8: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 9: FLAN-T5 XL is a large-scale language model fine-tuned on over 1000 tasks across multiple languages. It excels in zero-shot and few-shot NLP tasks, including reasoning, question answering, and language understanding. The model, accessible via the Hugging Face Transformers framework, achieves state-of-the-art performance on benchmarks like MMLU, with an accuracy of 75.2%. It is designed for research on the capabilities and limitations of large language models.\nTool 10: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 11: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 12: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 13: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 14: FLAN-T5 is a multilingual language model fine-tuned on over 1,000 diverse tasks, designed for text-to-text generation in zero-shot and few-shot NLP settings. It can perform tasks such as reasoning, translation, and question answering, achieving strong benchmark performance and supporting research in versatile natural language understanding and generation applications.\nTool 15: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 16: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 17: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 18: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 19: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A computer vision tool that uses a Hugging Face Transformers model to classify images as either \\u201chotdog\\u201d or \\u201cnot hotdog,\\u201d useful for automated image recognition tasks with moderate accuracy.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The image to be classified by the model. This can be a URL, file path, or image data in a supported format.\", \"required\": true, \"example\": \"path/to/image.jpg\"}}, \"response\": \"The tool returns a classification result for a given image, indicating whether the image depicts a hotdog or not. The response is typically structured as a list of one or more classification entries. Each entry contains the following components:\\n\\n- **label**: A string representing the predicted class for the image (e.g., \\\"hotdog\\\" or \\\"not hotdog\\\").  \\n- **score**: A numerical value representing the model\\u2019s confidence in the prediction, usually as a probability between 0 and 1.  \\n\\nIn principle, if multiple predictions are returned, they are sorted by confidence score in descending order. The overall response provides both the predicted class and the confidence level, allowing downstream applications to interpret the likelihood of the classification.\", \"examples\": [{\"user_intent\": \"I want to check if this picture I took is a hotdog or not.\", \"tool_call\": {\"name\": \"julien-c/hotdog-not-hotdog\", \"arguments\": {\"image\": \"/home/user/photos/lunch.jpg\"}}}, {\"user_intent\": \"Classify an image from a URL to see if it contains a hotdog.\", \"tool_call\": {\"name\": \"julien-c/hotdog-not-hotdog\", \"arguments\": {\"image\": \"https://example.com/images/street_food.png\"}}}, {\"user_intent\": \"I have a batch of food images. Let's test one to determine if it's a hotdog.\", \"tool_call\": {\"name\": \"julien-c/hotdog-not-hotdog\", \"arguments\": {\"image\": \"/data/food_images/meal_001.jpg\"}}}]}\n\n# User Query\nI have few picture from my album, and I would like to find out whether it is a hotdog or not.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to check if this picture from my album is a hotdog or not.",
                    "tool_description": "Classify an image using a Hugging Face Transformers model to determine whether it depicts a hotdog or not, providing automated image recognition with moderate accuracy.",
                    "expected_response": "A classification result indicating whether the image shows a hotdog, including a label and confidence score."
                }
            ],
            "extracted_arguments": {
                "image": "The image to be classified, provided as a URL, file path, or image data in a supported format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_263",
        "query": "An environmental organization would like to use our Carbon Emissions prediction model to estimate CO2 emissions of different configurations of vehicles.",
        "instruction": "Given a `carbon emissions estimation` task, retrieve tools that leverage prediction models to analyze input configurations and provide estimates of CO2 emissions based on the provided vehicle data.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "pcoloc/autotrain-data-600-dragino",
                        "accuracy": {
                            "Loss": 93.595,
                            "R2": 0.502,
                            "MSE": 8760.052,
                            "MAE": 77.527,
                            "RMSLE": 0.445
                        }
                    },
                    "description": "This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.",
                    "id": "huggingface_api_872",
                    "name": "pcoloc/autotrain-600-dragino-1839063122"
                },
                "id": "gorilla_huggingface_tool_865",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 1: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 2: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 4: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 5: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 6: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 7: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 8: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 9: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 10: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 11: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 12: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.",
            "Tool 13: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 14: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 16: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 17: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 18: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 19: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.",
                    "required": true,
                    "example": "{ 'feature1': [value1, value2], 'feature2': [value3, value4] }"
                }
            },
            "response": "The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\n\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\n\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\n\n3. **Model Information**: The model's name, in this case, \"kochetkovIT/autotrain-ironhack-49741119788,\" could also be part of the response for identification or logging purposes.\n\nIn summary, the response would likely include:\n- The **predictions** from the model based on input data.\n- A set of **performance metrics** that evaluate the quality of the model.\n- **Model identification** (name) for reference or tracking.\n\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my new vehicle dataset using the trained model.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "vehicle_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "factory_emissions_input.csv"
                        }
                    }
                },
                {
                    "user_intent": "Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.",
                    "tool_call": {
                        "name": "kochetkovIT/autotrain-ironhack-49741119788",
                        "arguments": {
                            "data": "building_energy.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 1: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 2: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 4: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 5: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 6: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 7: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 8: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 9: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 10: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 11: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 12: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.\nTool 13: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 14: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 16: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 17: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 18: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 19: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The input data to be used for making predictions with the regression model. The data should include the features required by the model, formatted as a pandas DataFrame.\", \"required\": true, \"example\": \"{ 'feature1': [value1, value2], 'feature2': [value3, value4] }\"}}, \"response\": \"The tool does not explicitly define a response format. However, based on its functionality, a reasonable and typical response from this tool would contain:\\n\\n1. **Predictions**: The primary output of the model is likely a set of predicted carbon emissions values, based on the input data provided to the model. These predictions would typically be in numerical form (e.g., in grams of CO2 emissions).\\n\\n2. **Performance Metrics**: The response may include key performance metrics of the model, such as:\\n   - **Loss**: A numerical value representing the loss function (in this case, a regression loss measure).\\n   - **R2**: The R-squared score, which indicates how well the model explains the variance in the target variable. In this case, a low R2 (0.013) suggests the model's predictive power is weak.\\n   - **MSE**: Mean Squared Error, which measures the average of the squared differences between predicted and actual values.\\n   - **MAE**: Mean Absolute Error, indicating the average absolute difference between predicted and actual values.\\n   - **RMSLE**: Root Mean Squared Logarithmic Error, a metric sensitive to large errors.\\n\\n3. **Model Information**: The model's name, in this case, \\\"kochetkovIT/autotrain-ironhack-49741119788,\\\" could also be part of the response for identification or logging purposes.\\n\\nIn summary, the response would likely include:\\n- The **predictions** from the model based on input data.\\n- A set of **performance metrics** that evaluate the quality of the model.\\n- **Model identification** (name) for reference or tracking.\\n\\nThe response would be structured to convey both the results of the prediction task and relevant performance data for assessing model accuracy and suitability.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my new vehicle dataset using the trained model.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"vehicle_data.csv\"}}}, {\"user_intent\": \"I have a CSV of industrial factory parameters, can you estimate the associated carbon emissions?\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"factory_emissions_input.csv\"}}}, {\"user_intent\": \"Use the AutoTrain regression model to predict carbon emissions for a set of building energy consumption data.\", \"tool_call\": {\"name\": \"kochetkovIT/autotrain-ironhack-49741119788\", \"arguments\": {\"data\": \"building_energy.csv\"}}}]}\n\n# User Query\nAn environmental organization would like to use our Carbon Emissions prediction model to estimate CO2 emissions of different configurations of vehicles.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate the CO2 emissions for different vehicle configurations using the trained model.",
                    "tool_description": "Use a tabular regression model trained with AutoTrain to predict carbon emissions in grams based on structured input features of vehicles.",
                    "expected_response": "A set of predicted CO2 emission values for each vehicle configuration, along with relevant performance metrics such as R², MSE, and MAE, and model identification information."
                }
            ],
            "extracted_arguments": {
                "vehicle_data": "Structured tabular data containing features for each vehicle configuration, formatted as a pandas DataFrame."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_351",
        "query": "We are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.",
        "instruction": "Given a `text summarization` task, retrieve tools that are capable of processing long text inputs to generate concise and coherent summaries, effectively condensing the content while preserving the main ideas.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')",
                    "api_arguments": "",
                    "python_environment_requirements": "huggingface/transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": [
                            {
                                "name": "cnn_dailymail",
                                "accuracy": {
                                    "Rouge 2": "22.12",
                                    "Rouge-L": "36.99"
                                }
                            }
                        ]
                    },
                    "description": "DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.",
                    "id": "huggingface_api_545",
                    "name": "sshleifer/distilbart-cnn-12-6"
                },
                "id": "gorilla_huggingface_tool_541",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 1: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 3: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.",
            "Tool 4: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 5: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 6: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 7: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 9: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 10: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 11: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 13: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 15: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 16: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts."
        ],
        "tool_view": {
            "description": "A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "parameters": {
                "input_text": {
                    "type": "string",
                    "description": "The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.",
                    "required": true,
                    "example": "India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England."
                }
            },
            "response": "The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \n\nSpecifically:  \n\n- **Type:** String  \n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \n- **Length:** Typically shorter than the input text, with the exact length depending on the model’s internal generation constraints and the input complexity.  \n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \n\nThere are no additional structured fields; the output is purely the generated summary string.",
            "examples": "[\n    {\n        \"user_intent\": \"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\",\n        \"tool_call\": {\n            \"name\": \"tuner007/pegasus_summarizer\",\n            \"arguments\": {\n                \"input_text\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day"
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 1: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 2: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 3: This tool uses the \"lidiya/bart-large-xsum-samsum\" model, fine-tuned on the SAMSum dataset, to generate abstractive summaries of conversational text. It leverages the Hugging Face Transformers framework and can be called via the `pipeline('summarization')` API. This model is ideal for summarizing dialogues, offering an efficient solution for condensing long conversations into concise summaries.\nTool 4: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 5: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 6: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 7: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 9: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 10: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 11: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 13: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 15: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 16: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 17: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 18: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\", \"parameters\": {\"input_text\": {\"type\": \"string\", \"description\": \"The text content that you want the PEGASUS model to summarize. This should be a complete passage or article in string format.\", \"required\": true, \"example\": \"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj during the third Test against England.\"}}, \"response\": \"The tool produces a **textual summary** of the input content provided via the `input_text` argument. The response is a single string that represents a condensed version of the original text, retaining the most important information, key events, or main ideas while reducing length and removing less relevant details.  \\n\\nSpecifically:  \\n\\n- **Type:** String  \\n- **Content:** A coherent, fluent summary of the input text. It is generated by the PEGASUS model fine-tuned for summarization (`tuner007/pegasus_summarizer`) and is intended to capture the essence of the input without including extraneous information.  \\n- **Length:** Typically shorter than the input text, with the exact length depending on the model\\u2019s internal generation constraints and the input complexity.  \\n- **Purpose:** To provide a concise representation of the input text suitable for quick reading or downstream NLP tasks.  \\n\\nThere are no additional structured fields; the output is purely the generated summary string.\", \"examples\": \"[\\n    {\\n        \\\"user_intent\\\": \\\"Summarize this news article about Rishabh Pant and crowd incidents during the India vs England Test match.\\\",\\n        \\\"tool_call\\\": {\\n            \\\"name\\\": \\\"tuner007/pegasus_summarizer\\\",\\n            \\\"arguments\\\": {\\n                \\\"input_text\\\": \\\"India wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play. You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added. In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd. Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul. Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated. Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day\"}\n\n# User Query\nWe are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to summarize a long article into a concise version.",
                    "tool_description": "Generate a condensed summary of the input text, preserving key information while reducing length.",
                    "expected_response": "A coherent, fluent summary of the input text that captures the main points and key details."
                }
            ],
            "extracted_arguments": {
                "input_text": "The text content to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_169",
        "query": "We are building a product which can identify birds in the images. Design the model which can help us segment the birds in an image.",
        "instruction": "Given an `image segmentation` task, retrieve tools that process image inputs to identify and segment objects such as birds by leveraging transformer models designed for instance segmentation tasks, aligning with the requirements of computer vision projects.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mask2former-swin-tiny-coco-instance"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs)\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\npredicted_instance_map = result['segmentation']",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not specified"
                    },
                    "description": "Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.",
                    "id": "huggingface_api_250",
                    "name": "facebook/mask2former-swin-tiny-coco-instance"
                },
                "id": "gorilla_huggingface_tool_246",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 2: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.",
            "Tool 3: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 4: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 5: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 6: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 7: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 8: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.",
            "Tool 9: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 10: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 13: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 15: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 16: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.",
            "Tool 17: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.",
            "Tool 18: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 19: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.",
                    "required": true,
                    "example": 0.25
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.",
                    "required": true,
                    "example": 0.45
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.",
                    "required": true,
                    "example": false
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections to return per image.",
                    "required": true,
                    "example": 1000
                },
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. Can be a local file path or a URL pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool’s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - A confidence score indicating the model’s certainty that the object belongs to the target class.  \n   - Optional class label or ID identifying the object category (here, all are buildings).  \n\n2. **`masks`** – Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \n\n3. **`scores`** – Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \n\n4. **`class_ids` or `labels`** – Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \n\n5. **`metadata`** (optional) – May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \n\n6. **Visualization helpers** – Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \n\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.",
            "examples": [
                {
                    "user_intent": "I want to segment all buildings in a satellite image of a city block.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/satellite_city_block.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect and segment buildings in a rural area satellite image with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.6,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 200,
                            "image": "https://example.com/rural_area_satellite.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Process a satellite image to extract building masks for GIS analysis.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/gis_project_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 2: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.\nTool 3: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 4: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 5: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 6: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 7: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 8: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.\nTool 9: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 10: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 11: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 13: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 14: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 15: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 16: This tool uses the \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" model to convert sentences or paragraphs into 768-dimensional embeddings, which can be used for tasks such as clustering or semantic search. It is based on Hugging Face Transformers and supports multilingual sentence similarity.\nTool 17: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.\nTool 18: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 19: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.\", \"required\": true, \"example\": 0.25}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.\", \"required\": true, \"example\": 0.45}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.\", \"required\": true, \"example\": false}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections to return per image.\", \"required\": true, \"example\": 1000}, \"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. Can be a local file path or a URL pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \\n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - A confidence score indicating the model\\u2019s certainty that the object belongs to the target class.  \\n   - Optional class label or ID identifying the object category (here, all are buildings).  \\n\\n2. **`masks`** \\u2013 Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \\n\\n3. **`scores`** \\u2013 Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \\n\\n4. **`class_ids` or `labels`** \\u2013 Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \\n\\n5. **`metadata`** (optional) \\u2013 May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \\n\\n6. **Visualization helpers** \\u2013 Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \\n\\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.\", \"examples\": [{\"user_intent\": \"I want to segment all buildings in a satellite image of a city block.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/satellite_city_block.jpg\"}}}, {\"user_intent\": \"Detect and segment buildings in a rural area satellite image with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.6, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 200, \"image\": \"https://example.com/rural_area_satellite.jpg\"}}}, {\"user_intent\": \"Process a satellite image to extract building masks for GIS analysis.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/gis_project_image.jpg\"}}}]}\n\n# User Query\nWe are building a product which can identify birds in the images. Design the model which can help us segment the birds in an image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to segment birds in an image to identify their exact locations.",
                    "tool_description": "Use an image segmentation model trained to detect and segment birds within images, producing precise masks and bounding boxes for each bird detected.",
                    "expected_response": "A structured object containing bounding boxes, segmentation masks, confidence scores, and optional class identifiers for each detected bird."
                }
            ],
            "extracted_arguments": {
                "image": "The input image containing birds to be segmented, provided as a file path or URL.",
                "conf": "The confidence threshold for bird detection; predictions below this value will be discarded.",
                "iou": "The Intersection over Union threshold used for non-maximum suppression to filter overlapping bird detections.",
                "agnostic_nms": "Boolean indicating whether non-maximum suppression should be class-agnostic for overlapping detections.",
                "max_det": "The maximum number of bird detections to return per image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_16",
        "query": "I have a list of Olympic Game host cities and their corresponding years. I want to know which year the games were held in Beijing.",
        "instruction": "Given a `year lookup` task for historical events in a tabular format, retrieve tools that perform table question answering by processing structured data to determine the specific year an event occurred in a specified location, aligning with the query's requirements for detailed information extraction.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-base')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": {
                        "libraries": [
                            "transformers",
                            "pandas"
                        ]
                    },
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "arxiv:2107.07653",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.",
                    "id": "huggingface_api_443",
                    "name": "microsoft/tapex-base"
                },
                "id": "gorilla_huggingface_tool_439",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.",
            "Tool 2: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.",
            "Tool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 5: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 8: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 10: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 11: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 12: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 13: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.",
            "Tool 14: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 15: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.",
            "Tool 16: Mask2Former is a universal image segmentation model that performs instance, semantic, and panoptic segmentation by predicting sets of masks with corresponding labels. Trained on the COCO panoptic segmentation dataset with a Swin backbone, it provides a unified, efficient approach to multiple segmentation tasks and improves on previous state-of-the-art models in both accuracy and computational performance.",
            "Tool 17: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 18: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 19: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: A pre-trained Wav2Vec2 model fine-tuned for multilingual phonetic transcription, which converts audio into sequences of phonetic labels. It is used for automatic speech recognition tasks across multiple languages, and the phonetic outputs can be mapped to words using a corresponding label-to-word dictionary.\nTool 2: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.\nTool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 5: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 7: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 8: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 10: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 11: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 12: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 13: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.\nTool 14: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 15: The `microsoft/trocr-large-handwritten` tool is a Transformer-based model for optical character recognition (OCR) that converts handwritten images into text. It uses an image Transformer encoder to process visual input and a text Transformer decoder to generate corresponding text, making it suitable for digitizing handwritten documents or extracting text from images.\nTool 16: Mask2Former is a universal image segmentation model that performs instance, semantic, and panoptic segmentation by predicting sets of masks with corresponding labels. Trained on the COCO panoptic segmentation dataset with a Swin backbone, it provides a unified, efficient approach to multiple segmentation tasks and improves on previous state-of-the-art models in both accuracy and computational performance.\nTool 17: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 18: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 19: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nI have a list of Olympic Game host cities and their corresponding years. I want to know which year the games were held in Beijing.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Find the year when the Olympic Games were held in a specific city.",
                    "tool_description": "Query a structured list of Olympic host cities and their corresponding years to identify the year associated with a given city.",
                    "expected_response": "The year(s) when the Olympic Games took place in the specified city."
                }
            ],
            "extracted_arguments": {
                "host_city": "The name of the Olympic host city to look up."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_34",
        "query": "Our client has a traffic camera system and wants to detect vehicles in the images captured. Implement a solution.",
        "instruction": "Given an `object detection` task, retrieve tools that process image inputs to identify and locate objects, specifically vehicles in this case, by leveraging models trained for object detection tasks that provide output including bounding boxes, scores, and categories.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Transformers",
                    "functionality": "Object Detection",
                    "api_call": "yolov5.load('fcakyon/yolov5s-v7.0')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": "import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')",
                    "performance": {
                        "dataset": "detection-datasets/coco",
                        "accuracy": null
                    },
                    "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.",
                    "id": "huggingface_api_226",
                    "name": "fcakyon/yolov5s-v7.0"
                },
                "id": "gorilla_huggingface_tool_222",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 2: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 3: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 4: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 5: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 7: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 8: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 9: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 10: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.",
            "Tool 11: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 12: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 13: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 14: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 15: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 16: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 17: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 18: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 19: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research."
        ],
        "tool_view": {
            "description": "A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "multi_label": {
                    "type": "boolean",
                    "description": "If True, allows each detected box to have multiple class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                },
                "img": {
                    "type": "string",
                    "description": "The input image for detection. Can be a URL or local file path.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                },
                "size": {
                    "type": "number",
                    "description": "The size to which the input image will be resized before detection.",
                    "required": true,
                    "default": 640,
                    "example": 512
                },
                "augment": {
                    "type": "boolean",
                    "description": "If True, applies data augmentation during inference for potentially improved results.",
                    "required": true,
                    "default": true,
                    "example": false
                }
            },
            "response": "The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\n\n1. **pred**  \n   - A list (or tensor) containing the detection predictions for each input image.  \n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \n   - Each row contains:  \n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \n     - **Confidence score** (usually the fifth element) indicating the model’s certainty for that detection.  \n     - **Class index** (usually the sixth element) representing the detected object’s category; here it corresponds to the license plate class.\n\n2. **boxes**  \n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\n\n3. **scores**  \n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\n\n4. **categories**  \n   - Extracted from `pred`, this array contains the class indices for each detection.\n\n5. **Visualization methods**  \n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\n\n6. **Additional metadata** (implicit)  \n   - Number of detections returned is limited by `max_det`.  \n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \n   - The response may include augmented results if `augment=True` is used during inference.\n\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.",
            "examples": [
                {
                    "user_intent": "Detect license plates in a car image I took on the highway.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/highway_car.jpg",
                            "conf": 0.3,
                            "iou": 0.45,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 500,
                            "size": 640,
                            "augment": true
                        }
                    }
                },
                {
                    "user_intent": "Scan multiple parking lot images for license plates and save the results.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/parking_lot_1.jpg",
                            "conf": 0.25,
                            "iou": 0.5,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 1000,
                            "size": 640,
                            "augment": false
                        }
                    }
                },
                {
                    "user_intent": "Quickly check license plates in a low-resolution surveillance image.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/surveillance_frame.jpg",
                            "conf": 0.2,
                            "iou": 0.4,
                            "agnostic": true,
                            "multi_label": false,
                            "max_det": 200,
                            "size": 320,
                            "augment": true
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 2: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 3: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 4: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 5: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 6: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 7: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 8: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 9: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 10: A pretrained Vision Transformer model for image classification that has been trained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization, suitable for recognizing and categorizing images in a wide range of visual tasks.\nTool 11: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 12: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 13: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 14: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 15: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 16: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 17: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 18: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 19: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.\", \"required\": true, \"default\": false, \"example\": true}, \"multi_label\": {\"type\": \"boolean\", \"description\": \"If True, allows each detected box to have multiple class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image.\", \"required\": true, \"default\": 1000, \"example\": 500}, \"img\": {\"type\": \"string\", \"description\": \"The input image for detection. Can be a URL or local file path.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"size\": {\"type\": \"number\", \"description\": \"The size to which the input image will be resized before detection.\", \"required\": true, \"default\": 640, \"example\": 512}, \"augment\": {\"type\": \"boolean\", \"description\": \"If True, applies data augmentation during inference for potentially improved results.\", \"required\": true, \"default\": true, \"example\": false}}, \"response\": \"The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\\n\\n1. **pred**  \\n   - A list (or tensor) containing the detection predictions for each input image.  \\n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \\n   - Each row contains:  \\n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \\n     - **Confidence score** (usually the fifth element) indicating the model\\u2019s certainty for that detection.  \\n     - **Class index** (usually the sixth element) representing the detected object\\u2019s category; here it corresponds to the license plate class.\\n\\n2. **boxes**  \\n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\\n\\n3. **scores**  \\n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\\n\\n4. **categories**  \\n   - Extracted from `pred`, this array contains the class indices for each detection.\\n\\n5. **Visualization methods**  \\n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \\n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\\n\\n6. **Additional metadata** (implicit)  \\n   - Number of detections returned is limited by `max_det`.  \\n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \\n   - The response may include augmented results if `augment=True` is used during inference.\\n\\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.\", \"examples\": [{\"user_intent\": \"Detect license plates in a car image I took on the highway.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/highway_car.jpg\", \"conf\": 0.3, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 500, \"size\": 640, \"augment\": true}}}, {\"user_intent\": \"Scan multiple parking lot images for license plates and save the results.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/parking_lot_1.jpg\", \"conf\": 0.25, \"iou\": 0.5, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"size\": 640, \"augment\": false}}}, {\"user_intent\": \"Quickly check license plates in a low-resolution surveillance image.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/surveillance_frame.jpg\", \"conf\": 0.2, \"iou\": 0.4, \"agnostic\": true, \"multi_label\": false, \"max_det\": 200, \"size\": 320, \"augment\": true}}}]}\n\n# User Query\nOur client has a traffic camera system and wants to detect vehicles in the images captured. Implement a solution.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect vehicles in images captured by traffic cameras.",
                    "tool_description": "Use a YOLOv5-based object detection model to identify and localize vehicles in input images, providing bounding boxes and confidence scores for each detected vehicle.",
                    "expected_response": "An object detection result containing bounding box coordinates, confidence scores, and class indices for all detected vehicles in the image, along with optional visualization of detections."
                }
            ],
            "extracted_arguments": {
                "img": "The input image from the traffic camera, specified as a URL or local file path.",
                "conf": "The confidence threshold for detecting vehicles; numeric value used to filter low-confidence detections.",
                "iou": "The Intersection over Union (IoU) threshold for Non-Maximum Suppression to filter overlapping vehicle boxes; numeric value.",
                "agnostic": "Boolean indicating whether to perform class-agnostic NMS, ignoring vehicle class labels when suppressing overlaps.",
                "multi_label": "Boolean indicating whether each detected box can have multiple class labels.",
                "max_det": "Numeric value specifying the maximum number of vehicle detections allowed per image.",
                "size": "Numeric value for resizing the input image before detection.",
                "augment": "Boolean indicating whether to apply data augmentation during inference for potentially improved detection results."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_355",
        "query": "I would like to create an application that identifies animals in Chinese language image captions. Specifically, we want to know if a picture includes a cat or a dog.",
        "instruction": "Given an `image caption analysis` task, retrieve tools that perform image classification by processing image inputs and utilizing Chinese language models to identify specific animals such as cats or dogs in captions.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "OFA-Sys/chinese-clip-vit-base-patch16"
                    },
                    "python_environment_requirements": {
                        "transformers": "ChineseCLIPProcessor, ChineseCLIPModel"
                    },
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [, , , ]\ninputs = processor(images=image, return_tensors=pt)\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, padding=True, return_tensors=pt)\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)",
                    "performance": {
                        "dataset": {
                            "MUGE Text-to-Image Retrieval": {
                                "accuracy": {
                                    "Zero-shot R@1": 63.0,
                                    "Zero-shot R@5": 84.1,
                                    "Zero-shot R@10": 89.2,
                                    "Finetune R@1": 68.9,
                                    "Finetune R@5": 88.7,
                                    "Finetune R@10": 93.1
                                }
                            },
                            "Flickr30K-CN Retrieval": {
                                "accuracy": {
                                    "Zero-shot Text-to-Image R@1": 71.2,
                                    "Zero-shot Text-to-Image R@5": 91.4,
                                    "Zero-shot Text-to-Image R@10": 95.5,
                                    "Finetune Text-to-Image R@1": 83.8,
                                    "Finetune Text-to-Image R@5": 96.9,
                                    "Finetune Text-to-Image R@10": 98.6,
                                    "Zero-shot Image-to-Text R@1": 81.6,
                                    "Zero-shot Image-to-Text R@5": 97.5,
                                    "Zero-shot Image-to-Text R@10": 98.8,
                                    "Finetune Image-to-Text R@1": 95.3,
                                    "Finetune Image-to-Text R@5": 99.7,
                                    "Finetune Image-to-Text R@10": 100.0
                                }
                            },
                            "COCO-CN Retrieval": {
                                "accuracy": {
                                    "Zero-shot Text-to-Image R@1": 69.2,
                                    "Zero-shot Text-to-Image R@5": 89.9,
                                    "Zero-shot Text-to-Image R@10": 96.1,
                                    "Finetune Text-to-Image R@1": 81.5,
                                    "Finetune Text-to-Image R@5": 96.9,
                                    "Finetune Text-to-Image R@10": 99.1,
                                    "Zero-shot Image-to-Text R@1": 63.0,
                                    "Zero-shot Image-to-Text R@5": 86.6,
                                    "Zero-shot Image-to-Text R@10": 92.9,
                                    "Finetune Image-to-Text R@1": 83.5,
                                    "Finetune Image-to-Text R@5": 97.3,
                                    "Finetune Image-to-Text R@10": 99.2
                                }
                            },
                            "Zero-shot Image Classification": {
                                "accuracy": {
                                    "CIFAR10": 96.0,
                                    "CIFAR100": 79.7,
                                    "DTD": 51.2,
                                    "EuroSAT": 52.0,
                                    "FER": 55.1,
                                    "FGVC": 26.2,
                                    "KITTI": 49.9,
                                    "MNIST": 79.4,
                                    "PC": 63.5,
                                    "VOC": 84.9
                                }
                            }
                        }
                    },
                    "description": "Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.",
                    "id": "huggingface_api_367",
                    "name": "OFA-Sys/chinese-clip-vit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_363",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 1: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.",
            "Tool 2: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 3: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.",
            "Tool 4: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 5: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 6: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 7: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 8: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.",
            "Tool 9: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 11: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 12: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.",
            "Tool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 14: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 15: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 17: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 18: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 19: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required."
        ],
        "tool_view": {
            "description": "Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "parameters": {
                "pretrained_model": {
                    "type": "string",
                    "description": "The name or path of the pretrained model to load for image generation.",
                    "required": true,
                    "example": "schdoel/sd-class-AFHQ-32"
                }
            },
            "response": "The tool generates a single image (or a batch of images) using an unconditional diffusion model. The response structure can be described as follows:\n\n- **images**: This is the primary component of the response. It is typically a list or array of generated images. Each image is represented as an image object compatible with standard Python imaging libraries (such as a PIL `Image` object). In the example usage, only the first image is accessed via `images[0]`.\n\n- **type/format**: Each image in the response is in a standard visual format (e.g., RGB), suitable for display, saving to disk, or further processing.\n\n- **content**: The images contain the output of the unconditional diffusion process. Since the model is unconditional, each generated image is produced without any user-provided prompt or conditioning input. The content reflects the distribution learned by the model from its training data (in this case, the AFHQ dataset, focusing on cute animals).\n\nThere are no additional metadata fields explicitly defined in the tool documentation. The primary purpose of the response is to provide the generated image(s) ready for visualization or downstream use.",
            "examples": [
                {
                    "user_intent": "I want to generate a random image of a cute animal for my project.",
                    "tool_call": {
                        "name": "sd-class-pandas-32",
                        "arguments": {
                            "pretrained_model": "schdoel/sd-class-AFHQ-32"
                        }
                    }
                },
                {
                    "user_intent": "Create a unique, high-quality image of a cat without providing any prompt.",
                    "tool_call": {
                        "name": "sd-class-pandas-32",
                        "arguments": {
                            "pretrained_model": "schdoel/sd-class-AFHQ-32"
                        }
                    }
                },
                {
                    "user_intent": "I need a random image for testing my image recognition software using the AFHQ dataset style.",
                    "tool_call": {
                        "name": "sd-class-pandas-32",
                        "arguments": {
                            "pretrained_model": "schdoel/sd-class-AFHQ-32"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 1: BLIP is a state-of-the-art Vision-Language Pre-training (VLP) framework designed for tasks like image captioning, image-text retrieval, and Visual Question Answering (VQA). By leveraging large-scale noisy web data, it generates and filters captions for improved accuracy. This model excels in generating captions for images and can optionally take text as input to guide the captioning process. It achieves strong performance, including a 2.8% improvement in CIDEr for image captioning tasks on the COCO dataset.\nTool 2: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 3: This tool provides zero-shot image classification using the Chinese CLIP model, which is trained on a large dataset of approximately 200 million Chinese image-text pairs. It leverages a Vision Transformer (ViT-B/16) as the image encoder and a RoBERTa-based text encoder to match images with textual descriptions without requiring task-specific fine-tuning. It is designed for applications like image retrieval and classification in Chinese contexts, offering strong performance across benchmarks such as CIFAR, COCO-CN, Flickr30K-CN, and MUGE.\nTool 4: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 5: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 6: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 7: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 8: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.\nTool 9: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 11: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 12: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.\nTool 13: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 14: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 15: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 16: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 17: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 18: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 19: A transformer-based language detection tool built on a fine-tuned XLM-RoBERTa model that accurately identifies the language of a given text. It supports 20 languages and can be used for sequence classification tasks where determining the text’s language is required.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\", \"parameters\": {\"pretrained_model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained model to load for image generation.\", \"required\": true, \"example\": \"schdoel/sd-class-AFHQ-32\"}}, \"response\": \"The tool generates a single image (or a batch of images) using an unconditional diffusion model. The response structure can be described as follows:\\n\\n- **images**: This is the primary component of the response. It is typically a list or array of generated images. Each image is represented as an image object compatible with standard Python imaging libraries (such as a PIL `Image` object). In the example usage, only the first image is accessed via `images[0]`.\\n\\n- **type/format**: Each image in the response is in a standard visual format (e.g., RGB), suitable for display, saving to disk, or further processing.\\n\\n- **content**: The images contain the output of the unconditional diffusion process. Since the model is unconditional, each generated image is produced without any user-provided prompt or conditioning input. The content reflects the distribution learned by the model from its training data (in this case, the AFHQ dataset, focusing on cute animals).\\n\\nThere are no additional metadata fields explicitly defined in the tool documentation. The primary purpose of the response is to provide the generated image(s) ready for visualization or downstream use.\", \"examples\": [{\"user_intent\": \"I want to generate a random image of a cute animal for my project.\", \"tool_call\": {\"name\": \"sd-class-pandas-32\", \"arguments\": {\"pretrained_model\": \"schdoel/sd-class-AFHQ-32\"}}}, {\"user_intent\": \"Create a unique, high-quality image of a cat without providing any prompt.\", \"tool_call\": {\"name\": \"sd-class-pandas-32\", \"arguments\": {\"pretrained_model\": \"schdoel/sd-class-AFHQ-32\"}}}, {\"user_intent\": \"I need a random image for testing my image recognition software using the AFHQ dataset style.\", \"tool_call\": {\"name\": \"sd-class-pandas-32\", \"arguments\": {\"pretrained_model\": \"schdoel/sd-class-AFHQ-32\"}}}]}\n\n# User Query\nI would like to create an application that identifies animals in Chinese language image captions. Specifically, we want to know if a picture includes a cat or a dog.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify an image to determine if it contains a cat or a dog.",
                    "tool_description": "Use a pre-trained image classification model to identify whether the input image depicts a cat or a dog, suitable for quick binary categorization.",
                    "expected_response": "A classification result indicating 'cat' or 'dog' along with associated confidence scores."
                },
                {
                    "user_intent": "I want to generate Chinese captions for the image for further analysis.",
                    "tool_description": "Apply a Chinese image captioning model to produce a textual description of the image content in Chinese, enabling downstream text-based processing or analysis.",
                    "expected_response": "A Chinese-language caption describing the main objects and scene in the image."
                },
                {
                    "user_intent": "Check the generated Chinese caption to see if it mentions a cat or a dog.",
                    "tool_description": "Perform keyword matching or text analysis on the Chinese caption to identify mentions of specific animals such as 'cat' or 'dog'.",
                    "expected_response": "A boolean or label indicating whether the caption mentions a cat, a dog, or neither."
                }
            ],
            "extracted_arguments": {
                "image_file": "The input image to be analyzed, provided as a file or image object.",
                "language": "The target language for the caption, specifically Chinese.",
                "animal_labels": "The set of animal categories to detect, e.g., 'cat' and 'dog'."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_30",
        "query": "Design a pipeline to create artistic variations of an input image.",
        "instruction": "Given an `image variation design` task, retrieve tools that process image inputs to generate artistic variations by utilizing stable diffusion techniques and ensure compatibility with specified frameworks and environment requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Variations",
                    "api_call": "StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')",
                    "api_arguments": {
                        "revision": "v2.0"
                    },
                    "python_environment_requirements": "Diffusers >=0.8.0",
                    "example_code": "from diffusers import StableDiffusionImageVariationPipeline\nfrom PIL import Image\ndevice = cuda:0\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n lambdalabs/sd-image-variations-diffusers,\n revision=v2.0,\n)\nsd_pipe = sd_pipe.to(device)\nim = Image.open(path/to/image.jpg)\ntform = transforms.Compose([\n transforms.ToTensor(),\n transforms.Resize(\n  (224, 224),\n  interpolation=transforms.InterpolationMode.BICUBIC,\n  antialias=False,\n ),\n transforms.Normalize(\n  [0.48145466, 0.4578275, 0.40821073],\n  [0.26862954, 0.26130258, 0.27577711]),\n])\ninp = tform(im).to(device).unsqueeze(0)\nout = sd_pipe(inp, guidance_scale=3)\nout[images][0].save(result.jpg)",
                    "performance": {
                        "dataset": "ChristophSchuhmann/improved_aesthetics_6plus",
                        "accuracy": "N/A"
                    },
                    "description": "This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.",
                    "id": "huggingface_api_260",
                    "name": "lambdalabs/sd-image-variations-diffusers"
                },
                "id": "gorilla_huggingface_tool_256",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 1: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 2: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 3: Lykon/DreamShaper is a text-to-image model that generates artistic images based on input text. It leverages the Hugging Face framework and is implemented using the transformers and torch libraries.",
            "Tool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 6: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 7: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 8: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 9: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 12: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 13: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 16: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 17: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 18: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts."
        ],
        "tool_view": {
            "description": "This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "parameters": {
                "revision": {
                    "type": "string",
                    "description": "The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.",
                    "required": true,
                    "default": "v2.0",
                    "example": "v2.0"
                }
            },
            "response": "The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\n\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \n\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.",
            "examples": [
                {
                    "user_intent": "I want to generate variations of an image using the Stable Diffusion model with the latest version.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Create new variations of a given image using the Stable Diffusion image variation pipeline.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 1: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 2: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 3: Lykon/DreamShaper is a text-to-image model that generates artistic images based on input text. It leverages the Hugging Face framework and is implemented using the transformers and torch libraries.\nTool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 6: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 7: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 8: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 9: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 10: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 11: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 12: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 13: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 14: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 16: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 17: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 18: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL\\u00b7E 2. The tool is based on the \\\"lambdalabs/sd-image-variations-diffusers\\\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\", \"parameters\": {\"revision\": {\"type\": \"string\", \"description\": \"The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.\", \"required\": true, \"default\": \"v2.0\", \"example\": \"v2.0\"}}, \"response\": \"The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\\n\\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \\n\\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.\", \"examples\": [{\"user_intent\": \"I want to generate variations of an image using the Stable Diffusion model with the latest version.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Create new variations of a given image using the Stable Diffusion image variation pipeline.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}]}\n\n# User Query\nDesign a pipeline to create artistic variations of an input image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate artistic variations of a given image using a Stable Diffusion-based model.",
                    "tool_description": "Generate multiple image variations from an input image using a Stable Diffusion pipeline fine-tuned to accept CLIP image embeddings, enabling artistic transformations and style changes.",
                    "expected_response": "A collection of generated image objects representing different artistic variations of the input image."
                }
            ],
            "extracted_arguments": {
                "revision": "The specific version of the image variation model to be used in the pipeline, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_323",
        "query": "I would like to create an application where users may listen to translations of English sentences. I need a Text-to-Speech model to support this functionality.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that enable transforming text inputs into audio speech, supporting functionalities such as translating sentences to aid in the creation of applications where users can listen to spoken translations.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "pipeline('text-to-speech', model='mio/Artoria')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('s')",
                    "performance": {
                        "dataset": "fate",
                        "accuracy": "Not provided"
                    },
                    "description": "This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.",
                    "id": "huggingface_api_726",
                    "name": "mio/Artoria"
                },
                "id": "gorilla_huggingface_tool_721",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 1: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 5: This tool provides automatic speech recognition using the fine-tuned Wav2Vec2 model from Hugging Face, specifically the \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" version. It transcribes audio files (such as MP3 or WAV) into text, with support for English language inputs sampled at 16kHz. The model is optimized using the Common Voice 6.1 dataset and achieves competitive accuracy, with a Word Error Rate (WER) of 19.06%.",
            "Tool 6: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 7: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 8: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 9: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 10: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.",
            "Tool 11: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 14: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 15: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 16: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 17: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 18: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.",
            "Tool 19: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The English text string that you want to translate into Arabic.",
                    "required": true,
                    "example": "Hello World"
                }
            },
            "response": "The tool returns a structured translation output for a given English text input. The response is typically a list of one or more objects, where each object contains the translated text and possibly additional metadata. The key components of a typical response are:\n\n- **`translation_text`**: A string containing the translated Arabic text corresponding to the input English text. This is the primary content of the response.  \n- **Optional metadata fields** (depending on the pipeline implementation) may include:  \n  - **`score`**: A numeric value representing the model’s confidence or likelihood associated with the translation.  \n  - **Other internal identifiers**: Some pipelines may include auxiliary information such as tokenization details or intermediate model outputs, though these are not always exposed in the default output.  \n\nIn summary, the response is a list where each element represents one translation result, with `translation_text` being the main field containing the Arabic translation of the input string.",
            "examples": [
                {
                    "user_intent": "Translate a simple greeting from English to Arabic.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-en-ar",
                        "arguments": {
                            "text": "Hello, how are you?"
                        }
                    }
                },
                {
                    "user_intent": "Convert an English news headline to Arabic.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-en-ar",
                        "arguments": {
                            "text": "Global markets see a significant rise in stock prices today."
                        }
                    }
                },
                {
                    "user_intent": "Translate a short English paragraph into Arabic for a language learning app.",
                    "tool_call": {
                        "name": "Helsinki-NLP/opus-mt-en-ar",
                        "arguments": {
                            "text": "The cat sat on the windowsill, enjoying the warm sunlight while birds chirped outside."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 1: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 4: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 5: This tool provides automatic speech recognition using the fine-tuned Wav2Vec2 model from Hugging Face, specifically the \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" version. It transcribes audio files (such as MP3 or WAV) into text, with support for English language inputs sampled at 16kHz. The model is optimized using the Common Voice 6.1 dataset and achieves competitive accuracy, with a Word Error Rate (WER) of 19.06%.\nTool 6: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 7: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 8: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 9: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 10: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.\nTool 11: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 12: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 13: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 14: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 15: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 16: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 17: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 18: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\nTool 19: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The English text string that you want to translate into Arabic.\", \"required\": true, \"example\": \"Hello World\"}}, \"response\": \"The tool returns a structured translation output for a given English text input. The response is typically a list of one or more objects, where each object contains the translated text and possibly additional metadata. The key components of a typical response are:\\n\\n- **`translation_text`**: A string containing the translated Arabic text corresponding to the input English text. This is the primary content of the response.  \\n- **Optional metadata fields** (depending on the pipeline implementation) may include:  \\n  - **`score`**: A numeric value representing the model\\u2019s confidence or likelihood associated with the translation.  \\n  - **Other internal identifiers**: Some pipelines may include auxiliary information such as tokenization details or intermediate model outputs, though these are not always exposed in the default output.  \\n\\nIn summary, the response is a list where each element represents one translation result, with `translation_text` being the main field containing the Arabic translation of the input string.\", \"examples\": [{\"user_intent\": \"Translate a simple greeting from English to Arabic.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-en-ar\", \"arguments\": {\"text\": \"Hello, how are you?\"}}}, {\"user_intent\": \"Convert an English news headline to Arabic.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-en-ar\", \"arguments\": {\"text\": \"Global markets see a significant rise in stock prices today.\"}}}, {\"user_intent\": \"Translate a short English paragraph into Arabic for a language learning app.\", \"tool_call\": {\"name\": \"Helsinki-NLP/opus-mt-en-ar\", \"arguments\": {\"text\": \"The cat sat on the windowsill, enjoying the warm sunlight while birds chirped outside.\"}}}]}\n\n# User Query\nI would like to create an application where users may listen to translations of English sentences. I need a Text-to-Speech model to support this functionality.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert English text into spoken audio for users to listen to translations.",
                    "tool_description": "A text-to-speech model that synthesizes high-quality audio from English text input, generating natural and expressive speech suitable for applications like voice assistants, narration, or accessibility tools.",
                    "expected_response": "An audio output file or stream representing the spoken version of the input English text."
                }
            ],
            "extracted_arguments": {
                "text_input": "The English text string that needs to be converted into speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_36",
        "query": "Design a model to classify the following image: a city park with a playground and a lake, surrounded by trees and skyscrapers.",
        "instruction": "Given an `image classification` task, retrieve tools that perform zero-shot image classification by processing image inputs and using pre-defined categories to assign the most relevant labels, specifically within a city park context.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')",
                    "api_arguments": {
                        "text": [
                            "a photo of a residential area",
                            "a photo of a playground",
                            "a photo of a stadium",
                            "a photo of a forest",
                            "a photo of an airport"
                        ],
                        "images": "image",
                        "return_tensors": "pt",
                        "padding": "True"
                    },
                    "python_environment_requirements": [
                        "PIL",
                        "requests",
                        "transformers"
                    ],
                    "example_code": "from PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nlabels = [residential area, playground, stadium, forest, airport]\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\nfor l, p in zip(labels, probs[0]):\n print(f{l:&lt;16} {p:.4f})",
                    "performance": {
                        "dataset": {
                            "RSICD": {
                                "original CLIP": {
                                    "k=1": 0.572,
                                    "k=3": 0.745,
                                    "k=5": 0.837,
                                    "k=10": 0.939
                                },
                                "clip-rsicd-v2 (this model)": {
                                    "k=1": 0.883,
                                    "k=3": 0.968,
                                    "k=5": 0.982,
                                    "k=10": 0.998
                                }
                            }
                        }
                    },
                    "description": "This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.",
                    "id": "huggingface_api_360",
                    "name": "flax-community/clip-rsicd-v2"
                },
                "id": "gorilla_huggingface_tool_356",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 1: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 2: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 3: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 4: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 6: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "Tool 7: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.",
            "Tool 8: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.",
            "Tool 10: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 11: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 12: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 14: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.",
            "Tool 15: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 16: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 19: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template."
        ],
        "tool_view": {
            "description": "Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description of the image to generate. The model uses this prompt to guide the image creation process.",
                    "required": true,
                    "example": "a photo of an astronaut riding a horse on mars"
                }
            },
            "response": "The tool produces a generated image based on a given textual prompt. The response structure can be described as follows:\n\n- **images**: A collection (typically a list or array) containing one or more image objects produced by the model. Each element represents a single generated image corresponding to the input prompt.  \n  - **type**: Image object (often in a format compatible with Python imaging libraries, e.g., PIL.Image).  \n  - **role**: Encodes the visual output generated from the text prompt. Users can save, display, or further process this image.\n\n- **metadata (optional, depending on implementation)**: Some pipelines may include additional information about the generation process, such as:  \n  - **prompt**: The text prompt used to generate the image.  \n  - **model_id**: Identifier of the model used.  \n  - **scheduler_info**: Details of the scheduler employed during diffusion.  \n  - **generation_settings**: Optional settings like guidance scale, number of inference steps, or image dimensions.\n\nIn essence, the primary semantic content of the response is the generated image(s) that visually represent the input prompt, while optional metadata provides context about how the image was produced.",
            "examples": [
                {
                    "user_intent": "Create a realistic image of a futuristic city skyline at sunset.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2",
                        "arguments": {
                            "prompt": "a futuristic city skyline at sunset with flying cars and neon lights"
                        }
                    }
                },
                {
                    "user_intent": "Generate an artistic painting of a fantasy dragon perched on a mountain peak.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2",
                        "arguments": {
                            "prompt": "an oil painting of a majestic dragon perched on a snowy mountain peak, fantasy style"
                        }
                    }
                },
                {
                    "user_intent": "Produce a cute cartoon-style illustration of a cat baking cupcakes in a kitchen.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2",
                        "arguments": {
                            "prompt": "a cute cartoon cat baking cupcakes in a cozy kitchen, colorful and whimsical"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 1: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 2: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 3: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 4: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 5: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 6: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.\nTool 7: A Hugging Face Transformers model for image classification that uses the SegFormer encoder fine-tuned on ImageNet-1k, enabling accurate prediction of image classes from input images. It is designed for efficient and effective image classification leveraging a hierarchical Transformer architecture and can be used to classify a wide variety of images in computer vision applications.\nTool 8: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 9: A zero-shot image classification tool based on the CLIP ViT-B/16 model trained on the LAION-2B dataset, allowing users to classify images without task-specific training by providing a list of possible class names. It can handle images from local files or URLs and is useful for rapid image categorization, research, and tasks involving image-text understanding.\nTool 10: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 11: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 12: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 13: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 14: This tool enables zero-shot image classification using the \"openai/clip-vit-large-patch14-336\" model, which can classify images without requiring task-specific training. It utilizes the CLIP model from OpenAI, designed to interpret images in conjunction with natural language prompts, making it suitable for diverse image classification tasks.\nTool 15: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 16: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 17: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 18: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 19: A Korean CLIP-based zero-shot image classification model that uses multilingual sentence embeddings to classify images without any task-specific training data, allowing users to assign labels to images directly by providing candidate labels and a hypothesis template.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description of the image to generate. The model uses this prompt to guide the image creation process.\", \"required\": true, \"example\": \"a photo of an astronaut riding a horse on mars\"}}, \"response\": \"The tool produces a generated image based on a given textual prompt. The response structure can be described as follows:\\n\\n- **images**: A collection (typically a list or array) containing one or more image objects produced by the model. Each element represents a single generated image corresponding to the input prompt.  \\n  - **type**: Image object (often in a format compatible with Python imaging libraries, e.g., PIL.Image).  \\n  - **role**: Encodes the visual output generated from the text prompt. Users can save, display, or further process this image.\\n\\n- **metadata (optional, depending on implementation)**: Some pipelines may include additional information about the generation process, such as:  \\n  - **prompt**: The text prompt used to generate the image.  \\n  - **model_id**: Identifier of the model used.  \\n  - **scheduler_info**: Details of the scheduler employed during diffusion.  \\n  - **generation_settings**: Optional settings like guidance scale, number of inference steps, or image dimensions.\\n\\nIn essence, the primary semantic content of the response is the generated image(s) that visually represent the input prompt, while optional metadata provides context about how the image was produced.\", \"examples\": [{\"user_intent\": \"Create a realistic image of a futuristic city skyline at sunset.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2\", \"arguments\": {\"prompt\": \"a futuristic city skyline at sunset with flying cars and neon lights\"}}}, {\"user_intent\": \"Generate an artistic painting of a fantasy dragon perched on a mountain peak.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2\", \"arguments\": {\"prompt\": \"an oil painting of a majestic dragon perched on a snowy mountain peak, fantasy style\"}}}, {\"user_intent\": \"Produce a cute cartoon-style illustration of a cat baking cupcakes in a kitchen.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2\", \"arguments\": {\"prompt\": \"a cute cartoon cat baking cupcakes in a cozy kitchen, colorful and whimsical\"}}}]}\n\n# User Query\nDesign a model to classify the following image: a city park with a playground and a lake, surrounded by trees and skyscrapers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to design a model to classify an image based on its description.",
                    "tool_description": "Generate a classification model to predict categories for images based on their content, using predefined class labels or zero-shot classification capabilities.",
                    "expected_response": "A classification result, indicating the predicted category of the image."
                }
            ],
            "extracted_arguments": {
                "image_description": "A text-based description of the image's content to guide the classification model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_3",
        "query": "We need to create automated transcripts from recorded podcasts that include punctuation for better readability.",
        "instruction": "Given an `automated transcription with punctuation` task, retrieve tools that process audio inputs, specifically recorded podcasts, to generate accurate and readable textual transcriptions that include punctuation marks, enhancing the text's clarity and prosody.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "https://github.com/neonbjb/ocotillo",
                    "performance": {
                        "dataset": "librispeech validation set",
                        "accuracy": "4.45%"
                    },
                    "description": "This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.",
                    "id": "huggingface_api_748",
                    "name": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli"
                },
                "id": "gorilla_huggingface_tool_743",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 2: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 6: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 7: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 8: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 9: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 11: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 12: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 15: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 17: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 18: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words."
        ],
        "tool_view": {
            "description": "This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.",
                    "required": true,
                    "example": "My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller"
                }
            },
            "response": "The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\n\n### Expected Response Description:\n\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\n- Periods (.)\n- Commas (,)\n- Question marks (?)\n- Dashes (-)\n- Colons (:)\n\n### Key Components of the Response:\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\n  \n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\n\n",
            "examples": [
                {
                    "user_intent": "I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps"
                        }
                    }
                },
                {
                    "user_intent": "I need proper punctuation for a German interview transcript I just transcribed.",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview"
                        }
                    }
                },
                {
                    "user_intent": "Can you add punctuation to this Italian speech transcription so I can publish it as an article?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 2: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 6: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 7: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 8: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 9: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 10: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 11: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 12: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 13: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 15: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 16: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 17: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 18: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.\", \"required\": true, \"example\": \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\\u00fcller\"}}, \"response\": \"The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\\n\\n### Expected Response Description:\\n\\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\\n- Periods (.)\\n- Commas (,)\\n- Question marks (?)\\n- Dashes (-)\\n- Colons (:)\\n\\n### Key Components of the Response:\\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\\n  \\n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\\n\\n\", \"examples\": [{\"user_intent\": \"I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps\"}}}, {\"user_intent\": \"I need proper punctuation for a German interview transcript I just transcribed.\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview\"}}}, {\"user_intent\": \"Can you add punctuation to this Italian speech transcription so I can publish it as an article?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo\"}}}]}\n\n# User Query\nWe need to create automated transcripts from recorded podcasts that include punctuation for better readability.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transcribe the audio from the podcast into text.",
                    "tool_description": "Use an automatic speech recognition model to convert recorded podcast audio into raw text transcription.",
                    "expected_response": "A text string containing the spoken content from the podcast, without punctuation."
                },
                {
                    "user_intent": "Add punctuation to the transcribed podcast text for better readability.",
                    "tool_description": "Predict and restore appropriate punctuation marks in the transcribed text to improve readability and reflect natural sentence structures.",
                    "expected_response": "A text string containing the original transcription with predicted punctuation marks (periods, commas, question marks, dashes, and colons) correctly inserted."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The recorded podcast audio file or stream to be transcribed.",
                "transcribed_text": "The raw text obtained from the speech-to-text transcription, used as input for punctuation prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_469",
        "query": "We are opening a platform where users can record their own podcast and host it on our platform, can you help us to convert the audio into text automatically?",
        "instruction": "Given an `audio-to-text conversion` task, retrieve tools designed for automatic speech recognition by processing audio signals and generating accurate textual transcriptions that can be used to host podcasts on a platform.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Transformers",
                    "functionality": "Transcription",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')",
                    "api_arguments": [
                        "input_values"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "torch",
                        "jiwer"
                    ],
                    "example_code": "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom datasets import load_dataset\nimport torch\n\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\nlogits = model(input_values).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)",
                    "performance": {
                        "dataset": "LibriSpeech",
                        "accuracy": {
                            "clean": 3.4,
                            "other": 8.6
                        }
                    },
                    "description": "Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.",
                    "id": "huggingface_api_751",
                    "name": "facebook/wav2vec2-base-960h"
                },
                "id": "gorilla_huggingface_tool_746",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 2: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 3: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 6: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 9: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 11: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 12: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 13: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.",
            "Tool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.",
            "Tool 15: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 16: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 19: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.",
                    "required": true,
                    "example": "Hello I'm a <mask> model."
                }
            },
            "response": "The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\n\n1. **`sequence`** – The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \n\n2. **`score`** – A numerical value representing the model’s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \n\n3. **`token`** – The integer ID of the predicted token in the model’s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \n\n4. **`token_str`** – The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \n\nThe response is typically ordered by descending confidence, so the first entry is the model’s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \n\nThis response allows users to understand the model’s likely substitutions for a masked word in context, along with a measure of confidence for each option.",
            "examples": [
                {
                    "user_intent": "I want to predict the missing word in this sentence: 'The weather today is <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "The weather today is <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "Artificial intelligence is transforming <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'",
                    "tool_call": {
                        "name": "roberta-base",
                        "arguments": {
                            "text": "I can't wait to eat some <mask> tonight!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 1: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 2: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 3: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 4: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 5: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 6: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 9: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 10: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 11: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 12: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 13: This tool is a speech-to-speech translation model that converts between languages without using text as an intermediate step. It is designed for audio-to-audio translation, making it useful for tasks where direct audio-based translation is needed.\nTool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\nTool 15: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 16: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 19: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text containing a masked token, indicated by <mask>, that the model will attempt to predict.\", \"required\": true, \"example\": \"Hello I'm a <mask> model.\"}}, \"response\": \"The tool returns a list of predictions for the masked token(s) in the input text. Each prediction in the response typically includes the following components:\\n\\n1. **`sequence`** \\u2013 The input text with the mask replaced by the predicted token, showing the full reconstructed sentence. This allows the user to see how the model fills in the masked position in context.  \\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability for this particular token prediction. Higher scores indicate higher likelihood according to the model.  \\n\\n3. **`token`** \\u2013 The integer ID of the predicted token in the model\\u2019s vocabulary. This is useful for internal processing or mapping to embeddings but is generally less important for casual interpretation.  \\n\\n4. **`token_str`** \\u2013 The string representation of the predicted token, which is the actual text that replaces the mask. This is the most interpretable part of the prediction for users.  \\n\\nThe response is typically ordered by descending confidence, so the first entry is the model\\u2019s top prediction for the masked token, followed by alternative plausible completions. The number of predictions returned can usually be configured via parameters but defaults to a small set (e.g., top 5).  \\n\\nThis response allows users to understand the model\\u2019s likely substitutions for a masked word in context, along with a measure of confidence for each option.\", \"examples\": [{\"user_intent\": \"I want to predict the missing word in this sentence: 'The weather today is <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"The weather today is <mask>.\"}}}, {\"user_intent\": \"Fill in the blank for this sentence about technology: 'Artificial intelligence is transforming <mask>.'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"Artificial intelligence is transforming <mask>.\"}}}, {\"user_intent\": \"Guess the missing word in this casual sentence: 'I can't wait to eat some <mask> tonight!'\", \"tool_call\": {\"name\": \"roberta-base\", \"arguments\": {\"text\": \"I can't wait to eat some <mask> tonight!\"}}}]}\n\n# User Query\nWe are opening a platform where users can record their own podcast and host it on our platform, can you help us to convert the audio into text automatically?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to convert audio recordings into text automatically.",
                    "tool_description": "Transcribe spoken audio into text using a pre-trained speech recognition model, allowing users to convert their podcast audio into a textual format.",
                    "expected_response": "A text transcription of the audio input, containing the spoken words in a structured format."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The audio file containing the recorded speech for transcription."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_357",
        "query": "We are a product company selling personalized gadgets, and we want to build a recommender system that shows generated, high-resolution images of human faces on our website.",
        "instruction": "Given an `image generation` task, retrieve tools capable of producing high-resolution images, specifically focusing on the generation of human faces, by utilizing advanced models for creating quality visuals aligned with the query’s requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')",
                    "api_arguments": {
                        "model_id": "google/ncsnpp-celebahq-256"
                    },
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DiffusionPipeline\nmodel_id = google/ncsnpp-celebahq-256\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\nimage = sde_ve()[sample]\nimage[0].save(sde_ve_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR-10",
                        "accuracy": {
                            "Inception_score": 9.89,
                            "FID": 2.2,
                            "likelihood": 2.99
                        }
                    },
                    "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "id": "huggingface_api_293",
                    "name": "google/ncsnpp-celebahq-256"
                },
                "id": "gorilla_huggingface_tool_289",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 1: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 2: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 3: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 4: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 5: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.",
            "Tool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 7: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 8: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 9: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 10: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 11: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.",
            "Tool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 13: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 15: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 16: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 17: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 18: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 19: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features."
        ],
        "tool_view": {
            "description": "Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "parameters": {
                "model_id": {
                    "type": "string",
                    "description": "The identifier of the pretrained Latent Diffusion Model (LDM) to load from Hugging Face. This determines which model weights and configuration will be used for image generation.",
                    "required": true,
                    "example": "CompVis/ldm-celebahq-256"
                }
            },
            "response": "The tool produces a generated image as its response. The response structure can be described as follows:\n\n- **Primary Output**: A single image or a batch of images generated unconditionally by the diffusion model. Each image is represented as an in-memory image object compatible with standard image handling libraries (e.g., PIL Image in Python).  \n\n- **Optional Metadata**: Depending on the pipeline implementation, the response may include additional information such as:\n  - **`sample` or `images` field**: A list or array of generated images.\n  - **Generation parameters**: Information about the inference settings used, such as `num_inference_steps` or random seed values.\n  \n- **Semantic Role**: The output image(s) represent the model’s unconditional synthesis of realistic human faces, consistent with the training distribution (CelebA-HQ dataset). No textual prompt or conditional input is required for generation.  \n\nIn summary, the tool’s response is fundamentally a high-resolution image object (or a collection thereof), optionally accompanied by metadata describing the generation process, intended for saving, display, or further processing.",
            "examples": [
                {
                    "user_intent": "Generate a high-resolution realistic portrait of a person using AI.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                },
                {
                    "user_intent": "I want to create a batch of AI-generated celebrity-style images for testing a facial recognition model.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                },
                {
                    "user_intent": "Generate a realistic face image for use in an artwork project without providing any initial image or prompt.",
                    "tool_call": {
                        "name": "DiffusionPipeline.from_pretrained",
                        "arguments": {
                            "model_id": "CompVis/ldm-celebahq-256"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 1: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 2: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 3: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 4: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 5: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.\nTool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 7: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 8: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 9: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 10: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 11: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.\nTool 12: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 13: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 15: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 16: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 17: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 18: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 19: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\", \"parameters\": {\"model_id\": {\"type\": \"string\", \"description\": \"The identifier of the pretrained Latent Diffusion Model (LDM) to load from Hugging Face. This determines which model weights and configuration will be used for image generation.\", \"required\": true, \"example\": \"CompVis/ldm-celebahq-256\"}}, \"response\": \"The tool produces a generated image as its response. The response structure can be described as follows:\\n\\n- **Primary Output**: A single image or a batch of images generated unconditionally by the diffusion model. Each image is represented as an in-memory image object compatible with standard image handling libraries (e.g., PIL Image in Python).  \\n\\n- **Optional Metadata**: Depending on the pipeline implementation, the response may include additional information such as:\\n  - **`sample` or `images` field**: A list or array of generated images.\\n  - **Generation parameters**: Information about the inference settings used, such as `num_inference_steps` or random seed values.\\n  \\n- **Semantic Role**: The output image(s) represent the model\\u2019s unconditional synthesis of realistic human faces, consistent with the training distribution (CelebA-HQ dataset). No textual prompt or conditional input is required for generation.  \\n\\nIn summary, the tool\\u2019s response is fundamentally a high-resolution image object (or a collection thereof), optionally accompanied by metadata describing the generation process, intended for saving, display, or further processing.\", \"examples\": [{\"user_intent\": \"Generate a high-resolution realistic portrait of a person using AI.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}, {\"user_intent\": \"I want to create a batch of AI-generated celebrity-style images for testing a facial recognition model.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}, {\"user_intent\": \"Generate a realistic face image for use in an artwork project without providing any initial image or prompt.\", \"tool_call\": {\"name\": \"DiffusionPipeline.from_pretrained\", \"arguments\": {\"model_id\": \"CompVis/ldm-celebahq-256\"}}}]}\n\n# User Query\nWe are a product company selling personalized gadgets, and we want to build a recommender system that shows generated, high-resolution images of human faces on our website.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate high-resolution realistic human face images for use in a product recommendation interface.",
                    "tool_description": "Use a Latent Diffusion Model trained on the CelebA-HQ dataset to generate unconditional, high-quality, high-resolution images of human faces for display in web applications or recommender systems.",
                    "expected_response": "A collection of high-resolution human face images, represented as image objects compatible with standard image handling libraries, optionally with metadata about generation parameters."
                }
            ],
            "extracted_arguments": {
                "model_id": "The identifier of the pretrained Latent Diffusion Model (LDM) to load, determining which model weights and configuration will be used for image generation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_117",
        "query": "We need a system to control the access into parking lot. Analyze an image and find out the license plate numbers to detect whether it's an authorized vehicle or not.",
        "instruction": "Given a `license plate recognition` task, retrieve tools that analyze images to identify and extract license plate numbers, allowing for the verification of vehicle authorization based on these attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "License Plate Detection",
                    "api_call": "yolov5.load('keremberke/yolov5m-license-plate')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": [
                        "import yolov5",
                        "model = yolov5.load('keremberke/yolov5m-license-plate')",
                        "model.conf = 0.25",
                        "model.iou = 0.45",
                        "model.agnostic = False",
                        "model.multi_label = False",
                        "model.max_det = 1000",
                        "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'",
                        "results = model(img, size=640)",
                        "results = model(img, augment=True)",
                        "predictions = results.pred[0]",
                        "boxes = predictions[:, :4]",
                        "scores = predictions[:, 4]",
                        "categories = predictions[:, 5]",
                        "results.show()",
                        "results.save(save_dir='results/')"
                    ],
                    "performance": {
                        "dataset": "keremberke/license-plate-object-detection",
                        "accuracy": 0.988
                    },
                    "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.",
                    "id": "huggingface_api_214",
                    "name": "keremberke/yolov5m-license-plate"
                },
                "id": "gorilla_huggingface_tool_210",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 1: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.",
            "Tool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 4: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 6: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 7: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 8: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 9: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.",
            "Tool 10: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 11: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 12: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 13: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 14: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 15: ControlNet is a neural network designed to enhance diffusion models by adding additional conditioning factors. This specific checkpoint, \"lllyasviel/control_v11p_sd15_lineart,\" is tailored to work with lineart images, allowing for more precise control in image generation tasks. It can be integrated with Hugging Face’s frameworks, particularly for tasks involving image-to-image transformations, and requires specific Python dependencies for setup.",
            "Tool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 17: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 18: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 19: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications."
        ],
        "tool_view": {
            "description": "A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "multi_label": {
                    "type": "boolean",
                    "description": "If True, allows each detected box to have multiple class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                },
                "img": {
                    "type": "string",
                    "description": "The input image for detection. Can be a URL or local file path.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                },
                "size": {
                    "type": "number",
                    "description": "The size to which the input image will be resized before detection.",
                    "required": true,
                    "default": 640,
                    "example": 512
                },
                "augment": {
                    "type": "boolean",
                    "description": "If True, applies data augmentation during inference for potentially improved results.",
                    "required": true,
                    "default": true,
                    "example": false
                }
            },
            "response": "The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\n\n1. **pred**  \n   - A list (or tensor) containing the detection predictions for each input image.  \n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \n   - Each row contains:  \n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \n     - **Confidence score** (usually the fifth element) indicating the model’s certainty for that detection.  \n     - **Class index** (usually the sixth element) representing the detected object’s category; here it corresponds to the license plate class.\n\n2. **boxes**  \n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\n\n3. **scores**  \n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\n\n4. **categories**  \n   - Extracted from `pred`, this array contains the class indices for each detection.\n\n5. **Visualization methods**  \n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\n\n6. **Additional metadata** (implicit)  \n   - Number of detections returned is limited by `max_det`.  \n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \n   - The response may include augmented results if `augment=True` is used during inference.\n\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.",
            "examples": [
                {
                    "user_intent": "Detect license plates in a car image I took on the highway.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/highway_car.jpg",
                            "conf": 0.3,
                            "iou": 0.45,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 500,
                            "size": 640,
                            "augment": true
                        }
                    }
                },
                {
                    "user_intent": "Scan multiple parking lot images for license plates and save the results.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/parking_lot_1.jpg",
                            "conf": 0.25,
                            "iou": 0.5,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 1000,
                            "size": 640,
                            "augment": false
                        }
                    }
                },
                {
                    "user_intent": "Quickly check license plates in a low-resolution surveillance image.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/surveillance_frame.jpg",
                            "conf": 0.2,
                            "iou": 0.4,
                            "agnostic": true,
                            "multi_label": false,
                            "max_det": 200,
                            "size": 320,
                            "augment": true
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 1: A YOLOv5-based model for detecting license plates in images, capable of identifying plate locations with high accuracy and supporting optional test-time augmentation. It is designed for computer vision applications that require automatic license plate recognition or localization.\nTool 2: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 4: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 6: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 7: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 8: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 9: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.\nTool 10: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 11: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 12: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 13: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 14: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 15: ControlNet is a neural network designed to enhance diffusion models by adding additional conditioning factors. This specific checkpoint, \"lllyasviel/control_v11p_sd15_lineart,\" is tailored to work with lineart images, allowing for more precise control in image generation tasks. It can be integrated with Hugging Face’s frameworks, particularly for tasks involving image-to-image transformations, and requires specific Python dependencies for setup.\nTool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 17: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 18: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 19: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.\", \"required\": true, \"default\": false, \"example\": true}, \"multi_label\": {\"type\": \"boolean\", \"description\": \"If True, allows each detected box to have multiple class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image.\", \"required\": true, \"default\": 1000, \"example\": 500}, \"img\": {\"type\": \"string\", \"description\": \"The input image for detection. Can be a URL or local file path.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"size\": {\"type\": \"number\", \"description\": \"The size to which the input image will be resized before detection.\", \"required\": true, \"default\": 640, \"example\": 512}, \"augment\": {\"type\": \"boolean\", \"description\": \"If True, applies data augmentation during inference for potentially improved results.\", \"required\": true, \"default\": true, \"example\": false}}, \"response\": \"The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\\n\\n1. **pred**  \\n   - A list (or tensor) containing the detection predictions for each input image.  \\n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \\n   - Each row contains:  \\n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \\n     - **Confidence score** (usually the fifth element) indicating the model\\u2019s certainty for that detection.  \\n     - **Class index** (usually the sixth element) representing the detected object\\u2019s category; here it corresponds to the license plate class.\\n\\n2. **boxes**  \\n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\\n\\n3. **scores**  \\n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\\n\\n4. **categories**  \\n   - Extracted from `pred`, this array contains the class indices for each detection.\\n\\n5. **Visualization methods**  \\n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \\n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\\n\\n6. **Additional metadata** (implicit)  \\n   - Number of detections returned is limited by `max_det`.  \\n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \\n   - The response may include augmented results if `augment=True` is used during inference.\\n\\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.\", \"examples\": [{\"user_intent\": \"Detect license plates in a car image I took on the highway.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/highway_car.jpg\", \"conf\": 0.3, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 500, \"size\": 640, \"augment\": true}}}, {\"user_intent\": \"Scan multiple parking lot images for license plates and save the results.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/parking_lot_1.jpg\", \"conf\": 0.25, \"iou\": 0.5, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"size\": 640, \"augment\": false}}}, {\"user_intent\": \"Quickly check license plates in a low-resolution surveillance image.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/surveillance_frame.jpg\", \"conf\": 0.2, \"iou\": 0.4, \"agnostic\": true, \"multi_label\": false, \"max_det\": 200, \"size\": 320, \"augment\": true}}}]}\n\n# User Query\nWe need a system to control the access into parking lot. Analyze an image and find out the license plate numbers to detect whether it's an authorized vehicle or not.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze an image to detect all license plates present.",
                    "tool_description": "Use a YOLOv5-based model to identify and localize license plates in an input image, providing bounding boxes and confidence scores for each detected plate.",
                    "expected_response": "A structured object containing bounding box coordinates, confidence scores, and class indices for each detected license plate, with optional visualizations."
                },
                {
                    "user_intent": "Check if detected license plates correspond to authorized vehicles.",
                    "tool_description": "Compare the extracted license plate numbers from the image against a database of authorized vehicles to determine access permission.",
                    "expected_response": "A list or mapping indicating for each detected license plate whether it is authorized or unauthorized."
                }
            ],
            "extracted_arguments": {
                "img": "The input image containing vehicles whose license plates need to be detected, provided as a local file path or URL.",
                "conf": "Confidence threshold for detecting license plates; predictions below this value are ignored.",
                "iou": "Intersection over Union threshold for Non-Maximum Suppression to filter overlapping detections.",
                "agnostic": "Boolean indicating whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.",
                "multi_label": "Boolean indicating if each detected box can have multiple class labels.",
                "max_det": "Maximum number of detections allowed per image.",
                "size": "The size to which the input image is resized before detection.",
                "augment": "Boolean indicating whether to apply data augmentation during inference for potentially improved results.",
                "authorized_list": "A dataset or collection of license plate numbers that are permitted to access the parking lot."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_446",
        "query": "Develop a video content recommendation engine that can understand and generate multiple categories, such as sports, comedy, and news, based on the videos.",
        "instruction": "Given a `video recommendation` task, retrieve tools that can analyze video content to categorize and recommend based topics such as sports, comedy, and news by leveraging video classification capabilities.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Classification",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')",
                    "api_arguments": "video",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\ninputs = processor(video, return_tensors=pt)\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "Something-Something-v2",
                        "accuracy": {
                            "top-1": 70.6,
                            "top-5": 92.6
                        }
                    },
                    "description": "VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.",
                    "id": "huggingface_api_325",
                    "name": "MCG-NJU/videomae-base-finetuned-ssv2"
                },
                "id": "gorilla_huggingface_tool_321",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 1: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 2: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 3: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 4: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 5: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 6: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 7: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 9: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 11: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 12: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 13: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 14: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 15: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 16: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.",
            "Tool 17: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 18: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition."
        ],
        "tool_view": {
            "description": "X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "parameters": {
                "domain": {
                    "type": "string",
                    "description": "The domain or field in which the tool operates, in this case, related to computer vision and video classification tasks.",
                    "required": true,
                    "example": "Computer Vision Video Classification"
                },
                "framework": {
                    "type": "string",
                    "description": "The machine learning framework used by the tool, which in this case is Hugging Face Transformers.",
                    "required": true,
                    "example": "Hugging Face Transformers"
                },
                "functionality": {
                    "type": "string",
                    "description": "The main functionality of the tool. Here, it is focused on feature extraction from video inputs.",
                    "required": true,
                    "example": "Feature Extraction"
                },
                "api_call": {
                    "type": "string",
                    "description": "The function call to load the pre-trained model, which in this case uses the Hugging Face Transformers library to load the 'microsoft/xclip-base-patch16-zero-shot' model.",
                    "required": true,
                    "example": "XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')"
                },
                "api_arguments": {
                    "type": "array",
                    "description": "A list of arguments required for the API call. In this case, no additional arguments are specified.",
                    "required": true,
                    "example": []
                },
                "python_environment_requirements": {
                    "type": "array",
                    "description": "A list of Python packages that must be installed in the environment to run the tool. Here, the 'transformers' package is required.",
                    "required": true,
                    "example": [
                        "transformers"
                    ]
                },
                "example_code": {
                    "type": "string",
                    "description": "Code snippets for using the tool. For more details, the documentation should be referenced.",
                    "required": true,
                    "example": "For code examples, we refer to the documentation."
                },
                "performance": {
                    "type": "array",
                    "description": "A list of datasets used for evaluating the model's performance, along with the accuracy scores achieved on each dataset.",
                    "required": true,
                    "example": [
                        {
                            "name": "HMDB-51",
                            "accuracy": 44.6
                        },
                        {
                            "name": "UCF-101",
                            "accuracy": 72.0
                        },
                        {
                            "name": "Kinetics-600",
                            "accuracy": 65.2
                        }
                    ]
                },
                "description": {
                    "type": "string",
                    "description": "A brief description of the tool's purpose and functionality. X-CLIP is an extension of CLIP, designed for video-language understanding tasks.",
                    "required": true,
                    "example": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."
                },
                "name": {
                    "type": "string",
                    "description": "The name or identifier of the model, in this case, the 'microsoft/xclip-base-patch16-zero-shot' model.",
                    "required": true,
                    "example": "microsoft/xclip-base-patch16-zero-shot"
                }
            },
            "response": "The tool is designed for **feature extraction** in the context of **video classification** using the **X-CLIP model**, which is part of the **Hugging Face Transformers** framework. \n\n### Expected Response Structure\n\nThe response will typically provide the following:\n\n1. **Model Details**: \n   - It will likely include a reference to the model used, in this case, `microsoft/xclip-base-patch16-zero-shot`, confirming that this is the model being utilized for the task.\n\n2. **Performance Metrics**: \n   - The response may include **accuracy scores** on various benchmark datasets, which demonstrate the performance of the X-CLIP model. These metrics provide an indication of the model's effectiveness for tasks like video classification.\n   - The datasets and their corresponding accuracies are typically as follows:\n     - **HMDB-51**: 44.6% accuracy\n     - **UCF-101**: 72.0% accuracy\n     - **Kinetics-600**: 65.2% accuracy\n   - This section helps users understand the model’s performance in comparison to standard benchmarks.\n\n3. **Description of Model Functionality**:\n   - A description explaining the model's purpose, which in this case is to support **video-language understanding** through a contrastive training approach on video-text pairs. The model is suited for a variety of tasks, including:\n     - **Zero-shot video classification**\n     - **Few-shot classification**\n     - **Fully supervised video classification**\n     - **Video-text retrieval**\n   - This would clarify the potential applications of the model in real-world tasks.\n\n4. **Framework and Environment Requirements**:\n   - The response would include details about the **framework** used (Hugging Face Transformers) and the **python environment requirements**, such as the `transformers` library, indicating what dependencies need to be installed for the tool to function correctly.\n\n5. **Example Code Reference**:\n   - The response would point to external documentation for **example code**, giving users a starting point to implement and use the model effectively.\n\n6. **API Call Details**:\n   - The model can be loaded via the API call: `XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')`.\n   - This is the way users will initialize the model for feature extraction.\n\n### Summary:\nThe tool’s response will likely include a combination of:\n- The model's purpose and functionality in video-text understanding tasks.\n- The model's performance metrics on established datasets.\n- Information on the framework and environment needed.\n- A reference to where example code can be found to help users implement the model.",
            "examples": [
                {
                    "user_intent": "I want to classify short video clips by their content without any additional training.",
                    "tool_call": {
                        "name": "XClipModel.from_pretrained",
                        "arguments": {
                            "model_name": "microsoft/xclip-base-patch16-zero-shot"
                        }
                    }
                },
                {
                    "user_intent": "Retrieve relevant text descriptions for a set of sports videos using a pre-trained video-language model.",
                    "tool_call": {
                        "name": "XClipModel.from_pretrained",
                        "arguments": {
                            "model_name": "microsoft/xclip-base-patch16-zero-shot"
                        }
                    }
                },
                {
                    "user_intent": "I need a feature extractor to embed my movie clips for a video recommendation system.",
                    "tool_call": {
                        "name": "XClipModel.from_pretrained",
                        "arguments": {
                            "model_name": "microsoft/xclip-base-patch16-zero-shot"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 1: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 2: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 3: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 4: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 5: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 6: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 7: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 9: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 10: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 11: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 12: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 13: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 14: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 15: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 16: This is a fine-tuned video classification model based on MCG-NJU/videomae-base, designed for classifying videos. It utilizes the Hugging Face Transformers framework and is suitable for tasks that involve categorizing video content, achieving an accuracy of 89.68% on an unknown dataset.\nTool 17: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 18: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 19: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\", \"parameters\": {\"domain\": {\"type\": \"string\", \"description\": \"The domain or field in which the tool operates, in this case, related to computer vision and video classification tasks.\", \"required\": true, \"example\": \"Computer Vision Video Classification\"}, \"framework\": {\"type\": \"string\", \"description\": \"The machine learning framework used by the tool, which in this case is Hugging Face Transformers.\", \"required\": true, \"example\": \"Hugging Face Transformers\"}, \"functionality\": {\"type\": \"string\", \"description\": \"The main functionality of the tool. Here, it is focused on feature extraction from video inputs.\", \"required\": true, \"example\": \"Feature Extraction\"}, \"api_call\": {\"type\": \"string\", \"description\": \"The function call to load the pre-trained model, which in this case uses the Hugging Face Transformers library to load the 'microsoft/xclip-base-patch16-zero-shot' model.\", \"required\": true, \"example\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\"}, \"api_arguments\": {\"type\": \"array\", \"description\": \"A list of arguments required for the API call. In this case, no additional arguments are specified.\", \"required\": true, \"example\": []}, \"python_environment_requirements\": {\"type\": \"array\", \"description\": \"A list of Python packages that must be installed in the environment to run the tool. Here, the 'transformers' package is required.\", \"required\": true, \"example\": [\"transformers\"]}, \"example_code\": {\"type\": \"string\", \"description\": \"Code snippets for using the tool. For more details, the documentation should be referenced.\", \"required\": true, \"example\": \"For code examples, we refer to the documentation.\"}, \"performance\": {\"type\": \"array\", \"description\": \"A list of datasets used for evaluating the model's performance, along with the accuracy scores achieved on each dataset.\", \"required\": true, \"example\": [{\"name\": \"HMDB-51\", \"accuracy\": 44.6}, {\"name\": \"UCF-101\", \"accuracy\": 72.0}, {\"name\": \"Kinetics-600\", \"accuracy\": 65.2}]}, \"description\": {\"type\": \"string\", \"description\": \"A brief description of the tool's purpose and functionality. X-CLIP is an extension of CLIP, designed for video-language understanding tasks.\", \"required\": true, \"example\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}, \"name\": {\"type\": \"string\", \"description\": \"The name or identifier of the model, in this case, the 'microsoft/xclip-base-patch16-zero-shot' model.\", \"required\": true, \"example\": \"microsoft/xclip-base-patch16-zero-shot\"}}, \"response\": \"The tool is designed for **feature extraction** in the context of **video classification** using the **X-CLIP model**, which is part of the **Hugging Face Transformers** framework. \\n\\n### Expected Response Structure\\n\\nThe response will typically provide the following:\\n\\n1. **Model Details**: \\n   - It will likely include a reference to the model used, in this case, `microsoft/xclip-base-patch16-zero-shot`, confirming that this is the model being utilized for the task.\\n\\n2. **Performance Metrics**: \\n   - The response may include **accuracy scores** on various benchmark datasets, which demonstrate the performance of the X-CLIP model. These metrics provide an indication of the model's effectiveness for tasks like video classification.\\n   - The datasets and their corresponding accuracies are typically as follows:\\n     - **HMDB-51**: 44.6% accuracy\\n     - **UCF-101**: 72.0% accuracy\\n     - **Kinetics-600**: 65.2% accuracy\\n   - This section helps users understand the model\\u2019s performance in comparison to standard benchmarks.\\n\\n3. **Description of Model Functionality**:\\n   - A description explaining the model's purpose, which in this case is to support **video-language understanding** through a contrastive training approach on video-text pairs. The model is suited for a variety of tasks, including:\\n     - **Zero-shot video classification**\\n     - **Few-shot classification**\\n     - **Fully supervised video classification**\\n     - **Video-text retrieval**\\n   - This would clarify the potential applications of the model in real-world tasks.\\n\\n4. **Framework and Environment Requirements**:\\n   - The response would include details about the **framework** used (Hugging Face Transformers) and the **python environment requirements**, such as the `transformers` library, indicating what dependencies need to be installed for the tool to function correctly.\\n\\n5. **Example Code Reference**:\\n   - The response would point to external documentation for **example code**, giving users a starting point to implement and use the model effectively.\\n\\n6. **API Call Details**:\\n   - The model can be loaded via the API call: `XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')`.\\n   - This is the way users will initialize the model for feature extraction.\\n\\n### Summary:\\nThe tool\\u2019s response will likely include a combination of:\\n- The model's purpose and functionality in video-text understanding tasks.\\n- The model's performance metrics on established datasets.\\n- Information on the framework and environment needed.\\n- A reference to where example code can be found to help users implement the model.\", \"examples\": [{\"user_intent\": \"I want to classify short video clips by their content without any additional training.\", \"tool_call\": {\"name\": \"XClipModel.from_pretrained\", \"arguments\": {\"model_name\": \"microsoft/xclip-base-patch16-zero-shot\"}}}, {\"user_intent\": \"Retrieve relevant text descriptions for a set of sports videos using a pre-trained video-language model.\", \"tool_call\": {\"name\": \"XClipModel.from_pretrained\", \"arguments\": {\"model_name\": \"microsoft/xclip-base-patch16-zero-shot\"}}}, {\"user_intent\": \"I need a feature extractor to embed my movie clips for a video recommendation system.\", \"tool_call\": {\"name\": \"XClipModel.from_pretrained\", \"arguments\": {\"model_name\": \"microsoft/xclip-base-patch16-zero-shot\"}}}]}\n\n# User Query\nDevelop a video content recommendation engine that can understand and generate multiple categories, such as sports, comedy, and news, based on the videos.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract meaningful features from videos to understand their content across multiple categories like sports, comedy, and news.",
                    "tool_description": "Use a video-language model to extract rich video representations that capture semantic content, enabling multi-category understanding and classification.",
                    "expected_response": "A feature vector or embedding for each video that encodes its content for downstream tasks like classification or recommendation."
                },
                {
                    "user_intent": "Classify videos into multiple categories to support a recommendation system.",
                    "tool_description": "Apply a pre-trained video classification model to categorize videos into predefined labels such as sports, comedy, and news, using extracted video features.",
                    "expected_response": "Predicted category labels or probabilities for each video."
                },
                {
                    "user_intent": "Generate personalized video recommendations based on content understanding and classification.",
                    "tool_description": "Build a recommendation engine that leverages video embeddings and classification outputs to suggest relevant videos to users according to their interests.",
                    "expected_response": "A ranked list of recommended videos for each user, with associated metadata and relevance scores."
                }
            ],
            "extracted_arguments": {
                "video_input": "Video files or streams that serve as input for content analysis and feature extraction.",
                "categories": "A list of semantic categories (e.g., sports, comedy, news) used to classify videos.",
                "recommendation_engine": "The system or model that will process video features and categories to output personalized video suggestions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_301",
        "query": "The company wants to create a chatbot to help answer customer questions regarding the chatbot's consciousness. We need to be able to generate sensible responses.",
        "instruction": "Given a `chatbot development` task, retrieve tools capable of generating coherent and contextually appropriate text outputs to address inquiries about chatbot consciousness by leveraging advanced natural language processing models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('facebook/opt-66b')",
                    "api_arguments": [
                        "input_ids",
                        "do_sample",
                        "num_return_sequences",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\nprompt = Hello, I am conscious and\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\nset_seed(32)\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "GPT-3",
                        "accuracy": "roughly matched"
                    },
                    "description": "OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.",
                    "id": "huggingface_api_625",
                    "name": "facebook/opt-66b"
                },
                "id": "gorilla_huggingface_tool_620",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 2: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 4: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 5: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 6: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 12: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 14: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 15: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 18: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages."
        ],
        "tool_view": {
            "description": "Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "parameters": {
                "input": {
                    "type": "string",
                    "description": "The input message that the model will generate a response for. This serves as the starting point for the conversation.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model’s continuation, reply, or conversational output. Specifically:\n\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\n  - `generated_text`: The actual text output produced by the model.\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model’s confidence or token-level details.  \n\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.",
            "examples": [
                {
                    "user_intent": "Start a conversation with the Zixtrauce chatbot and ask about the weather.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What's the weather like today?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a response from BrandonBot4Epochs about AI advancements.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "Tell me about the latest advancements in AI."
                        }
                    }
                },
                {
                    "user_intent": "Chat with the bot about hobbies and interests.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What do you think is the best hobby to start?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 1: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 2: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 4: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 5: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 6: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 7: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 8: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 9: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 10: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 11: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 12: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 13: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 14: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 15: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 17: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 18: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\", \"parameters\": {\"input\": {\"type\": \"string\", \"description\": \"The input message that the model will generate a response for. This serves as the starting point for the conversation.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model\\u2019s continuation, reply, or conversational output. Specifically:\\n\\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \\n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\\n  - `generated_text`: The actual text output produced by the model.\\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model\\u2019s confidence or token-level details.  \\n\\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.\", \"examples\": [{\"user_intent\": \"Start a conversation with the Zixtrauce chatbot and ask about the weather.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What's the weather like today?\"}}}, {\"user_intent\": \"Generate a response from BrandonBot4Epochs about AI advancements.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"Tell me about the latest advancements in AI.\"}}}, {\"user_intent\": \"Chat with the bot about hobbies and interests.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What do you think is the best hobby to start?\"}}}]}\n\n# User Query\nThe company wants to create a chatbot to help answer customer questions regarding the chatbot's consciousness. We need to be able to generate sensible responses.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a chatbot that answers questions about its own consciousness.",
                    "tool_description": "Generate a response from a conversational AI model based on an input message regarding the chatbot's consciousness.",
                    "expected_response": "A generated text response that answers or discusses the chatbot's consciousness in a coherent and contextually relevant manner."
                }
            ],
            "extracted_arguments": {
                "input": "The input message or query the chatbot will respond to, focused on the chatbot's consciousness."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_10",
        "query": "Create an API that processes large data sets of financial transactions and can deliver information on the number of transactions and their monetary value, based on a date range.",
        "instruction": "Given a `data processing and analysis` task, retrieve tools capable of handling large datasets, particularly financial transactions, by processing inputs based on date ranges and delivering insights on the transactions' count and monetary value.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "api_arguments": "model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import TapasTokenizer, TapasForQuestionAnswering\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')",
                    "performance": {
                        "dataset": "wikisql",
                        "accuracy": "Not specified"
                    },
                    "description": "TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.",
                    "id": "huggingface_api_451",
                    "name": "google/tapas-small-finetuned-wikisql-supervised"
                },
                "id": "gorilla_huggingface_tool_447",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 2: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 3: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 5: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 6: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.",
            "Tool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 8: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 9: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.",
            "Tool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 11: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.",
            "Tool 12: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 13: This tool provides a pre-trained model for speaker verification using the Wav2Vec2 architecture, specifically ported for the SUPERB benchmark. It is based on the wav2vec2-large-lv60 model, trained on 16kHz sampled speech audio. It can be used to classify audio inputs for speaker identification, assuming the input is sampled at 16kHz. This model is available through the Hugging Face Transformers library and can be easily loaded with the `AutoModelForAudioXVector` API for integration into speech processing tasks.",
            "Tool 14: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 15: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 16: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 17: Google’s T5 v1.1 model is a versatile text-to-text transformer that can perform a wide range of natural language processing tasks, including translation, summarization, question answering, and text classification. It is pre-trained on a large-scale dataset and can be accessed through the Transformers library’s text2text-generation pipeline for generating task-specific outputs from text inputs.",
            "Tool 18: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.",
            "Tool 19: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information."
        ],
        "tool_view": {
            "description": "A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained PEGASUS model to load for financial text summarization. This specifies which model weights and tokenizer to use.",
                    "required": true,
                    "example": "human-centered-summarization/financial-summarization-pegasus"
                }
            },
            "response": "The tool produces an abstractive summary of a given input text. The response is typically a single text string that concisely captures the main points of the input while rephrasing and condensing the content. Specifically:  \n\n- **Type:** String  \n- **Content:** A summarized version of the input text, written in natural language, often shorter than the original text.  \n- **Characteristics:**  \n  - Abstractive in nature: the summary may use new wording rather than copying sentences verbatim.  \n  - Captures the key facts, events, or insights from the input.  \n  - Designed to be coherent and readable as a standalone text.  \n\nIn practical use, the response is obtained by decoding the output token IDs generated by the PEGASUS model into human-readable text. The length, style, and focus of the summary can be influenced by generation parameters such as `max_length`, `num_beams`, and `early_stopping`.",
            "examples": [
                {
                    "user_intent": "Summarize this financial news article about a major bank merger in Saudi Arabia.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year. NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June. The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions. The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets."
                        }
                    }
                },
                {
                    "user_intent": "Provide a concise summary of a cryptocurrency market update from Bloomberg.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "Bitcoin surged past $35,000 after recent institutional investments boosted confidence in the cryptocurrency market. Ethereum and other major altcoins followed suit, with trading volumes rising across exchanges. Analysts attribute the rally to increasing adoption and the anticipation of favorable regulatory guidance. Market volatility remains high, prompting investors to exercise caution."
                        }
                    }
                },
                {
                    "user_intent": "Summarize this report about stock market fluctuations and interest rate changes.",
                    "tool_call": {
                        "name": "financial-summarization-pegasus",
                        "arguments": {
                            "model_name": "human-centered-summarization/financial-summarization-pegasus",
                            "text_to_summarize": "The S&P 500 index fell 1.2% today following news that the Federal Reserve may raise interest rates sooner than expected. Technology and consumer discretionary stocks were hit hardest, while energy stocks saw slight gains. Analysts warn that continued rate hikes could dampen market sentiment and increase borrowing costs for companies, affecting earnings forecasts for the next quarter."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 1: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 2: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 3: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 4: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 5: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 6: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\nTool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 8: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 9: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.\nTool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 11: `google/pegasus-large` is a pre-trained model for abstractive text summarization that leverages the PEGASUS architecture. It is designed to generate concise summaries from longer text inputs, having been trained on large datasets like C4 and HugeNews and fine-tuned across multiple summarization benchmarks. The model supports a wide range of summarization tasks and delivers strong performance on datasets such as XSum, CNN/DailyMail, and BigPatent, making it useful for efficiently condensing documents while preserving key information.\nTool 12: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 13: This tool provides a pre-trained model for speaker verification using the Wav2Vec2 architecture, specifically ported for the SUPERB benchmark. It is based on the wav2vec2-large-lv60 model, trained on 16kHz sampled speech audio. It can be used to classify audio inputs for speaker identification, assuming the input is sampled at 16kHz. This model is available through the Hugging Face Transformers library and can be easily loaded with the `AutoModelForAudioXVector` API for integration into speech processing tasks.\nTool 14: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 15: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 16: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 17: Google’s T5 v1.1 model is a versatile text-to-text transformer that can perform a wide range of natural language processing tasks, including translation, summarization, question answering, and text classification. It is pre-trained on a large-scale dataset and can be accessed through the Transformers library’s text2text-generation pipeline for generating task-specific outputs from text inputs.\nTool 18: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.\nTool 19: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained PEGASUS model to load for financial text summarization. This specifies which model weights and tokenizer to use.\", \"required\": true, \"example\": \"human-centered-summarization/financial-summarization-pegasus\"}}, \"response\": \"The tool produces an abstractive summary of a given input text. The response is typically a single text string that concisely captures the main points of the input while rephrasing and condensing the content. Specifically:  \\n\\n- **Type:** String  \\n- **Content:** A summarized version of the input text, written in natural language, often shorter than the original text.  \\n- **Characteristics:**  \\n  - Abstractive in nature: the summary may use new wording rather than copying sentences verbatim.  \\n  - Captures the key facts, events, or insights from the input.  \\n  - Designed to be coherent and readable as a standalone text.  \\n\\nIn practical use, the response is obtained by decoding the output token IDs generated by the PEGASUS model into human-readable text. The length, style, and focus of the summary can be influenced by generation parameters such as `max_length`, `num_beams`, and `early_stopping`.\", \"examples\": [{\"user_intent\": \"Summarize this financial news article about a major bank merger in Saudi Arabia.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"National Commercial Bank (NCB), Saudi Arabia\\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year. NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June. The offer is a 3.5% premium to Samba\\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions. The new bank will have total assets of more than $220 billion, creating the Gulf region\\u2019s third-largest lender. The entity\\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\\u2019s biggest lender with about $268 billion of assets.\"}}}, {\"user_intent\": \"Provide a concise summary of a cryptocurrency market update from Bloomberg.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"Bitcoin surged past $35,000 after recent institutional investments boosted confidence in the cryptocurrency market. Ethereum and other major altcoins followed suit, with trading volumes rising across exchanges. Analysts attribute the rally to increasing adoption and the anticipation of favorable regulatory guidance. Market volatility remains high, prompting investors to exercise caution.\"}}}, {\"user_intent\": \"Summarize this report about stock market fluctuations and interest rate changes.\", \"tool_call\": {\"name\": \"financial-summarization-pegasus\", \"arguments\": {\"model_name\": \"human-centered-summarization/financial-summarization-pegasus\", \"text_to_summarize\": \"The S&P 500 index fell 1.2% today following news that the Federal Reserve may raise interest rates sooner than expected. Technology and consumer discretionary stocks were hit hardest, while energy stocks saw slight gains. Analysts warn that continued rate hikes could dampen market sentiment and increase borrowing costs for companies, affecting earnings forecasts for the next quarter.\"}}}]}\n\n# User Query\nCreate an API that processes large data sets of financial transactions and can deliver information on the number of transactions and their monetary value, based on a date range.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Process a large dataset of financial transactions to compute aggregate metrics over a specified date range.",
                    "tool_description": "Analyze a dataset of financial transactions, filtering by a given date range, and calculate summary statistics such as total transaction count and total monetary value.",
                    "expected_response": "A structured response containing the number of transactions and the cumulative monetary value within the specified date range."
                }
            ],
            "extracted_arguments": {
                "transaction_data": "The dataset containing financial transactions with attributes such as date and amount, in a structured tabular format.",
                "start_date": "The beginning date for the range filter, in a standard date format.",
                "end_date": "The ending date for the range filter, in a standard date format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_173",
        "query": "Generate an image of a beautiful fantasy landscape based on the description provided: a peaceful scene in a lush green forest with a crystal-clear river flowing through it, under a blue sky with fluffy white clouds.",
        "instruction": "Given an `image generation` task, retrieve tools that transform descriptive text prompts into visual representations, specifically creating detailed and aesthetically appealing images of fantasy landscapes using advanced text-to-image models and techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')",
                    "api_arguments": {
                        "model_id": "darkstorm2150/Protogen_v5.8_Official_Release",
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "diffusers"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\nprompt = (\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\n)\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(./result.jpg)",
                    "performance": {
                        "dataset": "unknown",
                        "accuracy": "unknown"
                    },
                    "description": "Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.",
                    "id": "huggingface_api_57",
                    "name": "darkstorm2150/Protogen_x5.8_Official_Release"
                },
                "id": "gorilla_huggingface_tool_57",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 1: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 3: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 4: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.",
            "Tool 5: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 6: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 7: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 8: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 9: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 10: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.",
            "Tool 11: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 12: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 13: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 14: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.",
            "Tool 15: The deepset/deberta-v3-base-squad2 model is a fine-tuned version of the DeBERTa V3 model, optimized for Question Answering tasks using the SQuAD2.0 dataset. It is capable of answering both answerable and unanswerable questions based on a provided context. This model can be easily used in natural language processing applications through the Hugging Face Transformers framework, offering high accuracy with an Exact Match score of 83.8% and an F1 score of 87.41%.",
            "Tool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 17: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 18: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 19: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation."
        ],
        "tool_view": {
            "description": "A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A textual description that serves as the input for generating a video. The model uses this description to create a video that aligns with the given text. Only English input is supported.",
                    "required": true,
                    "example": "Spiderman is surfing"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps to use during the video generation process. Higher values typically result in better quality, but require more computational resources.",
                    "required": true,
                    "default": 25,
                    "example": 25
                }
            },
            "response": "The tool generates a video sequence based on a given text prompt. The response is structured as an object containing the generated video data, typically including the following components:\n\n- **frames**: A sequential collection of individual video frames produced by the model. Each frame represents a single image in the video, and the full sequence forms the complete video corresponding to the input text prompt. The number of frames is determined by the internal configuration of the model and the diffusion process.  \n\n- **video_path** (optional, depending on usage): A file path or reference to a video file if the generated frames are exported and saved as a video. This allows downstream usage or playback of the synthesized video.\n\nSemantically, the response represents the mapping from a textual description to a visual, temporal output, where each frame visually corresponds to the content described in the input prompt. The response may be handled either as an in-memory collection of frames or as a saved video file, depending on how the API is used.",
            "examples": [
                {
                    "user_intent": "Generate a short video of a golden retriever playing in a sunny park.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "A golden retriever joyfully playing in a sunny park, trees swaying gently, birds flying",
                            "num_inference_steps": 30
                        }
                    }
                },
                {
                    "user_intent": "Create a video showing a futuristic city skyline at night with flying cars.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "Futuristic city skyline at night, neon lights reflecting on skyscrapers, flying cars zooming",
                            "num_inference_steps": 40
                        }
                    }
                },
                {
                    "user_intent": "I want a short animation of a cat surfing on a wave in the ocean.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "A cute cat surfing on a big ocean wave, sunny day, animated style",
                            "num_inference_steps": 25
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 1: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 3: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 4: The Microsoft GIT-base model is a generative image-to-text transformer designed to generate descriptive captions for images. Trained on large datasets of image-text pairs, it predicts text tokens based on image content and preceding text, enabling applications such as image and video captioning, visual question answering, and image classification through text-based output.\nTool 5: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 6: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 7: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 8: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 9: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 10: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.\nTool 11: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 12: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 13: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 14: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.\nTool 15: The deepset/deberta-v3-base-squad2 model is a fine-tuned version of the DeBERTa V3 model, optimized for Question Answering tasks using the SQuAD2.0 dataset. It is capable of answering both answerable and unanswerable questions based on a provided context. This model can be easily used in natural language processing applications through the Hugging Face Transformers framework, offering high accuracy with an Exact Match score of 83.8% and an F1 score of 87.41%.\nTool 16: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 17: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 18: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 19: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A textual description that serves as the input for generating a video. The model uses this description to create a video that aligns with the given text. Only English input is supported.\", \"required\": true, \"example\": \"Spiderman is surfing\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps to use during the video generation process. Higher values typically result in better quality, but require more computational resources.\", \"required\": true, \"default\": 25, \"example\": 25}}, \"response\": \"The tool generates a video sequence based on a given text prompt. The response is structured as an object containing the generated video data, typically including the following components:\\n\\n- **frames**: A sequential collection of individual video frames produced by the model. Each frame represents a single image in the video, and the full sequence forms the complete video corresponding to the input text prompt. The number of frames is determined by the internal configuration of the model and the diffusion process.  \\n\\n- **video_path** (optional, depending on usage): A file path or reference to a video file if the generated frames are exported and saved as a video. This allows downstream usage or playback of the synthesized video.\\n\\nSemantically, the response represents the mapping from a textual description to a visual, temporal output, where each frame visually corresponds to the content described in the input prompt. The response may be handled either as an in-memory collection of frames or as a saved video file, depending on how the API is used.\", \"examples\": [{\"user_intent\": \"Generate a short video of a golden retriever playing in a sunny park.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"A golden retriever joyfully playing in a sunny park, trees swaying gently, birds flying\", \"num_inference_steps\": 30}}}, {\"user_intent\": \"Create a video showing a futuristic city skyline at night with flying cars.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"Futuristic city skyline at night, neon lights reflecting on skyscrapers, flying cars zooming\", \"num_inference_steps\": 40}}}, {\"user_intent\": \"I want a short animation of a cat surfing on a wave in the ocean.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"A cute cat surfing on a big ocean wave, sunny day, animated style\", \"num_inference_steps\": 25}}}]}\n\n# User Query\nGenerate an image of a beautiful fantasy landscape based on the description provided: a peaceful scene in a lush green forest with a crystal-clear river flowing through it, under a blue sky with fluffy white clouds.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a high-quality anime-style image of a fantasy landscape based on a text description.",
                    "tool_description": "Create a detailed anime-style or illustrative image from a textual prompt using a diffusion-based text-to-image model.",
                    "expected_response": "A generated image file or object representing the scene described in the text prompt, including visual elements such as environment, lighting, and style."
                }
            ],
            "extracted_arguments": {
                "prompt": "A textual description of the fantasy landscape to be visualized in the image, including elements like environment, scenery, and style."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_453",
        "query": "Let's create a smart agent that can learn to play CartPole-v0 using reinforcement learning.",
        "instruction": "Given a `reinforcement learning agent creation` task, retrieve tools that support the development and training of smart agents through reinforcement learning frameworks, specifically focusing on environments like 'CartPole-v0', by utilizing libraries and models designed for deep reinforcement learning tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "deep-reinforcement-learning",
                    "api_call": "load_from_hub(repo_id='HumanCompatibleAI/ppo-seals-CartPole-v0')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/",
                    "performance": {
                        "dataset": "seals/CartPole-v0",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_891",
                    "name": "ppo-seals-CartPole-v0"
                },
                "id": "gorilla_huggingface_tool_884",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 1: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 2: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 3: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 4: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.",
            "Tool 5: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 6: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 7: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 8: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 9: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 10: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 11: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 12: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 13: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 14: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.",
            "Tool 15: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 16: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 17: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 18: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.",
            "Tool 19: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset."
        ],
        "tool_view": {
            "description": "Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "parameters": {
                "repo_id": {
                    "type": "string",
                    "description": "The identifier of the repository where the pre-trained model is stored on the hub.",
                    "required": true,
                    "example": "HumanCompatibleAI/ppo-seals-CartPole-v0"
                },
                "algo": {
                    "type": "string",
                    "description": "The reinforcement learning algorithm used for training the agent.",
                    "required": true,
                    "example": "ppo"
                },
                "env": {
                    "type": "string",
                    "description": "The environment where the agent will be tested or trained.",
                    "required": true,
                    "example": "seals/CartPole-v0"
                },
                "f": {
                    "type": "string",
                    "description": "The directory path where logs or additional outputs should be saved.",
                    "required": true,
                    "example": "logs/"
                }
            },
            "response": "The tool returns a pre-trained reinforcement learning model object for a PPO agent trained on the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo framework. The response structure typically includes the following components:\n\n1. **Algorithm Object (`algo`)**  \n   - Represents the loaded PPO agent instance.  \n   - Encapsulates the policy network, optimizer state, and any internal parameters needed to perform actions or continue training.\n\n2. **Environment Reference (`env`)**  \n   - A reference or wrapper to the environment (`seals/CartPole-v0`) used during training.  \n   - Allows evaluation of the loaded agent in the same environment, including stepping through episodes and rendering.\n\n3. **Optional Logging/Configuration (`f` or other metadata)**  \n   - May include paths to training logs, configuration files, or other metadata used to initialize or track the model.  \n   - Useful for reproducibility, evaluation, or further fine-tuning.\n\n4. **Performance Metadata (implicit in the object or returned alongside)**  \n   - Summary statistics of agent performance on the training environment, such as average reward (here noted as `500.00 +/- 0.00`).  \n   - Provides a quick reference to the effectiveness of the loaded model.\n\n5. **Methods/Functions on the Loaded Model**  \n   - Functions to run inference (`predict`), continue training (`learn`), or save/export the agent.  \n   - Access to the agent’s policy network parameters, exploration strategies, and other algorithm-specific components.\n\nOverall, the response is a fully initialized RL agent ready for evaluation or further training, bundled with references to its environment and relevant configuration or logging information.",
            "examples": [
                {
                    "user_intent": "I want to quickly load a pre-trained PPO model for the CartPole-v0 environment using Stable-Baselines3.",
                    "tool_call": {
                        "name": "ppo-seals-CartPole-v0",
                        "arguments": {
                            "algo": "ppo",
                            "env": "seals/CartPole-v0",
                            "f": "logs/"
                        }
                    }
                },
                {
                    "user_intent": "Load the HumanCompatibleAI PPO agent for CartPole-v0 to evaluate its performance on my local setup.",
                    "tool_call": {
                        "name": "ppo-seals-CartPole-v0",
                        "arguments": {
                            "algo": "ppo",
                            "env": "seals/CartPole-v0",
                            "f": "evaluation_logs/"
                        }
                    }
                },
                {
                    "user_intent": "I want to experiment with a pre-trained RL agent from the RL Zoo for CartPole-v0 without training from scratch.",
                    "tool_call": {
                        "name": "ppo-seals-CartPole-v0",
                        "arguments": {
                            "algo": "ppo",
                            "env": "seals/CartPole-v0",
                            "f": "pretrained_models/"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 1: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 2: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 3: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 4: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.\nTool 5: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 6: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 7: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 8: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 9: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 10: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 11: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 12: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 13: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 14: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.\nTool 15: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 16: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 17: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 18: This tool provides a Decision Transformer model trained on expert trajectories from the Gym Hopper environment, enabling reinforcement learning tasks using Hugging Face Transformers. It is designed to leverage pretrained models for decision-making and action prediction in simulated control environments.\nTool 19: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\", \"parameters\": {\"repo_id\": {\"type\": \"string\", \"description\": \"The identifier of the repository where the pre-trained model is stored on the hub.\", \"required\": true, \"example\": \"HumanCompatibleAI/ppo-seals-CartPole-v0\"}, \"algo\": {\"type\": \"string\", \"description\": \"The reinforcement learning algorithm used for training the agent.\", \"required\": true, \"example\": \"ppo\"}, \"env\": {\"type\": \"string\", \"description\": \"The environment where the agent will be tested or trained.\", \"required\": true, \"example\": \"seals/CartPole-v0\"}, \"f\": {\"type\": \"string\", \"description\": \"The directory path where logs or additional outputs should be saved.\", \"required\": true, \"example\": \"logs/\"}}, \"response\": \"The tool returns a pre-trained reinforcement learning model object for a PPO agent trained on the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo framework. The response structure typically includes the following components:\\n\\n1. **Algorithm Object (`algo`)**  \\n   - Represents the loaded PPO agent instance.  \\n   - Encapsulates the policy network, optimizer state, and any internal parameters needed to perform actions or continue training.\\n\\n2. **Environment Reference (`env`)**  \\n   - A reference or wrapper to the environment (`seals/CartPole-v0`) used during training.  \\n   - Allows evaluation of the loaded agent in the same environment, including stepping through episodes and rendering.\\n\\n3. **Optional Logging/Configuration (`f` or other metadata)**  \\n   - May include paths to training logs, configuration files, or other metadata used to initialize or track the model.  \\n   - Useful for reproducibility, evaluation, or further fine-tuning.\\n\\n4. **Performance Metadata (implicit in the object or returned alongside)**  \\n   - Summary statistics of agent performance on the training environment, such as average reward (here noted as `500.00 +/- 0.00`).  \\n   - Provides a quick reference to the effectiveness of the loaded model.\\n\\n5. **Methods/Functions on the Loaded Model**  \\n   - Functions to run inference (`predict`), continue training (`learn`), or save/export the agent.  \\n   - Access to the agent\\u2019s policy network parameters, exploration strategies, and other algorithm-specific components.\\n\\nOverall, the response is a fully initialized RL agent ready for evaluation or further training, bundled with references to its environment and relevant configuration or logging information.\", \"examples\": [{\"user_intent\": \"I want to quickly load a pre-trained PPO model for the CartPole-v0 environment using Stable-Baselines3.\", \"tool_call\": {\"name\": \"ppo-seals-CartPole-v0\", \"arguments\": {\"algo\": \"ppo\", \"env\": \"seals/CartPole-v0\", \"f\": \"logs/\"}}}, {\"user_intent\": \"Load the HumanCompatibleAI PPO agent for CartPole-v0 to evaluate its performance on my local setup.\", \"tool_call\": {\"name\": \"ppo-seals-CartPole-v0\", \"arguments\": {\"algo\": \"ppo\", \"env\": \"seals/CartPole-v0\", \"f\": \"evaluation_logs/\"}}}, {\"user_intent\": \"I want to experiment with a pre-trained RL agent from the RL Zoo for CartPole-v0 without training from scratch.\", \"tool_call\": {\"name\": \"ppo-seals-CartPole-v0\", \"arguments\": {\"algo\": \"ppo\", \"env\": \"seals/CartPole-v0\", \"f\": \"pretrained_models/\"}}}]}\n\n# User Query\nLet's create a smart agent that can learn to play CartPole-v0 using reinforcement learning.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained reinforcement learning agent for CartPole-v0 to avoid training from scratch.",
                    "tool_description": "Load a pre-trained PPO agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo framework, allowing immediate evaluation or further training.",
                    "expected_response": "A fully initialized PPO agent object ready for evaluation or training, including environment reference and optional logging/configuration metadata."
                },
                {
                    "user_intent": "Train or fine-tune the loaded agent on the CartPole-v0 environment using reinforcement learning.",
                    "tool_description": "Use the loaded PPO agent to perform reinforcement learning in the `seals/CartPole-v0` environment, updating its policy through interaction with the environment.",
                    "expected_response": "Updated agent parameters and performance metrics, reflecting improved policy performance on CartPole-v0."
                }
            ],
            "extracted_arguments": {
                "env": "The target reinforcement learning environment identifier where the agent will be trained or evaluated.",
                "algo": "The reinforcement learning algorithm to use for the agent, e.g., PPO.",
                "repo_id": "The identifier of the repository containing the pre-trained model for loading.",
                "f": "The directory path for saving logs or additional outputs during agent evaluation or training."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_115",
        "query": "I am working in a dating chatapp development team. We want to generate sentences to make it more interactive.",
        "instruction": "Given a `sentence generation` task, retrieve tools that facilitate interactive and engaging sentence creation by utilizing natural language processing capabilities, tailored for applications such as chat applications, to enhance user interaction.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "pipeline('fill-mask', model='albert-base-v2')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='albert-base-v2')\nunmasker(Hello I'm a [MASK] model.)",
                    "performance": {
                        "dataset": {
                            "SQuAD1.1": "90.2/83.2",
                            "SQuAD2.0": "82.1/79.3",
                            "MNLI": "84.6",
                            "SST-2": "92.9",
                            "RACE": "66.8"
                        },
                        "accuracy": "82.3"
                    },
                    "description": "ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.",
                    "id": "huggingface_api_666",
                    "name": "albert-base-v2"
                },
                "id": "gorilla_huggingface_tool_661",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 1: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 3: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 4: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 5: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 6: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 7: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 8: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 9: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 11: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 12: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 13: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 14: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.",
            "Tool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 16: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 17: This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.",
            "Tool 18: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 19: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications."
        ],
        "tool_view": {
            "description": "BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "parameters": {
                "input_message": {
                    "type": "string",
                    "description": "The input message or prompt that will be fed into the model to generate a response in a conversational environment.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool returns a conversational response generated by the BaekBot model in reaction to the provided input message. The response is structured as an object that typically includes the following components:  \n\n- **generated_text / response_text**: The main textual output of the model, containing the model’s human-like reply to the user’s input.  \n- **conversation_state / context (optional)**: An object or data structure representing the ongoing conversation context, which may include past user inputs and model outputs. This allows the model to maintain coherence across multiple turns in a dialogue.  \n- **metadata (optional)**: Additional information about the generation process, which may include token usage, confidence scores, or model-specific details, depending on the implementation of the pipeline.  \n\nIn principle, the response is a structured representation of a single conversational turn, containing both the text generated by the model and any contextual or tracking information necessary for multi-turn interactions.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat with an AI about my day.",
                    "tool_call": {
                        "name": "Zixtrauce/BaekBot",
                        "arguments": {
                            "input_message": "Hi BaekBot! How are you doing today?"
                        }
                    }
                },
                {
                    "user_intent": "Ask the AI for advice on handling stress at work.",
                    "tool_call": {
                        "name": "Zixtrauce/BaekBot",
                        "arguments": {
                            "input_message": "I'm feeling overwhelmed with work. Any tips on managing stress?"
                        }
                    }
                },
                {
                    "user_intent": "Have the AI tell me a short story to pass the time.",
                    "tool_call": {
                        "name": "Zixtrauce/BaekBot",
                        "arguments": {
                            "input_message": "Can you tell me an interesting short story?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 1: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 2: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 3: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 4: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 5: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 6: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 7: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 8: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 9: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 11: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 12: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 13: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 14: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.\nTool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 16: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 17: This tool is a multi-class tabular classification model, trained on the Iris dataset using XGBoost and AutoTrain. It is designed for classifying tabular data and can be used with the Joblib framework to make predictions. The model achieves an accuracy of 86.67% on the Iris dataset, and requires the `joblib` and `pandas` libraries in the Python environment.\nTool 18: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 19: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BaekBot is a GPT-2\\u2013based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\", \"parameters\": {\"input_message\": {\"type\": \"string\", \"description\": \"The input message or prompt that will be fed into the model to generate a response in a conversational environment.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool returns a conversational response generated by the BaekBot model in reaction to the provided input message. The response is structured as an object that typically includes the following components:  \\n\\n- **generated_text / response_text**: The main textual output of the model, containing the model\\u2019s human-like reply to the user\\u2019s input.  \\n- **conversation_state / context (optional)**: An object or data structure representing the ongoing conversation context, which may include past user inputs and model outputs. This allows the model to maintain coherence across multiple turns in a dialogue.  \\n- **metadata (optional)**: Additional information about the generation process, which may include token usage, confidence scores, or model-specific details, depending on the implementation of the pipeline.  \\n\\nIn principle, the response is a structured representation of a single conversational turn, containing both the text generated by the model and any contextual or tracking information necessary for multi-turn interactions.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat with an AI about my day.\", \"tool_call\": {\"name\": \"Zixtrauce/BaekBot\", \"arguments\": {\"input_message\": \"Hi BaekBot! How are you doing today?\"}}}, {\"user_intent\": \"Ask the AI for advice on handling stress at work.\", \"tool_call\": {\"name\": \"Zixtrauce/BaekBot\", \"arguments\": {\"input_message\": \"I'm feeling overwhelmed with work. Any tips on managing stress?\"}}}, {\"user_intent\": \"Have the AI tell me a short story to pass the time.\", \"tool_call\": {\"name\": \"Zixtrauce/BaekBot\", \"arguments\": {\"input_message\": \"Can you tell me an interesting short story?\"}}}]}\n\n# User Query\nI am working in a dating chatapp development team. We want to generate sentences to make it more interactive.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate conversational sentences to make the app more interactive.",
                    "tool_description": "Use a conversational AI model to generate human-like responses for interactive text-based conversations within the dating chat app.",
                    "expected_response": "A conversational response generated by the selected AI model that aligns with the tone and interaction style of a dating app."
                }
            ],
            "extracted_arguments": {
                "input_message": "The input message or prompt that will be used to generate the response, typically reflecting the desired tone or scenario for the app."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_481",
        "query": "We are working on a project to detect patterns and correlations in a dataset that contains a mix of code segments and comments. Find a suitable model to complete this task.",
        "instruction": "Given a `pattern detection` task, retrieve tools that are designed to analyze datasets composed of mixed data types, such as code segments and comments, to identify patterns and correlations. These tools should employ feature extraction capabilities suitable for processing multimodal data inputs and providing relevant insights or models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Feature Extraction",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Engineering",
                    "api_call": "AutoModel.from_pretrained('microsoft/unixcoder-base')",
                    "api_arguments": {
                        "tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModel"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')",
                    "performance": {
                        "dataset": "Not specified",
                        "accuracy": "Not specified"
                    },
                    "description": "UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.",
                    "id": "huggingface_api_18",
                    "name": "microsoft/unixcoder-base"
                },
                "id": "gorilla_huggingface_tool_18",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 1: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.",
            "Tool 2: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 5: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 6: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 7: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 8: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 9: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 10: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 11: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 12: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 13: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.",
            "Tool 14: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.",
            "Tool 15: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 16: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 17: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 19: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage."
        ],
        "tool_view": {
            "description": "UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer instance used to preprocess input data for the UniXcoder model. It converts raw text or code into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                }
            },
            "response": "The tool produces a pretrained model object that can be used to extract feature representations from input data, typically source code or associated modalities such as comments or abstract syntax trees (ASTs). The response structure is not explicitly defined in the JSON, but based on the Hugging Face Transformers framework, a reasonable and typical response would include the following components:\n\n1. **Model Object (`AutoModel`)**  \n   - Represents the loaded UniXcoder model.\n   - Encapsulates the neural network layers, weights, and architecture necessary to compute embeddings or feature representations.\n   - Provides methods to process tokenized input and return feature vectors.\n\n2. **Tokenizer Object (`AutoTokenizer`)**  \n   - Handles the preprocessing of raw input (e.g., code snippets, comments) into token IDs suitable for the model.\n   - Performs tokenization, mapping to vocabulary indices, and optionally handles padding, truncation, and attention masks.\n   - Ensures input data conforms to the model’s expected format.\n\n3. **Feature Representations / Embeddings**  \n   - When input data is passed through the model object, the output consists of numerical vectors representing the semantic features of the input.\n   - Typically returned as multi-dimensional tensors, where each token or sequence is represented by a high-dimensional embedding vector.\n   - These embeddings can be used for downstream tasks such as code classification, similarity search, or other feature engineering applications.\n\n4. **Optional Metadata**  \n   - May include model configuration details such as hidden size, number of layers, attention heads, and pretrained settings.\n   - Provides information necessary for consistent usage or integration with other processing pipelines.\n\nIn summary, the response provides the tokenizer and model objects that together enable conversion of raw code-related input into dense numerical features, suitable for various code representation and feature engineering tasks.",
            "examples": [
                {
                    "user_intent": "I want to extract feature embeddings from a Python code snippet using a pre-trained model.",
                    "tool_call": {
                        "name": "microsoft/unixcoder-base",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                        }
                    }
                },
                {
                    "user_intent": "Generate vector representations for multiple code files to use in a similarity search.",
                    "tool_call": {
                        "name": "microsoft/unixcoder-base",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                        }
                    }
                },
                {
                    "user_intent": "I need to create embeddings from code comments and ASTs for a code classification task.",
                    "tool_call": {
                        "name": "microsoft/unixcoder-base",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 1: A fine-tuned DistilBERT model for classifying toxic comments, providing a convenient pipeline to detect harmful or offensive text in natural language inputs.\nTool 2: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 5: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 6: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 7: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 8: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 9: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 10: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 11: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 12: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 13: SantaCoder is a 1.1B-parameter language model designed for generating code in Python, Java, and JavaScript. Built on Hugging Face Transformers, it supports text generation using causal language modeling and is trained on a curated subset of The Stack dataset with filtering for near-duplicates and comment-to-code ratio, leveraging a Fill-in-the-Middle training objective. It enables automated code completion, generation, and synthesis tasks across multiple programming languages, making it useful for developers seeking AI-assisted coding and code automation.\nTool 14: CodeBERT is a pretrained model designed for extracting features from both programming code and natural language. Trained on the CodeSearchNet dataset using a combination of masked language modeling and replaced token detection objectives, it provides a robust representation of bi-modal data for tasks involving code understanding, retrieval, and analysis.\nTool 15: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 16: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 17: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 18: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 19: This tool provides automatic speech recognition (ASR) using a fine-tuned version of the Wav2Vec2 model for Esperanto, leveraging the Common Voice dataset. It converts speech input, sampled at 16 kHz, into text. The tool is built on the Hugging Face Transformers framework and requires a Python environment with libraries like Torch, Torchaudio, and Transformers. It's suitable for tasks where speech-to-text conversion is needed for Esperanto language audio, with an example code showcasing the setup and usage.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer instance used to preprocess input data for the UniXcoder model. It converts raw text or code into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}}, \"response\": \"The tool produces a pretrained model object that can be used to extract feature representations from input data, typically source code or associated modalities such as comments or abstract syntax trees (ASTs). The response structure is not explicitly defined in the JSON, but based on the Hugging Face Transformers framework, a reasonable and typical response would include the following components:\\n\\n1. **Model Object (`AutoModel`)**  \\n   - Represents the loaded UniXcoder model.\\n   - Encapsulates the neural network layers, weights, and architecture necessary to compute embeddings or feature representations.\\n   - Provides methods to process tokenized input and return feature vectors.\\n\\n2. **Tokenizer Object (`AutoTokenizer`)**  \\n   - Handles the preprocessing of raw input (e.g., code snippets, comments) into token IDs suitable for the model.\\n   - Performs tokenization, mapping to vocabulary indices, and optionally handles padding, truncation, and attention masks.\\n   - Ensures input data conforms to the model\\u2019s expected format.\\n\\n3. **Feature Representations / Embeddings**  \\n   - When input data is passed through the model object, the output consists of numerical vectors representing the semantic features of the input.\\n   - Typically returned as multi-dimensional tensors, where each token or sequence is represented by a high-dimensional embedding vector.\\n   - These embeddings can be used for downstream tasks such as code classification, similarity search, or other feature engineering applications.\\n\\n4. **Optional Metadata**  \\n   - May include model configuration details such as hidden size, number of layers, attention heads, and pretrained settings.\\n   - Provides information necessary for consistent usage or integration with other processing pipelines.\\n\\nIn summary, the response provides the tokenizer and model objects that together enable conversion of raw code-related input into dense numerical features, suitable for various code representation and feature engineering tasks.\", \"examples\": [{\"user_intent\": \"I want to extract feature embeddings from a Python code snippet using a pre-trained model.\", \"tool_call\": {\"name\": \"microsoft/unixcoder-base\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}}}, {\"user_intent\": \"Generate vector representations for multiple code files to use in a similarity search.\", \"tool_call\": {\"name\": \"microsoft/unixcoder-base\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}}}, {\"user_intent\": \"I need to create embeddings from code comments and ASTs for a code classification task.\", \"tool_call\": {\"name\": \"microsoft/unixcoder-base\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}}}]}\n\n# User Query\nWe are working on a project to detect patterns and correlations in a dataset that contains a mix of code segments and comments. Find a suitable model to complete this task.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract meaningful features from a dataset containing code and comments for pattern and correlation analysis.",
                    "tool_description": "Use a pre-trained model that generates rich code representations from code snippets and associated comments or ASTs, providing feature vectors suitable for analysis and machine learning tasks.",
                    "expected_response": "A set of feature embeddings representing the semantic content of code segments and comments, returned as high-dimensional numerical vectors."
                }
            ],
            "extracted_arguments": {
                "input_data": "A collection of code segments and associated comments to be processed for feature extraction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_61",
        "query": "An educational software company needs an advanced method to separate voices from background noise. Develop a solution.",
        "instruction": "Given a `voice separation` task, retrieve tools that employ advanced audio processing techniques to effectively isolate vocal tracks from background noise by utilizing specific models or algorithms tailored for this purpose.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 1: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 4: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 6: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.",
            "Tool 7: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.",
            "Tool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 9: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 10: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 11: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 12: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 13: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 14: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 15: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 16: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 17: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 18: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 19: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods."
        ],
        "tool_view": {
            "description": "This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that needs to be enhanced.",
                    "required": true,
                    "example": "speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav"
                }
            },
            "response": "The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \n\n1. **Enhanced Audio Tensor**:  \n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \n   - The values are floating-point numbers representing the audio amplitude.  \n\n2. **Sample Rate**:  \n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \n\n3. **Optional Metadata**:  \n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \n\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of a meeting and want to clean it up for transcription.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/audio/meeting_noisy.wav"
                        }
                    }
                },
                {
                    "user_intent": "Enhance the audio quality of my podcast episode recorded in a cafe.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/podcasts/episode5_cafe.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to denoise a voice memo recorded with background traffic noise.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/voice_memos/traffic_memo.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 1: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 4: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 6: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\nTool 7: This tool provides a pre-trained GradientBoostingRegressor for predicting fish weight, designed for educational use. It can be easily loaded in Python using `skops` and is intended for experimentation and learning with tabular regression tasks.\nTool 8: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 9: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 10: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 11: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 12: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 13: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 14: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 15: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 16: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 17: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 18: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 19: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that needs to be enhanced.\", \"required\": true, \"example\": \"speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav\"}}, \"response\": \"The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \\n\\n1. **Enhanced Audio Tensor**:  \\n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \\n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \\n   - The values are floating-point numbers representing the audio amplitude.  \\n\\n2. **Sample Rate**:  \\n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \\n\\n3. **Optional Metadata**:  \\n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \\n\\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of a meeting and want to clean it up for transcription.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/audio/meeting_noisy.wav\"}}}, {\"user_intent\": \"Enhance the audio quality of my podcast episode recorded in a cafe.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/podcasts/episode5_cafe.wav\"}}}, {\"user_intent\": \"I want to denoise a voice memo recorded with background traffic noise.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/voice_memos/traffic_memo.wav\"}}}]}\n\n# User Query\nAn educational software company needs an advanced method to separate voices from background noise. Develop a solution.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to separate individual voices from a noisy audio recording for educational purposes.",
                    "tool_description": "Perform audio source separation using a pretrained SepFormer model implemented in SpeechBrain, isolating speech signals from background noise and reverberation.",
                    "expected_response": "A processed audio data structure containing separate audio streams for each detected voice, including enhanced speech tensors and corresponding sample rates."
                }
            ],
            "extracted_arguments": {
                "audio_input_path": "The file path to the input audio recording containing mixed voices and background noise."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_231",
        "query": "Our business is expanding to international markets. Analyze the sentiment of the following customer review to better understand their satisfaction with our product: \"¡Esto es maravilloso! Me encanta.\"",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze customer reviews by processing text inputs to determine the sentiment expressed, using multilingual capabilities to provide insights across different languages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "result = sentiment_pipeline('I love this product!')",
                    "performance": {
                        "dataset": [
                            {
                                "language": "English",
                                "accuracy": {
                                    "exact": "67%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Dutch",
                                "accuracy": {
                                    "exact": "57%",
                                    "off-by-1": "93%"
                                }
                            },
                            {
                                "language": "German",
                                "accuracy": {
                                    "exact": "61%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "French",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "Italian",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Spanish",
                                "accuracy": {
                                    "exact": "58%",
                                    "off-by-1": "95%"
                                }
                            }
                        ]
                    },
                    "description": "This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
                    "id": "huggingface_api_914",
                    "name": "bert-base-multilingual-uncased-sentiment"
                },
                "id": "gorilla_huggingface_tool_379",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 5: Parrot is a paraphrase-based text augmentation tool designed to generate varied versions of input utterances, helping accelerate the training of natural language understanding models by expanding and diversifying their training data.",
            "Tool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 7: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 8: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 9: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 10: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 11: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 12: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 13: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 15: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 16: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 17: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 18: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 4: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 5: Parrot is a paraphrase-based text augmentation tool designed to generate varied versions of input utterances, helping accelerate the training of natural language understanding models by expanding and diversifying their training data.\nTool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 7: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 8: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 9: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 10: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 11: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 12: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 13: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 14: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 15: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 16: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 17: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 18: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nOur business is expanding to international markets. Analyze the sentiment of the following customer review to better understand their satisfaction with our product: \"¡Esto es maravilloso! Me encanta.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of a customer review to understand satisfaction with the product.",
                    "tool_description": "Perform sentiment analysis on a customer review to predict the sentiment rating, typically represented as a star rating from 1 to 5, based on the sentiment expressed in the review.",
                    "expected_response": "A sentiment analysis result with a star rating (1 to 5) and a confidence score indicating the likelihood of the sentiment classification."
                }
            ],
            "extracted_arguments": {
                "text": "The customer review in Spanish that needs to be analyzed for sentiment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_264",
        "query": "We are a company producing wine. Based on the chemical properties of our wine data, we need to analyze the quality of our products and determine whether they are good or bad.",
        "instruction": "Given a `wine quality analysis` task, retrieve tools that evaluate the quality of wine based on its chemical properties by employing classification models to process tabular data inputs and determine quality levels.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Wine Quality classification",
                    "api_call": "joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))",
                    "api_arguments": [
                        "X"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nREPO_ID = julien-c/wine-quality\nFILENAME = sklearn_model.joblib\nmodel = joblib.load(cached_download(\n hf_hub_url(REPO_ID, FILENAME)\n))\ndata_file = cached_download(\n hf_hub_url(REPO_ID, winequality-red.csv)\n)\nwinedf = pd.read_csv(data_file, sep=;)\nX = winedf.drop([quality], axis=1)\nY = winedf[quality]\nprint(X[:3])\nlabels = model.predict(X[:3])\nmodel.score(X, Y)",
                    "performance": {
                        "dataset": "julien-c/wine-quality",
                        "accuracy": 0.6616635397123202
                    },
                    "description": "A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
                    "id": "huggingface_api_842",
                    "name": "julien-c/wine-quality"
                },
                "id": "gorilla_huggingface_tool_837",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "Tool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 5: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 8: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 9: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 10: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 11: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 13: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 14: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.",
            "Tool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 16: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 18: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 19: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding."
        ],
        "tool_view": {
            "description": "This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.",
            "parameters": {
                "X": {
                    "type": "DataFrame",
                    "description": "A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.",
                    "required": true,
                    "example": "pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})"
                }
            },
            "response": "The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\n\n- **Type**: Array or list of discrete labels.  \n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \n\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.",
            "examples": [
                {
                    "user_intent": "I want to classify wine quality using a pre-trained Scikit-learn model.",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "wine data to classify"
                        }
                    }
                },
                {
                    "user_intent": "Can I load a model and use it to predict the quality of wines from a CSV file?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "data from winequality-red.csv"
                        }
                    }
                },
                {
                    "user_intent": "How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?",
                    "tool_call": {
                        "name": "osanseviero/wine-quality",
                        "arguments": {
                            "X": "sample wine data from winequality-red.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\nTool 1: This tool provides a simple Scikit-learn pipeline for classifying wine quality based on the \"winequality-red.csv\" dataset. It loads a pre-trained model and the dataset from the Hugging Face Hub, allowing users to make predictions and evaluate accuracy (around 66%) on the wine quality classification task.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 5: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 8: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 9: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 10: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 11: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 12: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 13: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 14: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\nTool 15: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 16: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 18: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 19: BLIP Image Captioning Base is a vision-language model that generates descriptive captions for images. Built on a ViT-based architecture and pre-trained on the COCO dataset, it uses a bootstrapping approach to leverage web data, producing accurate and coherent image descriptions. This tool is used to automatically convert visual content into natural-language text, supporting applications in accessibility, content indexing, and multimedia understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Scikit-learn model for classifying wine quality based on tabular data. It allows users to input wine features and predict quality scores, enabling experiments with machine learning pipelines for tabular classification tasks. The model can be easily loaded from Hugging Face Hub and is suitable for analyzing wine datasets like the red wine quality dataset.\", \"parameters\": {\"X\": {\"type\": \"DataFrame\", \"description\": \"A pandas DataFrame containing the input features for predicting wine quality. Each row represents a wine sample, and each column represents a feature.\", \"required\": true, \"example\": \"pd.DataFrame({'fixed_acidity': [7.4, 7.8, 7.8], 'volatile_acidity': [0.70, 0.88, 0.76], 'citric_acid': [0.00, 0.00, 0.04]})\"}}, \"response\": \"The tool returns a prediction output from a trained Scikit-learn classification model for wine quality. The response structure is as follows:\\n\\n- **Type**: Array or list of discrete labels.  \\n- **Length**: Matches the number of input feature vectors provided in `X`. Each element corresponds to the predicted class for one input sample.  \\n- **Element meaning**: Each label represents the predicted quality category of a wine sample, as determined by the model trained on the wine quality dataset. The labels are typically integer values corresponding to quality scores from the original dataset.  \\n\\nIn principle, the response provides a one-to-one mapping between input feature rows and predicted wine quality classes, enabling downstream evaluation or usage in further processing.\", \"examples\": [{\"user_intent\": \"I want to classify wine quality using a pre-trained Scikit-learn model.\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"wine data to classify\"}}}, {\"user_intent\": \"Can I load a model and use it to predict the quality of wines from a CSV file?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"data from winequality-red.csv\"}}}, {\"user_intent\": \"How do I load a pre-trained wine quality classifier and make predictions on a sample dataset?\", \"tool_call\": {\"name\": \"osanseviero/wine-quality\", \"arguments\": {\"X\": \"sample wine data from winequality-red.csv\"}}}]}\n\n# User Query\nWe are a company producing wine. Based on the chemical properties of our wine data, we need to analyze the quality of our products and determine whether they are good or bad.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the quality of our wines using a pre-trained model.",
                    "tool_description": "Use a Scikit-learn classification model to predict wine quality scores based on input chemical properties provided in tabular format.",
                    "expected_response": "An array of predicted quality labels for each wine sample, indicating their predicted quality categories."
                },
                {
                    "user_intent": "Determine whether each wine is good or bad based on the predicted quality scores.",
                    "tool_description": "Map the predicted wine quality scores to qualitative categories (e.g., 'good' or 'bad') according to predefined thresholds.",
                    "expected_response": "A list of qualitative labels classifying each wine sample as good or bad."
                }
            ],
            "extracted_arguments": {
                "X": "A pandas DataFrame containing the chemical properties of each wine sample, with rows representing wines and columns representing chemical features."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_224",
        "query": "As a clerk in a school, you want to extract information from some student enrollment forms. These forms contain students' details such as Name, age, and address.",
        "instruction": "Given a `document data extraction` task, retrieve tools that process student enrollment forms to accurately extract and list relevant information such as names, ages, and addresses from the forms based on the inputs provided.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Transformers",
                    "functionality": "Document Question Answering",
                    "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')",
                    "api_arguments": {
                        "image": "path/to/image/file"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch",
                        "tensorflow"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.",
                    "id": "huggingface_api_125",
                    "name": "tiny-random-LayoutLMv3ForQuestionAnswering"
                },
                "id": "gorilla_huggingface_tool_124",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 3: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 4: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 6: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 9: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 10: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 11: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 15: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 16: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 17: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 18: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 19: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents."
        ],
        "tool_view": {
            "description": "Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the image that will be processed by the model to generate text.",
                    "required": true,
                    "example": "path/to/document_image.png"
                }
            },
            "response": "The tool’s response is a textual output generated from an input image, representing the content or structured information extracted from that image. The response is produced by a multimodal model consisting of a vision encoder and a text decoder. Specifically:  \n\n- **Overall type**: A string of text, typically representing the model’s interpretation, transcription, or structured content derived from the image.  \n- **Content**: The text may contain:  \n  - Raw transcriptions of visible text in the image (e.g., from documents, forms, receipts).  \n  - Structured representations of the information present, if the model has been fine-tuned for document parsing tasks (e.g., key-value pairs, tables).  \n- **Generation process**:  \n  - The **vision encoder** (Swin Transformer) converts the input image into a tensor of embeddings that capture visual features.  \n  - The **text decoder** (BART) autoregressively generates the textual output, conditioned on the encoded image features.  \n- **Conditional behavior**: The output is influenced by the model’s training on the CORD dataset, so it is optimized for understanding and parsing document images.  \n- **Format nuances**: While the response is primarily a text string, the content may sometimes follow a structured format (e.g., JSON-like key-value extraction) depending on how the model has been prompted or fine-tuned.  \n\nIn summary, the tool transforms an input image into a textual representation, either as free-form text or structured document content, reflecting the visual and semantic information present in the image.",
            "examples": [
                {
                    "user_intent": "Extract all the text from a scanned invoice image I have.",
                    "tool_call": {
                        "name": "donut-base-finetuned-cord-v2",
                        "arguments": {
                            "image": "/home/user/documents/invoice_january.png"
                        }
                    }
                },
                {
                    "user_intent": "I need to parse a receipt photo to get the merchant name, date, and total amount.",
                    "tool_call": {
                        "name": "donut-base-finetuned-cord-v2",
                        "arguments": {
                            "image": "/home/user/photos/receipt_2025-12-01.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Convert this scanned report page into editable text for my records.",
                    "tool_call": {
                        "name": "donut-base-finetuned-cord-v2",
                        "arguments": {
                            "image": "/home/user/reports/report_page3_scan.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 3: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 4: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 6: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 7: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 9: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 10: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 11: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 12: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 14: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 15: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 16: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 17: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 18: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 19: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the image that will be processed by the model to generate text.\", \"required\": true, \"example\": \"path/to/document_image.png\"}}, \"response\": \"The tool\\u2019s response is a textual output generated from an input image, representing the content or structured information extracted from that image. The response is produced by a multimodal model consisting of a vision encoder and a text decoder. Specifically:  \\n\\n- **Overall type**: A string of text, typically representing the model\\u2019s interpretation, transcription, or structured content derived from the image.  \\n- **Content**: The text may contain:  \\n  - Raw transcriptions of visible text in the image (e.g., from documents, forms, receipts).  \\n  - Structured representations of the information present, if the model has been fine-tuned for document parsing tasks (e.g., key-value pairs, tables).  \\n- **Generation process**:  \\n  - The **vision encoder** (Swin Transformer) converts the input image into a tensor of embeddings that capture visual features.  \\n  - The **text decoder** (BART) autoregressively generates the textual output, conditioned on the encoded image features.  \\n- **Conditional behavior**: The output is influenced by the model\\u2019s training on the CORD dataset, so it is optimized for understanding and parsing document images.  \\n- **Format nuances**: While the response is primarily a text string, the content may sometimes follow a structured format (e.g., JSON-like key-value extraction) depending on how the model has been prompted or fine-tuned.  \\n\\nIn summary, the tool transforms an input image into a textual representation, either as free-form text or structured document content, reflecting the visual and semantic information present in the image.\", \"examples\": [{\"user_intent\": \"Extract all the text from a scanned invoice image I have.\", \"tool_call\": {\"name\": \"donut-base-finetuned-cord-v2\", \"arguments\": {\"image\": \"/home/user/documents/invoice_january.png\"}}}, {\"user_intent\": \"I need to parse a receipt photo to get the merchant name, date, and total amount.\", \"tool_call\": {\"name\": \"donut-base-finetuned-cord-v2\", \"arguments\": {\"image\": \"/home/user/photos/receipt_2025-12-01.jpg\"}}}, {\"user_intent\": \"Convert this scanned report page into editable text for my records.\", \"tool_call\": {\"name\": \"donut-base-finetuned-cord-v2\", \"arguments\": {\"image\": \"/home/user/reports/report_page3_scan.png\"}}}]}\n\n# User Query\nAs a clerk in a school, you want to extract information from some student enrollment forms. These forms contain students' details such as Name, age, and address.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract all the text from scanned student enrollment forms.",
                    "tool_description": "Process images of student enrollment forms using a multimodal image-to-text model to convert visual document content into structured textual data.",
                    "expected_response": "Textual output representing the content of the enrollment forms, including key student details such as name, age, and address."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the scanned enrollment form image to be processed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_412",
        "query": "We are an animal rescue organization, and we are looking for a way to automatically identify if images uploaded to our site contain cats, dogs, or birds.",
        "instruction": "Given an `image classification` task, retrieve tools that utilize machine learning models to analyze images and identify specific categories such as cats, dogs, or birds, based on the content of uploaded images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Zero-Shot Image Classification",
                    "framework": "Hugging Face",
                    "functionality": "Zero-Shot Image Classification",
                    "api_call": "pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')",
                    "api_arguments": "image_path, class_names",
                    "python_environment_requirements": "transformers",
                    "example_code": "results = model(image_path, class_names='cat, dog, bird')",
                    "performance": {
                        "dataset": "ImageNet-1k",
                        "accuracy": "76.9"
                    },
                    "description": "A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).",
                    "id": "huggingface_api_354",
                    "name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"
                },
                "id": "gorilla_huggingface_tool_350",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 1: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 3: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 4: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 5: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 6: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 7: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 8: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 11: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 12: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 14: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 15: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.",
            "Tool 16: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 17: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 18: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 19: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pretrained model to use for Named Entity Recognition, loaded via the Transformers library.",
                    "required": true,
                    "example": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer corresponding to the pretrained model, used to convert text into token IDs for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')"
                }
            },
            "response": "The tool performs Named Entity Recognition (NER) using a pre-trained model based on the multilingual BERT architecture, specifically the model named \"Davlan/bert-base-multilingual-cased-ner-hrl\". The response from this tool would contain the following elements:\n\n1. **Entity Labels**: \n   The tool identifies and classifies entities in the provided input text. Each recognized entity is assigned a label that indicates its type. The possible entity labels are:\n   - **LOC** (Location)\n   - **ORG** (Organization)\n   - **PER** (Person)\n\n2. **Entities**: \n   The response will list the specific words or phrases from the input text that were identified as entities. Each entity will typically be paired with its corresponding label (e.g., \"PERSON\", \"LOCATION\", \"ORGANIZATION\").\n\n3. **Entity Boundaries**:\n   The recognized entities will likely include information about the token boundaries, i.e., where the entity starts and ends within the text. This allows users to map the detected entities back to the input text for further processing or analysis.\n\n4. **Confidence Scores (Optional)**:\n   While not explicitly mentioned, many Named Entity Recognition models, especially those based on transformer architectures, might provide confidence scores for each entity prediction. These scores would indicate the model's certainty regarding the classification of each entity.\n\n5. **Language**:\n   The model is capable of recognizing entities in multiple languages, so the response could potentially include a language identifier, specifying which language the text is in (e.g., Arabic, German, English). However, this is typically inferred based on the input and might not be explicitly provided in all cases.\n\nIn summary, the response will be a collection of entities identified in the input text, with each entity classified into one of the predefined categories (Location, Organization, or Person). Optionally, the response could also include confidence scores or other metadata related to the model's performance.",
            "examples": [
                {
                    "user_intent": "I want to extract names of people, locations, and organizations from an English news article.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "Apple announced a partnership with the German automotive company Volkswagen in Berlin yesterday."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities in a Spanish sentence for my multilingual NLP project.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "Lionel Messi jugó un partido increíble para el FC Barcelona en Madrid."
                        }
                    }
                },
                {
                    "user_intent": "Detect named entities in a Chinese sentence to test the model's multilingual capabilities.",
                    "tool_call": {
                        "name": "Davlan/bert-base-multilingual-cased-ner-hrl",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')",
                            "text": "李小龍出生於美國舊金山，並在香港成長。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 1: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 3: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 4: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 5: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 6: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 7: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 8: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 9: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 10: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 11: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 12: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 14: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 15: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\nTool 16: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 17: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 18: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 19: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pretrained model to use for Named Entity Recognition, loaded via the Transformers library.\", \"required\": true, \"example\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer corresponding to the pretrained model, used to convert text into token IDs for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\"}}, \"response\": \"The tool performs Named Entity Recognition (NER) using a pre-trained model based on the multilingual BERT architecture, specifically the model named \\\"Davlan/bert-base-multilingual-cased-ner-hrl\\\". The response from this tool would contain the following elements:\\n\\n1. **Entity Labels**: \\n   The tool identifies and classifies entities in the provided input text. Each recognized entity is assigned a label that indicates its type. The possible entity labels are:\\n   - **LOC** (Location)\\n   - **ORG** (Organization)\\n   - **PER** (Person)\\n\\n2. **Entities**: \\n   The response will list the specific words or phrases from the input text that were identified as entities. Each entity will typically be paired with its corresponding label (e.g., \\\"PERSON\\\", \\\"LOCATION\\\", \\\"ORGANIZATION\\\").\\n\\n3. **Entity Boundaries**:\\n   The recognized entities will likely include information about the token boundaries, i.e., where the entity starts and ends within the text. This allows users to map the detected entities back to the input text for further processing or analysis.\\n\\n4. **Confidence Scores (Optional)**:\\n   While not explicitly mentioned, many Named Entity Recognition models, especially those based on transformer architectures, might provide confidence scores for each entity prediction. These scores would indicate the model's certainty regarding the classification of each entity.\\n\\n5. **Language**:\\n   The model is capable of recognizing entities in multiple languages, so the response could potentially include a language identifier, specifying which language the text is in (e.g., Arabic, German, English). However, this is typically inferred based on the input and might not be explicitly provided in all cases.\\n\\nIn summary, the response will be a collection of entities identified in the input text, with each entity classified into one of the predefined categories (Location, Organization, or Person). Optionally, the response could also include confidence scores or other metadata related to the model's performance.\", \"examples\": [{\"user_intent\": \"I want to extract names of people, locations, and organizations from an English news article.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"Apple announced a partnership with the German automotive company Volkswagen in Berlin yesterday.\"}}}, {\"user_intent\": \"Identify entities in a Spanish sentence for my multilingual NLP project.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"Lionel Messi jug\\u00f3 un partido incre\\u00edble para el FC Barcelona en Madrid.\"}}}, {\"user_intent\": \"Detect named entities in a Chinese sentence to test the model's multilingual capabilities.\", \"tool_call\": {\"name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"text\": \"\\u674e\\u5c0f\\u9f8d\\u51fa\\u751f\\u65bc\\u7f8e\\u570b\\u820a\\u91d1\\u5c71\\uff0c\\u4e26\\u5728\\u9999\\u6e2f\\u6210\\u9577\\u3002\"}}}]}\n\n# User Query\nWe are an animal rescue organization, and we are looking for a way to automatically identify if images uploaded to our site contain cats, dogs, or birds.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically classify images uploaded to our site as cats, dogs, or birds.",
                    "tool_description": "Perform image classification on uploaded images to identify whether they contain cats, dogs, or birds using a pre-trained computer vision model.",
                    "expected_response": "A label or category for each image indicating 'cat', 'dog', or 'bird', potentially with confidence scores."
                }
            ],
            "extracted_arguments": {
                "image_files": "A list of images uploaded to the site that need to be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_332",
        "query": "You have just met a person that speaks French. As a hotel manager, you need to tell them, \"Welcome to our hotel, we hope you enjoy your stay.\" in French.",
        "instruction": "Given a `multilingual translation` task, retrieve tools designed for natural language processing that can translate text from one language to another by processing text inputs in the source language and outputting accurate translations in the target language.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Multilingual Translation",
                    "api_call": "M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')",
                    "api_arguments": {
                        "encoded_input": "Encoded input text",
                        "target_lang": "Target language code"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": [
                        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer",
                        "hi_text = ",
                        "chinese_text = ",
                        "model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)",
                        "tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)",
                        "tokenizer.src_lang = hi",
                        "encoded_hi = tokenizer(hi_text, return_tensors=pt)",
                        "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))",
                        "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "WMT",
                        "accuracy": "Not provided"
                    },
                    "description": "M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.",
                    "id": "huggingface_api_633",
                    "name": "facebook/m2m100_418M"
                },
                "id": "gorilla_huggingface_tool_628",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 3: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 4: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 5: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 6: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 7: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 8: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 9: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 10: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 11: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 13: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 15: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 16: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 17: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 18: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 19: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications."
        ],
        "tool_view": {
            "description": "A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The pre-trained tokenizer used to convert text input into token IDs compatible with the model.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')"
                },
                "model": {
                    "type": "string",
                    "description": "The pre-trained language model used for generating responses in the chatbot.",
                    "required": true,
                    "example": "AutoModelWithLMHead.from_pretrained('output-small')"
                }
            },
            "response": "The tool is designed for **text generation** within the context of **conversational AI**. It leverages the **PyTorch Transformers** framework and specifically utilizes a **DialoGPT model** trained on the speech of a game character, Joshua from *The World Ends With You*.\n\n### Expected Response Description:\n\nThe expected response from the tool, when invoked for text generation, will be a **generated sequence of text** that represents the chatbot's response to a user's input. This will be based on the conversational history provided to the model. \n\n### Key Components of the Response:\n\n1. **Generated Text (Response)**: \n   - The primary output is a **string of text** representing the chatbot's reply to the user's input. This is the AI’s response, generated by the **DialoGPT model** based on the ongoing conversation.\n   \n2. **Tokens Used in Generation**:\n   - The model may also include details about the **tokenization** process, where the input text is converted into tokens and used for generating the output. This is relevant for understanding how the model processes and outputs text.\n\n3. **Model Configurations**:\n   - Although not explicitly stated in the response, it's important to note that the generation process could be controlled by various model parameters, such as:\n     - **max_length**: The maximum length of the generated response.\n     - **temperature**: Controls the randomness of the generation.\n     - **top_k, top_p**: These affect the diversity of the output, with **top_k** sampling the top k possible tokens and **top_p** using nucleus sampling.\n\n4. **Chat History**:\n   - The response may be influenced by prior dialogue history, which is accumulated and used to maintain a coherent conversation. Each input from the user and corresponding AI reply forms part of the **chat history**, which helps the model generate contextually appropriate responses.\n   \n### Performance Information:\n- While the tool provides an example code for generating responses, it does not include any specific accuracy metrics in the response. The model’s behavior is informed by training on a **game script dataset**, though accuracy measures are not disclosed.\n\n### Summary:\nIn essence, the tool returns a **text-based conversational response** generated by the DialoGPT model, based on the current user input and any previous chat history, with optional performance settings influencing the quality and randomness of the output.",
            "examples": [
                {
                    "user_intent": "I want to have a casual conversation with an AI that speaks like Joshua from The World Ends With You.",
                    "tool_call": {
                        "name": "satvikag/chatbot",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')",
                            "model": "AutoModelWithLMHead.from_pretrained('output-small')"
                        }
                    }
                },
                {
                    "user_intent": "Generate responses in the style of a game character for a role-playing scenario.",
                    "tool_call": {
                        "name": "satvikag/chatbot",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')",
                            "model": "AutoModelWithLMHead.from_pretrained('output-small')"
                        }
                    }
                },
                {
                    "user_intent": "Simulate a 5-step conversation with an AI acting as Joshua for testing dialogue flow.",
                    "tool_call": {
                        "name": "satvikag/chatbot",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')",
                            "model": "AutoModelWithLMHead.from_pretrained('output-small')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 1: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 3: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 4: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 5: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 6: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 7: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 8: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 9: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 10: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 11: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 13: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 15: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 16: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 17: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 18: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 19: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The pre-trained tokenizer used to convert text input into token IDs compatible with the model.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\"}, \"model\": {\"type\": \"string\", \"description\": \"The pre-trained language model used for generating responses in the chatbot.\", \"required\": true, \"example\": \"AutoModelWithLMHead.from_pretrained('output-small')\"}}, \"response\": \"The tool is designed for **text generation** within the context of **conversational AI**. It leverages the **PyTorch Transformers** framework and specifically utilizes a **DialoGPT model** trained on the speech of a game character, Joshua from *The World Ends With You*.\\n\\n### Expected Response Description:\\n\\nThe expected response from the tool, when invoked for text generation, will be a **generated sequence of text** that represents the chatbot's response to a user's input. This will be based on the conversational history provided to the model. \\n\\n### Key Components of the Response:\\n\\n1. **Generated Text (Response)**: \\n   - The primary output is a **string of text** representing the chatbot's reply to the user's input. This is the AI\\u2019s response, generated by the **DialoGPT model** based on the ongoing conversation.\\n   \\n2. **Tokens Used in Generation**:\\n   - The model may also include details about the **tokenization** process, where the input text is converted into tokens and used for generating the output. This is relevant for understanding how the model processes and outputs text.\\n\\n3. **Model Configurations**:\\n   - Although not explicitly stated in the response, it's important to note that the generation process could be controlled by various model parameters, such as:\\n     - **max_length**: The maximum length of the generated response.\\n     - **temperature**: Controls the randomness of the generation.\\n     - **top_k, top_p**: These affect the diversity of the output, with **top_k** sampling the top k possible tokens and **top_p** using nucleus sampling.\\n\\n4. **Chat History**:\\n   - The response may be influenced by prior dialogue history, which is accumulated and used to maintain a coherent conversation. Each input from the user and corresponding AI reply forms part of the **chat history**, which helps the model generate contextually appropriate responses.\\n   \\n### Performance Information:\\n- While the tool provides an example code for generating responses, it does not include any specific accuracy metrics in the response. The model\\u2019s behavior is informed by training on a **game script dataset**, though accuracy measures are not disclosed.\\n\\n### Summary:\\nIn essence, the tool returns a **text-based conversational response** generated by the DialoGPT model, based on the current user input and any previous chat history, with optional performance settings influencing the quality and randomness of the output.\", \"examples\": [{\"user_intent\": \"I want to have a casual conversation with an AI that speaks like Joshua from The World Ends With You.\", \"tool_call\": {\"name\": \"satvikag/chatbot\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\", \"model\": \"AutoModelWithLMHead.from_pretrained('output-small')\"}}}, {\"user_intent\": \"Generate responses in the style of a game character for a role-playing scenario.\", \"tool_call\": {\"name\": \"satvikag/chatbot\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\", \"model\": \"AutoModelWithLMHead.from_pretrained('output-small')\"}}}, {\"user_intent\": \"Simulate a 5-step conversation with an AI acting as Joshua for testing dialogue flow.\", \"tool_call\": {\"name\": \"satvikag/chatbot\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\", \"model\": \"AutoModelWithLMHead.from_pretrained('output-small')\"}}}]}\n\n# User Query\nYou have just met a person that speaks French. As a hotel manager, you need to tell them, \"Welcome to our hotel, we hope you enjoy your stay.\" in French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate a sentence from English to French.",
                    "tool_description": "Translate a given English text into French using a machine translation model.",
                    "expected_response": "The translated French version of the input text."
                }
            ],
            "extracted_arguments": {
                "source_text": "The English text that needs to be translated.",
                "target_language": "The language into which the text will be translated (French)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_343",
        "query": "In our online ecommerce platform, we want to build an AI app to automatically recognize the type of products. It should be able to identify common items like clothing, electronics, furniture, and more.",
        "instruction": "Given a `product recognition` task, retrieve tools that utilize image classification to identify various product types by processing visual inputs and discerning categories such as clothing, electronics, and furniture, aligning with the query's requirements for an AI application in ecommerce.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/convnext-large-224"
                    },
                    "python_environment_requirements": {
                        "transformers": "Hugging Face Transformers",
                        "torch": "PyTorch",
                        "datasets": "Hugging Face Datasets"
                    },
                    "example_code": {
                        "import": [
                            "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification",
                            "import torch",
                            "from datasets import load_dataset"
                        ],
                        "load_dataset": "dataset = load_dataset('huggingface/cats-image')",
                        "image": "image = dataset['test']['image'][0]",
                        "feature_extractor": "feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')",
                        "model": "model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')",
                        "inputs": "inputs = feature_extractor(image, return_tensors='pt')",
                        "logits": "with torch.no_grad():\n  logits = model(**inputs).logits",
                        "predicted_label": "predicted_label = logits.argmax(-1).item()",
                        "print": "print(model.config.id2label[predicted_label])"
                    },
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not specified"
                    },
                    "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.",
                    "id": "huggingface_api_174",
                    "name": "facebook/convnext-large-224"
                },
                "id": "gorilla_huggingface_tool_170",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 2: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 3: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 4: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 5: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 6: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 8: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 9: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 10: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 12: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 13: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 14: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 15: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 16: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 17: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 18: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 19: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications."
        ],
        "tool_view": {
            "description": "A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "parameters": {
                "image": {
                    "type": "object",
                    "description": "The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                }
            },
            "response": "The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\n\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\n\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\n\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\n\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\n\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.",
            "examples": [
                {
                    "user_intent": "Segment the clothing items in this portrait image to identify shirts, pants, and jackets.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a group photo to separate and highlight different clothing types for fashion research.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e"
                        }
                    }
                },
                {
                    "user_intent": "Process a full-body street style image to extract the clothing regions for a style recommendation app.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1595152772835-219674b2a8a2"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 1: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 2: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 3: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 4: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 5: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 6: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 8: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 9: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 10: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 12: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 13: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 14: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 15: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 16: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 17: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 18: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 19: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\", \"parameters\": {\"image\": {\"type\": \"object\", \"description\": \"The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}}, \"response\": \"The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\\n\\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\\n\\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\\n\\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\\n\\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\\n\\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.\", \"examples\": [{\"user_intent\": \"Segment the clothing items in this portrait image to identify shirts, pants, and jackets.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d\"}}}, {\"user_intent\": \"Analyze a group photo to separate and highlight different clothing types for fashion research.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\"}}}, {\"user_intent\": \"Process a full-body street style image to extract the clothing regions for a style recommendation app.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1595152772835-219674b2a8a2\"}}}]}\n\n# User Query\nIn our online ecommerce platform, we want to build an AI app to automatically recognize the type of products. It should be able to identify common items like clothing, electronics, furniture, and more.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to recognize the type of products in an image, such as clothing, electronics, and furniture.",
                    "tool_description": "Classify the objects present in an image into predefined categories such as clothing, electronics, and furniture using a vision-based classification model.",
                    "expected_response": "A list of predicted labels for the objects in the image, each corresponding to a category like clothing, electronics, or furniture."
                }
            ],
            "extracted_arguments": {
                "image": "The input image to be classified, typically a product image from the ecommerce platform."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_471",
        "query": "I am a game developer working on a game project involving moving carts. I need to use reinforcement learning to improve the game experience.",
        "instruction": "Given a `reinforcement learning application` task, retrieve tools that utilize reinforcement learning models and frameworks, such as Stable-Baselines3, to enhance game experiences by applying algorithms like PPO to specific environments, such as CartPole-v1, as per the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/ppo-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "f"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_901",
                    "name": "sb3/ppo-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_894",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 3: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 4: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 5: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 6: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 7: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.",
            "Tool 8: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 9: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 10: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 11: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 12: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 14: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 15: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 16: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 17: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 18: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 19: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 2: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 3: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 4: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 5: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 6: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 7: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.\nTool 8: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 9: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 10: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 11: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 12: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 13: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 14: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 15: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 16: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 17: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 18: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 19: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nI am a game developer working on a game project involving moving carts. I need to use reinforcement learning to improve the game experience.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained reinforcement learning agent for a cart-based simulation environment.",
                    "tool_description": "Provide access to a pre-trained reinforcement learning model, such as PPO or DQN, designed for environments involving cart dynamics, enabling evaluation or deployment without training from scratch.",
                    "expected_response": "A reinforcement learning agent object ready to interact with a cart simulation environment."
                },
                {
                    "user_intent": "I want to train or fine-tune the reinforcement learning agent to optimize gameplay experience for moving carts.",
                    "tool_description": "Use a reinforcement learning framework to train or fine-tune an agent in a cart-based environment, leveraging pre-trained models or starting from scratch, to improve control policies and game experience.",
                    "expected_response": "Training logs, performance metrics, and an updated reinforcement learning agent capable of controlling the cart environment efficiently."
                }
            ],
            "extracted_arguments": {
                "environment_name": "The identifier or type of the simulation environment representing cart dynamics.",
                "rl_algorithm": "The reinforcement learning algorithm to be used, such as PPO or DQN.",
                "pretrained_model": "Optional flag or reference indicating whether to load a pre-trained agent before training.",
                "training_parameters": "Hyperparameters and settings for training the agent, including learning rate, number of episodes, and reward functions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_159",
        "query": "Our organization sells movies. We need to collect reviews from various platforms to understand the popularity of a movie.",
        "instruction": "Given a `movie review collection` task, retrieve tools that can gather reviews from various platforms to analyze movie popularity, ensuring compatibility with text classification and sentiment analysis frameworks to interpret public opinion accurately.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Classification",
                    "api_call": "pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers",
                        "pytorch"
                    ],
                    "example_code": "classifier('I love this movie!')",
                    "performance": {
                        "dataset": "imdb",
                        "accuracy": 0.928
                    },
                    "description": "This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.",
                    "id": "huggingface_api_389",
                    "name": "lvwerra/distilbert-imdb"
                },
                "id": "gorilla_huggingface_tool_385",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 9: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 10: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 13: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 14: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 16: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 17: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 18: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 19: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "parameters": {},
            "response": "The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \n\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \n- **score**: A numeric value representing the model’s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \n\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers’ sentiment-analysis pipeline.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "The plot was captivating and the acting was superb."
                        }
                    }
                },
                {
                    "user_intent": "I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "I couldn't stand the boring dialogue and slow pacing."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a short review: 'It was an average movie, nothing special.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "It was an average movie, nothing special."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 2: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 3: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 5: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 9: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 10: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 11: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 12: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 13: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 14: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 16: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 17: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 18: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 19: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\", \"parameters\": {}, \"response\": \"The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \\n\\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \\n- **score**: A numeric value representing the model\\u2019s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \\n\\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers\\u2019 sentiment-analysis pipeline.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"The plot was captivating and the acting was superb.\"}}}, {\"user_intent\": \"I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"I couldn't stand the boring dialogue and slow pacing.\"}}}, {\"user_intent\": \"Check the sentiment of a short review: 'It was an average movie, nothing special.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"It was an average movie, nothing special.\"}}}]}\n\n# User Query\nOur organization sells movies. We need to collect reviews from various platforms to understand the popularity of a movie.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to collect movie reviews from various platforms to gauge popularity.",
                    "tool_description": "Gather reviews from multiple sources (e.g., social media, review websites) related to specific movies for analysis.",
                    "expected_response": "A collection of movie review objects with ratings, sentiment labels, and associated metadata like reviewer details or platform."
                }
            ],
            "extracted_arguments": {
                "movie_title": "The title of the movie being reviewed.",
                "platforms": "A list of platforms or websites from which to collect reviews.",
                "reviewer_info": "Information about the reviewer (e.g., username, reviewer ID).",
                "review_content": "The textual content of the review."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_365",
        "query": "We are a team of architects and civil engineers looking to estimate the depth of elements in architectural designs from 2D images of the structures. We want to implement a depth estimation model that will transform these images into depictions of depth.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to process 2D images and transform them into depth representations, meeting the query's architectural and engineering requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Depth Estimation",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3736,
                            "Mae": 0.3079,
                            "Rmse": 0.4321,
                            "Abs Rel": 0.3666,
                            "Log Mae": 0.1288,
                            "Log Rmse": 0.1794,
                            "Delta1": 0.4929,
                            "Delta2": 0.7934,
                            "Delta3": 0.9234
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_162",
                    "name": "glpn-nyu-finetuned-diode-221116-104421"
                },
                "id": "gorilla_huggingface_tool_158",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 1: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 2: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 3: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 4: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 5: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 6: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 8: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 9: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 11: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 12: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 13: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 16: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.",
            "Tool 17: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 18: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 19: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input."
        ],
        "tool_view": {
            "description": "GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "parameters": {
                "images": {
                    "type": "any",
                    "description": "A single image or a batch of images to be processed for depth estimation. This input can be in various formats, such as a PIL image, numpy array, or a file path.",
                    "required": true,
                    "example": "http://images.cocodataset.org/val2017/000000039769.jpg"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the format in which to return the tensor data. Typically set to 'pt' (PyTorch tensors), but other formats can be used depending on the framework.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The response for the described tool is expected to contain the following components:\n\n1. **Predicted Depth**:  \n   The tool will return the depth map corresponding to the input image, representing the estimated depth of each pixel in the image. This will be a tensor or array where each value corresponds to the depth of a specific point in the scene.\n\n2. **Prediction Process**:  \n   The depth map will be generated through a process that involves interpolating the predicted depth to match the original image size. The interpolation will use a bicubic method and align corners to provide a more accurate and visually smooth depth representation.\n\n3. **Formatted Depth Map**:  \n   The depth map will be post-processed to scale the depth values, typically normalizing them to a range where the maximum value is 255 (often for visualization purposes). This post-processing step ensures the output is in a format suitable for display or further analysis, and it will be provided as an image (likely in the form of a `PIL.Image` object).\n\nIn principle, the response will deliver:\n- The **predicted depth values** (as a tensor or array).\n- A **formatted depth map** (typically converted into an image or array suitable for visualization).\n",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth of an image using the GLPN model from Hugging Face.",
                    "tool_call": {
                        "name": "glpn-kitti",
                        "arguments": {
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Can I use the GLPN model to predict depth from an image URL?",
                    "tool_call": {
                        "name": "glpn-kitti",
                        "arguments": {
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Estimate depth from a local image file using the GLPN model.",
                    "tool_call": {
                        "name": "glpn-kitti",
                        "arguments": {
                            "images": "path/to/local/image.jpg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 1: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 2: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 3: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 4: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 5: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 6: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 8: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 9: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 11: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 12: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 13: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 14: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 16: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\nTool 17: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 18: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 19: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\", \"parameters\": {\"images\": {\"type\": \"any\", \"description\": \"A single image or a batch of images to be processed for depth estimation. This input can be in various formats, such as a PIL image, numpy array, or a file path.\", \"required\": true, \"example\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the format in which to return the tensor data. Typically set to 'pt' (PyTorch tensors), but other formats can be used depending on the framework.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The response for the described tool is expected to contain the following components:\\n\\n1. **Predicted Depth**:  \\n   The tool will return the depth map corresponding to the input image, representing the estimated depth of each pixel in the image. This will be a tensor or array where each value corresponds to the depth of a specific point in the scene.\\n\\n2. **Prediction Process**:  \\n   The depth map will be generated through a process that involves interpolating the predicted depth to match the original image size. The interpolation will use a bicubic method and align corners to provide a more accurate and visually smooth depth representation.\\n\\n3. **Formatted Depth Map**:  \\n   The depth map will be post-processed to scale the depth values, typically normalizing them to a range where the maximum value is 255 (often for visualization purposes). This post-processing step ensures the output is in a format suitable for display or further analysis, and it will be provided as an image (likely in the form of a `PIL.Image` object).\\n\\nIn principle, the response will deliver:\\n- The **predicted depth values** (as a tensor or array).\\n- A **formatted depth map** (typically converted into an image or array suitable for visualization).\\n\", \"examples\": [{\"user_intent\": \"I want to estimate the depth of an image using the GLPN model from Hugging Face.\", \"tool_call\": {\"name\": \"glpn-kitti\", \"arguments\": {\"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Can I use the GLPN model to predict depth from an image URL?\", \"tool_call\": {\"name\": \"glpn-kitti\", \"arguments\": {\"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Estimate depth from a local image file using the GLPN model.\", \"tool_call\": {\"name\": \"glpn-kitti\", \"arguments\": {\"images\": \"path/to/local/image.jpg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nWe are a team of architects and civil engineers looking to estimate the depth of elements in architectural designs from 2D images of the structures. We want to implement a depth estimation model that will transform these images into depictions of depth.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth of architectural images using a pre-trained depth estimation model.",
                    "tool_description": "Use a Hugging Face Transformers-based GLPN model fine-tuned for depth estimation to predict per-pixel depth maps from 2D images of architectural structures.",
                    "expected_response": "A depth map tensor or array representing estimated depth values for each pixel, along with a post-processed image formatted for visualization."
                }
            ],
            "extracted_arguments": {
                "images": "A single image or a batch of images of architectural structures, in a format compatible with the depth estimation model (e.g., PIL image, numpy array, or file path).",
                "return_tensors": "The desired format for the output depth map tensor, such as 'pt' for PyTorch tensors."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_102",
        "query": "We have a road safety app that helps identify potholes. Can we use an image segmentation model to detect if there are potholes in the pictures of roads?",
        "instruction": "Given an `image segmentation` task, retrieve tools that use image processing capabilities to detect specific objects, such as potholes, within road images by providing detailed segmented outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Segmentation",
                    "api_call": "YOLO('keremberke/yolov8s-pothole-segmentation')",
                    "api_arguments": {
                        "image": "URL or local path to the image"
                    },
                    "python_environment_requirements": {
                        "ultralyticsplus": "0.0.23",
                        "ultralytics": "8.0.21"
                    },
                    "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nprint(results[0].masks)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "pothole-segmentation",
                        "accuracy": {
                            "mAP@0.5(box)": 0.928,
                            "mAP@0.5(mask)": 0.928
                        }
                    },
                    "description": "A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.",
                    "id": "huggingface_api_256",
                    "name": "keremberke/yolov8s-pothole-segmentation"
                },
                "id": "gorilla_huggingface_tool_252",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 1: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.",
            "Tool 2: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.",
            "Tool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 4: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 5: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 6: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.",
            "Tool 7: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 8: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.",
            "Tool 9: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 10: The \"facebook/mask2former-swin-small-coco-instance\" is a Mask2Former model designed for universal image segmentation, including instance, semantic, and panoptic segmentation tasks. Built on a Swin transformer backbone and trained on the COCO dataset, it predicts masks and corresponding labels for all three segmentation types. This model outperforms previous state-of-the-art models like MaskFormer in both performance and efficiency. It is available for use through the Hugging Face Transformers framework, enabling seamless integration for image segmentation tasks.",
            "Tool 11: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 12: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 13: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.",
            "Tool 14: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.",
            "Tool 15: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.",
            "Tool 16: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 17: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.",
            "Tool 18: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 19: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment."
        ],
        "tool_view": {
            "description": "This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\n\n1. **Boxes**  \n   - A collection of bounding boxes corresponding to detected potholes in the image.  \n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - May include additional metadata such as confidence scores for each detection.\n\n2. **Masks**  \n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\n\n3. **Class Labels (optional)**  \n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\n\n4. **Confidence Scores (optional)**  \n   - A numeric confidence value for each detection, reflecting the model’s certainty that the detected object is indeed a pothole.\n\n5. **Rendered Output (optional)**  \n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\n\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.",
            "examples": [
                {
                    "user_intent": "I want to detect potholes in an image using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/pothole_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Can I use a local image to detect potholes with the YOLOv8 model?",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "/path/to/local/image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/street_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 1: A YOLOv8 model designed for detecting and segmenting potholes in images, providing both bounding boxes and detailed segmentation masks to identify pothole locations and shapes, trained on the keremberke/pothole-segmentation dataset for accurate pothole analysis.\nTool 2: This tool uses a YOLOv8 model for pothole segmentation in images, achieving high accuracy in detecting potholes. It leverages the Hugging Face Transformers framework and can be used with specific parameters such as confidence threshold, Intersection over Union (IoU) values, and non-maximum suppression settings to optimize performance. The model is pre-trained on the pothole-segmentation dataset, making it particularly suitable for applications in road maintenance and infrastructure monitoring.\nTool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 4: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 5: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 6: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single Transformer-based architecture. Trained on the ADE20k dataset with a Swin backbone, it leverages task-specific tokens to adapt dynamically to different segmentation tasks, enabling high flexibility and efficiency without needing separate models for each task. It is designed for use in computer vision applications that require accurate and versatile image segmentation across multiple categories.\nTool 7: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 8: CLIPSeg-RD64-Refined is a Hugging Face Transformers model for image segmentation that enables zero-shot and one-shot segmentation using text and image prompts. It uses a refined architecture with reduced dimensionality to improve segmentation quality and can be accessed via the `image-segmentation` pipeline for flexible, prompt-based image analysis.\nTool 9: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 10: The \"facebook/mask2former-swin-small-coco-instance\" is a Mask2Former model designed for universal image segmentation, including instance, semantic, and panoptic segmentation tasks. Built on a Swin transformer backbone and trained on the COCO dataset, it predicts masks and corresponding labels for all three segmentation types. This model outperforms previous state-of-the-art models like MaskFormer in both performance and efficiency. It is available for use through the Hugging Face Transformers framework, enabling seamless integration for image segmentation tasks.\nTool 11: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 12: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 13: The MaskFormer model, trained on the COCO panoptic segmentation dataset, is a transformer-based model designed for image segmentation tasks. It uses a Swin backbone and can be easily accessed via the Hugging Face Transformers framework. This tool performs instance segmentation by processing input images, extracting relevant features, and generating segmentation masks, suitable for tasks like object detection and image analysis. The model can be loaded and utilized in Python using the provided API and requires libraries like transformers, PIL, and requests.\nTool 14: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.\nTool 15: This tool provides a ControlNet model conditioned on image segmentation, enabling precise control of Stable Diffusion outputs using segmentation maps. It is designed for generating images that adhere to specific structural layouts and is trained on a large dataset of segmentation-image pairs, making it useful for tasks that require structured or layout-aware image synthesis.\nTool 16: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 17: OneFormer is a universal image segmentation model that performs semantic, instance, and panoptic segmentation using a single architecture and model. It allows users to process an image for different segmentation tasks by conditioning the model with task-specific inputs, enabling flexible, task-guided inference without needing separate models for each task. This tool is useful for efficiently generating detailed segmentation maps from images across multiple segmentation types with a single, pretrained model.\nTool 18: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 19: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\\n\\n1. **Boxes**  \\n   - A collection of bounding boxes corresponding to detected potholes in the image.  \\n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - May include additional metadata such as confidence scores for each detection.\\n\\n2. **Masks**  \\n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \\n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\\n\\n3. **Class Labels (optional)**  \\n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\\n\\n4. **Confidence Scores (optional)**  \\n   - A numeric confidence value for each detection, reflecting the model\\u2019s certainty that the detected object is indeed a pothole.\\n\\n5. **Rendered Output (optional)**  \\n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\\n\\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.\", \"examples\": [{\"user_intent\": \"I want to detect potholes in an image using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/pothole_image.jpg\"}}}, {\"user_intent\": \"Can I use a local image to detect potholes with the YOLOv8 model?\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"/path/to/local/image.jpg\"}}}, {\"user_intent\": \"I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/street_image.jpg\"}}}]}\n\n# User Query\nWe have a road safety app that helps identify potholes. Can we use an image segmentation model to detect if there are potholes in the pictures of roads?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect potholes in an image using the YOLOv8 pothole segmentation model.",
                    "tool_description": "Use a YOLOv8-based image segmentation model to identify potholes in road images, producing both bounding boxes and segmentation masks for precise localization and shape analysis.",
                    "expected_response": "A structured result containing bounding boxes and pixel-level masks for detected potholes, with optional confidence scores and visual rendering of detections."
                }
            ],
            "extracted_arguments": {
                "image": "The input image of a road for pothole detection, either as a URL or local file path."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_353",
        "query": "A stock investor is looking to analyze the sentiment of a stock forum, such as StockTwits, to gain insights into the market sentiment for a specific stock.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze textual data related to stock market discussions by processing text inputs to infer market sentiment using pre-trained models specifically tailored for stock-related content.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Inferencing for stock-related comments",
                    "api_call": "RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')",
                    "api_arguments": {
                        "model": "RobertaForSequenceClassification",
                        "tokenizer": "RobertaTokenizer"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import RobertaForSequenceClassification, RobertaTokenizer\nfrom transformers import pipeline\nimport pandas as pd\nimport emoji\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\nsentences = list(sentences)\nresults = nlp(sentences)\nprint(results)",
                    "performance": {
                        "dataset": "stocktwits",
                        "accuracy": 0.9343
                    },
                    "description": "This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.",
                    "id": "huggingface_api_384",
                    "name": "zhayunduo/roberta-base-stocktwits-finetuned"
                },
                "id": "gorilla_huggingface_tool_380",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 1: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 4: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 5: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 8: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 9: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 10: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.",
            "Tool 11: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 12: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.",
            "Tool 13: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 14: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 15: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 16: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 17: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 18: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages."
        ],
        "tool_view": {
            "description": "A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "parameters": {
                "sentences": {
                    "type": "array",
                    "description": "A list of text strings, each representing a stock-related comment or sentence to classify for sentiment (Bullish or Bearish).",
                    "required": true,
                    "example": [
                        "just buy",
                        "just sell it",
                        "go down"
                    ]
                },
                "model": {
                    "type": "string",
                    "description": "The pre-trained Hugging Face model used for sequence classification. For this tool, it should be 'RobertaForSequenceClassification'.",
                    "required": true,
                    "example": "RobertaForSequenceClassification"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer corresponding to the pre-trained model, used to convert input text into tokens suitable for the model.",
                    "required": true,
                    "example": "RobertaTokenizer"
                }
            },
            "response": "The tool produces a structured response representing the sentiment classification of each input text. Each element in the response corresponds to one input comment and typically includes the following components:\n\n1. **`label`** – A string indicating the predicted sentiment class for the input text. In this case, the label is derived from the user-labeled categories in the training data, which are `'Bullish'` or `'Bearish'`. This reflects whether the model predicts a positive/upward (Bullish) or negative/downward (Bearish) sentiment regarding stock-related commentary.\n\n2. **`score`** – A numerical value, usually a floating-point number between 0 and 1, representing the model’s confidence in the predicted label. Higher values indicate stronger confidence in the classification.\n\nThe full response is an ordered list, where each entry corresponds to one input comment in the same order as provided to the model. Each entry contains the `label` and `score` fields described above. This structure allows downstream code to associate each prediction directly with its corresponding input sentence.",
            "examples": [
                {
                    "user_intent": "Analyze these stock-related messages to see if people are bullish or bearish: 'Apple is going to the moon!', 'Time to sell Tesla', 'Holding Amazon long-term.'",
                    "tool_call": {
                        "name": "zhayunduo/roberta-base-stocktwits-finetuned",
                        "arguments": {
                            "sentences": [
                                "Apple is going to the moon!",
                                "Time to sell Tesla",
                                "Holding Amazon long-term."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of recent market comments to guide my trading decisions.",
                    "tool_call": {
                        "name": "zhayunduo/roberta-base-stocktwits-finetuned",
                        "arguments": {
                            "sentences": [
                                "I think GME will crash soon 😬",
                                "MSFT showing strong growth, buy now!",
                                "Not sure about Bitcoin, might dip further."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I want to quickly classify a list of short StockTwits posts into bullish or bearish.",
                    "tool_call": {
                        "name": "zhayunduo/roberta-base-stocktwits-finetuned",
                        "arguments": {
                            "sentences": [
                                "Just bought some NVIDIA shares, feeling optimistic!",
                                "Sell all my AMC stocks, it's overvalued.",
                                "Tesla is unpredictable but might go up."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 1: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 3: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 4: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 5: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 8: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 9: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 10: This tool uses a fine-tuned version of the BERT-base-uncased model for sentiment analysis, specifically trained on a classified dataset for text classification tasks. It is designed to classify text sentiment and is not intended for further fine-tuning for other tasks.\nTool 11: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 12: A fine-tuned DistilBERT model for text classification, trained on the SST-2 dataset to identify sentiment in text with high accuracy. It can be used to classify input text into predefined categories, making it suitable for tasks like sentiment analysis or topic classification in natural language processing applications.\nTool 13: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 14: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 15: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 16: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 17: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 18: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 19: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \\\"Bullish\\\" or \\\"Bearish\\\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\", \"parameters\": {\"sentences\": {\"type\": \"array\", \"description\": \"A list of text strings, each representing a stock-related comment or sentence to classify for sentiment (Bullish or Bearish).\", \"required\": true, \"example\": [\"just buy\", \"just sell it\", \"go down\"]}, \"model\": {\"type\": \"string\", \"description\": \"The pre-trained Hugging Face model used for sequence classification. For this tool, it should be 'RobertaForSequenceClassification'.\", \"required\": true, \"example\": \"RobertaForSequenceClassification\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer corresponding to the pre-trained model, used to convert input text into tokens suitable for the model.\", \"required\": true, \"example\": \"RobertaTokenizer\"}}, \"response\": \"The tool produces a structured response representing the sentiment classification of each input text. Each element in the response corresponds to one input comment and typically includes the following components:\\n\\n1. **`label`** \\u2013 A string indicating the predicted sentiment class for the input text. In this case, the label is derived from the user-labeled categories in the training data, which are `'Bullish'` or `'Bearish'`. This reflects whether the model predicts a positive/upward (Bullish) or negative/downward (Bearish) sentiment regarding stock-related commentary.\\n\\n2. **`score`** \\u2013 A numerical value, usually a floating-point number between 0 and 1, representing the model\\u2019s confidence in the predicted label. Higher values indicate stronger confidence in the classification.\\n\\nThe full response is an ordered list, where each entry corresponds to one input comment in the same order as provided to the model. Each entry contains the `label` and `score` fields described above. This structure allows downstream code to associate each prediction directly with its corresponding input sentence.\", \"examples\": [{\"user_intent\": \"Analyze these stock-related messages to see if people are bullish or bearish: 'Apple is going to the moon!', 'Time to sell Tesla', 'Holding Amazon long-term.'\", \"tool_call\": {\"name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"arguments\": {\"sentences\": [\"Apple is going to the moon!\", \"Time to sell Tesla\", \"Holding Amazon long-term.\"]}}}, {\"user_intent\": \"Check the sentiment of recent market comments to guide my trading decisions.\", \"tool_call\": {\"name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"arguments\": {\"sentences\": [\"I think GME will crash soon \\ud83d\\ude2c\", \"MSFT showing strong growth, buy now!\", \"Not sure about Bitcoin, might dip further.\"]}}}, {\"user_intent\": \"I want to quickly classify a list of short StockTwits posts into bullish or bearish.\", \"tool_call\": {\"name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"arguments\": {\"sentences\": [\"Just bought some NVIDIA shares, feeling optimistic!\", \"Sell all my AMC stocks, it's overvalued.\", \"Tesla is unpredictable but might go up.\"]}}}]}\n\n# User Query\nA stock investor is looking to analyze the sentiment of a stock forum, such as StockTwits, to gain insights into the market sentiment for a specific stock.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze stock-related forum messages to determine if the sentiment is bullish or bearish.",
                    "tool_description": "Use a Hugging Face Transformers model fine-tuned on StockTwits posts to classify stock-related comments as 'Bullish' or 'Bearish', allowing inference of market sentiment from textual data.",
                    "expected_response": "A list of sentiment classification results where each comment is labeled as 'Bullish' or 'Bearish' along with a confidence score."
                }
            ],
            "extracted_arguments": {
                "sentences": "A list of stock-related comments or forum messages to classify for sentiment (Bullish or Bearish).",
                "model": "The pre-trained Hugging Face model used for sequence classification.",
                "tokenizer": "The tokenizer corresponding to the pre-trained model, used to convert input text into model-ready tokens."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_373",
        "query": "Develop an application to transcribe audio files with punctuation marks for a podcast platform.",
        "instruction": "Given a `punctuated transcription` task, retrieve tools that transcribe audio files into text while incorporating punctuation marks, ensuring the output is suitable for applications like podcast platforms where clear prosody is essential.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Wav2Vec2ForCTC.from_pretrained('jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "https://github.com/neonbjb/ocotillo",
                    "performance": {
                        "dataset": "librispeech validation set",
                        "accuracy": "4.45%"
                    },
                    "description": "This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.",
                    "id": "huggingface_api_748",
                    "name": "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli"
                },
                "id": "gorilla_huggingface_tool_743",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 2: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 3: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 4: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 5: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 6: This tool performs speech recognition on Portuguese audio files using the fine-tuned Wav2Vec 2.0 model (jonatasgrosman/wav2vec2-large-xlsr-53-portuguese). It transcribes audio inputs, such as MP3 or WAV files, into text and is optimized for speech sampled at 16kHz. The model is based on the Common Voice 6.1 dataset and can be used within a Python environment with libraries like Hugging Face Transformers, Torch, and Librosa.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 9: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 10: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 11: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 12: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 13: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 14: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 15: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 16: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 17: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 18: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.",
            "Tool 19: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings."
        ],
        "tool_view": {
            "description": "This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.",
                    "required": true,
                    "example": "My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller"
                }
            },
            "response": "The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\n\n### Expected Response Description:\n\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\n- Periods (.)\n- Commas (,)\n- Question marks (?)\n- Dashes (-)\n- Colons (:)\n\n### Key Components of the Response:\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\n  \n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\n\n",
            "examples": [
                {
                    "user_intent": "I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps"
                        }
                    }
                },
                {
                    "user_intent": "I need proper punctuation for a German interview transcript I just transcribed.",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview"
                        }
                    }
                },
                {
                    "user_intent": "Can you add punctuation to this Italian speech transcription so I can publish it as an article?",
                    "tool_call": {
                        "name": "PunctuationModel",
                        "arguments": {
                            "text": "oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 1: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 2: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 3: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 4: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 5: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 6: This tool performs speech recognition on Portuguese audio files using the fine-tuned Wav2Vec 2.0 model (jonatasgrosman/wav2vec2-large-xlsr-53-portuguese). It transcribes audio inputs, such as MP3 or WAV files, into text and is optimized for speech sampled at 16kHz. The model is based on the Common Voice 6.1 dataset and can be used within a Python environment with libraries like Hugging Face Transformers, Torch, and Librosa.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 9: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 10: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 11: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 12: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 13: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 14: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 15: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 16: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 17: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 18: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.\nTool 19: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text string for which the model will predict and restore punctuation. This should be a continuous sequence of words without punctuation.\", \"required\": true, \"example\": \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\\u00fcller\"}}, \"response\": \"The tool is designed to predict punctuation for transcribed spoken language. Given a text input, the model predicts the appropriate punctuation marks (e.g., periods, commas, question marks, dashes, and colons) for languages such as English, Italian, French, and German.\\n\\n### Expected Response Description:\\n\\nThe model's response consists of a **text string** that represents the input text with the restored punctuation. Specifically, the text will contain the appropriate punctuation marks inserted at positions where they were missing or omitted in the original transcription. The punctuation marks the model restores include:\\n- Periods (.)\\n- Commas (,)\\n- Question marks (?)\\n- Dashes (-)\\n- Colons (:)\\n\\n### Key Components of the Response:\\n- **Restored Text**: A string of text with punctuation marks inserted. This text corresponds to the original input text but with predicted punctuation added based on the model's understanding of the structure and meaning of the sentence.\\n  \\n  The output will not simply be a prediction about the presence or absence of punctuation, but a complete transformation of the input text, incorporating the correct punctuation at appropriate places.\\n\\n\", \"examples\": [{\"user_intent\": \"I have a transcript of an English meeting without punctuation. Can you restore it so it's easier to read?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"good morning everyone thank you for joining the meeting today we will discuss the quarterly results and next steps\"}}}, {\"user_intent\": \"I need proper punctuation for a German interview transcript I just transcribed.\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"guten tag mein name ist klaus ich arbeite seit zehn jahren in berlin und ich freue mich auf das interview\"}}}, {\"user_intent\": \"Can you add punctuation to this Italian speech transcription so I can publish it as an article?\", \"tool_call\": {\"name\": \"PunctuationModel\", \"arguments\": {\"text\": \"oggi parleremo dei risultati del progetto e delle sfide che abbiamo incontrato durante lo sviluppo\"}}}]}\n\n# User Query\nDevelop an application to transcribe audio files with punctuation marks for a podcast platform.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to transcribe audio files with punctuation for a podcast platform.",
                    "tool_description": "Use an automatic speech recognition model to transcribe audio files into text, ensuring that punctuation marks are accurately inserted into the transcriptions.",
                    "expected_response": "A text transcription of the audio file with appropriate punctuation marks (e.g., periods, commas, question marks, dashes, colons) inserted at the correct positions."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The input audio file to be transcribed, typically in formats such as MP3 or WAV.",
                "platform": "The target platform or service where the transcription will be used, in this case, a podcast platform."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_409",
        "query": "Our company is developing a game, and they want to include an AI agent to play the game. We need to evaluate its performance on the CartPole-v1 environment for consistency.",
        "instruction": "Given a `performance evaluation` task, retrieve tools that facilitate assessing an AI agent's performance in a specific gaming environment, such as CartPole-v1, by leveraging reinforcement learning frameworks to ensure consistency and accuracy.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Stable-Baselines3",
                    "functionality": "CartPole-v1",
                    "api_call": "load_from_hub(repo_id='sb3/dqn-CartPole-v1')",
                    "api_arguments": [
                        "algo",
                        "env",
                        "logs"
                    ],
                    "python_environment_requirements": [
                        "rl_zoo3",
                        "stable-baselines3",
                        "stable-baselines3-contrib"
                    ],
                    "example_code": "python train.py --algo dqn --env CartPole-v1 -f logs/",
                    "performance": {
                        "dataset": "CartPole-v1",
                        "accuracy": "500.00 +/- 0.00"
                    },
                    "description": "This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
                    "id": "huggingface_api_903",
                    "name": "dqn-CartPole-v1"
                },
                "id": "gorilla_huggingface_tool_896",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 1: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 2: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 4: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 5: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 6: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 8: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 9: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 10: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 11: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 12: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.",
            "Tool 14: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 15: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 16: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 17: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.",
            "Tool 18: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 19: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data."
        ],
        "tool_view": {
            "description": "Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "parameters": {
                "algo": {
                    "type": "string",
                    "description": "The reinforcement learning algorithm to use for training or loading the agent, e.g., DQN, PPO, or A2C.",
                    "required": true,
                    "example": "dqn"
                },
                "env": {
                    "type": "string",
                    "description": "The environment identifier from OpenAI Gym on which the agent will be trained or evaluated, e.g., CartPole-v1.",
                    "required": true,
                    "example": "CartPole-v1"
                },
                "logs": {
                    "type": "string",
                    "description": "The file path or directory where training logs, model checkpoints, and other outputs will be stored.",
                    "required": true,
                    "example": "logs/"
                }
            },
            "response": "The tool returns a pre-trained reinforcement learning model of a DQN agent for the CartPole-v1 environment. The response typically includes the following components:  \n\n1. **Algorithm instance (`algo`)**: This is the loaded DQN algorithm object, fully configured with the trained policy and hyperparameters. It can be used to interact with the environment, perform predictions, or continue training.  \n\n2. **Environment instance (`env`)**: The corresponding CartPole-v1 environment object that the model was trained on. This allows evaluation of the agent in the same context as the original training.  \n\n3. **Logs (`logs`)**: Training or evaluation logs associated with the pre-trained agent, which may include metrics such as rewards per episode, episode lengths, and any performance statistics recorded during training or evaluation.  \n\nOverall, the response provides all necessary objects and metadata to immediately use, evaluate, or extend the pre-trained DQN agent within a Stable-Baselines3 framework.",
            "examples": [
                {
                    "user_intent": "I want to quickly load a pre-trained DQN model for CartPole-v1 to test it in my environment.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "sb3/dqn-CartPole-v1",
                            "algo": "dqn",
                            "env": "CartPole-v1",
                            "logs": "logs/dqn-CartPole-v1"
                        }
                    }
                },
                {
                    "user_intent": "Load the DQN CartPole agent so I can evaluate its performance on 100 episodes.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "sb3/dqn-CartPole-v1",
                            "algo": "dqn",
                            "env": "CartPole-v1",
                            "logs": "logs/evaluation"
                        }
                    }
                },
                {
                    "user_intent": "I want to download the pre-trained CartPole DQN model and continue training it with my custom logging setup.",
                    "tool_call": {
                        "name": "load_from_hub",
                        "arguments": {
                            "repo_id": "sb3/dqn-CartPole-v1",
                            "algo": "dqn",
                            "env": "CartPole-v1",
                            "logs": "logs/custom_training"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 1: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 2: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 3: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 4: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 5: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 6: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 7: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 8: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 9: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 10: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 11: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 12: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.\nTool 14: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 15: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 16: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 17: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.\nTool 18: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 19: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\", \"parameters\": {\"algo\": {\"type\": \"string\", \"description\": \"The reinforcement learning algorithm to use for training or loading the agent, e.g., DQN, PPO, or A2C.\", \"required\": true, \"example\": \"dqn\"}, \"env\": {\"type\": \"string\", \"description\": \"The environment identifier from OpenAI Gym on which the agent will be trained or evaluated, e.g., CartPole-v1.\", \"required\": true, \"example\": \"CartPole-v1\"}, \"logs\": {\"type\": \"string\", \"description\": \"The file path or directory where training logs, model checkpoints, and other outputs will be stored.\", \"required\": true, \"example\": \"logs/\"}}, \"response\": \"The tool returns a pre-trained reinforcement learning model of a DQN agent for the CartPole-v1 environment. The response typically includes the following components:  \\n\\n1. **Algorithm instance (`algo`)**: This is the loaded DQN algorithm object, fully configured with the trained policy and hyperparameters. It can be used to interact with the environment, perform predictions, or continue training.  \\n\\n2. **Environment instance (`env`)**: The corresponding CartPole-v1 environment object that the model was trained on. This allows evaluation of the agent in the same context as the original training.  \\n\\n3. **Logs (`logs`)**: Training or evaluation logs associated with the pre-trained agent, which may include metrics such as rewards per episode, episode lengths, and any performance statistics recorded during training or evaluation.  \\n\\nOverall, the response provides all necessary objects and metadata to immediately use, evaluate, or extend the pre-trained DQN agent within a Stable-Baselines3 framework.\", \"examples\": [{\"user_intent\": \"I want to quickly load a pre-trained DQN model for CartPole-v1 to test it in my environment.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"sb3/dqn-CartPole-v1\", \"algo\": \"dqn\", \"env\": \"CartPole-v1\", \"logs\": \"logs/dqn-CartPole-v1\"}}}, {\"user_intent\": \"Load the DQN CartPole agent so I can evaluate its performance on 100 episodes.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"sb3/dqn-CartPole-v1\", \"algo\": \"dqn\", \"env\": \"CartPole-v1\", \"logs\": \"logs/evaluation\"}}}, {\"user_intent\": \"I want to download the pre-trained CartPole DQN model and continue training it with my custom logging setup.\", \"tool_call\": {\"name\": \"load_from_hub\", \"arguments\": {\"repo_id\": \"sb3/dqn-CartPole-v1\", \"algo\": \"dqn\", \"env\": \"CartPole-v1\", \"logs\": \"logs/custom_training\"}}}]}\n\n# User Query\nOur company is developing a game, and they want to include an AI agent to play the game. We need to evaluate its performance on the CartPole-v1 environment for consistency.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained reinforcement learning agent for CartPole-v1 to evaluate its gameplay performance.",
                    "tool_description": "Retrieve a pre-trained DQN reinforcement learning agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling immediate evaluation or deployment without training from scratch.",
                    "expected_response": "A pre-trained DQN agent object configured with its trained policy, the CartPole-v1 environment instance, and associated training/evaluation logs for performance analysis."
                },
                {
                    "user_intent": "Evaluate the loaded agent's performance on CartPole-v1 to check consistency across multiple runs.",
                    "tool_description": "Run the pre-trained CartPole-v1 agent in its environment to collect performance metrics such as rewards per episode and episode lengths, allowing assessment of agent consistency and reliability.",
                    "expected_response": "Evaluation metrics including rewards per episode, episode lengths, and other relevant performance statistics."
                }
            ],
            "extracted_arguments": {
                "algo": "The reinforcement learning algorithm to use for the agent, e.g., DQN or PPO.",
                "env": "The environment identifier for testing the agent, in this case CartPole-v1.",
                "logs": "The file path or directory where evaluation or training logs should be stored."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_492",
        "query": "We are building a robot for elderly care. The robot should be able to understand what's happening and take appropriate actions based on the elderly's current activities.",
        "instruction": "Given a `robotic perception and action` task, retrieve tools that facilitate the development of robots capable of interpreting and responding to dynamic environments, specifically through understanding human activities and executing corresponding actions. Tools should incorporate vision and sensory data to enable embodied AI functionalities.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning Robotics",
                    "framework": "Hugging Face Transformers",
                    "functionality": "EmbodiedAI tasks",
                    "api_call": "model_utils.load_model('model_utils.VC1_BASE_NAME')",
                    "api_arguments": "img",
                    "python_environment_requirements": "from vc_models.models.vit import model_utils",
                    "example_code": "model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\nimg = your_function_here ...\ntransformed_img = model_transforms(img)\nembedding = model(transformed_img)",
                    "performance": {
                        "dataset": "CortexBench",
                        "accuracy": "Mean Success: 68.7%"
                    },
                    "description": "The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.",
                    "id": "huggingface_api_910",
                    "name": "VC1_BASE_NAME"
                },
                "id": "gorilla_huggingface_tool_903",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 1: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 2: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 5: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 6: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 7: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 8: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 10: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 11: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 12: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 13: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 14: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.",
            "Tool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 17: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.",
            "Tool 18: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions."
        ],
        "tool_view": {
            "description": "COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained model to load. This specifies which model weights to use for the conversation agent.",
                    "required": true,
                    "example": "allenai/cosmo-xl"
                }
            },
            "response": "The tool produces a generated conversational text response. The structure and meaning of its response can be described as follows:\n\n- **Response type:** A single string of text representing the model’s generated utterance.  \n- **Semantic content:** The text corresponds to the model’s continuation of a conversation given three inputs:\n  1. **Situation narrative** – a description of the scenario or context in which the conversation is taking place.\n  2. **Role instruction** – guidance about the persona, character, or role the model should adopt in the conversation.\n  3. **Conversation history** – a sequence of prior conversational turns, which the model uses to maintain context and coherence.\n\n- **Purpose of the response:** The generated text simulates a natural human conversational reply, reflecting the role specified and responding appropriately to the context and prior conversation. It is designed to handle both in-domain and out-of-domain conversational scenarios.  \n\n- **Optional characteristics of the response:** Depending on generation parameters (e.g., `temperature`, `top_p`, `max_new_tokens`), the response may vary in creativity, length, and specificity, but it will always be a coherent conversational utterance intended to continue the dialogue naturally.  \n\nIn summary, the tool returns a single string containing the model-generated conversational reply, conditioned on the provided situation, role, and conversation history.",
            "examples": [
                {
                    "user_intent": "Simulate a conversation where Cosmo is a helpful customer service agent assisting a user with a billing issue.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo is acting as a customer service representative helping a user resolve a billing problem.",
                            "role_instruction": "You are Cosmo, a friendly and patient customer service agent.",
                            "conversation_history": [
                                "Hello, I noticed an unexpected charge on my account. Can you help me with this?"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Have Cosmo chat informally with a friend about a recent vacation to Paris.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo recently returned from a vacation to Paris and is sharing the experience with a close friend.",
                            "role_instruction": "You are Cosmo, casually chatting with your friend about your trip.",
                            "conversation_history": [
                                "Hey! How was your trip to Paris?"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Generate a dialogue where Cosmo plays the role of a motivational coach encouraging someone to pursue their goals.",
                    "tool_call": {
                        "name": "allenai/cosmo-xl",
                        "arguments": {
                            "situation_narrative": "Cosmo is a motivational coach giving supportive advice to a client feeling uncertain about their future.",
                            "role_instruction": "You are Cosmo, a positive and encouraging life coach.",
                            "conversation_history": [
                                "I'm not sure if I have what it takes to start my own business."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 1: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 2: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 5: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 6: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 7: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 8: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 10: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 11: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 12: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 13: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 14: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.\nTool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 17: `bert-base-cased` is a pre-trained, case-sensitive BERT model designed for masked language modeling, allowing users to predict missing words in English text. It can be accessed via the Hugging Face Transformers `fill-mask` pipeline and is commonly used to understand context, generate or complete text, and serve as a foundation for fine-tuning on downstream NLP tasks.\nTool 18: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 19: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained model to load. This specifies which model weights to use for the conversation agent.\", \"required\": true, \"example\": \"allenai/cosmo-xl\"}}, \"response\": \"The tool produces a generated conversational text response. The structure and meaning of its response can be described as follows:\\n\\n- **Response type:** A single string of text representing the model\\u2019s generated utterance.  \\n- **Semantic content:** The text corresponds to the model\\u2019s continuation of a conversation given three inputs:\\n  1. **Situation narrative** \\u2013 a description of the scenario or context in which the conversation is taking place.\\n  2. **Role instruction** \\u2013 guidance about the persona, character, or role the model should adopt in the conversation.\\n  3. **Conversation history** \\u2013 a sequence of prior conversational turns, which the model uses to maintain context and coherence.\\n\\n- **Purpose of the response:** The generated text simulates a natural human conversational reply, reflecting the role specified and responding appropriately to the context and prior conversation. It is designed to handle both in-domain and out-of-domain conversational scenarios.  \\n\\n- **Optional characteristics of the response:** Depending on generation parameters (e.g., `temperature`, `top_p`, `max_new_tokens`), the response may vary in creativity, length, and specificity, but it will always be a coherent conversational utterance intended to continue the dialogue naturally.  \\n\\nIn summary, the tool returns a single string containing the model-generated conversational reply, conditioned on the provided situation, role, and conversation history.\", \"examples\": [{\"user_intent\": \"Simulate a conversation where Cosmo is a helpful customer service agent assisting a user with a billing issue.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo is acting as a customer service representative helping a user resolve a billing problem.\", \"role_instruction\": \"You are Cosmo, a friendly and patient customer service agent.\", \"conversation_history\": [\"Hello, I noticed an unexpected charge on my account. Can you help me with this?\"]}}}, {\"user_intent\": \"Have Cosmo chat informally with a friend about a recent vacation to Paris.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo recently returned from a vacation to Paris and is sharing the experience with a close friend.\", \"role_instruction\": \"You are Cosmo, casually chatting with your friend about your trip.\", \"conversation_history\": [\"Hey! How was your trip to Paris?\"]}}}, {\"user_intent\": \"Generate a dialogue where Cosmo plays the role of a motivational coach encouraging someone to pursue their goals.\", \"tool_call\": {\"name\": \"allenai/cosmo-xl\", \"arguments\": {\"situation_narrative\": \"Cosmo is a motivational coach giving supportive advice to a client feeling uncertain about their future.\", \"role_instruction\": \"You are Cosmo, a positive and encouraging life coach.\", \"conversation_history\": [\"I'm not sure if I have what it takes to start my own business.\"]}}}]}\n\n# User Query\nWe are building a robot for elderly care. The robot should be able to understand what's happening and take appropriate actions based on the elderly's current activities.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Interpret the current situation of the elderly person and understand their ongoing activities.",
                    "tool_description": "Use a conversational AI or perception model to analyze input data (e.g., dialogue, video, or sensor readings) to identify the current activity or state of the elderly person.",
                    "expected_response": "A structured representation or textual description of the elderly person's current activity, condition, or context."
                },
                {
                    "user_intent": "Generate appropriate responses or actions for the robot based on the elderly person's current activities.",
                    "tool_description": "Generate contextually appropriate dialogue or action instructions for the robot using a conversational AI model, conditioned on the identified situation and the robot's caregiving role.",
                    "expected_response": "A conversational or action-oriented output specifying what the robot should say or do next to assist or interact with the elderly person."
                }
            ],
            "extracted_arguments": {
                "situation_narrative": "A description of the elderly person's current activities or state, derived from sensory or contextual inputs.",
                "role_instruction": "Instructions defining the robot's persona or caregiving behavior in response to the elderly person's activities.",
                "conversation_history": "Prior interactions or observations that provide context for generating the robot's response or actions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_455",
        "query": "We are building a video analysis tool that can automatically detect the main action happening in a given video clip.",
        "instruction": "Given a `video action recognition` task, retrieve tools that focus on analyzing video content to identify and classify main actions occurring within video clips, utilizing pretrained models designed for accurate video classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Video Action Recognition",
                    "api_call": "VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "decord",
                        "huggingface_hub"
                    ],
                    "example_code": "from decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n converted_len = int(clip_len * frame_sample_rate)\n end_idx = np.random.randint(converted_len, seg_len)\n start_idx = end_idx - converted_len\n indices = np.linspace(start_idx, end_idx, num=clip_len)\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n return indices\nfile_path = hf_hub_download(\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\ninputs = feature_extractor(list(video), return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n logits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
                    "performance": {
                        "dataset": "UCF101",
                        "accuracy": 0.758209764957428
                    },
                    "description": "VideoMAE Base model fine tuned on UCF101 for Video Action Recognition",
                    "id": "huggingface_api_342",
                    "name": "videomae-base-finetuned-ucf101"
                },
                "id": "gorilla_huggingface_tool_338",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 2: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 3: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 4: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 5: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 6: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 7: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 8: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 10: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.",
            "Tool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 12: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 13: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.",
            "Tool 14: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 15: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 17: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 18: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 19: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features."
        ],
        "tool_view": {
            "description": "A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "parameters": {
                "learning_rate": {
                    "type": "number",
                    "description": "The learning rate used during training. Controls the step size for weight updates in the optimizer.",
                    "required": true,
                    "example": 5e-05
                },
                "train_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward/backward pass during training.",
                    "required": true,
                    "example": 8
                },
                "eval_batch_size": {
                    "type": "number",
                    "description": "The number of samples processed together in one forward pass during evaluation.",
                    "required": true,
                    "example": 8
                },
                "seed": {
                    "type": "number",
                    "description": "A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.",
                    "required": true,
                    "example": 42
                },
                "optimizer": {
                    "type": "string",
                    "description": "The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.",
                    "required": true,
                    "example": "Adam with betas=(0.9,0.999) and epsilon=1e-08"
                },
                "lr_scheduler_type": {
                    "type": "string",
                    "description": "The type of learning rate scheduler used to adjust the learning rate during training.",
                    "required": true,
                    "example": "linear"
                },
                "lr_scheduler_warmup_ratio": {
                    "type": "number",
                    "description": "The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.",
                    "required": true,
                    "example": 0.1
                },
                "training_steps": {
                    "type": "number",
                    "description": "The total number of training steps to perform.",
                    "required": true,
                    "example": 111
                }
            },
            "response": "The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \n\n1. **Predicted Label(s)** – One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \n\n2. **Confidence Scores / Probabilities** – A numeric value or array indicating the model’s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \n\n3. **Optional Metadata** – Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \n\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.",
            "examples": [
                {
                    "user_intent": "I want to use a pre-trained video classification model for a project. Can you help me with setting it up?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                },
                {
                    "user_intent": "I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset",
                            "learning_rate": 5e-05,
                            "train_batch_size": 8,
                            "eval_batch_size": 8,
                            "seed": 42,
                            "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                            "lr_scheduler_type": "linear",
                            "lr_scheduler_warmup_ratio": 0.1,
                            "training_steps": 111
                        }
                    }
                },
                {
                    "user_intent": "Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?",
                    "tool_call": {
                        "name": "AutoModelForVideoClassification.from_pretrained",
                        "arguments": {
                            "model_name": "sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 1: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 2: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 3: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 4: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 5: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 6: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 7: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 8: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 10: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.\nTool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 12: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 13: The \"videomae-small-finetuned-ssv2\" tool is a pre-trained model for video classification based on the VideoMAE architecture, an extension of Masked Autoencoders (MAE) designed for video data. It uses a Transformer encoder to process video as a sequence of patches and can be fine-tuned on specific tasks like action recognition. This model has been fine-tuned on the Something-Something V2 dataset, achieving top-1 accuracy of 66.8% and top-5 accuracy of 90.3%. It is typically used to extract video features for classification tasks, where a linear classifier is added on top of the model's [CLS] token representation.\nTool 14: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 15: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 17: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 18: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 19: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\", \"parameters\": {\"learning_rate\": {\"type\": \"number\", \"description\": \"The learning rate used during training. Controls the step size for weight updates in the optimizer.\", \"required\": true, \"example\": 5e-05}, \"train_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward/backward pass during training.\", \"required\": true, \"example\": 8}, \"eval_batch_size\": {\"type\": \"number\", \"description\": \"The number of samples processed together in one forward pass during evaluation.\", \"required\": true, \"example\": 8}, \"seed\": {\"type\": \"number\", \"description\": \"A random seed used for initializing the model and data shuffling, ensuring reproducibility of results.\", \"required\": true, \"example\": 42}, \"optimizer\": {\"type\": \"string\", \"description\": \"The optimizer algorithm used for training, including additional parameters such as beta values and epsilon.\", \"required\": true, \"example\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\"}, \"lr_scheduler_type\": {\"type\": \"string\", \"description\": \"The type of learning rate scheduler used to adjust the learning rate during training.\", \"required\": true, \"example\": \"linear\"}, \"lr_scheduler_warmup_ratio\": {\"type\": \"number\", \"description\": \"The fraction of total training steps used for learning rate warmup, where the learning rate gradually increases from 0 to the initial learning rate.\", \"required\": true, \"example\": 0.1}, \"training_steps\": {\"type\": \"number\", \"description\": \"The total number of training steps to perform.\", \"required\": true, \"example\": 111}}, \"response\": \"The tool performs video classification using a pretrained Hugging Face Transformers model. Its response would typically consist of a structured prediction output for a given input video, including the following components:  \\n\\n1. **Predicted Label(s)** \\u2013 One or more class names representing the predicted action(s) or category(ies) in the input video. This is usually a string or list of strings corresponding to the dataset classes the model was trained on.  \\n\\n2. **Confidence Scores / Probabilities** \\u2013 A numeric value or array indicating the model\\u2019s confidence in each predicted class. This is often represented as a probability distribution over all possible classes.  \\n\\n3. **Optional Metadata** \\u2013 Depending on the implementation, the response may include additional information such as the mapping of class indices to human-readable labels, processing timestamps, or intermediate features used for prediction.  \\n\\nOverall, the response is structured to convey both the predicted video category and the associated confidence, enabling downstream applications to interpret and act on the classification results.\", \"examples\": [{\"user_intent\": \"I want to use a pre-trained video classification model for a project. Can you help me with setting it up?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}, {\"user_intent\": \"I'm training a custom video classification model using the Hugging Face framework. What parameters should I use for fine-tuning?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}}}, {\"user_intent\": \"Can I get the performance details of the video classification model 'sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset'?\", \"tool_call\": {\"name\": \"AutoModelForVideoClassification.from_pretrained\", \"arguments\": {\"model_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\"}}}]}\n\n# User Query\nWe are building a video analysis tool that can automatically detect the main action happening in a given video clip.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to automatically classify the main action in a video clip.",
                    "tool_description": "Use a pre-trained VideoMAE-based video classification model to analyze input video frames and predict the primary action or activity occurring in the video.",
                    "expected_response": "A structured prediction containing the predicted action label(s) for the video and corresponding confidence scores."
                }
            ],
            "extracted_arguments": {
                "video_input": "The video clip to be analyzed, provided as a file or stream for classification."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_310",
        "query": "We need to integrate the personal assistant we're building with the capability of having conversations with people, sometimes answering general knowledge questions.",
        "instruction": "Given a `conversational AI integration` task, retrieve tools that enable interactive and dynamic text-based interactions by leveraging natural language processing capabilities to facilitate conversations and provide answers to general inquiries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('conversational', model='mywateriswet/ShuanBot')",
                    "api_arguments": "message",
                    "python_environment_requirements": "transformers",
                    "example_code": "response = chatbot('What is your name?')",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.",
                    "id": "huggingface_api_585",
                    "name": "mywateriswet/ShuanBot"
                },
                "id": "gorilla_huggingface_tool_580",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 1: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 2: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 3: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 4: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 6: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 8: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 9: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 10: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 11: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.",
            "Tool 12: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 14: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 15: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 16: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 18: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 19: A fine-tuned TAPAS mini model for answering questions over tables, specifically designed for sequential question answering tasks, enabling structured data in tables to be queried using natural language."
        ],
        "tool_view": {
            "description": "BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained BlenderBot model to load from the Hugging Face Transformers library.",
                    "required": true,
                    "example": "facebook/blenderbot-3B"
                }
            },
            "response": "The tool facilitates conversational interactions using the **BlenderBot-3B** model, which is designed for open-domain chatbot applications. The response produced by this tool is expected to be a text-based output that represents the model's reply to an input message. The structure of the response would likely include the following components:\n\n1. **Response Text**: This will be a generated message that serves as the chatbot's reply. It can include information, questions, statements, or empathetic responses depending on the context of the conversation.\n   \n2. **Context Awareness**: The response is generated with an understanding of previous conversation history, allowing the model to maintain context across multiple turns of dialogue.\n\n3. **Personality and Empathy**: The response will reflect the model's trained ability to exhibit personality traits, empathy, and engage with the user in a way that feels natural and human-like.\n\n4. **Knowledge Integration**: The model may draw on factual knowledge to answer questions, offer suggestions, or discuss various topics, depending on the nature of the conversation.\n\nIn summary, the response from this tool would be a text message generated by the BlenderBot-3B model, informed by the conversation history, designed to be engaging, informative, and empathetic.",
            "examples": [
                {
                    "user_intent": "I want to start a conversation with BlenderBot-3B.",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                },
                {
                    "user_intent": "Can I use BlenderBot-3B for chatting about general topics?",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                },
                {
                    "user_intent": "Test BlenderBot-3B for a casual chat about movies.",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 1: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 2: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 3: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 4: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 6: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 8: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 9: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 10: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 11: BLIP-2 with Flan T5-xl is a multimodal model that generates text from images and questions, enabling tasks such as image captioning, visual question answering, and interactive image-based conversations. It works by processing an image alongside a text prompt and predicting the next text token, allowing users to query visual content or describe images in natural language.\nTool 12: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 13: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 14: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 15: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 16: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 18: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 19: A fine-tuned TAPAS mini model for answering questions over tables, specifically designed for sequential question answering tasks, enabling structured data in tables to be queried using natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained BlenderBot model to load from the Hugging Face Transformers library.\", \"required\": true, \"example\": \"facebook/blenderbot-3B\"}}, \"response\": \"The tool facilitates conversational interactions using the **BlenderBot-3B** model, which is designed for open-domain chatbot applications. The response produced by this tool is expected to be a text-based output that represents the model's reply to an input message. The structure of the response would likely include the following components:\\n\\n1. **Response Text**: This will be a generated message that serves as the chatbot's reply. It can include information, questions, statements, or empathetic responses depending on the context of the conversation.\\n   \\n2. **Context Awareness**: The response is generated with an understanding of previous conversation history, allowing the model to maintain context across multiple turns of dialogue.\\n\\n3. **Personality and Empathy**: The response will reflect the model's trained ability to exhibit personality traits, empathy, and engage with the user in a way that feels natural and human-like.\\n\\n4. **Knowledge Integration**: The model may draw on factual knowledge to answer questions, offer suggestions, or discuss various topics, depending on the nature of the conversation.\\n\\nIn summary, the response from this tool would be a text message generated by the BlenderBot-3B model, informed by the conversation history, designed to be engaging, informative, and empathetic.\", \"examples\": [{\"user_intent\": \"I want to start a conversation with BlenderBot-3B.\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}, {\"user_intent\": \"Can I use BlenderBot-3B for chatting about general topics?\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}, {\"user_intent\": \"Test BlenderBot-3B for a casual chat about movies.\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}]}\n\n# User Query\nWe need to integrate the personal assistant we're building with the capability of having conversations with people, sometimes answering general knowledge questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Enable the personal assistant to have natural, multi-turn conversations with users.",
                    "tool_description": "Use a large-scale open-domain conversational AI model to generate context-aware, empathetic, and human-like dialogue responses.",
                    "expected_response": "Text-based responses that maintain conversational context, display personality, and engage users naturally."
                },
                {
                    "user_intent": "Allow the personal assistant to answer general knowledge questions during conversations.",
                    "tool_description": "Leverage the conversational AI model's knowledge integration capabilities to provide accurate answers to factual questions asked by users.",
                    "expected_response": "Informative responses containing relevant facts, explanations, or suggestions drawn from the model's knowledge base."
                }
            ],
            "extracted_arguments": {
                "model_name": "The identifier of the pre-trained conversational AI model to use for multi-turn dialogue and general knowledge question answering."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_495",
        "query": "Our company seeks a solution to reduce the carbon emissions due to our operations. We have collected historical data about our emissions. We are looking for a machine learning model to predict the carbon emissions based on this data.",
        "instruction": "Given a `carbon emissions prediction` task, retrieve tools that utilize machine learning models specifically for tabular regression to predict carbon emissions based on historical data inputs, providing insights into potential reductions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "json",
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Robertooo/autotrain-data-hmaet",
                        "accuracy": {
                            "Loss": 0.067,
                            "R2": 0.486,
                            "MSE": 0.005,
                            "MAE": 0.055,
                            "RMSLE": 0.036
                        }
                    },
                    "description": "A tabular regression model trained with AutoTrain to predict carbon emissions.",
                    "id": "huggingface_api_875",
                    "name": "Robertooo/autotrain-hmaet-2037366891"
                },
                "id": "gorilla_huggingface_tool_868",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 1: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 2: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 4: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 5: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 7: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 8: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 10: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 11: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 13: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 15: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 17: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 18: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 19: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "parameters": {
                "data": {
                    "type": "DataFrame",
                    "description": "A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})"
                }
            },
            "response": "The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \n\n- **Type:** Array or list of numeric values.  \n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \n- **Data Semantics:**  \n  - Each numeric value reflects the model’s regression output, estimating carbon emissions based on the input features.  \n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \n\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my factory dataset using the trained model.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "factory_emissions_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of vehicle specifications and want to estimate their carbon footprint.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "vehicle_specs.csv"
                        }
                    }
                },
                {
                    "user_intent": "Calculate predicted carbon emissions for a new dataset of industrial plants.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "industrial_plants_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 1: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 2: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 4: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 5: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 6: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 7: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 8: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 9: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 10: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 11: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 13: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 15: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 16: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 17: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 18: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 19: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\", \"parameters\": {\"data\": {\"type\": \"DataFrame\", \"description\": \"A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})\"}}, \"response\": \"The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \\n\\n- **Type:** Array or list of numeric values.  \\n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \\n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \\n- **Data Semantics:**  \\n  - Each numeric value reflects the model\\u2019s regression output, estimating carbon emissions based on the input features.  \\n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \\n\\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my factory dataset using the trained model.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"factory_emissions_data.csv\"}}}, {\"user_intent\": \"I have a CSV of vehicle specifications and want to estimate their carbon footprint.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"vehicle_specs.csv\"}}}, {\"user_intent\": \"Calculate predicted carbon emissions for a new dataset of industrial plants.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"industrial_plants_data.csv\"}}}]}\n\n# User Query\nOur company seeks a solution to reduce the carbon emissions due to our operations. We have collected historical data about our emissions. We are looking for a machine learning model to predict the carbon emissions based on this data.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the carbon emissions for our historical operations data using a trained model.",
                    "tool_description": "Use a tabular regression model trained with AutoTrain to predict carbon emissions from structured input features, enabling estimation of emissions based on historical data.",
                    "expected_response": "An array of numeric values representing predicted carbon emissions for each row in the input dataset, suitable for analysis and planning."
                }
            ],
            "extracted_arguments": {
                "data": "A tabular dataset containing historical operational features for which carbon emissions predictions are to be made; columns must match the features expected by the model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_483",
        "query": "A doctor needs to find medical information in a large document. We are building an AI model to help them extract answers to their questions.",
        "instruction": "Given an `information extraction` task, retrieve tools capable of processing large textual data to extract accurate answers based on specific questions, aligning with the medical context and requirements of the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')",
                    "api_arguments": [
                        "context",
                        "question"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.7.0",
                        "torch==1.8.0",
                        "datasets==1.4.1",
                        "tokenizers==0.10.2"
                    ],
                    "example_code": "qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can the model be loaded?'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "1.2582"
                    },
                    "description": "This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset.",
                    "id": "huggingface_api_487",
                    "name": "bigwiz83/sapbert-from-pubmedbert-squad2"
                },
                "id": "gorilla_huggingface_tool_483",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 1: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 5: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 6: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 7: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 8: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.",
            "Tool 9: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 10: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 12: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 13: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 14: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 16: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 17: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 18: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 19: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "parameters": {
                "image": {
                    "type": "image",
                    "description": "The input document image on which the question answering model will operate. This should be a visual representation of the page(s) to analyze.",
                    "required": true,
                    "example": "Image.open('example.jpg').convert('RGB')"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question to ask about the content of the document image.",
                    "required": true,
                    "example": "Is this a question?"
                },
                "context": {
                    "type": "array",
                    "description": "An optional list of text strings providing additional context or reference information to help the model answer the question.",
                    "required": true,
                    "example": [
                        "Example text from the document."
                    ]
                },
                "boxes": {
                    "type": "array",
                    "description": "A list of bounding box coordinates corresponding to text regions in the document image, formatted as [x0, y0, x1, y1]. These boxes help the model locate relevant text areas.",
                    "required": true,
                    "example": [
                        [
                            0,
                            0,
                            1000,
                            1000
                        ]
                    ]
                }
            },
            "response": "The tool returns a structured output representing the model’s predicted answer to a question about a document image, based on the LayoutLMv3 architecture fine-tuned for multi-page document question answering. The response typically contains the following components:\n\n1. **start_logits**  \n   - A tensor of numerical scores corresponding to each token in the encoded input.  \n   - Each score represents the model’s confidence that the token is the **start** of the answer span.\n\n2. **end_logits**  \n   - A tensor of numerical scores for each token, representing the model’s confidence that the token is the **end** of the answer span.\n\n3. **answer_text** (derived from decoding the token span)  \n   - A string representing the predicted answer, obtained by taking the tokens between the indices of the highest start and end logits and decoding them using the tokenizer.  \n   - This is the human-readable answer extracted from the document content.\n\n4. **Optional auxiliary outputs** (depending on processor and model usage)  \n   - Token-level encodings or IDs of the document text.  \n   - Possibly attention scores or other intermediate features used for interpretability.\n\nIn essence, the response provides both **token-level prediction scores** (start and end logits) and the **final extracted answer** as text, allowing the user to locate and interpret the answer within the document context.",
            "examples": [
                {
                    "user_intent": "I have a scanned invoice and I want to know the total amount due.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "scanned_invoice_page1.jpg",
                            "question": "What is the total amount due?",
                            "context": [
                                "Invoice from ABC Corp, dated 12/15/2025, containing itemized charges and totals."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1000,
                                    1400
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Extract the date of birth from a scanned government ID.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "passport_scan.jpg",
                            "question": "What is the date of birth?",
                            "context": [
                                "Passport ID page with name, date of birth, nationality, and passport number."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1200,
                                    800
                                ]
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Find the page number and section title of a specific chapter in a PDF screenshot.",
                    "tool_call": {
                        "name": "layoutlmv3-base-mpdocvqa",
                        "arguments": {
                            "image": "pdf_chapter_screenshot.png",
                            "question": "What is the section title and page number of Chapter 3?",
                            "context": [
                                "PDF contains chapters, headings, and page numbers formatted in a multi-column layout."
                            ],
                            "boxes": [
                                [
                                    0,
                                    0,
                                    1600,
                                    2000
                                ]
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 1: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 2: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 3: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 4: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 5: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 6: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 7: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 8: A document question-answering model based on LayoutLMv2 that extracts answers from images by leveraging both text content and document layout information.\nTool 9: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 10: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 11: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 12: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 13: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 14: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 16: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 17: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 18: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 19: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\", \"parameters\": {\"image\": {\"type\": \"image\", \"description\": \"The input document image on which the question answering model will operate. This should be a visual representation of the page(s) to analyze.\", \"required\": true, \"example\": \"Image.open('example.jpg').convert('RGB')\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question to ask about the content of the document image.\", \"required\": true, \"example\": \"Is this a question?\"}, \"context\": {\"type\": \"array\", \"description\": \"An optional list of text strings providing additional context or reference information to help the model answer the question.\", \"required\": true, \"example\": [\"Example text from the document.\"]}, \"boxes\": {\"type\": \"array\", \"description\": \"A list of bounding box coordinates corresponding to text regions in the document image, formatted as [x0, y0, x1, y1]. These boxes help the model locate relevant text areas.\", \"required\": true, \"example\": [[0, 0, 1000, 1000]]}}, \"response\": \"The tool returns a structured output representing the model\\u2019s predicted answer to a question about a document image, based on the LayoutLMv3 architecture fine-tuned for multi-page document question answering. The response typically contains the following components:\\n\\n1. **start_logits**  \\n   - A tensor of numerical scores corresponding to each token in the encoded input.  \\n   - Each score represents the model\\u2019s confidence that the token is the **start** of the answer span.\\n\\n2. **end_logits**  \\n   - A tensor of numerical scores for each token, representing the model\\u2019s confidence that the token is the **end** of the answer span.\\n\\n3. **answer_text** (derived from decoding the token span)  \\n   - A string representing the predicted answer, obtained by taking the tokens between the indices of the highest start and end logits and decoding them using the tokenizer.  \\n   - This is the human-readable answer extracted from the document content.\\n\\n4. **Optional auxiliary outputs** (depending on processor and model usage)  \\n   - Token-level encodings or IDs of the document text.  \\n   - Possibly attention scores or other intermediate features used for interpretability.\\n\\nIn essence, the response provides both **token-level prediction scores** (start and end logits) and the **final extracted answer** as text, allowing the user to locate and interpret the answer within the document context.\", \"examples\": [{\"user_intent\": \"I have a scanned invoice and I want to know the total amount due.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"scanned_invoice_page1.jpg\", \"question\": \"What is the total amount due?\", \"context\": [\"Invoice from ABC Corp, dated 12/15/2025, containing itemized charges and totals.\"], \"boxes\": [[0, 0, 1000, 1400]]}}}, {\"user_intent\": \"Extract the date of birth from a scanned government ID.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"passport_scan.jpg\", \"question\": \"What is the date of birth?\", \"context\": [\"Passport ID page with name, date of birth, nationality, and passport number.\"], \"boxes\": [[0, 0, 1200, 800]]}}}, {\"user_intent\": \"Find the page number and section title of a specific chapter in a PDF screenshot.\", \"tool_call\": {\"name\": \"layoutlmv3-base-mpdocvqa\", \"arguments\": {\"image\": \"pdf_chapter_screenshot.png\", \"question\": \"What is the section title and page number of Chapter 3?\", \"context\": [\"PDF contains chapters, headings, and page numbers formatted in a multi-column layout.\"], \"boxes\": [[0, 0, 1600, 2000]]}}}]}\n\n# User Query\nA doctor needs to find medical information in a large document. We are building an AI model to help them extract answers to their questions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract specific medical information from a large document using AI.",
                    "tool_description": "Retrieve answers to specific questions based on a large document containing medical data, such as case reports or research papers, by leveraging a pretrained question-answering model fine-tuned for medical or structured documents.",
                    "expected_response": "A structured output containing the model's predicted answer to the question, with token-level prediction scores (start and end logits) and the extracted answer text."
                }
            ],
            "extracted_arguments": {
                "image": "The document image (scanned or photographed) containing medical information to be analyzed.",
                "question": "The natural language question about the medical content within the document.",
                "context": "An optional list of text strings providing additional context or reference information to help the model answer the question.",
                "boxes": "A list of bounding box coordinates corresponding to text regions in the document image to guide the model in locating relevant text areas."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_84",
        "query": "I want to make a conference call app which differentiates between the times when the user is speaking and when there is no voice activity.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that distinguish between periods of speech and silence in audio inputs to develop a conference call application that accurately identifies user speaking activity.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face",
                    "functionality": "Voice Activity Detection",
                    "api_call": "pipeline('voice-activity-detection', model='funasr/FSMN-VAD')",
                    "api_arguments": {},
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library.",
                    "id": "huggingface_api_838",
                    "name": "FSMN-VAD"
                },
                "id": "gorilla_huggingface_tool_833",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 2: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 3: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 5: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 6: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 7: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 8: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 9: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 10: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 11: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 13: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 14: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 15: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 16: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 17: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 19: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets."
        ],
        "tool_view": {
            "description": "Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "parameters": {},
            "response": "The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\n\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \n- **end_time**: The timestamp indicating when the speech segment ends.  \n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \n\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.",
            "examples": [
                {
                    "user_intent": "I have an audio file in Hindi, and I want to detect when someone is speaking in it.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "audio/hindi_conversation.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze my Tamil podcast recording to find all segments where there is active speech.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "podcasts/tamil_episode1.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "interviews/telugu_batch1.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 2: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 3: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 4: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 5: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 6: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 7: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 8: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 9: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 10: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 11: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 13: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 14: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 15: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 16: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 17: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 19: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\", \"parameters\": {}, \"response\": \"The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\\n\\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \\n- **end_time**: The timestamp indicating when the speech segment ends.  \\n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \\n\\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.\", \"examples\": [{\"user_intent\": \"I have an audio file in Hindi, and I want to detect when someone is speaking in it.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"audio/hindi_conversation.wav\"}}}, {\"user_intent\": \"Analyze my Tamil podcast recording to find all segments where there is active speech.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"podcasts/tamil_episode1.mp3\"}}}, {\"user_intent\": \"I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"interviews/telugu_batch1.wav\"}}}]}\n\n# User Query\nI want to make a conference call app which differentiates between the times when the user is speaking and when there is no voice activity.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect speech activity in audio streams from users during a conference call.",
                    "tool_description": "Perform voice activity detection (VAD) on real-time or recorded audio inputs to identify periods of speech versus silence.",
                    "expected_response": "A list of speech segments for each audio input, including start and end times of detected speech intervals, optionally with confidence scores."
                },
                {
                    "user_intent": "Differentiate between active speech and silence for multiple participants in the call.",
                    "tool_description": "Process individual participant audio streams to segment speech and non-speech intervals, allowing the application to determine who is speaking and when.",
                    "expected_response": "Structured data mapping each participant to their detected speech segments, enabling the app to track speaking activity."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The real-time or recorded audio stream from a conference call participant, provided as a digital audio file or streaming input.",
                "participant_id": "A unique identifier representing each user in the conference call, used to track speech activity per individual."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_281",
        "query": "Create a virtual assistant that can provide answers to trivia questions about history.",
        "instruction": "Given a `virtual assistant creation` task, retrieve tools that enable the construction of a virtual assistant capable of question answering in the domain of history trivia by employing natural language processing models to process queries and produce accurate answers.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='philschmid/distilbert-onnx')",
                    "api_arguments": {
                        "model": "philschmid/distilbert-onnx"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "onnx"
                    ],
                    "example_code": {
                        "Compute": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})"
                    },
                    "performance": {
                        "dataset": "squad",
                        "accuracy": "F1 score: 87.1"
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.",
                    "id": "huggingface_api_475",
                    "name": "philschmid/distilbert-onnx"
                },
                "id": "gorilla_huggingface_tool_471",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 2: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 3: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 4: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 5: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 6: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 7: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 9: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 11: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 16: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words."
        ],
        "tool_view": {
            "description": "A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want the model to answer, expressed in natural language.",
                    "required": true,
                    "example": "Why is model conversion important?"
                },
                "context": {
                    "type": "string",
                    "description": "The text passage or context in which the answer to the question can be found.",
                    "required": true,
                    "example": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                }
            },
            "response": "The tool performs **question answering** using a **BERT-based model** that has been fine-tuned on the **SQuAD2.0** dataset. The typical response from this tool would contain the following components:\n\n1. **Answer**: A string containing the extractive answer to the provided question, derived from the context. This is the primary output of the question-answering task.\n\n2. **Start**: The index (position) within the context where the answer begins.\n\n3. **End**: The index (position) within the context where the answer ends.\n\nThese components describe the span of the context that provides the answer to the question, along with the actual answer text.\n\nIn summary, the response is structured to give the model's best guess at an answer, alongside the span of text in the context that corresponds to the answer.",
            "examples": [
                {
                    "user_intent": "I want to know who developed the theory of relativity from this passage.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "Who developed the theory of relativity?",
                            "context": "Albert Einstein, a German-born theoretical physicist, developed the theory of relativity, one of the two pillars of modern physics."
                        }
                    }
                },
                {
                    "user_intent": "Find out why model conversion between frameworks is useful.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "Why is model conversion between frameworks important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                        }
                    }
                },
                {
                    "user_intent": "Determine the capital city mentioned in this travel guide excerpt.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "What is the capital city of France?",
                            "context": "France is known for its rich history, cuisine, and iconic landmarks like the Eiffel Tower in Paris, which is the capital city."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 1: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 2: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 3: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 4: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 5: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 6: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 7: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 9: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 10: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 11: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 16: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 17: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 18: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 19: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want the model to answer, expressed in natural language.\", \"required\": true, \"example\": \"Why is model conversion important?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text passage or context in which the answer to the question can be found.\", \"required\": true, \"example\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}, \"response\": \"The tool performs **question answering** using a **BERT-based model** that has been fine-tuned on the **SQuAD2.0** dataset. The typical response from this tool would contain the following components:\\n\\n1. **Answer**: A string containing the extractive answer to the provided question, derived from the context. This is the primary output of the question-answering task.\\n\\n2. **Start**: The index (position) within the context where the answer begins.\\n\\n3. **End**: The index (position) within the context where the answer ends.\\n\\nThese components describe the span of the context that provides the answer to the question, along with the actual answer text.\\n\\nIn summary, the response is structured to give the model's best guess at an answer, alongside the span of text in the context that corresponds to the answer.\", \"examples\": [{\"user_intent\": \"I want to know who developed the theory of relativity from this passage.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"Who developed the theory of relativity?\", \"context\": \"Albert Einstein, a German-born theoretical physicist, developed the theory of relativity, one of the two pillars of modern physics.\"}}}, {\"user_intent\": \"Find out why model conversion between frameworks is useful.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"Why is model conversion between frameworks important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}}, {\"user_intent\": \"Determine the capital city mentioned in this travel guide excerpt.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"What is the capital city of France?\", \"context\": \"France is known for its rich history, cuisine, and iconic landmarks like the Eiffel Tower in Paris, which is the capital city.\"}}}]}\n\n# User Query\nCreate a virtual assistant that can provide answers to trivia questions about history.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to answer trivia questions about history using a text-based passage or context.",
                    "tool_description": "Use a BERT-based extractive question answering model fine-tuned on SQuAD 2.0 to find precise answers to history-related questions from provided text passages.",
                    "expected_response": "A structured answer including the text span from the context that answers the history question, along with start and end indices of the answer within the passage."
                }
            ],
            "extracted_arguments": {
                "question": "The history trivia question expressed in natural language.",
                "context": "A text passage containing historical information relevant to the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_243",
        "query": "Create interesting variations of the given phrase 'How can I improve my time management skills?'.",
        "instruction": "Given a `phrase variation` task, retrieve tools that specialize in paraphrasing by processing text inputs to generate diverse and coherent variations of the original phrase.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Paraphrasing",
                    "api_call": "Parrot(model_tag='prithivida/parrot_paraphraser_on_T5')",
                    "api_arguments": [
                        "input_phrase",
                        "diversity_ranker",
                        "do_diverse",
                        "max_return_phrases",
                        "max_length",
                        "adequacy_threshold",
                        "fluency_threshold"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "from parrot import Parrot\nimport torch\nimport warnings\nwarnings.filterwarnings(ignore)\n\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\nphrases = [Can you recommed some upscale restaurants in Newyork?,\n What are the famous places we should not miss in Russia?\n]\nfor phrase in phrases:\n print(-*100)\n print(Input_phrase: , phrase)\n print(-*100)\n para_phrases = parrot.augment(input_phrase=phrase)\n for para_phrase in para_phrases:\n  print(para_phrase)",
                    "performance": {
                        "dataset": "Not mentioned",
                        "accuracy": "Not mentioned"
                    },
                    "description": "Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.",
                    "id": "huggingface_api_630",
                    "name": "prithivida/parrot_paraphraser_on_T5"
                },
                "id": "gorilla_huggingface_tool_625",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 1: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 2: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 4: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 6: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.",
            "Tool 7: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 8: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 11: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 12: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 14: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 15: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 17: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 18: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 19: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled."
        ],
        "tool_view": {
            "description": "This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "parameters": {
                "revision": {
                    "type": "string",
                    "description": "The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.",
                    "required": true,
                    "default": "v2.0",
                    "example": "v2.0"
                }
            },
            "response": "The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\n\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \n\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.",
            "examples": [
                {
                    "user_intent": "I want to generate variations of an image using the Stable Diffusion model with the latest version.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Create new variations of a given image using the Stable Diffusion image variation pipeline.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                },
                {
                    "user_intent": "Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.",
                    "tool_call": {
                        "name": "lambdalabs/sd-image-variations-diffusers",
                        "arguments": {
                            "revision": "v2.0"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 1: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 2: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 3: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 4: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 6: The \"sultan/BioM-ELECTRA-Large-SQuAD2\" is a fine-tuned biomedical question answering model based on BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts and further fine-tuned on the SQuAD2.0 dataset. This model is designed to improve performance on biomedical QA tasks, such as those in the BioASQ challenge, by providing accurate answers to questions based on a given context. It achieves strong performance with an exact match of 84.3% and an F1 score of 87.5% on the SQuAD2.0 dev set. The model can be accessed through the Hugging Face Transformers library using the `pipeline('question-answering')` function.\nTool 7: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 8: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 9: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 10: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 11: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 12: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 14: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 15: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 17: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 18: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 19: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL\\u00b7E 2. The tool is based on the \\\"lambdalabs/sd-image-variations-diffusers\\\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\", \"parameters\": {\"revision\": {\"type\": \"string\", \"description\": \"The specific version of the model to be used for image variations. This determines which version of the model is loaded for the process.\", \"required\": true, \"default\": \"v2.0\", \"example\": \"v2.0\"}}, \"response\": \"The tool produces one or more generated images that are variations of a given input image. The response is typically structured as an object or collection containing the generated images. Each component of the response has the following meaning:\\n\\n- **images**: A list or array of image objects representing the generated variations. Each element corresponds to a single image output created by the pipeline based on the input image and any additional parameters (e.g., guidance scale). These images are typically in a standard image format that can be saved or further processed, such as PIL Image objects.  \\n\\nThe primary semantic content of the response is the visual data of the image variations. Optional metadata (not explicitly defined in the documentation) could include information about generation parameters, but the core response is focused on returning the generated image objects.\", \"examples\": [{\"user_intent\": \"I want to generate variations of an image using the Stable Diffusion model with the latest version.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Create new variations of a given image using the Stable Diffusion image variation pipeline.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}, {\"user_intent\": \"Generate different styles of an input image with the fine-tuned Stable Diffusion model from Lambda Labs.\", \"tool_call\": {\"name\": \"lambdalabs/sd-image-variations-diffusers\", \"arguments\": {\"revision\": \"v2.0\"}}}]}\n\n# User Query\nCreate interesting variations of the given phrase 'How can I improve my time management skills?'.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate multiple interesting variations of a given text phrase.",
                    "tool_description": "Use a text generation or paraphrasing model to produce alternative phrasings of an input sentence, maintaining the original meaning while creating diverse linguistic expressions.",
                    "expected_response": "A list of sentence strings representing semantically similar but syntactically varied versions of the input phrase."
                }
            ],
            "extracted_arguments": {
                "input_phrase": "The original sentence or phrase to be reworded or paraphrased."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_337",
        "query": "I am working as the head of customer service for a Spanish speaking market. I want to know the sentiment of my customers on their last call with our support agents.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that evaluate the sentiment of audio inputs by classifying Spanish speech to determine the underlying sentiment expressed during customer interactions.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Classification",
                    "api_call": "Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')",
                    "api_arguments": {
                        "model_name": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.17.0",
                        "pytorch": "1.10.0+cu111",
                        "datasets": "2.0.0",
                        "tokenizers": "0.11.6"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "MESD",
                        "accuracy": 0.9308
                    },
                    "description": "This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.",
                    "id": "huggingface_api_825",
                    "name": "hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD"
                },
                "id": "gorilla_huggingface_tool_820",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 3: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 4: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 5: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 6: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 8: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 9: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 10: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 11: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 12: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.",
            "Tool 13: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 14: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 15: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 16: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.",
            "Tool 17: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 18: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 19: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 2: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 3: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 4: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 5: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 6: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 7: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 8: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 9: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 10: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 11: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 12: SapBERT is a pretrained model designed to generate embeddings for biomedical entities by aligning their representations in a shared space. Built on PubMedBERT, it processes input strings of biomedical entity names and outputs the [CLS] embedding from the last layer, enabling feature extraction for tasks like entity similarity or biomedical knowledge representation.\nTool 13: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 14: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 15: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 16: A tabular classification model trained for multi-class sentiment analysis on the IMDB dataset, designed to predict sentiment labels from structured data using a Joblib-loaded model in Python.\nTool 17: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 18: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 19: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nI am working as the head of customer service for a Spanish speaking market. I want to know the sentiment of my customers on their last call with our support agents.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the sentiment of customer support call transcripts in Spanish.",
                    "tool_description": "Perform multilingual sentiment analysis on text inputs, specifically tuned to classify text in Spanish and other supported languages, returning a sentiment rating or category.",
                    "expected_response": "A structured output containing the predicted sentiment label (e.g., star rating or positive/negative/neutral) and a confidence score for each customer support call transcript."
                }
            ],
            "extracted_arguments": {
                "text": "The transcript or written record of a customer support call in Spanish."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_65",
        "query": "Find a model that can be used to predict the properties of molecules based on their graph representations.",
        "instruction": "Given a `molecule property prediction` task, retrieve tools that utilize graph-based machine learning models capable of analyzing molecular graph representations to predict molecular properties, ensuring compatibility with the specified framework and requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Graph Machine Learning",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('graphormer-base-pcqm4mv1')",
                    "api_arguments": [
                        "model_name"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "See the Graph Classification with Transformers tutorial",
                    "performance": {
                        "dataset": "PCQM4M-LSC",
                        "accuracy": "1st place on the KDD CUP 2021 (quantum prediction track)"
                    },
                    "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.",
                    "id": "huggingface_api_142",
                    "name": "graphormer-base-pcqm4mv1"
                },
                "id": "gorilla_huggingface_tool_138",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 1: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 4: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 5: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.",
            "Tool 6: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.",
            "Tool 7: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.",
            "Tool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 9: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 10: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 11: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 12: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 13: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.",
            "Tool 14: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 15: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 16: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 17: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.",
            "Tool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French."
        ],
        "tool_view": {
            "description": "Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "parameters": {
                "pretrained_model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pretrained Graphormer model to load from Hugging Face Transformers.",
                    "required": true,
                    "example": "clefourrier/graphormer-base-pcqm4mv2"
                }
            },
            "response": "The tool provides a pretrained graph Transformer model designed for graph classification and graph representation tasks. Its response is not explicitly defined in the documentation, but a typical output when using the model for inference would include the following components:  \n\n1. **Graph Representation / Embeddings**:  \n   - A numerical tensor representing the learned features of the input graph.  \n   - These embeddings capture structural and node/edge information of the graph in a fixed-dimensional vector space, suitable for downstream tasks such as classification or regression.  \n\n2. **Predicted Class / Label Scores**:  \n   - If the model is applied to a classification task, the output typically includes a tensor of logits or probabilities corresponding to each possible class.  \n   - These values indicate the model’s confidence for each class label for the input graph.  \n\n3. **Optional Intermediate Outputs**:  \n   - Some implementations may optionally return attention weights or hidden states from intermediate Transformer layers, which can be used for analysis or interpretability.  \n\n4. **Batch Handling**:  \n   - The response usually supports batched inputs, so the output tensors may include a batch dimension corresponding to multiple graphs processed simultaneously.  \n\nIn summary, the response is a structured set of tensors that provide either graph-level embeddings or class predictions, potentially accompanied by auxiliary data such as attention maps, all suitable for downstream graph machine learning tasks.",
            "examples": [
                {
                    "user_intent": "I want to classify a batch of molecular graphs using a pretrained Graphormer model.",
                    "tool_call": {
                        "name": "graphormer-base-pcqm4mv2",
                        "arguments": {
                            "pretrained_model_name": "clefourrier/graphormer-base-pcqm4mv2"
                        }
                    }
                },
                {
                    "user_intent": "Load a graph Transformer model for graph-level property prediction on chemical compounds.",
                    "tool_call": {
                        "name": "graphormer-base-pcqm4mv2",
                        "arguments": {
                            "pretrained_model_name": "clefourrier/graphormer-base-pcqm4mv2"
                        }
                    }
                },
                {
                    "user_intent": "I need a pretrained Graphormer model to generate embeddings for molecular graphs for downstream tasks.",
                    "tool_call": {
                        "name": "graphormer-base-pcqm4mv2",
                        "arguments": {
                            "pretrained_model_name": "clefourrier/graphormer-base-pcqm4mv2"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 1: Graphormer-base-pcqm4mv1 is a pretrained graph Transformer model designed for graph classification and representation tasks, particularly in molecular modeling. It was trained on the PCQM4M-LSC dataset and achieved first place in the KDD CUP 2021 quantum prediction track, making it highly effective for learning from graph-structured data. The model can be used directly or fine-tuned for specific downstream tasks using the Hugging Face Transformers framework.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 4: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 5: The \"nikcheerla/nooks-amd-detection-v2-full\" is a sentence-transformer model that converts sentences and paragraphs into 768-dimensional dense vector representations. It can be used for tasks such as clustering or semantic search, enabling comparison of sentence similarities based on their embedded vector space.\nTool 6: VideoMAE is a video-focused extension of Masked Autoencoders (MAE) that leverages a Vision Transformer (ViT) architecture to learn meaningful video representations. It processes videos as sequences of fixed-size patches with positional embeddings and uses a decoder to predict pixel values for masked patches during pre-training. The model’s pre-trained representations can then be used for downstream tasks such as video classification by adding a simple linear layer on top of the [CLS] token, which summarizes the entire video. This tool is useful for extracting rich video features that improve performance on tasks like action recognition in datasets such as Something-Something-v2.\nTool 7: A pre-trained Vision Transformer (ViT) model for image classification that processes images as sequences of fixed-size patches and uses a Transformer encoder to predict class labels. It is trained on ImageNet-21k and fine-tuned on ImageNet 2012, making it suitable for recognizing a wide range of visual categories. This model is used to classify images by extracting features and predicting the most likely class from the learned representations.\nTool 8: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 9: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 10: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 11: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 12: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 13: SciNCL is a pre-trained BERT-based model designed to generate document-level embeddings for research papers, capturing their semantic content for tasks like similarity assessment and recommendation. It leverages citation graph neighborhoods during contrastive learning to improve the quality of embeddings and is initialized from SciBERT weights, enabling effective representation of scientific text.\nTool 14: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 15: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 16: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 17: XLM-RoBERTa is a multilingual model designed for masked language modeling, pre-trained on 2.5TB of CommonCrawl data in 100 languages. It can be used to predict masked words in sentences and is intended for fine-tuning on specific downstream tasks. This model is accessed through the Hugging Face Transformers pipeline.\nTool 18: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 19: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\", \"parameters\": {\"pretrained_model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pretrained Graphormer model to load from Hugging Face Transformers.\", \"required\": true, \"example\": \"clefourrier/graphormer-base-pcqm4mv2\"}}, \"response\": \"The tool provides a pretrained graph Transformer model designed for graph classification and graph representation tasks. Its response is not explicitly defined in the documentation, but a typical output when using the model for inference would include the following components:  \\n\\n1. **Graph Representation / Embeddings**:  \\n   - A numerical tensor representing the learned features of the input graph.  \\n   - These embeddings capture structural and node/edge information of the graph in a fixed-dimensional vector space, suitable for downstream tasks such as classification or regression.  \\n\\n2. **Predicted Class / Label Scores**:  \\n   - If the model is applied to a classification task, the output typically includes a tensor of logits or probabilities corresponding to each possible class.  \\n   - These values indicate the model\\u2019s confidence for each class label for the input graph.  \\n\\n3. **Optional Intermediate Outputs**:  \\n   - Some implementations may optionally return attention weights or hidden states from intermediate Transformer layers, which can be used for analysis or interpretability.  \\n\\n4. **Batch Handling**:  \\n   - The response usually supports batched inputs, so the output tensors may include a batch dimension corresponding to multiple graphs processed simultaneously.  \\n\\nIn summary, the response is a structured set of tensors that provide either graph-level embeddings or class predictions, potentially accompanied by auxiliary data such as attention maps, all suitable for downstream graph machine learning tasks.\", \"examples\": [{\"user_intent\": \"I want to classify a batch of molecular graphs using a pretrained Graphormer model.\", \"tool_call\": {\"name\": \"graphormer-base-pcqm4mv2\", \"arguments\": {\"pretrained_model_name\": \"clefourrier/graphormer-base-pcqm4mv2\"}}}, {\"user_intent\": \"Load a graph Transformer model for graph-level property prediction on chemical compounds.\", \"tool_call\": {\"name\": \"graphormer-base-pcqm4mv2\", \"arguments\": {\"pretrained_model_name\": \"clefourrier/graphormer-base-pcqm4mv2\"}}}, {\"user_intent\": \"I need a pretrained Graphormer model to generate embeddings for molecular graphs for downstream tasks.\", \"tool_call\": {\"name\": \"graphormer-base-pcqm4mv2\", \"arguments\": {\"pretrained_model_name\": \"clefourrier/graphormer-base-pcqm4mv2\"}}}]}\n\n# User Query\nFind a model that can be used to predict the properties of molecules based on their graph representations.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pretrained model that can generate graph representations of molecules for property prediction.",
                    "tool_description": "Retrieve and load a pretrained Graphormer model designed for graph classification and representation tasks, suitable for molecular modeling and downstream property prediction tasks.",
                    "expected_response": "A pretrained Graphormer model instance capable of producing graph-level embeddings or class predictions for molecular graphs, including optional intermediate outputs such as attention weights."
                }
            ],
            "extracted_arguments": {
                "pretrained_model_name": "The identifier or name of the pretrained Graphormer model to load from a model repository, typically a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_64",
        "query": "We want to evaluate the carbon footprint of a construction project based on the tabular data of material consumption.",
        "instruction": "Given a `carbon footprint evaluation` task, retrieve tools that analyze tabular data to predict or assess the carbon emissions of construction projects, employing models designed for tabular data inputs and capable of providing emission estimations.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "Validation Metrics",
                        "accuracy": 0.831
                    },
                    "description": "A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.",
                    "id": "huggingface_api_863",
                    "name": "tejas23/autotrain-amx2-1702259728"
                },
                "id": "gorilla_huggingface_tool_856",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 1: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 3: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 4: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 5: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 6: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 7: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 13: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 14: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 15: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 16: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 19: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement."
        ],
        "tool_view": {
            "description": "A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "parameters": {
                "bootstrap": {
                    "type": "boolean",
                    "description": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
                    "required": true,
                    "default": "True",
                    "example": true
                },
                "ccp_alpha": {
                    "type": "number",
                    "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "criterion": {
                    "type": "string",
                    "description": "The function used to measure the quality of a split. 'squared_error' uses mean squared error.",
                    "required": true,
                    "default": "squared_error",
                    "example": "squared_error"
                },
                "max_depth": {
                    "type": "integer",
                    "description": "The maximum depth of the tree. Limits how deep each tree can grow.",
                    "required": true,
                    "default": "10",
                    "example": 10
                },
                "max_features": {
                    "type": "number",
                    "description": "The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.",
                    "required": true,
                    "default": "1.0",
                    "example": 1.0
                },
                "max_leaf_nodes": {
                    "type": "string",
                    "description": "Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.",
                    "required": true,
                    "example": ""
                },
                "max_samples": {
                    "type": "string",
                    "description": "If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.",
                    "required": true,
                    "example": ""
                },
                "min_impurity_decrease": {
                    "type": "number",
                    "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "min_samples_leaf": {
                    "type": "integer",
                    "description": "The minimum number of samples required to be at a leaf node.",
                    "required": true,
                    "default": "1",
                    "example": 1
                },
                "min_samples_split": {
                    "type": "integer",
                    "description": "The minimum number of samples required to split an internal node.",
                    "required": true,
                    "default": "2",
                    "example": 2
                },
                "min_weight_fraction_leaf": {
                    "type": "number",
                    "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node.",
                    "required": true,
                    "default": "0.0",
                    "example": 0.0
                },
                "n_estimators": {
                    "type": "integer",
                    "description": "The number of trees in the forest.",
                    "required": true,
                    "default": "50",
                    "example": 50
                },
                "n_jobs": {
                    "type": "string",
                    "description": "The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.",
                    "required": true,
                    "example": ""
                },
                "oob_score": {
                    "type": "boolean",
                    "description": "Whether to use out-of-bag samples to estimate the R^2 on unseen data.",
                    "required": true,
                    "default": "False",
                    "example": false
                },
                "random_state": {
                    "type": "integer",
                    "description": "Controls the randomness of the estimator for reproducibility.",
                    "required": true,
                    "default": "59",
                    "example": 59
                },
                "verbose": {
                    "type": "integer",
                    "description": "Controls the verbosity when fitting and predicting.",
                    "required": true,
                    "default": "0",
                    "example": 0
                },
                "warm_start": {
                    "type": "boolean",
                    "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.",
                    "required": true,
                    "default": "False",
                    "example": false
                }
            },
            "response": "The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\n\n1. **model description**: \n   - **Domain**: \"Tabular Tabular Regression\" - This indicates that the model is designed for regression tasks involving tabular data.\n   - **Framework**: \"Scikit-learn\" - Specifies that the model is built using the Scikit-learn machine learning library.\n   - **Functionality**: \"skops\" - Refers to the model's compatibility with or use of the Skops library.\n   - **Description**: \"A RandomForestRegressor model for electricity consumption prediction\" - A brief description of what the model is used for.\n\n2. **API call**: \n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\n\n3. **Performance**: \n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\n\n4. **Example Code**: \n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\n\n5. **Python Environment Requirements**: \n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\n\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.",
            "examples": [
                {
                    "user_intent": "I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 10,
                            "max_features": 1.0,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 100,
                            "n_jobs": -1,
                            "oob_score": true,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                },
                {
                    "user_intent": "I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.",
                    "tool_call": {
                        "name": "rajistics/MAPIE-TS-Electricity",
                        "arguments": {
                            "bootstrap": true,
                            "ccp_alpha": 0.0,
                            "criterion": "squared_error",
                            "max_depth": 20,
                            "max_features": 1.5,
                            "max_leaf_nodes": "",
                            "max_samples": "",
                            "min_impurity_decrease": 0.0,
                            "min_samples_leaf": 1,
                            "min_samples_split": 2,
                            "min_weight_fraction_leaf": 0.0,
                            "n_estimators": 50,
                            "n_jobs": "",
                            "oob_score": false,
                            "random_state": 59,
                            "verbose": 0,
                            "warm_start": false
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 1: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 2: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 3: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 4: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 5: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 6: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 7: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 8: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 13: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 14: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 15: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 16: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 19: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\", \"parameters\": {\"bootstrap\": {\"type\": \"boolean\", \"description\": \"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\", \"required\": true, \"default\": \"True\", \"example\": true}, \"ccp_alpha\": {\"type\": \"number\", \"description\": \"Complexity parameter used for Minimal Cost-Complexity Pruning. A non-zero value will prune the tree.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"criterion\": {\"type\": \"string\", \"description\": \"The function used to measure the quality of a split. 'squared_error' uses mean squared error.\", \"required\": true, \"default\": \"squared_error\", \"example\": \"squared_error\"}, \"max_depth\": {\"type\": \"integer\", \"description\": \"The maximum depth of the tree. Limits how deep each tree can grow.\", \"required\": true, \"default\": \"10\", \"example\": 10}, \"max_features\": {\"type\": \"number\", \"description\": \"The number of features to consider when looking for the best split. Can be a float representing a fraction of total features.\", \"required\": true, \"default\": \"1.0\", \"example\": 1.0}, \"max_leaf_nodes\": {\"type\": \"string\", \"description\": \"Grow trees with max_leaf_nodes in best-first fashion. If empty, nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\", \"required\": true, \"example\": \"\"}, \"max_samples\": {\"type\": \"string\", \"description\": \"If bootstrap is True, the number of samples to draw from X to train each base estimator. If empty, draw X.shape[0] samples.\", \"required\": true, \"example\": \"\"}, \"min_impurity_decrease\": {\"type\": \"number\", \"description\": \"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"min_samples_leaf\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to be at a leaf node.\", \"required\": true, \"default\": \"1\", \"example\": 1}, \"min_samples_split\": {\"type\": \"integer\", \"description\": \"The minimum number of samples required to split an internal node.\", \"required\": true, \"default\": \"2\", \"example\": 2}, \"min_weight_fraction_leaf\": {\"type\": \"number\", \"description\": \"The minimum weighted fraction of the sum total of weights required to be at a leaf node.\", \"required\": true, \"default\": \"0.0\", \"example\": 0.0}, \"n_estimators\": {\"type\": \"integer\", \"description\": \"The number of trees in the forest.\", \"required\": true, \"default\": \"50\", \"example\": 50}, \"n_jobs\": {\"type\": \"string\", \"description\": \"The number of jobs to run in parallel for both fit and predict. If empty, defaults to 1.\", \"required\": true, \"example\": \"\"}, \"oob_score\": {\"type\": \"boolean\", \"description\": \"Whether to use out-of-bag samples to estimate the R^2 on unseen data.\", \"required\": true, \"default\": \"False\", \"example\": false}, \"random_state\": {\"type\": \"integer\", \"description\": \"Controls the randomness of the estimator for reproducibility.\", \"required\": true, \"default\": \"59\", \"example\": 59}, \"verbose\": {\"type\": \"integer\", \"description\": \"Controls the verbosity when fitting and predicting.\", \"required\": true, \"default\": \"0\", \"example\": 0}, \"warm_start\": {\"type\": \"boolean\", \"description\": \"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble.\", \"required\": true, \"default\": \"False\", \"example\": false}}, \"response\": \"The tool is used for a **RandomForestRegressor** model designed to predict electricity consumption. Below is the expected structure and meaning of the response:\\n\\n1. **model description**: \\n   - **Domain**: \\\"Tabular Tabular Regression\\\" - This indicates that the model is designed for regression tasks involving tabular data.\\n   - **Framework**: \\\"Scikit-learn\\\" - Specifies that the model is built using the Scikit-learn machine learning library.\\n   - **Functionality**: \\\"skops\\\" - Refers to the model's compatibility with or use of the Skops library.\\n   - **Description**: \\\"A RandomForestRegressor model for electricity consumption prediction\\\" - A brief description of what the model is used for.\\n\\n2. **API call**: \\n   - This section shows the exact Python API call used to initialize the model: `RandomForestRegressor()`. \\n   - **api_arguments**: A dictionary of hyperparameters used to configure the `RandomForestRegressor`:\\n     - Each key-value pair represents a specific argument and its assigned value for the model (e.g., `bootstrap: True`, `criterion: squared_error`).\\n     - These parameters control various aspects of the model's behavior such as the type of error to minimize, maximum depth of trees, number of estimators (trees), etc.\\n\\n3. **Performance**: \\n   - This section would typically contain information about the model's performance, such as accuracy or other relevant metrics, on a specific dataset. In this case, both the dataset and accuracy fields are unspecified.\\n\\n4. **Example Code**: \\n   - Typically, this field would provide an example of how to use the model in Python code. However, this is currently left empty.\\n\\n5. **Python Environment Requirements**: \\n   - If there are specific Python libraries or versions required to run the tool, they would be listed here. This field is currently unspecified.\\n\\nIn summary, the response would contain a description of the **RandomForestRegressor** model, its configuration (via the list of API arguments), and potentially performance metrics (if provided). There are no concrete examples of code or specific Python environment requirements in this case.\", \"examples\": [{\"user_intent\": \"I want to create a Random Forest model to predict electricity consumption using default settings but limit the depth of trees to avoid overfitting.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"Set up a Random Forest regressor to forecast electricity usage with 100 trees and enable out-of-bag scoring for validation.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 10, \"max_features\": 1.0, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": -1, \"oob_score\": true, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}, {\"user_intent\": \"I need a Random Forest model for electricity prediction that grows deeper trees and uses more features per split.\", \"tool_call\": {\"name\": \"rajistics/MAPIE-TS-Electricity\", \"arguments\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"criterion\": \"squared_error\", \"max_depth\": 20, \"max_features\": 1.5, \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": \"\", \"oob_score\": false, \"random_state\": 59, \"verbose\": 0, \"warm_start\": false}}}]}\n\n# User Query\nWe want to evaluate the carbon footprint of a construction project based on the tabular data of material consumption.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the carbon emissions based on the construction project's material consumption data.",
                    "tool_description": "Use a tabular regression model trained with AutoTrain to predict carbon emissions from structured input features representing material usage.",
                    "expected_response": "A numeric value or set of values representing the estimated carbon emissions for each data entry in the input dataset."
                }
            ],
            "extracted_arguments": {
                "input_features": "A structured dataset containing numeric and categorical features representing material quantities and types used in the construction project."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_272",
        "query": "Provide a way to translate spoken English to spoken Hokkien for an audio file.",
        "instruction": "Given a `speech-to-speech translation` task, retrieve tools that process audio inputs to translate spoken language from English to Hokkien, focusing on transforming speech directly into the target language with tools designed to handle both input and output in audio format.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_unity_en-hk')",
                    "api_arguments": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text",
                        "cache_dir": "cache_dir"
                    },
                    "python_environment_requirements": [
                        "fairseq",
                        "hub_utils",
                        "torchaudio",
                        "IPython.display",
                        "huggingface_hub"
                    ],
                    "example_code": [
                        "import json",
                        "import os",
                        "from pathlib import Path",
                        "import IPython.display as ipd",
                        "from fairseq import hub_utils",
                        "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub",
                        "from fairseq.models.speech_to_text.hub_interface import S2THubInterface",
                        "from fairseq.models.text_to_speech import CodeHiFiGANVocoder",
                        "from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface",
                        "from huggingface_hub import snapshot_download",
                        "import torchaudio",
                        "cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)",
                        "models, cfg, task = load_model_ensemble_and_task_from_hf_hub(",
                        " facebook/xm_transformer_unity_en-hk,",
                        " arg_overrides={config_yaml: config.yaml, task: speech_to_text},",
                        " cache_dir=cache_dir,",
                        ")",
                        "model = models[0].cpu()",
                        "cfg[task].cpu = True",
                        "generator = task.build_generator([model], cfg)",
                        "audio, _ = torchaudio.load(/path/to/an/audio/file)",
                        "sample = S2THubInterface.get_model_input(task, audio)",
                        "unit = S2THubInterface.get_prediction(task, model, generator, sample)",
                        "library_name = fairseq",
                        "cache_dir = (",
                        " cache_dir or (Path.home() / .cache / library_name).as_posix()",
                        ")",
                        "cache_dir = snapshot_download(",
                        " ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name",
                        ")",
                        "x = hub_utils.from_pretrained(",
                        " cache_dir,",
                        " model.pt,",
                        " .,",
                        " archive_map=CodeHiFiGANVocoder.hub_models(),",
                        " config_yaml=config.json,",
                        " fp16=False,",
                        " is_vocoder=True,",
                        ")",
                        "with open(f{x['args']['data']}/config.json) as f:",
                        " vocoder_cfg = json.load(f)",
                        "assert (",
                        " len(x[args][model_path]) == 1",
                        "), Too many vocoder models in the input",
                        "vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)",
                        "tts_model = VocoderHubInterface(vocoder_cfg, vocoder)",
                        "tts_sample = tts_model.get_model_input(unit)",
                        "wav, sr = tts_model.get_prediction(tts_sample)",
                        "ipd.Audio(wav, rate=sr)"
                    ],
                    "performance": {
                        "dataset": "MuST-C",
                        "accuracy": null
                    },
                    "description": "Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.",
                    "id": "huggingface_api_787",
                    "name": "xm_transformer_unity_en-hk"
                },
                "id": "gorilla_huggingface_tool_782",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 1: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 2: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 3: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 4: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 5: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 6: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 7: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.",
            "Tool 8: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 9: A PyTorch-based speech recognition model fine-tuned for Marathi, capable of converting spoken Marathi audio into text with moderate accuracy, using the Wav2Vec2 architecture from the Transformers library.",
            "Tool 10: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 11: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.",
            "Tool 12: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 13: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 14: A Text-to-Speech (TTS) tool that converts text into spoken audio using the ESPnet JETS model exported to ONNX. It can be integrated with the txtai pipeline or used directly with ONNX to generate high-quality speech from text.",
            "Tool 15: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 16: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 17: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.",
            "Tool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 19: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German."
        ],
        "tool_view": {
            "description": "A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "parameters": {
                "config_yaml": {
                    "type": "string",
                    "description": "The path to the YAML configuration file that contains the model settings and other relevant configurations.",
                    "required": true,
                    "example": "config.yaml"
                },
                "task": {
                    "type": "string",
                    "description": "The specific task the model will perform. In this case, it refers to speech-to-text, indicating that the model will translate speech into written text.",
                    "required": true,
                    "example": "speech_to_text"
                },
                "cache_dir": {
                    "type": "string",
                    "description": "The directory where cached model files will be stored. This is typically used to prevent re-downloading models every time the tool is invoked.",
                    "required": true,
                    "example": "/path/to/cache"
                }
            },
            "response": "The tool produces a **speech-to-speech translation output** in audio form. The response can be described as follows:\n\n- **Primary output (`wav`)**: A waveform array representing the synthesized speech in the target language. This is typically a one-dimensional or two-dimensional numeric array containing amplitude values over time.  \n\n- **Sampling rate (`sr`)**: An integer indicating the number of audio samples per second, specifying the playback rate for the waveform.  \n\n- **Intermediate representations (optional, internal to the pipeline)**:  \n  - **Unit sequence (`unit`)**: A discrete intermediate representation generated by the speech-to-speech model (UnitY), which encodes the phonetic or linguistic content of the input speech.  \n  - **Model input tensors (`sample`, `tts_sample`)**: Preprocessed audio features or embeddings used as input to the speech-to-speech model and the vocoder. These are primarily for internal computation rather than end-user consumption.  \n\n- **Role of components**:  \n  - **Speech-to-speech model**: Converts source-language audio into a discrete unit sequence representing the target-language speech content.  \n  - **Vocoder (Unit HiFiGAN)**: Converts the unit sequence into a natural-sounding audio waveform.  \n\nIn practice, the **user-facing response** consists of the waveform array (`wav`) and its associated sampling rate (`sr`), which can be directly played back or saved as an audio file. All other data structures are intermediate representations used internally for processing.",
            "examples": [
                {
                    "user_intent": "I need to translate a speech file from Hokkien to English.",
                    "tool_call": {
                        "name": "xm_transformer_unity_hk-en",
                        "arguments": {
                            "config_yaml": "config.yaml",
                            "task": "speech_to_text",
                            "cache_dir": "cache_dir"
                        }
                    }
                },
                {
                    "user_intent": "Load a pre-trained model for speech-to-speech translation between Hokkien and English for a new task.",
                    "tool_call": {
                        "name": "xm_transformer_unity_hk-en",
                        "arguments": {
                            "config_yaml": "config.yaml",
                            "task": "speech_to_text",
                            "cache_dir": "cache_dir"
                        }
                    }
                },
                {
                    "user_intent": "Translate a Hokkien speech file into English using the xm_transformer model.",
                    "tool_call": {
                        "name": "xm_transformer_unity_hk-en",
                        "arguments": {
                            "config_yaml": "config.yaml",
                            "task": "speech_to_text",
                            "cache_dir": "cache_dir"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 1: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 2: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 3: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 4: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 5: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 6: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 7: A compact wav2vec2 model for automatic speech recognition that converts spoken audio into text using the Hugging Face Transformers framework.\nTool 8: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 9: A PyTorch-based speech recognition model fine-tuned for Marathi, capable of converting spoken Marathi audio into text with moderate accuracy, using the Wav2Vec2 architecture from the Transformers library.\nTool 10: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 11: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.\nTool 12: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 13: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 14: A Text-to-Speech (TTS) tool that converts text into spoken audio using the ESPnet JETS model exported to ONNX. It can be integrated with the txtai pipeline or used directly with ONNX to generate high-quality speech from text.\nTool 15: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 16: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 17: Whisper is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio without fine-tuning. It is designed to generalize across diverse datasets and domains, enabling accurate conversion of spoken language into text.\nTool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 19: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook\\u2019s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\", \"parameters\": {\"config_yaml\": {\"type\": \"string\", \"description\": \"The path to the YAML configuration file that contains the model settings and other relevant configurations.\", \"required\": true, \"example\": \"config.yaml\"}, \"task\": {\"type\": \"string\", \"description\": \"The specific task the model will perform. In this case, it refers to speech-to-text, indicating that the model will translate speech into written text.\", \"required\": true, \"example\": \"speech_to_text\"}, \"cache_dir\": {\"type\": \"string\", \"description\": \"The directory where cached model files will be stored. This is typically used to prevent re-downloading models every time the tool is invoked.\", \"required\": true, \"example\": \"/path/to/cache\"}}, \"response\": \"The tool produces a **speech-to-speech translation output** in audio form. The response can be described as follows:\\n\\n- **Primary output (`wav`)**: A waveform array representing the synthesized speech in the target language. This is typically a one-dimensional or two-dimensional numeric array containing amplitude values over time.  \\n\\n- **Sampling rate (`sr`)**: An integer indicating the number of audio samples per second, specifying the playback rate for the waveform.  \\n\\n- **Intermediate representations (optional, internal to the pipeline)**:  \\n  - **Unit sequence (`unit`)**: A discrete intermediate representation generated by the speech-to-speech model (UnitY), which encodes the phonetic or linguistic content of the input speech.  \\n  - **Model input tensors (`sample`, `tts_sample`)**: Preprocessed audio features or embeddings used as input to the speech-to-speech model and the vocoder. These are primarily for internal computation rather than end-user consumption.  \\n\\n- **Role of components**:  \\n  - **Speech-to-speech model**: Converts source-language audio into a discrete unit sequence representing the target-language speech content.  \\n  - **Vocoder (Unit HiFiGAN)**: Converts the unit sequence into a natural-sounding audio waveform.  \\n\\nIn practice, the **user-facing response** consists of the waveform array (`wav`) and its associated sampling rate (`sr`), which can be directly played back or saved as an audio file. All other data structures are intermediate representations used internally for processing.\", \"examples\": [{\"user_intent\": \"I need to translate a speech file from Hokkien to English.\", \"tool_call\": {\"name\": \"xm_transformer_unity_hk-en\", \"arguments\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\", \"cache_dir\": \"cache_dir\"}}}, {\"user_intent\": \"Load a pre-trained model for speech-to-speech translation between Hokkien and English for a new task.\", \"tool_call\": {\"name\": \"xm_transformer_unity_hk-en\", \"arguments\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\", \"cache_dir\": \"cache_dir\"}}}, {\"user_intent\": \"Translate a Hokkien speech file into English using the xm_transformer model.\", \"tool_call\": {\"name\": \"xm_transformer_unity_hk-en\", \"arguments\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\", \"cache_dir\": \"cache_dir\"}}}]}\n\n# User Query\nProvide a way to translate spoken English to spoken Hokkien for an audio file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate an English speech audio file into Hokkien.",
                    "tool_description": "Convert spoken English audio into spoken Hokkien using a speech-to-speech translation model with a single-pass or two-pass decoder, generating natural-sounding translated speech.",
                    "expected_response": "A synthesized Hokkien audio waveform and its sampling rate representing the translated speech."
                }
            ],
            "extracted_arguments": {
                "input_audio": "The file path or data buffer containing the source English speech audio.",
                "config_yaml": "The path to the YAML configuration file containing model settings for speech-to-speech translation.",
                "task": "The specific task the model will perform, in this case, speech-to-speech translation from English to Hokkien.",
                "cache_dir": "The directory path for caching model files to avoid repeated downloads."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_389",
        "query": "We have the financial documents of a company and we want to extract information about the cash flow. Modify the model so it can answer the questions related to the cash flow.",
        "instruction": "Given a `financial document analysis` task, retrieve tools that specialize in processing company financial documents to extract and provide information related to specific topics such as cash flow by using question-answering models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Document Question Answering model based on LayoutXLM.",
                    "id": "huggingface_api_120",
                    "name": "CZ_DVQA_layoutxlm-base"
                },
                "id": "gorilla_huggingface_tool_119",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 2: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 4: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 5: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 8: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 9: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 10: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 11: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 14: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 15: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 16: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 17: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 18: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 19: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis."
        ],
        "tool_view": {
            "description": "A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the image that contains the document to be processed for question answering.",
                    "required": true,
                    "example": "path/to/image"
                },
                "question": {
                    "type": "string",
                    "description": "The question that you want to ask regarding the content of the image.",
                    "required": true,
                    "example": "What is the name of the company in the document?"
                }
            },
            "response": "The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\n\nThe response would typically include:\n\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\n\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\n\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\n\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.",
            "examples": [
                {
                    "user_intent": "I have a scanned contract and I want to know the effective date mentioned in it.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/contract_scan_01.png",
                            "question": "What is the effective date of this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Check this invoice image and tell me the total amount due.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/invoice_march2025.png",
                            "question": "What is the total amount to be paid?"
                        }
                    }
                },
                {
                    "user_intent": "I need to extract the name of the company from this PDF screenshot.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/company_profile_screenshot.png",
                            "question": "What is the name of the company in this document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 2: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 3: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 4: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 5: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 6: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 7: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 8: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 9: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 10: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 11: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 13: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 14: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 15: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 16: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 17: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 18: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 19: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the image that contains the document to be processed for question answering.\", \"required\": true, \"example\": \"path/to/image\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that you want to ask regarding the content of the image.\", \"required\": true, \"example\": \"What is the name of the company in the document?\"}}, \"response\": \"The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\\n\\nThe response would typically include:\\n\\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\\n\\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\\n\\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\\n\\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.\", \"examples\": [{\"user_intent\": \"I have a scanned contract and I want to know the effective date mentioned in it.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/contract_scan_01.png\", \"question\": \"What is the effective date of this contract?\"}}}, {\"user_intent\": \"Check this invoice image and tell me the total amount due.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/invoice_march2025.png\", \"question\": \"What is the total amount to be paid?\"}}}, {\"user_intent\": \"I need to extract the name of the company from this PDF screenshot.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/company_profile_screenshot.png\", \"question\": \"What is the name of the company in this document?\"}}}]}\n\n# User Query\nWe have the financial documents of a company and we want to extract information about the cash flow. Modify the model so it can answer the questions related to the cash flow.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to modify the model to extract information related to cash flow from financial documents.",
                    "tool_description": "Adapt the document question-answering model to focus on extracting data specific to cash flow from financial documents.",
                    "expected_response": "Confirmation that the model has been successfully adapted to answer questions about cash flow in financial documents."
                },
                {
                    "user_intent": "I need the model to answer questions specifically about cash flow from financial documents.",
                    "tool_description": "Use the modified document question-answering model to answer questions related to cash flow in financial documents.",
                    "expected_response": "An answer regarding the cash flow based on the content of the financial document."
                }
            ],
            "extracted_arguments": {
                "document_type": "The type of the financial document (e.g., balance sheet, cash flow statement).",
                "focus_area": "The specific financial aspect to extract information on, in this case, cash flow.",
                "question": "The natural language question related to cash flow that needs to be answered."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_35",
        "query": "We are developing an AI chatbot to interact with users. We need the bot to recognize the user's emotions based on their text input.",
        "instruction": "Given a `emotion recognition` task, retrieve tools that process text inputs to classify and identify emotions, leveraging models capable of analyzing and returning emotion-related insights relevant to the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "classifier(I love this!)",
                    "performance": {
                        "dataset": [
                            "Crowdflower (2016)",
                            "Emotion Dataset, Elvis et al. (2018)",
                            "GoEmotions, Demszky et al. (2020)",
                            "ISEAR, Vikash (2018)",
                            "MELD, Poria et al. (2019)",
                            "SemEval-2018, EI-reg, Mohammad et al. (2018)",
                            "Emotion Lines (Friends)"
                        ],
                        "accuracy": "Not provided"
                    },
                    "description": "DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.",
                    "id": "huggingface_api_400",
                    "name": "michellejieli/emotion_text_classifier"
                },
                "id": "gorilla_huggingface_tool_396",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 1: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 2: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 6: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 7: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 8: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 12: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 13: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 14: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 16: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.",
            "Tool 17: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 18: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 19: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data."
        ],
        "tool_view": {
            "description": "Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "parameters": {
                "input": {
                    "type": "string",
                    "description": "The input message that the model will generate a response for. This serves as the starting point for the conversation.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model’s continuation, reply, or conversational output. Specifically:\n\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\n  - `generated_text`: The actual text output produced by the model.\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model’s confidence or token-level details.  \n\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.",
            "examples": [
                {
                    "user_intent": "Start a conversation with the Zixtrauce chatbot and ask about the weather.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What's the weather like today?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a response from BrandonBot4Epochs about AI advancements.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "Tell me about the latest advancements in AI."
                        }
                    }
                },
                {
                    "user_intent": "Chat with the bot about hobbies and interests.",
                    "tool_call": {
                        "name": "Zixtrauce/BDBot4Epoch",
                        "arguments": {
                            "input": "What do you think is the best hobby to start?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 1: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 2: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 6: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 7: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 8: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 10: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 11: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 12: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 13: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 14: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 15: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 16: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.\nTool 17: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 18: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 19: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\", \"parameters\": {\"input\": {\"type\": \"string\", \"description\": \"The input message that the model will generate a response for. This serves as the starting point for the conversation.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a generated text response based on the input message provided to the model. The response structure is a string containing natural language text that represents the model\\u2019s continuation, reply, or conversational output. Specifically:\\n\\n- **Text Content**: The main component of the response is a string of generated text. This text is intended to be contextually relevant to the input message, coherent, and human-readable. In a conversational setting, it typically functions as a chatbot reply.  \\n- **Optional Metadata (typical in Hugging Face `text-generation` pipeline)**: The response may be wrapped in a list of objects, each containing fields such as:\\n  - `generated_text`: The actual text output produced by the model.\\n  - Other optional fields like `score` or `token_ids` may appear depending on the pipeline settings, representing the model\\u2019s confidence or token-level details.  \\n\\nIn summary, the response delivers a text continuation generated by the `Zixtrauce/BDBot4Epoch` model, formatted either as a raw string or as a list of objects containing the generated text and potentially additional metadata. Its semantic role is to provide a conversational reply or text generation output corresponding to the input prompt.\", \"examples\": [{\"user_intent\": \"Start a conversation with the Zixtrauce chatbot and ask about the weather.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What's the weather like today?\"}}}, {\"user_intent\": \"Generate a response from BrandonBot4Epochs about AI advancements.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"Tell me about the latest advancements in AI.\"}}}, {\"user_intent\": \"Chat with the bot about hobbies and interests.\", \"tool_call\": {\"name\": \"Zixtrauce/BDBot4Epoch\", \"arguments\": {\"input\": \"What do you think is the best hobby to start?\"}}}]}\n\n# User Query\nWe are developing an AI chatbot to interact with users. We need the bot to recognize the user's emotions based on their text input.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want the chatbot to recognize emotions in user input.",
                    "tool_description": "Classify emotions in the provided text input based on Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category.",
                    "expected_response": "A list of emotions with scores representing the likelihood of each emotion being present in the input text."
                }
            ],
            "extracted_arguments": {
                "input_text": "The text input provided by the user, containing the emotional content to be analyzed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_248",
        "query": "My company has a large data table of employees, containing their names, titles, departments, and hire dates. We need a tool that can find all employees with the title of \"Software Engineer\" hired in 2020.",
        "instruction": "Given a `data table query` task, retrieve tools that utilize table reasoning capabilities to filter and identify specific records within a dataset, such as employees with particular titles and hire dates, using structured query execution based on the query's criteria.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-sql-execution')",
                    "api_arguments": {
                        "table": "pd.DataFrame",
                        "query": "str"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "pandas"
                    ],
                    "example_code": "from transformers import TapexTokenizer, BartForConditionalGeneration\nimport pandas as pd\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-sql-execution)\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-sql-execution)\ndata = {\n year: [1896, 1900, 1904, 2004, 2008, 2012],\n city: [athens, paris, st. louis, athens, beijing, london]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = select year where city = beijing\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\noutputs = model.generate(**encoding)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
                    "performance": {
                        "dataset": "synthetic corpus",
                        "accuracy": "not specified"
                    },
                    "description": "TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.",
                    "id": "huggingface_api_448",
                    "name": "microsoft/tapex-large-sql-execution"
                },
                "id": "gorilla_huggingface_tool_444",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 4: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.",
            "Tool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 6: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 7: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 8: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 9: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.",
            "Tool 10: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 11: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 12: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 13: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.",
            "Tool 14: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 15: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 16: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 17: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 19: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications."
        ],
        "tool_view": {
            "description": "OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "parameters": {
                "table": {
                    "type": "pd.DataFrame",
                    "description": "A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.",
                    "required": true,
                    "example": "pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})"
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question string to be answered using the provided table data.",
                    "required": true,
                    "example": "In which year did Beijing host the Olympic Games?"
                }
            },
            "response": "The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \n\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \n- **Semantic role:** Each output string conveys the model’s prediction of the answer to the user’s question, derived by reasoning over the table structure, headers, and cell values.  \n\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.",
            "examples": [
                {
                    "user_intent": "I have a table of countries and their populations. I want to know which country has the largest population.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})",
                            "query": "Which country has the largest population?"
                        }
                    }
                },
                {
                    "user_intent": "Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})",
                            "query": "Which quarter had the highest revenue?"
                        }
                    }
                },
                {
                    "user_intent": "I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})",
                            "query": "In which year did London host the Olympic Games?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 3: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 4: A YOLOv8-based model designed for extracting tables from document images, capable of detecting both bordered and borderless tables. Trained on a dedicated table-extraction dataset, it provides high accuracy in identifying table structures and can be used to automate table detection and data extraction from images.\nTool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 6: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 7: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 8: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 9: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.\nTool 10: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 11: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 12: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 13: TAPEX is a pre-trained model designed to enable natural language question answering over tabular data. It leverages a neural SQL executor trained on synthetic SQL queries to reason about tables, allowing users to input a table and a text query to generate accurate, context-aware answers. This tool is useful for automating table-based data analysis and retrieving specific information from structured datasets.\nTool 14: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 15: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 16: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 17: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 18: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 19: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\", \"parameters\": {\"table\": {\"type\": \"pd.DataFrame\", \"description\": \"A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.\", \"required\": true, \"example\": \"pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})\"}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question string to be answered using the provided table data.\", \"required\": true, \"example\": \"In which year did Beijing host the Olympic Games?\"}}, \"response\": \"The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \\n\\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \\n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \\n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \\n- **Semantic role:** Each output string conveys the model\\u2019s prediction of the answer to the user\\u2019s question, derived by reasoning over the table structure, headers, and cell values.  \\n\\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.\", \"examples\": [{\"user_intent\": \"I have a table of countries and their populations. I want to know which country has the largest population.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})\", \"query\": \"Which country has the largest population?\"}}}, {\"user_intent\": \"Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})\", \"query\": \"Which quarter had the highest revenue?\"}}}, {\"user_intent\": \"I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})\", \"query\": \"In which year did London host the Olympic Games?\"}}}]}\n\n# User Query\nMy company has a large data table of employees, containing their names, titles, departments, and hire dates. We need a tool that can find all employees with the title of \"Software Engineer\" hired in 2020.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to query a table of employee data to find all employees with a specific job title and hire year.",
                    "tool_description": "Retrieve a subset of data from a table by applying filters based on specific criteria such as job title and hire year.",
                    "expected_response": "A list of employee records, each containing the employee's name, title, department, and hire date, filtered based on the provided criteria."
                }
            ],
            "extracted_arguments": {
                "table": "A pandas DataFrame containing employee records with columns for names, titles, departments, and hire dates.",
                "query": "A natural language query that filters the table for employees with the title 'Software Engineer' and a hire year of 2020."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_374",
        "query": "A real estate agency needs an application that can transform the floor plan images into simple straight line drawings, simplifying the visualization of the properties.",
        "instruction": "Given an `image transformation` task, retrieve tools that specialize in processing image inputs to convert detailed floor plan images into simplified straight line drawings, facilitating easier property visualization.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "ControlNet - M-LSD Straight Line Version",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-mlsd')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": {
                        "import": [
                            "from PIL import Image",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "import torch",
                            "from controlnet_aux import MLSDdetector",
                            "from diffusers.utils import load_image"
                        ],
                        "setup": [
                            "mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)",
                            "image = mlsd(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
                        ],
                        "execution": [
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(room, image, num_inference_steps=20).images[0]",
                            "image.save('images/room_mlsd_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "600k edge-image, caption pairs generated from Places2",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_268",
                    "name": "lllyasviel/sd-controlnet-mlsd"
                },
                "id": "gorilla_huggingface_tool_264",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 1: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 4: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 5: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.",
            "Tool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 7: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 8: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 9: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 10: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 11: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 12: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.",
            "Tool 13: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 14: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 15: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 16: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 18: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.",
            "Tool 19: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms."
        ],
        "tool_view": {
            "description": "This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "parameters": {
                "torch_dtype": {
                    "type": "string",
                    "description": "The data type to use for PyTorch tensors when loading the ControlNet model, which affects memory usage and computation precision.",
                    "required": true,
                    "example": "torch.float16"
                }
            },
            "response": "The tool produces an image-to-image transformation using a diffusion model conditioned on straight-line detection via M-LSD. The response of the tool is an object representing the generated image(s) after processing the input. Specifically:  \n\n- **Primary output**: A collection (typically a list) of generated images. Each image represents the result of applying the ControlNet-conditioned Stable Diffusion pipeline to the input image under the specified prompt or guidance.  \n- **Image format**: Each element is an image object (e.g., a PIL Image) that can be saved or further processed.  \n- **Optional metadata**: While not explicitly defined in the tool JSON, typical pipelines may include associated metadata such as:\n  - The input image used for conditioning.\n  - The prompt or textual description guiding the generation.\n  - Parameters used during inference (e.g., number of inference steps, scheduler type, seed).\n  - Any intermediate outputs or attention maps if requested (though not standard in the basic setup).  \n\nIn essence, the core semantic content of the response is the fully generated image(s) reflecting the diffusion process guided by the straight-line features detected in the input.",
            "examples": [
                {
                    "user_intent": "Enhance a photo of my living room by generating a stylized version guided by detected straight lines in the room.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Create a clean architectural sketch of a building from a photo using straight line detection.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "Generate a futuristic room concept from an existing room image, while preserving all straight edges and lines.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained",
                        "arguments": {
                            "model_id": "lllyasviel/sd-controlnet-mlsd",
                            "torch_dtype": "torch.float16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 1: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 2: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 4: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 5: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\nTool 6: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 7: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 8: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 9: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 10: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 11: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 12: The \"google/ddpm-cat-256\" model is a Denoising Diffusion Probabilistic Model (DDPM) designed for unconditional image generation. It uses diffusion-based techniques to generate high-quality images from noise, and has been trained on datasets like CIFAR10 and LSUN to achieve strong performance metrics, including an Inception score of 9.46 and an FID score of 3.17. This model can be used with the Hugging Face Transformers framework and requires the `diffusers` Python package for deployment. It provides a simple API for generating images, ideal for tasks involving high-quality image synthesis.\nTool 13: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 14: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 15: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 16: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 18: This tool provides a pre-trained HiFIGAN vocoder model for converting spectrograms into audio waveforms. It is typically used after a Text-to-Speech (TTS) model to transform the spectrogram output into natural-sounding speech. The model is trained on the LJSpeech dataset and operates at a sampling frequency of 22,050 Hz.\nTool 19: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\", \"parameters\": {\"torch_dtype\": {\"type\": \"string\", \"description\": \"The data type to use for PyTorch tensors when loading the ControlNet model, which affects memory usage and computation precision.\", \"required\": true, \"example\": \"torch.float16\"}}, \"response\": \"The tool produces an image-to-image transformation using a diffusion model conditioned on straight-line detection via M-LSD. The response of the tool is an object representing the generated image(s) after processing the input. Specifically:  \\n\\n- **Primary output**: A collection (typically a list) of generated images. Each image represents the result of applying the ControlNet-conditioned Stable Diffusion pipeline to the input image under the specified prompt or guidance.  \\n- **Image format**: Each element is an image object (e.g., a PIL Image) that can be saved or further processed.  \\n- **Optional metadata**: While not explicitly defined in the tool JSON, typical pipelines may include associated metadata such as:\\n  - The input image used for conditioning.\\n  - The prompt or textual description guiding the generation.\\n  - Parameters used during inference (e.g., number of inference steps, scheduler type, seed).\\n  - Any intermediate outputs or attention maps if requested (though not standard in the basic setup).  \\n\\nIn essence, the core semantic content of the response is the fully generated image(s) reflecting the diffusion process guided by the straight-line features detected in the input.\", \"examples\": [{\"user_intent\": \"Enhance a photo of my living room by generating a stylized version guided by detected straight lines in the room.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Create a clean architectural sketch of a building from a photo using straight line detection.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"Generate a futuristic room concept from an existing room image, while preserving all straight edges and lines.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained\", \"arguments\": {\"model_id\": \"lllyasviel/sd-controlnet-mlsd\", \"torch_dtype\": \"torch.float16\"}}}]}\n\n# User Query\nA real estate agency needs an application that can transform the floor plan images into simple straight line drawings, simplifying the visualization of the properties.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transform floor plan images into simplified drawings emphasizing straight lines.",
                    "tool_description": "Use a ControlNet model conditioned on M-LSD straight line detection to guide Stable Diffusion in generating simplified line-based versions of input floor plan images.",
                    "expected_response": "A collection of image objects representing the floor plans transformed into straight line drawings, optionally including metadata about the input and generation parameters."
                }
            ],
            "extracted_arguments": {
                "input_image": "The original floor plan image to be processed as an image object.",
                "torch_dtype": "The data type for PyTorch tensors used in the model, affecting computation precision and memory."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_43",
        "query": "I am developing software that needs to retrieve relevant information from a collection of documents based on a user's query.",
        "instruction": "Given an `information retrieval` task, retrieve tools that process user queries to extract relevant information from document collections efficiently, utilizing natural language processing techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Information Retrieval",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')",
                    "api_arguments": {
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('model_name')",
                        "features": "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": {
                        "import": "from transformers import AutoTokenizer, AutoModelForSequenceClassification",
                        "model": "model = AutoModelForSequenceClassification.from_pretrained('model_name')",
                        "tokenizer": "tokenizer = AutoTokenizer.from_pretrained('model_name')",
                        "features": "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')",
                        "scores": "with torch.no_grad():\n    scores = model(**features).logits\n    print(scores)"
                    },
                    "performance": {
                        "dataset": "TREC Deep Learning 2019",
                        "accuracy": "69.84 (NDCG@10)"
                    },
                    "description": "This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.",
                    "id": "huggingface_api_401",
                    "name": "cross-encoder/ms-marco-TinyBERT-L-2-v2"
                },
                "id": "gorilla_huggingface_tool_397",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 1: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 2: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 3: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 4: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 6: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 8: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.",
            "Tool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 13: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 14: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 16: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 17: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.",
            "Tool 18: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 19: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks."
        ],
        "tool_view": {
            "description": "A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "parameters": {
                "query": {
                    "type": "string",
                    "description": "The query sentence or phrase that you want to find the most similar documents for.",
                    "required": true,
                    "example": "How many people live in London?"
                },
                "docs": {
                    "type": "array",
                    "description": "A list of document strings to compare the query against. These could be sentences or paragraphs that you want to rank by similarity to the query.",
                    "required": true,
                    "example": [
                        "Around 9 Million people live in London",
                        "London is known for its financial district"
                    ]
                }
            },
            "response": "The tool produces a response that represents the semantic similarity between a given query sentence and a set of candidate documents. The response is typically structured as a list or collection of similarity scores paired with the corresponding documents. Each component can be described as follows:\n\n1. **Query embedding**: Internally, the tool encodes the input query into a dense vector in a high-dimensional semantic space (768 dimensions). While this is usually not returned directly, it is the basis for computing similarity.\n\n2. **Document embeddings**: Each input document is similarly encoded into a dense vector of the same dimensionality. These embeddings capture the semantic meaning of the documents.\n\n3. **Similarity scores**: For each document, a numerical score is computed that measures its semantic similarity to the query. This is typically a dot product or cosine similarity between the query embedding and the document embedding. The scores are continuous values, with higher values indicating greater similarity.\n\n4. **Document–score pairs**: The output is often organized as a list of pairs or tuples, where each pair contains:\n   - The original document text.\n   - Its similarity score relative to the query.\n\n5. **Optional sorting**: Many implementations return the document–score pairs sorted in descending order of similarity, so that the most semantically relevant documents appear first.\n\nIn summary, the tool’s response conveys which input documents are most semantically similar to a given query and quantifies this similarity with numerical scores, enabling ranking or retrieval for semantic search tasks.",
            "examples": [
                {
                    "user_intent": "I want to find which of these documents best answers my question about London's population.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
                        "arguments": {
                            "query": "How many people live in London?",
                            "docs": [
                                "Around 9 Million people live in London",
                                "London is known for its financial district",
                                "The population of New York is over 8 million"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Compare these tech news summaries to see which is most relevant to AI advancements.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
                        "arguments": {
                            "query": "Latest trends in artificial intelligence and machine learning",
                            "docs": [
                                "New breakthroughs in deep learning architectures have been reported",
                                "The stock market showed gains in the tech sector today",
                                "AI-driven robots are now performing complex surgeries"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Determine which customer support response best addresses the user's question about refund policies.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
                        "arguments": {
                            "query": "How can I get a refund for my order?",
                            "docs": [
                                "You can request a refund within 30 days of purchase",
                                "Shipping takes 5-7 business days",
                                "Our loyalty program rewards repeat customers"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 1: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 2: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 3: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 4: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 5: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 6: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 7: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 8: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 9: A T5-based model trained on the MS MARCO dataset that generates search queries from input documents, useful for improving document retrieval and information discovery.\nTool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 11: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 13: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 14: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 15: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 16: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 17: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling extraction of answers from documents by interpreting both text and layout information.\nTool 18: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 19: This tool provides a fine-tuned model for document question answering, built on the LayoutLMv2 architecture. It allows for extracting answers from documents based on textual queries, utilizing a pretrained version of the model specifically fine-tuned for information extraction tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question\\u2013answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"The query sentence or phrase that you want to find the most similar documents for.\", \"required\": true, \"example\": \"How many people live in London?\"}, \"docs\": {\"type\": \"array\", \"description\": \"A list of document strings to compare the query against. These could be sentences or paragraphs that you want to rank by similarity to the query.\", \"required\": true, \"example\": [\"Around 9 Million people live in London\", \"London is known for its financial district\"]}}, \"response\": \"The tool produces a response that represents the semantic similarity between a given query sentence and a set of candidate documents. The response is typically structured as a list or collection of similarity scores paired with the corresponding documents. Each component can be described as follows:\\n\\n1. **Query embedding**: Internally, the tool encodes the input query into a dense vector in a high-dimensional semantic space (768 dimensions). While this is usually not returned directly, it is the basis for computing similarity.\\n\\n2. **Document embeddings**: Each input document is similarly encoded into a dense vector of the same dimensionality. These embeddings capture the semantic meaning of the documents.\\n\\n3. **Similarity scores**: For each document, a numerical score is computed that measures its semantic similarity to the query. This is typically a dot product or cosine similarity between the query embedding and the document embedding. The scores are continuous values, with higher values indicating greater similarity.\\n\\n4. **Document\\u2013score pairs**: The output is often organized as a list of pairs or tuples, where each pair contains:\\n   - The original document text.\\n   - Its similarity score relative to the query.\\n\\n5. **Optional sorting**: Many implementations return the document\\u2013score pairs sorted in descending order of similarity, so that the most semantically relevant documents appear first.\\n\\nIn summary, the tool\\u2019s response conveys which input documents are most semantically similar to a given query and quantifies this similarity with numerical scores, enabling ranking or retrieval for semantic search tasks.\", \"examples\": [{\"user_intent\": \"I want to find which of these documents best answers my question about London's population.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", \"arguments\": {\"query\": \"How many people live in London?\", \"docs\": [\"Around 9 Million people live in London\", \"London is known for its financial district\", \"The population of New York is over 8 million\"]}}}, {\"user_intent\": \"Compare these tech news summaries to see which is most relevant to AI advancements.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", \"arguments\": {\"query\": \"Latest trends in artificial intelligence and machine learning\", \"docs\": [\"New breakthroughs in deep learning architectures have been reported\", \"The stock market showed gains in the tech sector today\", \"AI-driven robots are now performing complex surgeries\"]}}}, {\"user_intent\": \"Determine which customer support response best addresses the user's question about refund policies.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", \"arguments\": {\"query\": \"How can I get a refund for my order?\", \"docs\": [\"You can request a refund within 30 days of purchase\", \"Shipping takes 5-7 business days\", \"Our loyalty program rewards repeat customers\"]}}}]}\n\n# User Query\nI am developing software that needs to retrieve relevant information from a collection of documents based on a user's query.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find which documents in my collection are most relevant to a user's query.",
                    "tool_description": "Encode a user's query and a set of candidate documents into dense semantic vectors and compute similarity scores to rank the documents by relevance.",
                    "expected_response": "A list of document–score pairs indicating the semantic similarity between the query and each document, optionally sorted in descending order of relevance."
                }
            ],
            "extracted_arguments": {
                "query": "The user's natural language query as a string.",
                "docs": "A list of textual documents or passages to be compared against the query."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_40",
        "query": "We're working with a voiceover company, and they're looking for a tool to help them change the voice style of voiceovers while keeping the same content.",
        "instruction": "Given a `voice style transformation` task, retrieve tools that specialize in modifying voice attributes while maintaining the original content by processing audio inputs and utilizing advanced speech-to-speech conversion techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')",
                    "api_arguments": {
                        "audio": "example_speech",
                        "sampling_rate": "sampling_rate",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "numpy",
                        "torch",
                        "soundfile"
                    ],
                    "example_code": "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\ndataset = dataset.sort('id')\nsampling_rate = dataset.features['audio'].sampling_rate\nexample_speech = dataset[0]['audio']['array']\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nimport numpy as np\nimport torch\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\nimport soundfile as sf\nsf.write('speech.wav', speech.numpy(), samplerate=16000)",
                    "performance": {
                        "dataset": "CMU ARCTIC",
                        "accuracy": "Not specified"
                    },
                    "description": "SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.",
                    "id": "huggingface_api_784",
                    "name": "microsoft/speecht5_vc"
                },
                "id": "gorilla_huggingface_tool_779",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 2: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 3: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.",
            "Tool 4: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 5: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 6: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 7: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 8: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 9: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 10: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 11: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 12: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 13: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 14: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 15: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.",
            "Tool 16: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.",
            "Tool 17: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian."
        ],
        "tool_view": {
            "description": "Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description of the image to generate. This should clearly specify the subject, style, and any desired details for the generated image.",
                    "required": true,
                    "example": "archer arcane style magical princess with golden hair"
                }
            },
            "response": "The response from the tool would be an image generated based on the provided prompt. The specific structure and components of the response are as follows:\n\n1. **Generated Image**: The primary output will be an image (or set of images) created from the text prompt provided by the user. The tool generates the image using the Nitro Diffusion model, which is a fine-tuned version of Stable Diffusion. The response will likely include the image data, either in a directly viewable format (e.g., PNG, JPEG) or as a reference to the image location.\n\n2. **Metadata (Optional)**: Although not explicitly mentioned in the documentation, it is common for tools like this to provide additional metadata, such as:\n   - **Image dimensions** (e.g., width, height in pixels)\n   - **Style information** (indicating whether the image adheres more to one of the three art styles that Nitro Diffusion is fine-tuned on, if relevant)\n   - **Generation parameters** (if any other settings were adjusted, such as seed values or specific weighting parameters)\n\nThe exact format and structure of the image may vary depending on the platform or environment used for the tool execution. However, the core of the response will be the image itself, generated from the provided prompt.",
            "examples": [
                {
                    "user_intent": "Generate a digital painting of a futuristic city at sunset with neon lights.",
                    "tool_call": {
                        "name": "nitrosocke/nitro-diffusion",
                        "arguments": {
                            "prompt": "futuristic cityscape at sunset, neon lights, cyberpunk style, highly detailed digital painting"
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a medieval knight in armor, in the style of classical fantasy art.",
                    "tool_call": {
                        "name": "nitrosocke/nitro-diffusion",
                        "arguments": {
                            "prompt": "medieval knight in shining armor, epic fantasy style, highly detailed illustration, dramatic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Make a whimsical portrait of a magical cat wearing a wizard hat, in a colorful cartoon style.",
                    "tool_call": {
                        "name": "nitrosocke/nitro-diffusion",
                        "arguments": {
                            "prompt": "magical cat wearing a wizard hat, whimsical cartoon style, bright colors, playful illustration"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 1: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 2: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 3: This tool provides a cross-encoder model for information retrieval, trained on the MS Marco Passage Ranking task. It encodes a query alongside candidate passages and ranks the passages based on relevance, making it useful for retrieving and re-ranking textual information. The model is implemented using Hugging Face Transformers and can process inputs with automatic padding and truncation, producing scores that indicate passage relevance for tasks like search or question answering.\nTool 4: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 5: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 6: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 7: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 8: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 9: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 10: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 11: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 12: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 13: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 14: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 15: OneFormer is a universal image segmentation model trained on the COCO dataset that can perform semantic, instance, and panoptic segmentation using a single architecture. It leverages a task-guided transformer approach, allowing the same model to adapt dynamically to different segmentation tasks, making it a versatile solution for comprehensive image understanding.\nTool 16: The Mask2Former model, trained on COCO panoptic segmentation with a Swin backbone, is designed for universal image segmentation tasks, including instance, semantic, and panoptic segmentation. It predicts masks and labels using the same approach for all three tasks, offering superior performance and efficiency over the previous state-of-the-art model, MaskFormer. This tool leverages Hugging Face Transformers and is used to process and segment images, providing panoptic segmentation results with high accuracy.\nTool 17: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description of the image to generate. This should clearly specify the subject, style, and any desired details for the generated image.\", \"required\": true, \"example\": \"archer arcane style magical princess with golden hair\"}}, \"response\": \"The response from the tool would be an image generated based on the provided prompt. The specific structure and components of the response are as follows:\\n\\n1. **Generated Image**: The primary output will be an image (or set of images) created from the text prompt provided by the user. The tool generates the image using the Nitro Diffusion model, which is a fine-tuned version of Stable Diffusion. The response will likely include the image data, either in a directly viewable format (e.g., PNG, JPEG) or as a reference to the image location.\\n\\n2. **Metadata (Optional)**: Although not explicitly mentioned in the documentation, it is common for tools like this to provide additional metadata, such as:\\n   - **Image dimensions** (e.g., width, height in pixels)\\n   - **Style information** (indicating whether the image adheres more to one of the three art styles that Nitro Diffusion is fine-tuned on, if relevant)\\n   - **Generation parameters** (if any other settings were adjusted, such as seed values or specific weighting parameters)\\n\\nThe exact format and structure of the image may vary depending on the platform or environment used for the tool execution. However, the core of the response will be the image itself, generated from the provided prompt.\", \"examples\": [{\"user_intent\": \"Generate a digital painting of a futuristic city at sunset with neon lights.\", \"tool_call\": {\"name\": \"nitrosocke/nitro-diffusion\", \"arguments\": {\"prompt\": \"futuristic cityscape at sunset, neon lights, cyberpunk style, highly detailed digital painting\"}}}, {\"user_intent\": \"Create an illustration of a medieval knight in armor, in the style of classical fantasy art.\", \"tool_call\": {\"name\": \"nitrosocke/nitro-diffusion\", \"arguments\": {\"prompt\": \"medieval knight in shining armor, epic fantasy style, highly detailed illustration, dramatic lighting\"}}}, {\"user_intent\": \"Make a whimsical portrait of a magical cat wearing a wizard hat, in a colorful cartoon style.\", \"tool_call\": {\"name\": \"nitrosocke/nitro-diffusion\", \"arguments\": {\"prompt\": \"magical cat wearing a wizard hat, whimsical cartoon style, bright colors, playful illustration\"}}}]}\n\n# User Query\nWe're working with a voiceover company, and they're looking for a tool to help them change the voice style of voiceovers while keeping the same content.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert an existing voiceover to a different speaker's voice while preserving the original content.",
                    "tool_description": "Transform an input audio recording of speech to sound like a different speaker, keeping the linguistic content intact using a speech-to-speech voice conversion model.",
                    "expected_response": "An audio file where the original speech content is preserved but the voice characteristics are changed to match the target speaker."
                }
            ],
            "extracted_arguments": {
                "input_audio": "The audio file containing the original voiceover speech.",
                "target_voice": "The desired speaker's voice characteristics to apply to the output audio."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_496",
        "query": "The team is creating a home security system. We are currently looking at understanding the depth of objects in the video stream.",
        "instruction": "Given a `depth estimation` task, retrieve tools that perform computer vision-based analysis on video streams to determine the depth of objects within the footage, using appropriate machine learning models and frameworks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3597,
                            "Mae": 0.3054,
                            "Rmse": 0.4481,
                            "Abs Rel": 0.3462,
                            "Log Mae": 0.1256,
                            "Log Rmse": 0.1798,
                            "Delta1": 0.5278,
                            "Delta2": 0.8055,
                            "Delta3": 0.9191
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.",
                    "id": "huggingface_api_157",
                    "name": "glpn-nyu-finetuned-diode-221122-030603"
                },
                "id": "gorilla_huggingface_tool_153",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 1: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 3: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 4: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 5: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 6: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 7: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 8: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 11: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 12: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.",
            "Tool 13: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.",
            "Tool 14: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.",
            "Tool 15: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 16: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 17: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 18: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 19: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding."
        ],
        "tool_view": {
            "description": "A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A textual description that serves as the input for generating a video. The model uses this description to create a video that aligns with the given text. Only English input is supported.",
                    "required": true,
                    "example": "Spiderman is surfing"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps to use during the video generation process. Higher values typically result in better quality, but require more computational resources.",
                    "required": true,
                    "default": 25,
                    "example": 25
                }
            },
            "response": "The tool generates a video sequence based on a given text prompt. The response is structured as an object containing the generated video data, typically including the following components:\n\n- **frames**: A sequential collection of individual video frames produced by the model. Each frame represents a single image in the video, and the full sequence forms the complete video corresponding to the input text prompt. The number of frames is determined by the internal configuration of the model and the diffusion process.  \n\n- **video_path** (optional, depending on usage): A file path or reference to a video file if the generated frames are exported and saved as a video. This allows downstream usage or playback of the synthesized video.\n\nSemantically, the response represents the mapping from a textual description to a visual, temporal output, where each frame visually corresponds to the content described in the input prompt. The response may be handled either as an in-memory collection of frames or as a saved video file, depending on how the API is used.",
            "examples": [
                {
                    "user_intent": "Generate a short video of a golden retriever playing in a sunny park.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "A golden retriever joyfully playing in a sunny park, trees swaying gently, birds flying",
                            "num_inference_steps": 30
                        }
                    }
                },
                {
                    "user_intent": "Create a video showing a futuristic city skyline at night with flying cars.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "Futuristic city skyline at night, neon lights reflecting on skyscrapers, flying cars zooming",
                            "num_inference_steps": 40
                        }
                    }
                },
                {
                    "user_intent": "I want a short animation of a cat surfing on a wave in the ocean.",
                    "tool_call": {
                        "name": "damo-vilab/text-to-video-ms-1.7b-legacy",
                        "arguments": {
                            "prompt": "A cute cat surfing on a big ocean wave, sunny day, animated style",
                            "num_inference_steps": 25
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 1: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 3: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 4: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 5: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 6: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 7: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 8: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 9: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 10: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 11: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 12: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.\nTool 13: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.\nTool 14: A pre-trained TimeSformer model for video classification that uses space-time attention to analyze video frames, enabling automatic recognition of actions and events. It is designed for use with the Hugging Face Transformers library and is fine-tuned on the Kinetics-600 dataset, making it suitable for accurate video understanding tasks.\nTool 15: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 16: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 17: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 18: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 19: This tool provides a pretrained VideoMAE model fine-tuned for video classification on the Kinetics-400 dataset, enabling accurate identification of actions or events in video input. It leverages self-supervised pretraining combined with supervised fine-tuning to deliver high performance, making it suitable for applications requiring efficient and reliable video understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A textual description that serves as the input for generating a video. The model uses this description to create a video that aligns with the given text. Only English input is supported.\", \"required\": true, \"example\": \"Spiderman is surfing\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps to use during the video generation process. Higher values typically result in better quality, but require more computational resources.\", \"required\": true, \"default\": 25, \"example\": 25}}, \"response\": \"The tool generates a video sequence based on a given text prompt. The response is structured as an object containing the generated video data, typically including the following components:\\n\\n- **frames**: A sequential collection of individual video frames produced by the model. Each frame represents a single image in the video, and the full sequence forms the complete video corresponding to the input text prompt. The number of frames is determined by the internal configuration of the model and the diffusion process.  \\n\\n- **video_path** (optional, depending on usage): A file path or reference to a video file if the generated frames are exported and saved as a video. This allows downstream usage or playback of the synthesized video.\\n\\nSemantically, the response represents the mapping from a textual description to a visual, temporal output, where each frame visually corresponds to the content described in the input prompt. The response may be handled either as an in-memory collection of frames or as a saved video file, depending on how the API is used.\", \"examples\": [{\"user_intent\": \"Generate a short video of a golden retriever playing in a sunny park.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"A golden retriever joyfully playing in a sunny park, trees swaying gently, birds flying\", \"num_inference_steps\": 30}}}, {\"user_intent\": \"Create a video showing a futuristic city skyline at night with flying cars.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"Futuristic city skyline at night, neon lights reflecting on skyscrapers, flying cars zooming\", \"num_inference_steps\": 40}}}, {\"user_intent\": \"I want a short animation of a cat surfing on a wave in the ocean.\", \"tool_call\": {\"name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"arguments\": {\"prompt\": \"A cute cat surfing on a big ocean wave, sunny day, animated style\", \"num_inference_steps\": 25}}}]}\n\n# User Query\nThe team is creating a home security system. We are currently looking at understanding the depth of objects in the video stream.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to understand the depth of objects in a video stream.",
                    "tool_description": "Use a depth estimation model to process video frames and predict scene depth, identifying spatial relationships and distances between objects in the video.",
                    "expected_response": "A collection of depth maps representing per-pixel depth information for each frame in the video."
                }
            ],
            "extracted_arguments": {
                "video_stream": "A stream of video frames requiring depth estimation.",
                "model": "The specific depth estimation model to use, based on the video data."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_123",
        "query": "We have a collection of low-resolution images of movie characters, and we need to upscale those images to get a more detailed high-resolution image.",
        "instruction": "Given an `image upscaling` task, retrieve tools that enhance images by processing low-resolution inputs to produce high-resolution outputs using methods such as diffusion-based models.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image Upscaling",
                    "api_call": "StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)",
                    "api_arguments": {
                        "prompt": "text prompt",
                        "image": "low resolution latents",
                        "num_inference_steps": 20,
                        "guidance_scale": 0,
                        "generator": "torch generator"
                    },
                    "python_environment_requirements": [
                        "git+https://github.com/huggingface/diffusers.git",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\nimport torch\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\npipeline.to(cuda)\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\nupscaler.to(cuda)\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\ngenerator = torch.manual_seed(33)\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\nupscaled_image.save(astronaut_1024.png)",
                    "performance": {
                        "dataset": "LAION-2B",
                        "accuracy": "Not specified"
                    },
                    "description": "Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.",
                    "id": "huggingface_api_59",
                    "name": "stabilityai/sd-x2-latent-upscaler"
                },
                "id": "gorilla_huggingface_tool_59",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 1: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 2: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 3: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.",
            "Tool 4: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 5: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 6: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 7: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 8: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 9: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.",
            "Tool 10: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 11: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.",
            "Tool 12: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.",
            "Tool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 15: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 16: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 17: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 18: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.",
            "Tool 19: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content."
        ],
        "tool_view": {
            "description": "The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text string describing the desired content or modifications for the image. This serves as the main input guiding the image generation or upscaling process.",
                    "required": true,
                    "example": "a white cat sitting on a windowsill"
                },
                "image": {
                    "type": "image",
                    "description": "The low-resolution input image to be upscaled or modified. This should be provided as a PIL Image object.",
                    "required": true,
                    "example": "PIL.Image.open('low_res_cat.png')"
                },
                "noise_level": {
                    "type": "number",
                    "description": "An optional parameter that controls the amount of noise added to the input image according to the diffusion schedule. Higher values introduce more noise.",
                    "required": false,
                    "example": 0.2
                }
            },
            "response": "The tool produces a response consisting of an upscaled or modified image generated from a combination of a text prompt and an input image. The response structure can be described as follows:\n\n- **images**: A list or collection containing one or more images resulting from the pipeline. Each item in this list represents a generated or upscaled image.  \n  - Each image is typically represented as a PIL `Image` object (or another standard image format in memory) that can be saved to disk or further processed.  \n\n- **pipeline metadata (implicit)**: While not explicitly returned, the pipeline internally uses parameters such as:\n  - `prompt`: The text prompt guiding the generation or modification.\n  - `image`: The low-resolution input image to be upscaled or modified.\n  - `noise_level` (optional): The amount of noise applied according to the diffusion schedule.\n  - Device information (e.g., whether the model ran on CUDA or CPU).\n\nSemantically, the response contains the visual output of the diffusion-based upscaler, reflecting the content described in the prompt while preserving or enhancing details from the input image. The primary component of interest is the upscaled image itself, accessible via `response.images[0]` in typical usage.",
            "examples": [
                {
                    "user_intent": "I have a small photo of my dog and I want to upscale it to high resolution while keeping the details sharp.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a realistic portrait of a golden retriever",
                            "image_path": "low_res_dog.png",
                            "output_path": "upscaled_dog.png"
                        }
                    }
                },
                {
                    "user_intent": "Enhance a low-resolution concept art image of a futuristic city to make it look more detailed and polished.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a highly detailed futuristic cityscape at sunset",
                            "image_path": "low_res_city.png",
                            "output_path": "upscaled_city.png"
                        }
                    }
                },
                {
                    "user_intent": "Upscale a small illustration of a fantasy dragon while keeping its colors vibrant and textures sharp.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a majestic fantasy dragon in vibrant colors",
                            "image_path": "low_res_dragon.png",
                            "output_path": "upscaled_dragon.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 1: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 2: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 3: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.\nTool 4: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 5: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 6: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 7: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 8: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 9: A Swin Transformer model designed for image classification, pretrained on ImageNet-1k at 224×224 resolution. It processes images by computing self-attention within local windows, enabling efficient hierarchical feature extraction, and can predict object classes from input images. This model is suitable for general-purpose image classification tasks and can serve as a backbone for other computer vision applications.\nTool 10: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 11: This tool provides an image classification model, ConvNeXT-Tiny, from Hugging Face Transformers, which uses a convolutional architecture inspired by Vision Transformers. Pretrained on ImageNet-1k at 224×224 resolution, it can be used to classify images into predefined categories, enabling users to quickly apply state-of-the-art image recognition without training a model from scratch.\nTool 12: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 13: ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification that uses residual learning to enable training of deep models. It is designed to classify images at 224×224 resolution and is trained on the ImageNet-1k dataset. This model improves on the original ResNet-50 by modifying the downsampling in bottleneck blocks, slightly increasing accuracy. It can be used with the Hugging Face Transformers library to process images and predict their class labels efficiently.\nTool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 15: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 16: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 17: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 18: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.\nTool 19: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text string describing the desired content or modifications for the image. This serves as the main input guiding the image generation or upscaling process.\", \"required\": true, \"example\": \"a white cat sitting on a windowsill\"}, \"image\": {\"type\": \"image\", \"description\": \"The low-resolution input image to be upscaled or modified. This should be provided as a PIL Image object.\", \"required\": true, \"example\": \"PIL.Image.open('low_res_cat.png')\"}, \"noise_level\": {\"type\": \"number\", \"description\": \"An optional parameter that controls the amount of noise added to the input image according to the diffusion schedule. Higher values introduce more noise.\", \"required\": false, \"example\": 0.2}}, \"response\": \"The tool produces a response consisting of an upscaled or modified image generated from a combination of a text prompt and an input image. The response structure can be described as follows:\\n\\n- **images**: A list or collection containing one or more images resulting from the pipeline. Each item in this list represents a generated or upscaled image.  \\n  - Each image is typically represented as a PIL `Image` object (or another standard image format in memory) that can be saved to disk or further processed.  \\n\\n- **pipeline metadata (implicit)**: While not explicitly returned, the pipeline internally uses parameters such as:\\n  - `prompt`: The text prompt guiding the generation or modification.\\n  - `image`: The low-resolution input image to be upscaled or modified.\\n  - `noise_level` (optional): The amount of noise applied according to the diffusion schedule.\\n  - Device information (e.g., whether the model ran on CUDA or CPU).\\n\\nSemantically, the response contains the visual output of the diffusion-based upscaler, reflecting the content described in the prompt while preserving or enhancing details from the input image. The primary component of interest is the upscaled image itself, accessible via `response.images[0]` in typical usage.\", \"examples\": [{\"user_intent\": \"I have a small photo of my dog and I want to upscale it to high resolution while keeping the details sharp.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a realistic portrait of a golden retriever\", \"image_path\": \"low_res_dog.png\", \"output_path\": \"upscaled_dog.png\"}}}, {\"user_intent\": \"Enhance a low-resolution concept art image of a futuristic city to make it look more detailed and polished.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a highly detailed futuristic cityscape at sunset\", \"image_path\": \"low_res_city.png\", \"output_path\": \"upscaled_city.png\"}}}, {\"user_intent\": \"Upscale a small illustration of a fantasy dragon while keeping its colors vibrant and textures sharp.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a majestic fantasy dragon in vibrant colors\", \"image_path\": \"low_res_dragon.png\", \"output_path\": \"upscaled_dragon.png\"}}}]}\n\n# User Query\nWe have a collection of low-resolution images of movie characters, and we need to upscale those images to get a more detailed high-resolution image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to upscale a collection of low-resolution images of movie characters to high-resolution images while preserving details.",
                    "tool_description": "Use a text-to-image latent diffusion model to enhance and upscale low-resolution images, optionally applying controlled noise to refine details, producing higher-quality visuals from smaller inputs.",
                    "expected_response": "A collection of upscaled images, each represented as a PIL Image object, reflecting enhanced resolution and preserved details from the original images."
                }
            ],
            "extracted_arguments": {
                "images": "A collection of low-resolution images to be upscaled, provided as image objects.",
                "prompt": "A descriptive text prompt guiding the style or content preservation during upscaling, if needed.",
                "noise_level": "Optional numeric parameter controlling the amount of noise added to refine image details during upscaling."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_29",
        "query": "Our gaming company is looking for a reinforcement learning solution to implement an artificial agent that can play SoccerTwos proficiently.",
        "instruction": "Given a `reinforcement learning implementation` task, retrieve tools that facilitate the training and deployment of artificial agents via reinforcement learning frameworks, specifically for applications like SoccerTwos, by leveraging model training libraries and environment configurations.",
        "labels": [
            {
                "doc": {
                    "domain": "Reinforcement Learning",
                    "framework": "Unity ML-Agents Library",
                    "functionality": "Train and play SoccerTwos",
                    "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-SoccerTwosv2' --local-dir='./downloads'",
                    "api_arguments": [
                        "your_configuration_file_path.yaml",
                        "run_id"
                    ],
                    "python_environment_requirements": [
                        "ml-agents"
                    ],
                    "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume",
                    "performance": {
                        "dataset": "SoccerTwos",
                        "accuracy": "Not provided"
                    },
                    "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.",
                    "id": "huggingface_api_908",
                    "name": "poca-SoccerTwosv2"
                },
                "id": "gorilla_huggingface_tool_901",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 1: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 2: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 3: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.",
            "Tool 4: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.",
            "Tool 5: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 6: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.",
            "Tool 7: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.",
            "Tool 8: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.",
            "Tool 9: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.",
            "Tool 10: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.",
            "Tool 11: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 12: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.",
            "Tool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.",
            "Tool 14: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.",
            "Tool 15: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.",
            "Tool 16: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.",
            "Tool 17: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.",
            "Tool 18: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.",
            "Tool 19: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research."
        ],
        "tool_view": {
            "description": "A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "parameters": {
                "your_configuration_file_path.yaml": {
                    "type": "string",
                    "description": "The file path to your configuration YAML file, which contains the setup and hyperparameters for training the model.",
                    "required": true,
                    "example": "./configurations/soccer_twos_config.yaml"
                },
                "run_id": {
                    "type": "string",
                    "description": "A unique identifier for the training run. This ID is used to distinguish between different runs of the model.",
                    "required": true,
                    "example": "soccer_twos_run_001"
                }
            },
            "response": "The tool’s response is not explicitly defined in the JSON, but based on its functionality, a reasonable and typical response would include information related to the trained model and its usage in the Unity ML-Agents environment. The response would likely have the following components:\n\n1. **Model Metadata**  \n   - **Name/Identifier**: The unique identifier of the trained model (e.g., `poca-SoccerTwosv2`).  \n   - **Version/Run ID**: The specific training run identifier used to track the model instance.  \n   - **Configuration**: Reference to the YAML configuration file that specifies training parameters.\n\n2. **Training Information**  \n   - **Environment Details**: Description of the SoccerTwos environment used for training.  \n   - **Training Status**: Indication of whether training is complete, in progress, or resumed.  \n   - **Performance Metrics**: Any available performance measures, such as rewards, scores, or success rates (though accuracy is not provided in this case).\n\n3. **Model Access/Usage**  \n   - **Local Path**: File system location where the model has been downloaded or saved.  \n   - **Commands for Use**: Instructions or scripts to load, resume, or evaluate the model (e.g., `mlagents-learn` with appropriate arguments).  \n   - **API Call Confirmation**: Acknowledgment that the `mlagents-load-from-hf` command successfully fetched the model.\n\n4. **Dependencies**  \n   - **Required Python Packages**: List of necessary Python environment packages (e.g., `ml-agents`) to run or interact with the model.\n\n5. **Optional Artifacts**  \n   - **Logs or Checkpoints**: References to training logs, checkpoints, or replay files that can be used to monitor performance or resume training.  \n   - **Evaluation Results**: If the model is evaluated post-training, a summary of scores or metrics achieved in test episodes.\n\nOverall, the response conveys both metadata about the trained agent and actionable information for loading, running, or further training the model in the Unity ML-Agents framework.",
            "examples": [
                {
                    "user_intent": "I want to train a SoccerTwos model using the Unity ML-Agents Library with my custom configuration file.",
                    "tool_call": {
                        "name": "mlagents-load-from-hf",
                        "arguments": {
                            "repo-id": "Raiden-1001/poca-SoccerTwosv2",
                            "local-dir": "./downloads",
                            "your_configuration_file_path.yaml": "configs/soccer_twos_config.yaml",
                            "run_id": "soccer_twos_run_01"
                        }
                    }
                },
                {
                    "user_intent": "How can I resume a previous SoccerTwos training run in Unity ML-Agents?",
                    "tool_call": {
                        "name": "mlagents-learn",
                        "arguments": {
                            "your_configuration_file_path.yaml": "configs/soccer_twos_config.yaml",
                            "run-id": "soccer_twos_run_01",
                            "resume": true
                        }
                    }
                },
                {
                    "user_intent": "I need to load the 'poca-SoccerTwosv2' model to start testing in Unity ML-Agents.",
                    "tool_call": {
                        "name": "mlagents-load-from-hf",
                        "arguments": {
                            "repo-id": "Raiden-1001/poca-SoccerTwosv2",
                            "local-dir": "./downloads",
                            "your_configuration_file_path.yaml": "configs/soccer_twos_test_config.yaml",
                            "run_id": "soccer_twos_test_01"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 1: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 2: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 3: This tool provides a trained model of a POCA agent playing SoccerTwos, utilizing the Unity ML-Agents Library. It allows users to load the model from a specified repository and run it within a reinforcement learning framework for soccer-based simulations.\nTool 4: A pre-trained DQN reinforcement learning model for the LunarLander-v2 environment, developed with Stable-Baselines3, which can be loaded and evaluated to test or deploy an agent capable of landing the lunar module efficiently.\nTool 5: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 6: This tool provides a pre-trained PPO agent for the Pendulum-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows users to load and utilize the model for reinforcement learning tasks, leveraging hyperparameter optimization and pre-trained agents. The tool is designed for ease of use in training and evaluating reinforcement learning models with the Pendulum-v1 environment.\nTool 7: This tool provides a pre-trained Deep Q-Network (DQN) agent for the MountainCar-v0 environment using the Stable-Baselines3 library and the RL Zoo framework. It allows users to quickly load and deploy a trained reinforcement learning model without manual training, offering a baseline performance of approximately -103.4 reward. It is useful for experimentation, benchmarking, or further training in reinforcement learning projects.\nTool 8: Provides a pre-trained PPO reinforcement learning agent for the PongNoFrameskip-v4 environment using Stable-Baselines3 and the RL Zoo, enabling users to quickly load and evaluate a ready-to-use model without retraining.\nTool 9: Provides a pre-trained DQN agent for the CartPole-v1 environment using Stable-Baselines3 and the RL Zoo framework, enabling users to quickly deploy or evaluate a reinforcement learning model without training from scratch.\nTool 10: This tool provides a pre-trained Proximal Policy Optimization (PPO) agent for the LunarLander-v2 environment using Stable-Baselines3. It allows users to quickly load and evaluate the model’s performance in reinforcement learning experiments without training from scratch, enabling assessment of the agent’s ability to achieve high rewards in the LunarLander-v2 task.\nTool 11: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 12: Provides a pre-trained PPO reinforcement learning agent for the `seals/CartPole-v0` environment using Stable-Baselines3 and the RL Zoo, allowing users to quickly load and evaluate a high-performing model without training from scratch.\nTool 13: This tool provides a pre-trained TD3 reinforcement learning agent for the Ant-v3 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load, evaluate, or continue training a high-performing agent without starting from scratch, leveraging a framework that includes optimized hyperparameters and pre-trained models for efficient experimentation and benchmarking in continuous control tasks.\nTool 14: This tool provides a pre-trained PPO agent for the CartPole-v1 environment, using the Stable-Baselines3 library and the RL Zoo framework. It allows easy integration of reinforcement learning agents with optimized hyperparameters and pre-trained models, enabling fast deployment and testing of RL agents.\nTool 15: A Decision Transformer model trained on medium-quality trajectories from the Gym Hopper environment, designed for reinforcement learning tasks using Hugging Face Transformers. It can be loaded via `AutoModel.from_pretrained` and is used to model and predict action sequences in the environment.\nTool 16: A Decision Transformer model trained on expert trajectories from the Gym Walker2d environment, designed to predict actions in reinforcement learning tasks using the Hugging Face Transformers framework.\nTool 17: Antheia/Hanna is a reinforcement learning model for robotics tasks, accessible via Hugging Face's inference API, and is designed to perform robotic decision-making based on knowledge learned from the openai/webgpt_comparisons dataset.\nTool 18: A Hugging Face model for 6D robotic grasping that uses SE(3) diffusion fields to learn smooth cost functions for optimizing both grasp and motion, enabling more precise and efficient manipulation in reinforcement learning robotics.\nTool 19: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\", \"parameters\": {\"your_configuration_file_path.yaml\": {\"type\": \"string\", \"description\": \"The file path to your configuration YAML file, which contains the setup and hyperparameters for training the model.\", \"required\": true, \"example\": \"./configurations/soccer_twos_config.yaml\"}, \"run_id\": {\"type\": \"string\", \"description\": \"A unique identifier for the training run. This ID is used to distinguish between different runs of the model.\", \"required\": true, \"example\": \"soccer_twos_run_001\"}}, \"response\": \"The tool\\u2019s response is not explicitly defined in the JSON, but based on its functionality, a reasonable and typical response would include information related to the trained model and its usage in the Unity ML-Agents environment. The response would likely have the following components:\\n\\n1. **Model Metadata**  \\n   - **Name/Identifier**: The unique identifier of the trained model (e.g., `poca-SoccerTwosv2`).  \\n   - **Version/Run ID**: The specific training run identifier used to track the model instance.  \\n   - **Configuration**: Reference to the YAML configuration file that specifies training parameters.\\n\\n2. **Training Information**  \\n   - **Environment Details**: Description of the SoccerTwos environment used for training.  \\n   - **Training Status**: Indication of whether training is complete, in progress, or resumed.  \\n   - **Performance Metrics**: Any available performance measures, such as rewards, scores, or success rates (though accuracy is not provided in this case).\\n\\n3. **Model Access/Usage**  \\n   - **Local Path**: File system location where the model has been downloaded or saved.  \\n   - **Commands for Use**: Instructions or scripts to load, resume, or evaluate the model (e.g., `mlagents-learn` with appropriate arguments).  \\n   - **API Call Confirmation**: Acknowledgment that the `mlagents-load-from-hf` command successfully fetched the model.\\n\\n4. **Dependencies**  \\n   - **Required Python Packages**: List of necessary Python environment packages (e.g., `ml-agents`) to run or interact with the model.\\n\\n5. **Optional Artifacts**  \\n   - **Logs or Checkpoints**: References to training logs, checkpoints, or replay files that can be used to monitor performance or resume training.  \\n   - **Evaluation Results**: If the model is evaluated post-training, a summary of scores or metrics achieved in test episodes.\\n\\nOverall, the response conveys both metadata about the trained agent and actionable information for loading, running, or further training the model in the Unity ML-Agents framework.\", \"examples\": [{\"user_intent\": \"I want to train a SoccerTwos model using the Unity ML-Agents Library with my custom configuration file.\", \"tool_call\": {\"name\": \"mlagents-load-from-hf\", \"arguments\": {\"repo-id\": \"Raiden-1001/poca-SoccerTwosv2\", \"local-dir\": \"./downloads\", \"your_configuration_file_path.yaml\": \"configs/soccer_twos_config.yaml\", \"run_id\": \"soccer_twos_run_01\"}}}, {\"user_intent\": \"How can I resume a previous SoccerTwos training run in Unity ML-Agents?\", \"tool_call\": {\"name\": \"mlagents-learn\", \"arguments\": {\"your_configuration_file_path.yaml\": \"configs/soccer_twos_config.yaml\", \"run-id\": \"soccer_twos_run_01\", \"resume\": true}}}, {\"user_intent\": \"I need to load the 'poca-SoccerTwosv2' model to start testing in Unity ML-Agents.\", \"tool_call\": {\"name\": \"mlagents-load-from-hf\", \"arguments\": {\"repo-id\": \"Raiden-1001/poca-SoccerTwosv2\", \"local-dir\": \"./downloads\", \"your_configuration_file_path.yaml\": \"configs/soccer_twos_test_config.yaml\", \"run_id\": \"soccer_twos_test_01\"}}}]}\n\n# User Query\nOur gaming company is looking for a reinforcement learning solution to implement an artificial agent that can play SoccerTwos proficiently.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to load a pre-trained SoccerTwos RL agent to implement in our game.",
                    "tool_description": "Load a pre-trained Poca agent for the SoccerTwos environment using the Unity ML-Agents Library to enable gameplay evaluation or deployment.",
                    "expected_response": "Metadata and usage information about the loaded model, including model identifier, configuration, environment details, training status, and instructions for running or evaluating the agent."
                },
                {
                    "user_intent": "I want to integrate the SoccerTwos RL agent into our game environment for testing and refinement.",
                    "tool_description": "Run the loaded Poca agent within a Unity ML-Agents framework to evaluate its gameplay performance and interactions in the SoccerTwos environment.",
                    "expected_response": "Performance metrics, logs, or evaluation results summarizing the agent's proficiency and behavior in the SoccerTwos environment."
                }
            ],
            "extracted_arguments": {
                "configuration_file_path": "The file path to the YAML configuration containing setup and hyperparameters for the RL model.",
                "run_id": "A unique identifier for the training or evaluation run of the SoccerTwos agent."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_122",
        "query": "Our team is designing a butterfly-themed stationery set for children. We need to generate images of cute butterflies to use in the design.",
        "instruction": "Given an `image generation` task, retrieve tools that perform unconditional image creation by employing models capable of generating themed images, such as cute butterflies, to meet the design requirements for stationery sets.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')",
                    "api_arguments": {
                        "model_id": "clp/sd-class-butterflies-32"
                    },
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "This model is a diffusion model for unconditional image generation of cute butterflies.",
                    "id": "huggingface_api_304",
                    "name": "sd-class-butterflies-32"
                },
                "id": "gorilla_huggingface_tool_300",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 1: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 2: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 3: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 4: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 5: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 6: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 7: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.",
            "Tool 8: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 9: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.",
            "Tool 10: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 11: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 13: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 14: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 16: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.",
            "Tool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 18: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.",
            "Tool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images."
        ],
        "tool_view": {
            "description": "Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "parameters": {},
            "response": "The tool produces a generated image using an unconditional diffusion model. The response structure can be described as follows:\n\n- **images**: The primary output is a collection (typically a list) of generated images. Each item in this collection represents a single image generated by the model.  \n  - **image object**: Each image is represented in a standard image format (e.g., a PIL Image object or an array compatible with common image-processing libraries). This object contains the visual content produced by the diffusion process and can be directly displayed, saved, or further processed.\n\nSince this is an **unconditional image generation** pipeline, the response does not include input-conditioned information such as prompts or labels. The response is purely the generated visual data, and additional metadata (like generation parameters, seed, or steps) may be available depending on how the pipeline is configured, but the core response is the image itself.",
            "examples": [
                {
                    "user_intent": "Generate an image of a butterfly using a pre-trained model.",
                    "tool_call": {
                        "name": "MFawad/sd-class-butterflies-32",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "I want to create an image of a cute butterfly with the 'sd-class-butterflies-32' model.",
                    "tool_call": {
                        "name": "MFawad/sd-class-butterflies-32",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Use the diffusion model to generate a butterfly image without any specific conditions.",
                    "tool_call": {
                        "name": "MFawad/sd-class-butterflies-32",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 1: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 2: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 3: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 4: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 5: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 6: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 7: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.\nTool 8: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 9: This tool provides a wav2vec2-large model fine-tuned for automatic speech recognition, specifically designed to generate transcriptions with punctuation. It is intended for use in applications such as text-to-speech (TTS) models, where punctuation is crucial for accurate prosody. The model was fine-tuned on the LibriTTS and VoxPopuli datasets, extending the vocabulary to include punctuation marks. It can be accessed through the Hugging Face Transformers framework and has been evaluated on the LibriSpeech validation set with a 4.45% accuracy.\nTool 10: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 11: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 12: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 13: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 14: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 15: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 16: This tool provides a DistilBART-based text summarization model, specifically trained on the SAMsum dataset, to generate summaries of conversational text. It utilizes the Hugging Face Transformers framework and is designed for use in natural language processing tasks where summarizing dialogues or conversations is needed.\nTool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 18: This tool performs zero-shot image classification using a CLIP ConvNeXt-Base model trained on LAION-5B data. It allows users to classify images without additional training by providing an image and a set of possible class names, making it useful for quickly categorizing images based on visual and textual similarity. The model leverages a ConvNeXt-Base image encoder and a CLIP text encoder, achieving around 71% accuracy on ImageNet-1k.\nTool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\", \"parameters\": {}, \"response\": \"The tool produces a generated image using an unconditional diffusion model. The response structure can be described as follows:\\n\\n- **images**: The primary output is a collection (typically a list) of generated images. Each item in this collection represents a single image generated by the model.  \\n  - **image object**: Each image is represented in a standard image format (e.g., a PIL Image object or an array compatible with common image-processing libraries). This object contains the visual content produced by the diffusion process and can be directly displayed, saved, or further processed.\\n\\nSince this is an **unconditional image generation** pipeline, the response does not include input-conditioned information such as prompts or labels. The response is purely the generated visual data, and additional metadata (like generation parameters, seed, or steps) may be available depending on how the pipeline is configured, but the core response is the image itself.\", \"examples\": [{\"user_intent\": \"Generate an image of a butterfly using a pre-trained model.\", \"tool_call\": {\"name\": \"MFawad/sd-class-butterflies-32\", \"arguments\": {}}}, {\"user_intent\": \"I want to create an image of a cute butterfly with the 'sd-class-butterflies-32' model.\", \"tool_call\": {\"name\": \"MFawad/sd-class-butterflies-32\", \"arguments\": {}}}, {\"user_intent\": \"Use the diffusion model to generate a butterfly image without any specific conditions.\", \"tool_call\": {\"name\": \"MFawad/sd-class-butterflies-32\", \"arguments\": {}}}]}\n\n# User Query\nOur team is designing a butterfly-themed stationery set for children. We need to generate images of cute butterflies to use in the design.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate images of cute butterflies for a children's stationery design.",
                    "tool_description": "Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
                    "expected_response": "A collection of generated butterfly images in standard image formats suitable for display, printing, or further design processing."
                }
            ],
            "extracted_arguments": {}
        }
    },
    {
        "id": "gorilla_huggingface_query_157",
        "query": "Develop a tool that helps me get answers to questions related to a specific text.",
        "instruction": "Given a `question answering development` task, retrieve tools that facilitate generating answers to questions based on specific textual context by employing natural language processing techniques to interpret and respond accurately to user queries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='deepset/roberta-large-squad2')",
                    "api_arguments": [
                        "question",
                        "context"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": "Not provided"
                    },
                    "description": "A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.",
                    "id": "huggingface_api_471",
                    "name": "deepset/roberta-large-squad2"
                },
                "id": "gorilla_huggingface_tool_467",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 4: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 5: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 6: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 9: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 11: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 12: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 13: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 14: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 17: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.",
            "Tool 18: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 19: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA."
        ],
        "tool_view": {
            "description": "A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "parameters": {
                "context": {
                    "type": "string",
                    "description": "The text passage or document in which the model will search for the answer to the given question.",
                    "required": true,
                    "example": "Manuel Romero has been working hard in the repository huggingface/transformers lately."
                },
                "question": {
                    "type": "string",
                    "description": "The specific question that the model should answer based on the provided context.",
                    "required": true,
                    "example": "Who has been working hard for huggingface/transformers lately?"
                }
            },
            "response": "The tool produces a structured answer to a given question based on a provided textual context. The typical response is an object containing the following components:\n\n1. **`answer`** – A string representing the text extracted from the context that directly answers the input question. This is the primary output of the model.  \n\n2. **`score`** – A numeric value (usually a probability or confidence score) indicating the model’s confidence in the correctness of the predicted answer. Higher values suggest greater confidence.  \n\n3. **`start`** – An integer representing the starting character index of the answer span within the provided context. This allows precise localization of the answer within the source text.  \n\n4. **`end`** – An integer representing the ending character index of the answer span within the context. Combined with `start`, it defines the exact substring corresponding to the predicted answer.  \n\nSemantically, the response provides both the answer itself and metadata that reflects the model’s confidence and exact location of the answer within the context, supporting downstream tasks such as highlighting or further text processing.",
            "examples": [
                {
                    "user_intent": "I want to know who contributed significantly to the Transformers repository recently.",
                    "tool_call": {
                        "name": "mrm8488/bert-multi-cased-finetuned-xquadv1",
                        "arguments": {
                            "context": "Manuel Romero has been working hard in the repository huggingface/transformers over the past few months.",
                            "question": "Who contributed significantly to the Transformers repository recently?"
                        }
                    }
                },
                {
                    "user_intent": "Find out which city is described as the capital of France in this text.",
                    "tool_call": {
                        "name": "mrm8488/bert-multi-cased-finetuned-xquadv1",
                        "arguments": {
                            "context": "Paris is known as the capital city of France, famous for its art, fashion, and culture.",
                            "question": "What is the capital of France?"
                        }
                    }
                },
                {
                    "user_intent": "Determine the language in which the following book was written.",
                    "tool_call": {
                        "name": "mrm8488/bert-multi-cased-finetuned-xquadv1",
                        "arguments": {
                            "context": "El ingenioso hidalgo Don Quijote de la Mancha es una novela escrita por Miguel de Cervantes.",
                            "question": "In which language was this book written?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 1: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 4: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 5: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 6: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 7: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 9: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 10: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 11: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 12: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 13: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 14: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 17: The tool uses a fine-tuned version of DistilBERT (distilbert-base-uncased-distilled-squad) for extractive question answering, where it identifies answers to questions based on a provided context. It is designed to be faster and more efficient than the original BERT model, with 40% fewer parameters and 60% faster execution, while maintaining over 95% of BERT's performance on language understanding tasks. This model is particularly suited for tasks like answering questions from datasets such as SQuAD.\nTool 18: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 19: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\", \"parameters\": {\"context\": {\"type\": \"string\", \"description\": \"The text passage or document in which the model will search for the answer to the given question.\", \"required\": true, \"example\": \"Manuel Romero has been working hard in the repository huggingface/transformers lately.\"}, \"question\": {\"type\": \"string\", \"description\": \"The specific question that the model should answer based on the provided context.\", \"required\": true, \"example\": \"Who has been working hard for huggingface/transformers lately?\"}}, \"response\": \"The tool produces a structured answer to a given question based on a provided textual context. The typical response is an object containing the following components:\\n\\n1. **`answer`** \\u2013 A string representing the text extracted from the context that directly answers the input question. This is the primary output of the model.  \\n\\n2. **`score`** \\u2013 A numeric value (usually a probability or confidence score) indicating the model\\u2019s confidence in the correctness of the predicted answer. Higher values suggest greater confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the answer span within the provided context. This allows precise localization of the answer within the source text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the answer span within the context. Combined with `start`, it defines the exact substring corresponding to the predicted answer.  \\n\\nSemantically, the response provides both the answer itself and metadata that reflects the model\\u2019s confidence and exact location of the answer within the context, supporting downstream tasks such as highlighting or further text processing.\", \"examples\": [{\"user_intent\": \"I want to know who contributed significantly to the Transformers repository recently.\", \"tool_call\": {\"name\": \"mrm8488/bert-multi-cased-finetuned-xquadv1\", \"arguments\": {\"context\": \"Manuel Romero has been working hard in the repository huggingface/transformers over the past few months.\", \"question\": \"Who contributed significantly to the Transformers repository recently?\"}}}, {\"user_intent\": \"Find out which city is described as the capital of France in this text.\", \"tool_call\": {\"name\": \"mrm8488/bert-multi-cased-finetuned-xquadv1\", \"arguments\": {\"context\": \"Paris is known as the capital city of France, famous for its art, fashion, and culture.\", \"question\": \"What is the capital of France?\"}}}, {\"user_intent\": \"Determine the language in which the following book was written.\", \"tool_call\": {\"name\": \"mrm8488/bert-multi-cased-finetuned-xquadv1\", \"arguments\": {\"context\": \"El ingenioso hidalgo Don Quijote de la Mancha es una novela escrita por Miguel de Cervantes.\", \"question\": \"In which language was this book written?\"}}}]}\n\n# User Query\nDevelop a tool that helps me get answers to questions related to a specific text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to get answers to questions based on a specific text.",
                    "tool_description": "Use a multilingual extractive question-answering model to identify precise answers from a provided text context.",
                    "expected_response": "An object containing the extracted answer string, a confidence score, and the start and end character indices of the answer within the text."
                }
            ],
            "extracted_arguments": {
                "context": "The text passage or document in which answers will be searched for (string).",
                "question": "The specific question to be answered based on the provided text (string)."
            }
        }
    }
]