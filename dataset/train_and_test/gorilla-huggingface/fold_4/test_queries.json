[
    {
        "id": "gorilla_huggingface_query_470",
        "query": "A new city planning company wants to estimate carbon emissions for different types of buildings to provide sustainable living solutions. We need a model to classify the carbon emissions.",
        "instruction": "Given an `emission classification` task, retrieve tools that can classify carbon emissions from building data by processing tabular inputs to provide predictions aligned with the requirements for sustainable city planning solutions.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Joblib",
                    "functionality": "Carbon Emissions",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": {
                            "accuracy": 0.827
                        }
                    },
                    "description": "Multi-class Classification Model for Carbon Emissions",
                    "id": "huggingface_api_861",
                    "name": "tejas23/autotrain-amx2-1702259725"
                },
                "id": "gorilla_huggingface_tool_854",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 1: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 2: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 3: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 4: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 8: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 9: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.",
            "Tool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 14: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.",
            "Tool 15: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 16: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.",
            "Tool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 19: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks."
        ],
        "tool_view": {
            "description": "A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "parameters": {
                "data": {
                    "type": "DataFrame",
                    "description": "A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})"
                }
            },
            "response": "The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \n\n- **Type:** Array or list of numeric values.  \n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \n- **Data Semantics:**  \n  - Each numeric value reflects the model’s regression output, estimating carbon emissions based on the input features.  \n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \n\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.",
            "examples": [
                {
                    "user_intent": "Predict the carbon emissions for my factory dataset using the trained model.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "factory_emissions_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV of vehicle specifications and want to estimate their carbon footprint.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "vehicle_specs.csv"
                        }
                    }
                },
                {
                    "user_intent": "Calculate predicted carbon emissions for a new dataset of industrial plants.",
                    "tool_call": {
                        "name": "bibekbehera/autotrain-numeric_prediction-40376105019",
                        "arguments": {
                            "data": "industrial_plants_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 1: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 2: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 3: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 4: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 5: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 6: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 8: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 9: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 10: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\nTool 11: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 13: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 14: A binary classification tool that predicts carbon emissions from tabular data, using a pre-trained model loaded with Joblib, enabling users to process input features and generate emission predictions efficiently.\nTool 15: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 16: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 17: This tool is a tabular classification model for predicting carbon emissions in grams, trained using AutoTrain. It utilizes the Joblib framework to load the pre-trained model and make predictions based on input data. The tool is designed to work with pandas for data manipulation and requires a Python environment with Joblib and pandas installed. It has shown high performance with a validation accuracy of 99.1%.\nTool 18: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 19: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\", \"parameters\": {\"data\": {\"type\": \"DataFrame\", \"description\": \"A tabular dataset containing the input features for which carbon emissions predictions are to be made. Columns should match the features expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_1': [1.2, 3.4], 'feat_2': [5.6, 7.8]})\"}}, \"response\": \"The tool produces a numeric prediction output corresponding to carbon emissions for the input tabular data. The response structure can be described as follows:  \\n\\n- **Type:** Array or list of numeric values.  \\n- **Length:** Equal to the number of rows in the input dataset; each entry corresponds to a single data instance.  \\n- **Elements:** Each element represents the predicted carbon emission value for the respective input row.  \\n- **Data Semantics:**  \\n  - Each numeric value reflects the model\\u2019s regression output, estimating carbon emissions based on the input features.  \\n  - Values are continuous and can be used for further analysis, evaluation, or aggregation.  \\n\\nThe tool does not return additional metadata by default (such as confidence intervals or feature importances) in the standard prediction call; it strictly outputs the predicted numeric values corresponding to the input dataset.\", \"examples\": [{\"user_intent\": \"Predict the carbon emissions for my factory dataset using the trained model.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"factory_emissions_data.csv\"}}}, {\"user_intent\": \"I have a CSV of vehicle specifications and want to estimate their carbon footprint.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"vehicle_specs.csv\"}}}, {\"user_intent\": \"Calculate predicted carbon emissions for a new dataset of industrial plants.\", \"tool_call\": {\"name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"arguments\": {\"data\": \"industrial_plants_data.csv\"}}}]}\n\n# User Query\nA new city planning company wants to estimate carbon emissions for different types of buildings to provide sustainable living solutions. We need a model to classify the carbon emissions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify the carbon emissions for different types of buildings based on input features.",
                    "tool_description": "Use a multi-class classification model to predict carbon emissions from input features related to different building types, enabling classification of emission levels.",
                    "expected_response": "A collection of classification results indicating the emission category for each building type based on the input features."
                }
            ],
            "extracted_arguments": {
                "data": "A tabular dataset containing features for different building types, which will be used to predict their carbon emissions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_433",
        "query": "Create a program to determine the depth map from an input image of a street filled with people.",
        "instruction": "Given an `image processing` and `depth estimation` task, retrieve tools that utilize neural network structures to process input images and generate depth maps by estimating the relative distance of objects within the scene.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Depth Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth')",
                    "api_arguments": {
                        "torch_dtype": "torch.float16"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "PIL",
                        "numpy",
                        "torch"
                    ],
                    "example_code": {
                        "install_packages": "pip install diffusers transformers accelerate",
                        "code": [
                            "from transformers import pipeline",
                            "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler",
                            "from PIL import Image",
                            "import numpy as np",
                            "import torch",
                            "from diffusers.utils import load_image",
                            "depth_estimator = pipeline('depth-estimation')",
                            "image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)",
                            "image = depth_estimator(image)['depth']",
                            "image = np.array(image)",
                            "image = image[:, :, None]",
                            "image = np.concatenate([image, image, image], axis=2)",
                            "image = Image.fromarray(image)",
                            "controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)",
                            "pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)",
                            "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)",
                            "pipe.enable_xformers_memory_efficient_attention()",
                            "pipe.enable_model_cpu_offload()",
                            "image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]",
                            "image.save('./images/stormtrooper_depth_out.png')"
                        ]
                    },
                    "performance": {
                        "dataset": "3M depth-image, caption pairs",
                        "accuracy": "500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_265",
                    "name": "lllyasviel/sd-controlnet-depth"
                },
                "id": "gorilla_huggingface_tool_261",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "Tool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 2: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.",
            "Tool 3: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 7: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 8: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 9: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 10: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 11: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 12: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.",
            "Tool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 17: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 18: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 19: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark."
        ],
        "tool_view": {
            "description": "This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.",
            "parameters": {
                "checkpoint": {
                    "type": "string",
                    "description": "The identifier or path of the pretrained ControlNet model checkpoint to load.",
                    "required": true,
                    "example": "lllyasviel/control_v11p_sd15_inpaint"
                },
                "torch_dtype": {
                    "type": "string",
                    "description": "The PyTorch data type to use for model tensors. Commonly used to reduce memory usage or improve performance.",
                    "required": true,
                    "example": "torch.float16"
                }
            },
            "response": "The tool produces a single inpainted image as its primary response. The response can be described as follows:\n\n- **Type:** Image object  \n  The output is a generated image resulting from the image inpainting process, where masked regions in the input image are filled or modified according to the specified prompt and model conditions.\n\n- **Content:**  \n  - **Image pixels:** The pixel data of the generated image, typically in RGB format.  \n  - **Dimensions:** The width and height match the expected output size of the pipeline, which is usually consistent with the input image dimensions or model default resolution.  \n  - **File format or representation:** Can be returned as a PIL Image object, NumPy array, or saved directly to a file, depending on the user’s handling of the pipeline output.\n\n- **Optional metadata (if exposed by the pipeline):**  \n  - **Prompt used:** The text prompt that guided the generation.  \n  - **Negative prompt:** Text specifying undesired features to avoid in the generated image.  \n  - **Inference settings:** Number of steps, random seed, and other scheduler or pipeline parameters.  \n  - **Control conditions:** The processed image and mask inputs used for inpainting, which influenced the generation.\n\nIn summary, the response is a generated image reflecting the model’s inpainting of masked regions in the input image, optionally accompanied by generation parameters and metadata describing the conditioning and prompts used.",
            "examples": [
                {
                    "user_intent": "I want to inpaint an image where certain parts are missing or damaged.",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "How can I use ControlNet to fix specific parts of an image using a mask?",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                },
                {
                    "user_intent": "I need to repair a damaged image with inpainting and use a mask to guide the restoration process.",
                    "tool_call": {
                        "name": "lllyasviel/control_v11p_sd15_inpaint",
                        "arguments": {
                            "checkpoint": "lllyasviel/control_v11p_sd15_inpaint",
                            "torch_dtype": "torch.float16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\nTool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 2: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\nTool 3: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 4: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 6: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 7: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 8: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 9: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 10: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 11: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 12: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.\nTool 13: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 14: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 15: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 16: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 17: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 18: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 19: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the ControlNet model for image inpainting, which enhances diffusion models by adding extra conditions to control the image generation process. It is specifically designed for inpainting tasks, where missing parts of an image are filled in based on the surrounding content. The model can be used with the Hugging Face framework and requires specific Python libraries like `diffusers`, `transformers`, and `accelerate`. The provided example demonstrates how to use the model to perform inpainting on an image using a mask to define the areas to be filled.\", \"parameters\": {\"checkpoint\": {\"type\": \"string\", \"description\": \"The identifier or path of the pretrained ControlNet model checkpoint to load.\", \"required\": true, \"example\": \"lllyasviel/control_v11p_sd15_inpaint\"}, \"torch_dtype\": {\"type\": \"string\", \"description\": \"The PyTorch data type to use for model tensors. Commonly used to reduce memory usage or improve performance.\", \"required\": true, \"example\": \"torch.float16\"}}, \"response\": \"The tool produces a single inpainted image as its primary response. The response can be described as follows:\\n\\n- **Type:** Image object  \\n  The output is a generated image resulting from the image inpainting process, where masked regions in the input image are filled or modified according to the specified prompt and model conditions.\\n\\n- **Content:**  \\n  - **Image pixels:** The pixel data of the generated image, typically in RGB format.  \\n  - **Dimensions:** The width and height match the expected output size of the pipeline, which is usually consistent with the input image dimensions or model default resolution.  \\n  - **File format or representation:** Can be returned as a PIL Image object, NumPy array, or saved directly to a file, depending on the user\\u2019s handling of the pipeline output.\\n\\n- **Optional metadata (if exposed by the pipeline):**  \\n  - **Prompt used:** The text prompt that guided the generation.  \\n  - **Negative prompt:** Text specifying undesired features to avoid in the generated image.  \\n  - **Inference settings:** Number of steps, random seed, and other scheduler or pipeline parameters.  \\n  - **Control conditions:** The processed image and mask inputs used for inpainting, which influenced the generation.\\n\\nIn summary, the response is a generated image reflecting the model\\u2019s inpainting of masked regions in the input image, optionally accompanied by generation parameters and metadata describing the conditioning and prompts used.\", \"examples\": [{\"user_intent\": \"I want to inpaint an image where certain parts are missing or damaged.\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"How can I use ControlNet to fix specific parts of an image using a mask?\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}, {\"user_intent\": \"I need to repair a damaged image with inpainting and use a mask to guide the restoration process.\", \"tool_call\": {\"name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}}}]}\n\n# User Query\nCreate a program to determine the depth map from an input image of a street filled with people.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a depth map from a single image of a street scene.",
                    "tool_description": "Use a monocular depth estimation model to predict per-pixel depth from an input image, producing a dense depth map representing the spatial distances of scene elements from the camera.",
                    "expected_response": "A depth map image or array where each pixel encodes the relative depth of the corresponding scene point."
                }
            ],
            "extracted_arguments": {
                "input_image": "The image of the street scene containing people, provided as a file path or image object."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_9",
        "query": "Estimate the depth of a pool using computational depth estimation, given an underwater photo.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to analyze underwater images and compute the depth based on visual data, optimizing accuracy through specialized algorithms and models.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.",
                    "id": "huggingface_api_146",
                    "name": "hf-tiny-model-private/tiny-random-GLPNForDepthEstimation"
                },
                "id": "gorilla_huggingface_tool_142",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 2: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 3: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 4: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 5: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 6: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.",
            "Tool 7: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 8: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 9: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 10: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 11: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 12: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.",
            "Tool 13: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 14: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 16: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 17: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.",
            "Tool 18: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 19: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality."
        ],
        "tool_view": {
            "description": "This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The specific model to use for depth estimation. This refers to the pre-trained model identifier available from the Hugging Face model hub.",
                    "required": true,
                    "example": "sayakpaul/glpn-kitti-finetuned-diode-221214-123047"
                }
            },
            "response": "The tool returns a depth map representation corresponding to an input image. The response is typically a multidimensional array (tensor) where each element represents the estimated depth value at a specific pixel location in the image. The depth values indicate the distance from the camera to objects in the scene, usually in meters or a normalized scale, depending on the model’s configuration.  \n\nKey aspects of the response:  \n\n1. **Depth Map Tensor**:  \n   - A 2D (height × width) or 3D array where each element encodes the predicted depth for the corresponding pixel in the input image.  \n   - The shape and resolution generally match or are proportional to the input image dimensions.  \n\n2. **Data Type and Scale**:  \n   - The values are numerical, often floating-point, representing distance measurements or normalized depth scores.  \n   - The scale may be absolute (meters) or relative, depending on model preprocessing and postprocessing.  \n\n3. **Optional Metadata (depending on framework/pipeline)**:  \n   - Some implementations may include auxiliary information such as image dimensions, processing time, or confidence maps, though this is not explicitly defined in the JSON.  \n\nOverall, the response provides a pixel-wise depth estimation of the input image, suitable for downstream computer vision tasks such as 3D reconstruction, scene understanding, or augmented reality.",
            "examples": [
                {
                    "user_intent": "I want to estimate depth from an image using a pre-trained model.",
                    "tool_call": {
                        "name": "glpn-kitti-finetuned-diode-221214-123047",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Run depth estimation on a scene using the GLPN model fine-tuned on the DIoDE dataset.",
                    "tool_call": {
                        "name": "glpn-kitti-finetuned-diode-221214-123047",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Use the Hugging Face transformer model to perform depth estimation in a computer vision project.",
                    "tool_call": {
                        "name": "glpn-kitti-finetuned-diode-221214-123047",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 1: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 2: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 3: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 4: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 5: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 6: A small, random DPT model for depth estimation built using the Hugging Face Transformers library.\nTool 7: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 8: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 9: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 10: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 11: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 12: This is a depth estimation model fine-tuned on the DIODE dataset, built using the Hugging Face Transformers framework. It can be used for predicting depth maps from images, leveraging a pre-trained model available via the Hugging Face API.\nTool 13: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 14: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 16: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 17: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.\nTool 18: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 19: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The specific model to use for depth estimation. This refers to the pre-trained model identifier available from the Hugging Face model hub.\", \"required\": true, \"example\": \"sayakpaul/glpn-kitti-finetuned-diode-221214-123047\"}}, \"response\": \"The tool returns a depth map representation corresponding to an input image. The response is typically a multidimensional array (tensor) where each element represents the estimated depth value at a specific pixel location in the image. The depth values indicate the distance from the camera to objects in the scene, usually in meters or a normalized scale, depending on the model\\u2019s configuration.  \\n\\nKey aspects of the response:  \\n\\n1. **Depth Map Tensor**:  \\n   - A 2D (height \\u00d7 width) or 3D array where each element encodes the predicted depth for the corresponding pixel in the input image.  \\n   - The shape and resolution generally match or are proportional to the input image dimensions.  \\n\\n2. **Data Type and Scale**:  \\n   - The values are numerical, often floating-point, representing distance measurements or normalized depth scores.  \\n   - The scale may be absolute (meters) or relative, depending on model preprocessing and postprocessing.  \\n\\n3. **Optional Metadata (depending on framework/pipeline)**:  \\n   - Some implementations may include auxiliary information such as image dimensions, processing time, or confidence maps, though this is not explicitly defined in the JSON.  \\n\\nOverall, the response provides a pixel-wise depth estimation of the input image, suitable for downstream computer vision tasks such as 3D reconstruction, scene understanding, or augmented reality.\", \"examples\": [{\"user_intent\": \"I want to estimate depth from an image using a pre-trained model.\", \"tool_call\": {\"name\": \"glpn-kitti-finetuned-diode-221214-123047\", \"arguments\": {}}}, {\"user_intent\": \"Run depth estimation on a scene using the GLPN model fine-tuned on the DIoDE dataset.\", \"tool_call\": {\"name\": \"glpn-kitti-finetuned-diode-221214-123047\", \"arguments\": {}}}, {\"user_intent\": \"Use the Hugging Face transformer model to perform depth estimation in a computer vision project.\", \"tool_call\": {\"name\": \"glpn-kitti-finetuned-diode-221214-123047\", \"arguments\": {}}}]}\n\n# User Query\nEstimate the depth of a pool using computational depth estimation, given an underwater photo.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate depth from an underwater image of a pool using a pre-trained model.",
                    "tool_description": "Perform pixel-wise depth estimation on an input underwater image using a fine-tuned GLPN model trained on the Diode-Subset dataset via the Hugging Face Transformers framework.",
                    "expected_response": "A depth map tensor representing the estimated distance from the camera to points in the underwater scene, with numerical depth values for each pixel."
                }
            ],
            "extracted_arguments": {
                "input_image": "An image file or array depicting the underwater pool scene for which depth is to be estimated.",
                "model": "The identifier of the pre-trained depth estimation model to use, available from the Hugging Face model hub."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_250",
        "query": "I want to build a tool to recognize urban landscapes and identify different objects in the image.",
        "instruction": "Given an `image recognition and segmentation` task, retrieve tools that perform semantic segmentation by processing image inputs to identify and categorize different objects within urban landscapes, providing detailed insights into the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Semantic Segmentation",
                    "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "packages": [
                            "transformers",
                            "PIL",
                            "requests"
                        ]
                    },
                    "example_code": "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits",
                    "performance": {
                        "dataset": "CityScapes",
                        "accuracy": "Not provided"
                    },
                    "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.",
                    "id": "huggingface_api_249",
                    "name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024"
                },
                "id": "gorilla_huggingface_tool_245",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 1: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 3: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 4: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 5: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 6: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 8: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 10: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 11: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 12: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 13: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 14: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.",
            "Tool 15: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 16: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 17: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 18: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation."
        ],
        "tool_view": {
            "description": "A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\") trained on the CityScapes dataset.\n\n### Expected Response Structure:\n\n1. **Segmentation Output (Logits)**:\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\n\n2. **Tensor Type**:\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\"pt\"` in the API call.\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\n\n3. **Image and Segmentation Context**:\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\n\n4. **Performance Metrics (Optional)**:\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\n\n### General Meaning of Components:\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\n",
            "examples": [
                {
                    "user_intent": "I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "city_street.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "traffic_snapshot.png",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Segment a drone image of a city to detect buildings and roads for urban planning.",
                    "tool_call": {
                        "name": "SegformerForSemanticSegmentation.from_pretrained",
                        "arguments": {
                            "images": "drone_city_view.jpeg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 1: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 3: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 4: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 5: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 6: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 7: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 8: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 10: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 11: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 12: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 13: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 14: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\nTool 15: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 16: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 17: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 18: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 19: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024\\u00d71024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or list of images to be segmented by the model. Each image should be in a format compatible with PIL or a similar image-handling library.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after preprocessing. Typically set to 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool performs semantic image segmentation using the SegFormer model, specifically the fine-tuned version of SegFormer (model name: \\\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\\\") trained on the CityScapes dataset.\\n\\n### Expected Response Structure:\\n\\n1. **Segmentation Output (Logits)**:\\n   - The response includes the raw output logits of the model after processing the input image. These logits represent the predicted segmentation classes for each pixel in the input image.\\n   - The logits are typically returned as a tensor, with dimensions corresponding to the number of pixels in the image and the number of segmentation classes.\\n   - The logits can be used to derive the final segmentation map, which assigns a class to each pixel.\\n\\n2. **Tensor Type**:\\n   - The logits tensor is returned in PyTorch format (`pt`), as indicated by the `return_tensors` argument set to `\\\"pt\\\"` in the API call.\\n   - This means the output tensor is in a format compatible with PyTorch-based processing and further model handling.\\n\\n3. **Image and Segmentation Context**:\\n   - The response does not directly provide the input image or a visual representation of the segmented image. However, the logits can be processed to generate a segmentation map that can be visualized.\\n   - The response would focus on the model's output in terms of logits, which represent probabilities for each pixel belonging to various predefined classes (such as buildings, roads, cars, etc.).\\n\\n4. **Performance Metrics (Optional)**:\\n   - While not explicitly provided in the response, performance metrics such as accuracy may be available as part of model documentation or evaluation, but are not part of the segmentation response itself.\\n\\n### General Meaning of Components:\\n- **Logits**: These are the raw, unnormalized predictions made by the model for each pixel, which can later be passed through a softmax function or thresholding technique to assign class labels.\\n- **Tensor**: The logits are in tensor form, which is a multi-dimensional array used in PyTorch, representing the model's predictions for the segmentation task.\\n\", \"examples\": [{\"user_intent\": \"I want to segment the streets, cars, and pedestrians in a cityscape image I downloaded.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"city_street.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Analyze this traffic camera snapshot to identify roads, vehicles, and sidewalks.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"traffic_snapshot.png\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Segment a drone image of a city to detect buildings and roads for urban planning.\", \"tool_call\": {\"name\": \"SegformerForSemanticSegmentation.from_pretrained\", \"arguments\": {\"images\": \"drone_city_view.jpeg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nI want to build a tool to recognize urban landscapes and identify different objects in the image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to segment urban landscapes in images to identify roads, buildings, vehicles, and pedestrians.",
                    "tool_description": "Perform semantic segmentation on urban scene images using a transformer-based SegFormer model fine-tuned on the CityScapes dataset, labeling each pixel with its corresponding class.",
                    "expected_response": "A tensor containing the predicted segmentation logits for each pixel, which can be used to derive a segmentation map of urban objects."
                }
            ],
            "extracted_arguments": {
                "images": "The input image or list of images representing urban landscapes to be segmented.",
                "return_tensors": "The type of tensor to return after segmentation, e.g., PyTorch tensor format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_165",
        "query": "We want to build an audiobook service. We need to convert text to speech for the user to listen.",
        "instruction": "Given a `text-to-speech conversion` task, retrieve tools that transform text inputs into synthesized speech outputs, facilitating the creation of audiobooks or other audio content aligned with the user's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "LJSpeech",
                        "accuracy": ""
                    },
                    "description": "A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.",
                    "id": "huggingface_api_738",
                    "name": "kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan"
                },
                "id": "gorilla_huggingface_tool_733",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 2: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 3: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 4: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 6: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.",
            "Tool 7: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 9: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 10: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 11: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 12: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.",
            "Tool 13: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.",
            "Tool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 16: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.",
            "Tool 17: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text."
        ],
        "tool_view": {
            "description": "The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "parameters": {
                "arg_overrides": {
                    "type": "object",
                    "description": "A dictionary of arguments that override default model or task configurations. For example, you can specify a custom configuration YAML file or set the task type.",
                    "required": true,
                    "example": {
                        "config_yaml": "config.yaml",
                        "task": "speech_to_text"
                    }
                },
                "cache_dir": {
                    "type": "string",
                    "description": "The directory path where downloaded model files and other cached data will be stored. If not provided, defaults to the environment variable 'HUGGINGFACE_HUB_CACHE' or a standard cache location.",
                    "required": true,
                    "example": "/home/user/.cache/huggingface"
                }
            },
            "response": "The response from this tool would likely contain several key components, organized around the functionality of performing speech-to-speech translation from English to Hokkien using a model from Fairseq. Below is a description of the expected response structure and its semantic components:\n\n1. **Model and Task Information**:\n   - The response would confirm the successful loading of the **speech-to-speech translation model** and associated **task**. It may include:\n     - The model type used (in this case, \"xm_transformer_s2ut_en-hk\").\n     - The task used (likely \"speech_to_text\" or similar).\n\n2. **Input and Output Audio Data**:\n   - **Input Audio**: The response would include some form of **audio input data**, such as an audio waveform or an indication of successful input loading (e.g., confirmation of loading the audio file).\n   - **Predicted Translation**: The **predicted output** from the speech-to-speech translation would be provided, which might include:\n     - Translated speech in Hokkien corresponding to the English audio input.\n     - The output audio could be in the form of a **waveform** or **audio file** with the corresponding sampling rate.\n\n3. **Generated Prediction**:\n   - There would likely be information about the model's **prediction generation process**, which may include:\n     - Details about how the input audio was processed (e.g., through the Fairseq `S2THubInterface`).\n     - The output **prediction unit** (likely a phonetic or linguistic unit after translation).\n\n4. **Speech Synthesis Output**:\n   - The response may describe or include details about the **speech synthesis** process used to generate the output audio. It might include:\n     - Confirmation of the **vocoder model** used for speech synthesis (e.g., \"CodeHiFiGANVocoder\").\n     - **Waveform data** (`wav`) along with the **sampling rate** (`sr`), allowing the synthesized audio to be played back or saved.\n\n5. **Performance Details**:\n   - The response would mention the performance of the model, which in this case includes:\n     - A reference to the **MuST-C dataset** as the training data.\n     - **Accuracy** details may not be explicitly specified but might be inferred from the dataset used (TED domain, Audiobook domain).\n\n6. **Configuration and Environment**:\n   - The response would mention the **configuration** used for the model, referencing files like `config.yaml` or `config.json` and possibly specifying that certain configurations were loaded for tasks like speech-to-text.\n   - **Python environment requirements** such as `fairseq`, `huggingface_hub`, and `torchaudio` would be implicitly required to execute the function, but would not be part of the direct output.\n\nIn summary, the response would contain:\n- A confirmation of successful model loading and task initialization.\n- Details of the input audio (possibly the waveform data) and output (translated speech in Hokkien).\n- The generated prediction (linguistic or phonetic unit).\n- Audio synthesis output (waveform and sampling rate).\n- Details about the performance dataset and any relevant configurations or model specifications used.\n\n",
            "examples": [
                {
                    "user_intent": "Translate an English TED talk audio into Hokkien speech.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/ted_talk_en.wav",
                            "output_audio_path": "/path/to/ted_talk_hk.wav",
                            "device": "cpu"
                        }
                    }
                },
                {
                    "user_intent": "Convert an audiobook chapter from English to Hokkien audio for language learning.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/audiobook_chapter_en.wav",
                            "output_audio_path": "/path/to/audiobook_chapter_hk.wav",
                            "device": "cpu"
                        }
                    }
                },
                {
                    "user_intent": "Quickly generate a Hokkien version of a short English podcast segment.",
                    "tool_call": {
                        "name": "xm_transformer_s2ut_en-hk",
                        "arguments": {
                            "input_audio_path": "/path/to/podcast_segment_en.wav",
                            "output_audio_path": "/path/to/podcast_segment_hk.wav",
                            "device": "cpu"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 1: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 2: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 3: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 4: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 6: This tool uses the SpeechT5 model from Hugging Face to convert text into natural-sounding speech. It leverages a unified encoder-decoder framework pre-trained on speech and text data, allowing users to generate speech with specific speaker characteristics by providing speaker embeddings. It is suitable for applications in speech synthesis and other spoken language tasks, enabling high-quality, customizable text-to-speech generation.\nTool 7: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 8: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 9: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 10: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 11: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 12: A pretrained Wav2Vec2 model from Facebook designed for automatic speech recognition, fine-tuned on 960 hours of Libri-Light and LibriSpeech audio at 16 kHz. It uses a self-training approach to convert spoken audio into text and can serve as a standalone acoustic model for transcribing speech with high accuracy.\nTool 13: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 14: This tool provides a fine-tuned version of the wav2vec2-xls-r-300m model for Automatic Speech Recognition tasks. It leverages the Hugging Face Transformers framework and can be used to convert audio input into phoneme-based text. The model is pre-trained and optimized for speech recognition, offering an accuracy with a loss of 0.3327 and a character error rate (CER) of 0.1332.\nTool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 16: The \"xm_transformer_s2ut_hk-en\" is a speech-to-speech translation model built using Fairseq, designed for Hokkien-English translation. It employs a single-pass decoder (S2UT) and is trained on both supervised TED, drama, and TAT domain data, as well as weakly supervised data from the drama domain. This model is used to convert spoken language in Hokkien into English speech directly, making it suitable for applications in speech translation and cross-lingual speech processing.\nTool 17: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 19: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"xm_transformer_s2ut_en-hk\\\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\", \"parameters\": {\"arg_overrides\": {\"type\": \"object\", \"description\": \"A dictionary of arguments that override default model or task configurations. For example, you can specify a custom configuration YAML file or set the task type.\", \"required\": true, \"example\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}}, \"cache_dir\": {\"type\": \"string\", \"description\": \"The directory path where downloaded model files and other cached data will be stored. If not provided, defaults to the environment variable 'HUGGINGFACE_HUB_CACHE' or a standard cache location.\", \"required\": true, \"example\": \"/home/user/.cache/huggingface\"}}, \"response\": \"The response from this tool would likely contain several key components, organized around the functionality of performing speech-to-speech translation from English to Hokkien using a model from Fairseq. Below is a description of the expected response structure and its semantic components:\\n\\n1. **Model and Task Information**:\\n   - The response would confirm the successful loading of the **speech-to-speech translation model** and associated **task**. It may include:\\n     - The model type used (in this case, \\\"xm_transformer_s2ut_en-hk\\\").\\n     - The task used (likely \\\"speech_to_text\\\" or similar).\\n\\n2. **Input and Output Audio Data**:\\n   - **Input Audio**: The response would include some form of **audio input data**, such as an audio waveform or an indication of successful input loading (e.g., confirmation of loading the audio file).\\n   - **Predicted Translation**: The **predicted output** from the speech-to-speech translation would be provided, which might include:\\n     - Translated speech in Hokkien corresponding to the English audio input.\\n     - The output audio could be in the form of a **waveform** or **audio file** with the corresponding sampling rate.\\n\\n3. **Generated Prediction**:\\n   - There would likely be information about the model's **prediction generation process**, which may include:\\n     - Details about how the input audio was processed (e.g., through the Fairseq `S2THubInterface`).\\n     - The output **prediction unit** (likely a phonetic or linguistic unit after translation).\\n\\n4. **Speech Synthesis Output**:\\n   - The response may describe or include details about the **speech synthesis** process used to generate the output audio. It might include:\\n     - Confirmation of the **vocoder model** used for speech synthesis (e.g., \\\"CodeHiFiGANVocoder\\\").\\n     - **Waveform data** (`wav`) along with the **sampling rate** (`sr`), allowing the synthesized audio to be played back or saved.\\n\\n5. **Performance Details**:\\n   - The response would mention the performance of the model, which in this case includes:\\n     - A reference to the **MuST-C dataset** as the training data.\\n     - **Accuracy** details may not be explicitly specified but might be inferred from the dataset used (TED domain, Audiobook domain).\\n\\n6. **Configuration and Environment**:\\n   - The response would mention the **configuration** used for the model, referencing files like `config.yaml` or `config.json` and possibly specifying that certain configurations were loaded for tasks like speech-to-text.\\n   - **Python environment requirements** such as `fairseq`, `huggingface_hub`, and `torchaudio` would be implicitly required to execute the function, but would not be part of the direct output.\\n\\nIn summary, the response would contain:\\n- A confirmation of successful model loading and task initialization.\\n- Details of the input audio (possibly the waveform data) and output (translated speech in Hokkien).\\n- The generated prediction (linguistic or phonetic unit).\\n- Audio synthesis output (waveform and sampling rate).\\n- Details about the performance dataset and any relevant configurations or model specifications used.\\n\\n\", \"examples\": [{\"user_intent\": \"Translate an English TED talk audio into Hokkien speech.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/ted_talk_en.wav\", \"output_audio_path\": \"/path/to/ted_talk_hk.wav\", \"device\": \"cpu\"}}}, {\"user_intent\": \"Convert an audiobook chapter from English to Hokkien audio for language learning.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/audiobook_chapter_en.wav\", \"output_audio_path\": \"/path/to/audiobook_chapter_hk.wav\", \"device\": \"cpu\"}}}, {\"user_intent\": \"Quickly generate a Hokkien version of a short English podcast segment.\", \"tool_call\": {\"name\": \"xm_transformer_s2ut_en-hk\", \"arguments\": {\"input_audio_path\": \"/path/to/podcast_segment_en.wav\", \"output_audio_path\": \"/path/to/podcast_segment_hk.wav\", \"device\": \"cpu\"}}}]}\n\n# User Query\nWe want to build an audiobook service. We need to convert text to speech for the user to listen.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert written text from the audiobook into natural-sounding speech for users to listen.",
                    "tool_description": "Use a text-to-speech (TTS) model to synthesize spoken audio from input text, generating natural-sounding speech suitable for audiobook playback.",
                    "expected_response": "An audio waveform or file containing the spoken version of the input text, along with optional metadata such as sampling rate or speaker characteristics."
                }
            ],
            "extracted_arguments": {
                "input_text": "The text content to be converted into speech, as a string.",
                "voice_type": "Optional parameter specifying the desired voice characteristics (e.g., male, female, language accent).",
                "output_audio_path": "The file path or storage location where the generated speech audio will be saved."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_306",
        "query": "A travel app we work on can translate language of a guide in real-time. We are targeting Spanish-speaking tourists.",
        "instruction": "Given a `real-time language translation` task, retrieve tools that facilitate speech-to-speech translation by processing audio inputs to deliver translated audio outputs, particularly for Spanish-speaking audiences in a travel context.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Fairseq",
                    "functionality": "speech-to-speech-translation",
                    "api_call": "textless_sm_sl_es()",
                    "api_arguments": null,
                    "python_environment_requirements": "fairseq",
                    "example_code": "https://huggingface.co/facebook/textless_sm_cs_en",
                    "performance": {
                        "dataset": null,
                        "accuracy": null
                    },
                    "description": "A Fairseq model for audio-to-audio speech-to-speech translation.",
                    "id": "huggingface_api_800",
                    "name": "textless_sm_sl_es"
                },
                "id": "gorilla_huggingface_tool_795",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 2: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 4: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 5: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 6: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.",
            "Tool 7: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.",
            "Tool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 9: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.",
            "Tool 10: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 11: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 12: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 13: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 14: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.",
            "Tool 15: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 16: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 17: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 18: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 19: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation."
        ],
        "tool_view": {
            "description": "Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "parameters": {
                "audio.wav": {
                    "type": "string",
                    "description": "Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\n\n1. **Timeline object**:  \n   - Represents the sequence of time intervals in the audio.  \n   - Each interval corresponds to a period where overlapped speech occurs.\n\n2. **Speech segments** (within the timeline):  \n   - Each segment has at least two key attributes:  \n     - **start**: The timestamp indicating when the overlapped speech begins.  \n     - **end**: The timestamp indicating when the overlapped speech ends.  \n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\n\n3. **Support or confidence information** (optional, depending on the output object):  \n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\n\n4. **Methods for iteration and access**:  \n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\n\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.",
            "examples": [
                {
                    "user_intent": "Detect when multiple speakers are talking simultaneously in a recording.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "conference_call.wav"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a meeting recording to identify parts with overlapping speech.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "team_meeting.wav"
                        }
                    }
                },
                {
                    "user_intent": "Check if there are any overlapping speech segments in this podcast episode.",
                    "tool_call": {
                        "name": "pyannote/overlapped-speech-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_3.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 1: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 2: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 4: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 5: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 6: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.\nTool 7: The `facebook/s2t-medium-librispeech-asr` is a Speech-to-Text Transformer model designed for automatic speech recognition. It converts spoken audio into written text, making it useful for transcribing audio datasets, voice recordings, or real-time speech input. Trained on the LibriSpeech dataset, it provides high accuracy on both clean and challenging speech and can be integrated using the Hugging Face Transformers library with support for preprocessing and decoding audio input.\nTool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 9: Whisper-medium is a pre-trained Transformer-based model for automatic speech recognition and speech translation, capable of transcribing and translating audio across multiple languages. Trained on 680,000 hours of labeled data, it generalizes effectively to diverse datasets and domains without fine-tuning, making it suitable for converting spoken audio into text or translating speech in real time.\nTool 10: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 11: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 12: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 13: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 14: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.\nTool 15: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 16: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 17: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 18: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 19: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\", \"parameters\": {\"audio.wav\": {\"type\": \"string\", \"description\": \"Path to the audio file (in .wav format) for which overlapped speech detection is to be performed.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool returns an analysis of an input audio file that identifies periods where multiple speakers are speaking simultaneously. The response is structured as an object representing the detected overlapped speech segments and typically includes the following components:\\n\\n1. **Timeline object**:  \\n   - Represents the sequence of time intervals in the audio.  \\n   - Each interval corresponds to a period where overlapped speech occurs.\\n\\n2. **Speech segments** (within the timeline):  \\n   - Each segment has at least two key attributes:  \\n     - **start**: The timestamp indicating when the overlapped speech begins.  \\n     - **end**: The timestamp indicating when the overlapped speech ends.  \\n   - These segments collectively describe all regions in the audio where two or more speakers are active simultaneously.\\n\\n3. **Support or confidence information** (optional, depending on the output object):  \\n   - Some implementations may include a confidence score or probability indicating how certain the model is that the segment contains overlapped speech.\\n\\n4. **Methods for iteration and access**:  \\n   - The response object provides methods such as `.get_timeline().support()` to iterate over or extract the relevant overlapped speech segments.\\n\\nIn summary, the response is a structured timeline of audio intervals with metadata indicating the presence of simultaneous speech, allowing downstream applications to locate and analyze regions of overlapped speech in an audio file.\", \"examples\": [{\"user_intent\": \"Detect when multiple speakers are talking simultaneously in a recording.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"conference_call.wav\"}}}, {\"user_intent\": \"I need to analyze a meeting recording to identify parts with overlapping speech.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"team_meeting.wav\"}}}, {\"user_intent\": \"Check if there are any overlapping speech segments in this podcast episode.\", \"tool_call\": {\"name\": \"pyannote/overlapped-speech-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_3.wav\"}}}]}\n\n# User Query\nA travel app we work on can translate language of a guide in real-time. We are targeting Spanish-speaking tourists.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate spoken language in real-time for a Spanish-speaking audience.",
                    "tool_description": "Convert spoken audio from one language into translated speech in Spanish, suitable for real-time applications.",
                    "expected_response": "Translated speech in Spanish, ready for real-time playback."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the original spoken audio.",
                "target_language": "The target language for translation (Spanish).",
                "audio_input": "The audio file or real-time audio stream to be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_0",
        "query": "You are building a virtual global tour guide that can identify languages from the audio of people speaking. Use a model to identify which language is being spoken.",
        "instruction": "Given a `language identification` task, retrieve tools that process audio inputs to accurately determine the language being spoken, utilizing models designed for audio classification and language detection purposes.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Language Identification",
                    "api_call": "AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')",
                    "api_arguments": [
                        "model = AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')",
                        "processor = Wav2Vec2Processor.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')"
                    ],
                    "python_environment_requirements": [
                        "transformers==4.27.0.dev0",
                        "pytorch==1.13.1",
                        "datasets==2.9.0",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "google/xtreme_s",
                        "accuracy": 0.8805
                    },
                    "description": "This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset. It is used for language identification in audio classification tasks.",
                    "id": "huggingface_api_820",
                    "name": "sanchit-gandhi/whisper-medium-fleurs-lang-id"
                },
                "id": "gorilla_huggingface_tool_815",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 1: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 3: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 5: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 6: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 7: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 8: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 9: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.",
            "Tool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 11: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 12: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.",
            "Tool 13: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 14: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 16: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 19: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use."
        ],
        "tool_view": {
            "description": "A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "parameters": {
                "signal": {
                    "type": "string",
                    "description": "A URL or file path to the audio signal (e.g., an audio file) that will be classified by the language identification model.",
                    "required": true,
                    "example": "https://omniglot.com/soundfiles/udhr/udhr_th.mp3"
                }
            },
            "response": "The response from this tool would provide the classification results for the spoken language input. The structure of the response would likely include the following components:\n\n1. **Predicted Language**: The language identified by the model from the spoken audio input. This is a label or identifier corresponding to one of the 107 languages the model is trained to recognize.\n\n2. **Confidence/Probability**: A value indicating the model's confidence or probability in the predicted language. This is often expressed as a numerical score or percentage, indicating how confident the model is that the prediction is correct.\n\n3. **Audio Processing Details**: Information regarding how the input signal (audio) was processed, such as the duration of the audio or any transformations applied to it during classification (e.g., feature extraction).\n\n4. **Model Information**: Details about the model used for classification, which might include the specific model version, architecture (ECAPA-TDNN), and the dataset it was trained on (VoxLingua107).\n\n5. **Additional Metadata (Optional)**: Any other relevant metadata related to the prediction, such as the input signal's format (e.g., MP3), sample rate, or any errors or warnings that may have occurred during the classification process.\n\nThis response would be structured in a way that provides the user with the predicted language and the associated confidence level, while also offering insights into the model and input characteristics.",
            "examples": [
                {
                    "user_intent": "I have a recording of someone speaking in Thai. Can you tell me what language it is?",
                    "tool_call": {
                        "name": "TalTechNLP/voxlingua107-epaca-tdnn",
                        "arguments": {
                            "signal": "https://omniglot.com/soundfiles/udhr/udhr_th.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I recorded a short audio clip in Spanish. Identify the language spoken.",
                    "tool_call": {
                        "name": "TalTechNLP/voxlingua107-epaca-tdnn",
                        "arguments": {
                            "signal": "https://example.com/audio/spanish_clip.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to automatically detect the language in a batch of English and French podcast snippets.",
                    "tool_call": {
                        "name": "TalTechNLP/voxlingua107-epaca-tdnn",
                        "arguments": {
                            "signal": [
                                "https://example.com/audio/english_snippet.mp3",
                                "https://example.com/audio/french_snippet.mp3"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 1: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 3: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 5: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 6: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 7: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 8: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 9: DistilBERT-base-cased-distilled-SQuAD is a compact, high-performance model fine-tuned for extractive question answering on the SQuAD v1.1 dataset. It can take a question and a context passage to identify and return the precise answer span, achieving strong accuracy while being faster and more resource-efficient than standard BERT models, making it ideal for applications that require efficient and accurate natural language question answering.\nTool 10: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 11: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 12: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.\nTool 13: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 14: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 16: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 17: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 18: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 19: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\", \"parameters\": {\"signal\": {\"type\": \"string\", \"description\": \"A URL or file path to the audio signal (e.g., an audio file) that will be classified by the language identification model.\", \"required\": true, \"example\": \"https://omniglot.com/soundfiles/udhr/udhr_th.mp3\"}}, \"response\": \"The response from this tool would provide the classification results for the spoken language input. The structure of the response would likely include the following components:\\n\\n1. **Predicted Language**: The language identified by the model from the spoken audio input. This is a label or identifier corresponding to one of the 107 languages the model is trained to recognize.\\n\\n2. **Confidence/Probability**: A value indicating the model's confidence or probability in the predicted language. This is often expressed as a numerical score or percentage, indicating how confident the model is that the prediction is correct.\\n\\n3. **Audio Processing Details**: Information regarding how the input signal (audio) was processed, such as the duration of the audio or any transformations applied to it during classification (e.g., feature extraction).\\n\\n4. **Model Information**: Details about the model used for classification, which might include the specific model version, architecture (ECAPA-TDNN), and the dataset it was trained on (VoxLingua107).\\n\\n5. **Additional Metadata (Optional)**: Any other relevant metadata related to the prediction, such as the input signal's format (e.g., MP3), sample rate, or any errors or warnings that may have occurred during the classification process.\\n\\nThis response would be structured in a way that provides the user with the predicted language and the associated confidence level, while also offering insights into the model and input characteristics.\", \"examples\": [{\"user_intent\": \"I have a recording of someone speaking in Thai. Can you tell me what language it is?\", \"tool_call\": {\"name\": \"TalTechNLP/voxlingua107-epaca-tdnn\", \"arguments\": {\"signal\": \"https://omniglot.com/soundfiles/udhr/udhr_th.mp3\"}}}, {\"user_intent\": \"I recorded a short audio clip in Spanish. Identify the language spoken.\", \"tool_call\": {\"name\": \"TalTechNLP/voxlingua107-epaca-tdnn\", \"arguments\": {\"signal\": \"https://example.com/audio/spanish_clip.wav\"}}}, {\"user_intent\": \"I want to automatically detect the language in a batch of English and French podcast snippets.\", \"tool_call\": {\"name\": \"TalTechNLP/voxlingua107-epaca-tdnn\", \"arguments\": {\"signal\": [\"https://example.com/audio/english_snippet.mp3\", \"https://example.com/audio/french_snippet.mp3\"]}}}]}\n\n# User Query\nYou are building a virtual global tour guide that can identify languages from the audio of people speaking. Use a model to identify which language is being spoken.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify the language being spoken in an audio recording.",
                    "tool_description": "Use a spoken language identification model based on ECAPA-TDNN trained on the VoxLingua107 dataset to classify the language of an input audio signal.",
                    "expected_response": "An object containing the predicted language, confidence score, audio processing details, and model metadata."
                }
            ],
            "extracted_arguments": {
                "signal": "The URL or file path to the audio recording containing spoken language."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_271",
        "query": "A game developer needs to predict the HP of a new Pokemon character based on several attributes. Provide the code to make that prediction.",
        "instruction": "Given a `prediction task` for game development, retrieve tools that implement tabular regression models to forecast the HP of a Pokemon based on input attributes, utilizing machine learning techniques to provide the most accurate outcomes.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Hugging Face",
                    "functionality": "Predicting Pokemon HP",
                    "api_call": "pipeline('regression', model='julien-c/pokemon-predict-hp')",
                    "api_arguments": "input_data",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "julien-c/kaggle-rounakbanik-pokemon",
                        "accuracy": {
                            "mean_absolute_error": 15.909,
                            "model_loss": 647.605
                        }
                    },
                    "description": "A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.",
                    "id": "huggingface_api_889",
                    "name": "julien-c/pokemon-predict-hp"
                },
                "id": "gorilla_huggingface_tool_882",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A tabular regression model that predicts the HP of Pokémon based on their attributes, trained on the julien-c/kaggle-rounakbanik-pokemon dataset and accessible via Hugging Face’s regression pipeline.",
            "Tool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 2: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.",
            "Tool 3: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 5: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.",
            "Tool 6: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 7: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 10: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 11: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 12: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 13: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 14: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 17: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 18: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 19: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data."
        ],
        "tool_view": {
            "description": "A tabular regression model that predicts the HP of Pokémon based on their attributes, trained on the julien-c/kaggle-rounakbanik-pokemon dataset and accessible via Hugging Face’s regression pipeline.",
            "parameters": {},
            "response": "The response from the tool is expected to be the predicted HP values of Pokemon based on the provided input data. The response will contain:\n\n1. **Predicted HP**: A numerical value (or a set of numerical values) representing the predicted HP for each Pokemon provided in the `input_data`. These values are the result of the regression model predicting the HP based on the model trained on the Pokémon dataset.\n\n2. **Error Metrics**: While not part of the direct response to each prediction, the model's general performance is described by error metrics in the tool's metadata:\n   - **Mean Absolute Error (MAE)**: This measures the average of the absolute differences between the predicted HP values and the actual HP values from the dataset, which in this case is 15.909.\n   - **Model Loss**: This indicates how well the model is performing overall during training, with a model loss of 647.605.\n\nThe response focuses on providing the predicted HP for the input data, possibly accompanied by metadata related to the model's performance during training (such as accuracy and loss metrics), though these metrics are not directly part of the per-prediction output.\n\n",
            "examples": [
                {
                    "user_intent": "Predict the HP of a Pikachu with level 35, attack 55, and defense 40.",
                    "tool_call": {
                        "name": "julien-c/pokemon-predict-hp",
                        "arguments": {
                            "input_data": {
                                "Name": "Pikachu",
                                "Level": 35,
                                "Attack": 55,
                                "Defense": 40,
                                "Speed": 90,
                                "SpecialAttack": 50,
                                "SpecialDefense": 50,
                                "Type1": "Electric",
                                "Type2": null
                            }
                        }
                    }
                },
                {
                    "user_intent": "Estimate the HP for a Charmander at level 20 with moderate stats.",
                    "tool_call": {
                        "name": "julien-c/pokemon-predict-hp",
                        "arguments": {
                            "input_data": {
                                "Name": "Charmander",
                                "Level": 20,
                                "Attack": 52,
                                "Defense": 43,
                                "Speed": 65,
                                "SpecialAttack": 60,
                                "SpecialDefense": 50,
                                "Type1": "Fire",
                                "Type2": null
                            }
                        }
                    }
                },
                {
                    "user_intent": "Predict HP for a team of Bulbasaur, Squirtle, and Eevee with given stats.",
                    "tool_call": {
                        "name": "julien-c/pokemon-predict-hp",
                        "arguments": {
                            "input_data": [
                                {
                                    "Name": "Bulbasaur",
                                    "Level": 18,
                                    "Attack": 49,
                                    "Defense": 49,
                                    "Speed": 45,
                                    "SpecialAttack": 65,
                                    "SpecialDefense": 65,
                                    "Type1": "Grass",
                                    "Type2": "Poison"
                                },
                                {
                                    "Name": "Squirtle",
                                    "Level": 18,
                                    "Attack": 48,
                                    "Defense": 65,
                                    "Speed": 43,
                                    "SpecialAttack": 50,
                                    "SpecialDefense": 64,
                                    "Type1": "Water",
                                    "Type2": null
                                },
                                {
                                    "Name": "Eevee",
                                    "Level": 18,
                                    "Attack": 55,
                                    "Defense": 50,
                                    "Speed": 55,
                                    "SpecialAttack": 45,
                                    "SpecialDefense": 65,
                                    "Type1": "Normal",
                                    "Type2": null
                                }
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A tabular regression model that predicts the HP of Pokémon based on their attributes, trained on the julien-c/kaggle-rounakbanik-pokemon dataset and accessible via Hugging Face’s regression pipeline.\nTool 1: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 2: CodeT5 is a pre-trained encoder-decoder Transformer model designed to understand and generate programming code by leveraging the semantics of developer-assigned identifiers. It supports a range of tasks including code summarization, generation, translation, refinement, defect detection, and clone detection, making it useful for improving code quality, automating coding tasks, and enabling multi-task learning across various code-related applications.\nTool 3: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 5: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\nTool 6: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 7: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 8: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 10: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 11: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 12: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 13: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 14: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 16: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 17: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 18: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 19: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A tabular regression model that predicts the HP of Pok\\u00e9mon based on their attributes, trained on the julien-c/kaggle-rounakbanik-pokemon dataset and accessible via Hugging Face\\u2019s regression pipeline.\", \"parameters\": {}, \"response\": \"The response from the tool is expected to be the predicted HP values of Pokemon based on the provided input data. The response will contain:\\n\\n1. **Predicted HP**: A numerical value (or a set of numerical values) representing the predicted HP for each Pokemon provided in the `input_data`. These values are the result of the regression model predicting the HP based on the model trained on the Pok\\u00e9mon dataset.\\n\\n2. **Error Metrics**: While not part of the direct response to each prediction, the model's general performance is described by error metrics in the tool's metadata:\\n   - **Mean Absolute Error (MAE)**: This measures the average of the absolute differences between the predicted HP values and the actual HP values from the dataset, which in this case is 15.909.\\n   - **Model Loss**: This indicates how well the model is performing overall during training, with a model loss of 647.605.\\n\\nThe response focuses on providing the predicted HP for the input data, possibly accompanied by metadata related to the model's performance during training (such as accuracy and loss metrics), though these metrics are not directly part of the per-prediction output.\\n\\n\", \"examples\": [{\"user_intent\": \"Predict the HP of a Pikachu with level 35, attack 55, and defense 40.\", \"tool_call\": {\"name\": \"julien-c/pokemon-predict-hp\", \"arguments\": {\"input_data\": {\"Name\": \"Pikachu\", \"Level\": 35, \"Attack\": 55, \"Defense\": 40, \"Speed\": 90, \"SpecialAttack\": 50, \"SpecialDefense\": 50, \"Type1\": \"Electric\", \"Type2\": null}}}}, {\"user_intent\": \"Estimate the HP for a Charmander at level 20 with moderate stats.\", \"tool_call\": {\"name\": \"julien-c/pokemon-predict-hp\", \"arguments\": {\"input_data\": {\"Name\": \"Charmander\", \"Level\": 20, \"Attack\": 52, \"Defense\": 43, \"Speed\": 65, \"SpecialAttack\": 60, \"SpecialDefense\": 50, \"Type1\": \"Fire\", \"Type2\": null}}}}, {\"user_intent\": \"Predict HP for a team of Bulbasaur, Squirtle, and Eevee with given stats.\", \"tool_call\": {\"name\": \"julien-c/pokemon-predict-hp\", \"arguments\": {\"input_data\": [{\"Name\": \"Bulbasaur\", \"Level\": 18, \"Attack\": 49, \"Defense\": 49, \"Speed\": 45, \"SpecialAttack\": 65, \"SpecialDefense\": 65, \"Type1\": \"Grass\", \"Type2\": \"Poison\"}, {\"Name\": \"Squirtle\", \"Level\": 18, \"Attack\": 48, \"Defense\": 65, \"Speed\": 43, \"SpecialAttack\": 50, \"SpecialDefense\": 64, \"Type1\": \"Water\", \"Type2\": null}, {\"Name\": \"Eevee\", \"Level\": 18, \"Attack\": 55, \"Defense\": 50, \"Speed\": 55, \"SpecialAttack\": 45, \"SpecialDefense\": 65, \"Type1\": \"Normal\", \"Type2\": null}]}}}]}\n\n# User Query\nA game developer needs to predict the HP of a new Pokemon character based on several attributes. Provide the code to make that prediction.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Predict the HP of a new Pokemon character using its attributes.",
                    "tool_description": "A tabular regression model that predicts the HP of Pokémon based on their attributes, trained on the julien-c/kaggle-rounakbanik-pokemon dataset and accessible via Hugging Face’s regression pipeline.",
                    "expected_response": "Predicted HP values for the input Pokemon character, possibly accompanied by metadata on model performance such as MAE and model loss."
                }
            ],
            "extracted_arguments": {
                "input_data": "A structured object containing the Pokémon's attributes including name, level, attack, defense, speed, special attack, special defense, and types."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_398",
        "query": "Our company is working on an autonomous robot and needs a solution to estimate the depth of objects in its environment.",
        "instruction": "Given a `depth estimation` task, retrieve tools that assist in determining object depth in a robotic environment by leveraging computer vision models to process visual data inputs and produce depth-related outputs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')",
                    "api_arguments": "pretrained_model_name",
                    "python_environment_requirements": "transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2",
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3421,
                            "Mae": 0.27,
                            "Rmse": 0.4042,
                            "Abs Rel": 0.3279,
                            "Log Mae": 0.1132,
                            "Log Rmse": 0.1688,
                            "Delta1": 0.5839,
                            "Delta2": 0.8408,
                            "Delta3": 0.9309
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.",
                    "id": "huggingface_api_168",
                    "name": "glpn-nyu-finetuned-diode-221122-082237"
                },
                "id": "gorilla_huggingface_tool_164",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 1: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.",
            "Tool 2: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 6: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 7: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 9: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 11: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 12: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 13: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 15: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 17: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 18: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding."
        ],
        "tool_view": {
            "description": "This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "parameters": {},
            "response": "The tool produces a depth estimation model that, when applied to an input image, outputs a predicted depth map representing the per-pixel distances from the camera. While the JSON does not explicitly define the response format, a typical response from this type of model would have the following structure:\n\n- **Depth Map Tensor**: A multi-dimensional array (tensor) corresponding to the spatial dimensions of the input image (height × width). Each element of the tensor represents the predicted depth value at the corresponding pixel. The tensor is usually a floating-point type, with higher values indicating greater distance from the camera.  \n\n- **Optional Metadata (depending on the framework)**: Some implementations may include additional information such as:\n  - **Shape information**: The height and width of the output depth map.\n  - **Data type**: The numeric type of the tensor (e.g., float32).\n  - **Batch dimension**: If the model processes multiple images at once, the first dimension corresponds to the batch size.\n\n- **Unit or Scale**: Although not always explicitly returned, the depth values are in a consistent unit determined by the dataset used for training (here, the diode-subset). This is important for interpreting the actual distance.\n\nIn practice, the primary component is the tensor containing per-pixel depth predictions, which can be further processed for visualization, evaluation, or integration into downstream computer vision tasks.",
            "examples": [
                {
                    "user_intent": "I want to estimate depth from a single indoor image using a pretrained model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221121-063504",
                        "arguments": {
                            "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')",
                            "image_path": "living_room.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Generate a depth map for an outdoor scene photo I took in my backyard.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221121-063504",
                        "arguments": {
                            "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')",
                            "image_path": "backyard_scene.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to compare depth estimation on multiple test images from the diode-subset dataset.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221121-063504",
                        "arguments": {
                            "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')",
                            "image_path": [
                                "test_image1.jpg",
                                "test_image2.jpg",
                                "test_image3.jpg"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 1: This tool provides a pre-trained reinforcement learning agent for the SoccerTwos environment using Unity ML-Agents, allowing users to load and run the Raiden-1001/poca-Soccerv7 model to observe or evaluate its gameplay performance.\nTool 2: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 6: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 7: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 8: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 9: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 10: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 11: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 12: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 13: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 14: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 15: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 16: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 17: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 18: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 19: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \\\"glpn-nyu\\\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\", \"parameters\": {}, \"response\": \"The tool produces a depth estimation model that, when applied to an input image, outputs a predicted depth map representing the per-pixel distances from the camera. While the JSON does not explicitly define the response format, a typical response from this type of model would have the following structure:\\n\\n- **Depth Map Tensor**: A multi-dimensional array (tensor) corresponding to the spatial dimensions of the input image (height \\u00d7 width). Each element of the tensor represents the predicted depth value at the corresponding pixel. The tensor is usually a floating-point type, with higher values indicating greater distance from the camera.  \\n\\n- **Optional Metadata (depending on the framework)**: Some implementations may include additional information such as:\\n  - **Shape information**: The height and width of the output depth map.\\n  - **Data type**: The numeric type of the tensor (e.g., float32).\\n  - **Batch dimension**: If the model processes multiple images at once, the first dimension corresponds to the batch size.\\n\\n- **Unit or Scale**: Although not always explicitly returned, the depth values are in a consistent unit determined by the dataset used for training (here, the diode-subset). This is important for interpreting the actual distance.\\n\\nIn practice, the primary component is the tensor containing per-pixel depth predictions, which can be further processed for visualization, evaluation, or integration into downstream computer vision tasks.\", \"examples\": [{\"user_intent\": \"I want to estimate depth from a single indoor image using a pretrained model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221121-063504\", \"arguments\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", \"image_path\": \"living_room.jpg\"}}}, {\"user_intent\": \"Generate a depth map for an outdoor scene photo I took in my backyard.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221121-063504\", \"arguments\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", \"image_path\": \"backyard_scene.png\"}}}, {\"user_intent\": \"I want to compare depth estimation on multiple test images from the diode-subset dataset.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221121-063504\", \"arguments\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", \"image_path\": [\"test_image1.jpg\", \"test_image2.jpg\", \"test_image3.jpg\"]}}}]}\n\n# User Query\nOur company is working on an autonomous robot and needs a solution to estimate the depth of objects in its environment.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate the depth of objects in the environment for an autonomous robot.",
                    "tool_description": "Use a fine-tuned GLPN-based depth estimation model to predict per-pixel depth from input images, enabling the robot to perceive distances and spatial relationships.",
                    "expected_response": "A depth map tensor corresponding to the input image, where each pixel contains a predicted distance from the camera, optionally including metadata such as shape and data type."
                }
            ],
            "extracted_arguments": {
                "input_image": "An image captured by the robot's camera representing the environment for depth estimation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_276",
        "query": "We are building a customer service analytics software. The software should recognize the voice of the customers.",
        "instruction": "Given a `speaker recognition` task, retrieve tools that enable the identification and verification of customer voices by processing audio data to extract speaker embeddings, which can be utilized in customer service analytics software.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker Verification",
                    "api_call": "EncoderClassifier.from_hparams(source='speechbrain/spkrec-xvect-voxceleb')",
                    "api_arguments": [
                        "source",
                        "savedir"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "import torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)",
                    "performance": {
                        "dataset": "Voxceleb1-test set (Cleaned)",
                        "accuracy": "EER(%) 3.2"
                    },
                    "description": "This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.",
                    "id": "huggingface_api_815",
                    "name": "speechbrain/spkrec-xvect-voxceleb"
                },
                "id": "gorilla_huggingface_tool_810",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 4: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 6: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 9: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 10: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 11: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 12: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 13: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 15: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 17: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 18: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 19: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents."
        ],
        "tool_view": {
            "description": "A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or a list of images to be classified by the MobileNet V2 model.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the format of the returned tensors. Typically set to 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response consists of a structured output representing the predicted class probabilities or labels for the input image(s). Specifically, the response has the following components:\n\n1. **logits**  \n   - A tensor or array containing the raw, unnormalized prediction scores produced by the model for each possible class.  \n   - Each element corresponds to one class in the model’s output space (e.g., the 1,000 ImageNet-1k classes).  \n   - Higher values indicate stronger confidence for that class, but these are not probabilities.\n\n2. **predicted class index**  \n   - An integer indicating the index of the class with the highest logit score.  \n   - This is typically computed by applying an `argmax` operation over the logits along the class dimension.\n\n3. **predicted class label**  \n   - A human-readable string corresponding to the predicted class index.  \n   - Derived using the model configuration’s `id2label` mapping, which links each index to a semantic label from the training dataset (e.g., ImageNet-1k class names).\n\nOptionally, the response may also include:\n\n4. **probabilities** (not always returned by default)  \n   - A tensor or array containing normalized probabilities for each class, obtained by applying a softmax over the logits.  \n   - Values sum to 1 across all classes and can be used to assess the model’s confidence for each prediction.\n\nIn summary, the response provides the model’s predictions for the input image in both raw score form (logits) and interpretable class label form, allowing users to identify the most likely object category.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a dog using the MobileNetV2 model.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "dog_image.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Classify an image I have from a URL using the pre-trained MobileNetV2 model.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I need to identify the object in a cat photo using MobileNetV2.",
                    "tool_call": {
                        "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')",
                        "api_arguments": {
                            "images": "cat_photo.jpg",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 1: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 3: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 4: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 5: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 6: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 9: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 10: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 11: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 12: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 13: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 15: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 16: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 17: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 18: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 19: TrOCR is a transformer-based optical character recognition (OCR) model that converts images of text into machine-readable text. It uses a Vision Transformer as an encoder to process images and a text Transformer as a decoder to generate text tokens autoregressively. Pre-trained on generic image and text representations, it can be applied to recognize and transcribe text from images, making it useful for digitizing handwritten or printed documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or a list of images to be classified by the MobileNet V2 model.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the format of the returned tensors. Typically set to 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response consists of a structured output representing the predicted class probabilities or labels for the input image(s). Specifically, the response has the following components:\\n\\n1. **logits**  \\n   - A tensor or array containing the raw, unnormalized prediction scores produced by the model for each possible class.  \\n   - Each element corresponds to one class in the model\\u2019s output space (e.g., the 1,000 ImageNet-1k classes).  \\n   - Higher values indicate stronger confidence for that class, but these are not probabilities.\\n\\n2. **predicted class index**  \\n   - An integer indicating the index of the class with the highest logit score.  \\n   - This is typically computed by applying an `argmax` operation over the logits along the class dimension.\\n\\n3. **predicted class label**  \\n   - A human-readable string corresponding to the predicted class index.  \\n   - Derived using the model configuration\\u2019s `id2label` mapping, which links each index to a semantic label from the training dataset (e.g., ImageNet-1k class names).\\n\\nOptionally, the response may also include:\\n\\n4. **probabilities** (not always returned by default)  \\n   - A tensor or array containing normalized probabilities for each class, obtained by applying a softmax over the logits.  \\n   - Values sum to 1 across all classes and can be used to assess the model\\u2019s confidence for each prediction.\\n\\nIn summary, the response provides the model\\u2019s predictions for the input image in both raw score form (logits) and interpretable class label form, allowing users to identify the most likely object category.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a dog using the MobileNetV2 model.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"dog_image.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Classify an image I have from a URL using the pre-trained MobileNetV2 model.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I need to identify the object in a cat photo using MobileNetV2.\", \"tool_call\": {\"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"cat_photo.jpg\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nWe are building a customer service analytics software. The software should recognize the voice of the customers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect when a customer is speaking in an audio recording.",
                    "tool_description": "Use a Voice Activity Detection (VAD) model to identify segments of speech within an audio stream.",
                    "expected_response": "A set of time-stamped segments indicating the start and end of detected speech."
                },
                {
                    "user_intent": "I want to recognize and identify the voice of individual customers from the detected speech segments.",
                    "tool_description": "Apply a speech recognition or speaker identification model to convert speech segments into textual or speaker-labeled data.",
                    "expected_response": "Transcriptions of the speech along with speaker identity labels for each segment."
                }
            ],
            "extracted_arguments": {
                "audio_input": "Audio data containing customer speech, in a format suitable for VAD and speech recognition processing."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_82",
        "query": "We'd like our chatbot to act as a fictional character for engaging with our users.",
        "instruction": "Given a `conversational character simulation` task, retrieve tools that facilitate text generation for dialogue by processing inputs that define a character's persona and dialogue history to engage users interactively.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')",
                    "api_arguments": [
                        "input_ids",
                        "max_length",
                        "num_return_sequences"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained('waifu-workshop/pygmalion-6b')\nmodel = AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\n\ninput_text = [CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\n<START>\\n[DIALOGUE HISTORY]\\nYou: [Your input message here]\\n[CHARACTER]:\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\n\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)",
                    "performance": {
                        "dataset": "56MB of dialogue data gathered from multiple sources",
                        "accuracy": "Not specified"
                    },
                    "description": "Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.",
                    "id": "huggingface_api_580",
                    "name": "pygmalion-6b"
                },
                "id": "gorilla_huggingface_tool_567",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 1: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 2: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 3: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 5: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 6: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 7: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 8: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 10: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 12: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 13: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 14: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 15: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.",
            "Tool 16: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.",
            "Tool 17: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 18: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 19: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes."
        ],
        "tool_view": {
            "description": "ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that the user sends to the chatbot for generating a response.",
                    "required": true,
                    "example": "What is your name?"
                }
            },
            "response": "The tool produces a response representing the chatbot’s generated reply to a user-provided message. The response typically includes the following components:\n\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \n\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot’s attempt to provide a coherent, contextually relevant, and human-like answer.  \n\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \n\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \n\nIn essence, the response is a structured object that contains the user’s message, the chatbot’s reply, and optionally any historical context to support multi-turn conversation continuity.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat and ask the bot how its day is going.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Hi ShuanBot, how's your day going?"
                        }
                    }
                },
                {
                    "user_intent": "Ask the chatbot for advice on improving my productivity.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Can you give me some tips to be more productive at work?"
                        }
                    }
                },
                {
                    "user_intent": "Start a conversation with ShuanBot about a fun topic like movies.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "What's your favorite movie genre and why?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 1: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 2: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 3: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 4: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 5: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 6: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 7: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 8: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 9: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 10: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 11: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 12: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 13: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 14: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 15: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.\nTool 16: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.\nTool 17: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 18: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 19: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that the user sends to the chatbot for generating a response.\", \"required\": true, \"example\": \"What is your name?\"}}, \"response\": \"The tool produces a response representing the chatbot\\u2019s generated reply to a user-provided message. The response typically includes the following components:\\n\\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \\n\\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot\\u2019s attempt to provide a coherent, contextually relevant, and human-like answer.  \\n\\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \\n\\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \\n\\nIn essence, the response is a structured object that contains the user\\u2019s message, the chatbot\\u2019s reply, and optionally any historical context to support multi-turn conversation continuity.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat and ask the bot how its day is going.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Hi ShuanBot, how's your day going?\"}}}, {\"user_intent\": \"Ask the chatbot for advice on improving my productivity.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Can you give me some tips to be more productive at work?\"}}}, {\"user_intent\": \"Start a conversation with ShuanBot about a fun topic like movies.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"What's your favorite movie genre and why?\"}}}]}\n\n# User Query\nWe'd like our chatbot to act as a fictional character for engaging with our users.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want the chatbot to respond as a specific fictional character in conversations with users.",
                    "tool_description": "Generate context-aware, character-driven dialogue by providing the chatbot with a persona profile, dialogue history, and user input, enabling interactive role-play and engaging conversational experiences.",
                    "expected_response": "A structured response containing the user's message, the chatbot's reply in-character, and optionally conversation history to maintain context across multiple turns."
                }
            ],
            "extracted_arguments": {
                "character_persona": "A structured description of the fictional character including traits, background, and personality for guiding the chatbot's responses.",
                "user_message": "The input text or query from the user that the chatbot should respond to in-character.",
                "dialogue_history": "Optional past conversation context used to maintain continuity in multi-turn interactions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_423",
        "query": "We need a classifier that can sort movie synopses in German into three categories: crime, tragedy, and theft.",
        "instruction": "Given a `text classification` task, retrieve tools that perform language processing to sort textual data, specifically movie synopses, into specified categories using zero-shot classification techniques. This involves analyzing language content and aligning it with predefined labels even without direct training on the specific task.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Transformers",
                    "functionality": "Zero-Shot Classification",
                    "api_call": "classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "api_arguments": {
                        "sequence": "string",
                        "candidate_labels": "list of strings",
                        "hypothesis_template": "string"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\ncandidate_labels = [Verbrechen,Tragödie,Stehlen]\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)",
                    "performance": {
                        "dataset": {
                            "XNLI DEV (german)": {
                                "accuracy": 85.5
                            },
                            "XNLI TEST (german)": {
                                "accuracy": 83.6
                            }
                        }
                    },
                    "description": "This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.",
                    "id": "huggingface_api_514",
                    "name": "AutoModelForSequenceClassification.from_pretrained('Sahajtomar/German_Zeroshot')"
                },
                "id": "gorilla_huggingface_tool_510",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "Tool 1: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 2: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.",
            "Tool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 4: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 5: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 7: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 8: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.",
            "Tool 9: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 10: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 11: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 12: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.",
            "Tool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 14: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 15: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 16: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 17: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 19: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories."
        ],
        "tool_view": {
            "description": "This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.",
            "parameters": {
                "sequence": {
                    "type": "string",
                    "description": "The input text sequence that you want to classify using the zero-shot classification model.",
                    "required": true,
                    "example": "Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie"
                },
                "candidate_labels": {
                    "type": "list of strings",
                    "description": "A list of possible labels or categories that the input sequence might belong to. The model will determine which label fits best.",
                    "required": true,
                    "example": [
                        "Verbrechen",
                        "Tragödie",
                        "Stehlen"
                    ]
                },
                "hypothesis_template": {
                    "type": "string",
                    "description": "A template string used to form a hypothesis for classification. For monolingual German models, it should be something like 'In diesem geht es um {}'. It can be experimented with to improve classification accuracy.",
                    "required": true,
                    "example": "In diesem geht es um {}"
                }
            },
            "response": "The tool returns a structured classification result for a given input text sequence in a zero-shot setting. The response typically contains the following components:  \n\n1. **`sequence`** – The original input text that was classified. This allows the user to verify which text the labels and scores correspond to.  \n\n2. **`labels`** – A list of the candidate labels provided by the user, usually sorted in descending order of likelihood. These are the categories that the model considered for the given sequence.  \n\n3. **`scores`** – A list of confidence scores corresponding to each label, indicating the model’s estimated probability that the input sequence belongs to that label. The scores are numeric values, typically between 0 and 1, and align positionally with the `labels` list.  \n\n4. **Optional `hypothesis_template`** – If provided or overridden, the template used to convert the candidate labels into hypothesis sentences for classification. This may appear in the response metadata depending on the API or implementation.  \n\nOverall, the response provides a mapping from the input text to each candidate label along with the model’s confidence, enabling users to interpret which category the text most likely belongs to in a zero-shot manner.",
            "examples": [
                {
                    "user_intent": "Classify this German news sentence into the correct category: 'Die Polizei hat gestern einen Betrüger verhaftet.'",
                    "tool_call": {
                        "name": "classifier",
                        "arguments": {
                            "sequence": "Die Polizei hat gestern einen Betrüger verhaftet.",
                            "candidate_labels": [
                                "Verbrechen",
                                "Nachrichten",
                                "Politik"
                            ],
                            "hypothesis_template": "In deisem geht es um {}."
                        }
                    }
                },
                {
                    "user_intent": "Determine whether the following German text is about a tragedy, crime, or theft: 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie.'",
                    "tool_call": {
                        "name": "classifier",
                        "arguments": {
                            "sequence": "Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie.",
                            "candidate_labels": [
                                "Verbrechen",
                                "Tragödie",
                                "Stehlen"
                            ],
                            "hypothesis_template": "In deisem geht es um {}."
                        }
                    }
                },
                {
                    "user_intent": "Identify the topic of this German sentence: 'Die Bundesregierung plant neue Umweltgesetze zur Reduzierung von CO2.'",
                    "tool_call": {
                        "name": "classifier",
                        "arguments": {
                            "sequence": "Die Bundesregierung plant neue Umweltgesetze zur Reduzierung von CO2.",
                            "candidate_labels": [
                                "Umwelt",
                                "Politik",
                                "Wirtschaft"
                            ],
                            "hypothesis_template": "In deisem geht es um {}."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\nTool 1: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 2: Classify emotions in German audio recordings using a wav2vec2-based model, detecting seven categories—anger, boredom, disgust, fear, happiness, sadness, and neutral—making it useful for analyzing speech sentiment and emotional content.\nTool 3: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 4: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 5: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 6: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 7: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 8: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.\nTool 9: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 10: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 11: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 12: MaskFormer is a computer vision model for instance, semantic, and panoptic image segmentation that uses a unified approach by predicting sets of masks with corresponding labels. The base-sized version with a Swin backbone is trained on the ADE20k dataset and can process images to generate segmented outputs for all three tasks, making it suitable for applications that require detailed object and scene understanding.\nTool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 14: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 15: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 16: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 17: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 18: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 19: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs zero-shot text classification in German, allowing users to assign a given text to one or more candidate labels without prior training on those labels. It uses a GBERT Large model fine-tuned on the XNLI German dataset and requires specifying a hypothesis template in German to produce accurate predictions. It is useful for categorizing text into custom labels in scenarios where labeled data for those categories is unavailable.\", \"parameters\": {\"sequence\": {\"type\": \"string\", \"description\": \"The input text sequence that you want to classify using the zero-shot classification model.\", \"required\": true, \"example\": \"Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie\"}, \"candidate_labels\": {\"type\": \"list of strings\", \"description\": \"A list of possible labels or categories that the input sequence might belong to. The model will determine which label fits best.\", \"required\": true, \"example\": [\"Verbrechen\", \"Trag\\u00f6die\", \"Stehlen\"]}, \"hypothesis_template\": {\"type\": \"string\", \"description\": \"A template string used to form a hypothesis for classification. For monolingual German models, it should be something like 'In diesem geht es um {}'. It can be experimented with to improve classification accuracy.\", \"required\": true, \"example\": \"In diesem geht es um {}\"}}, \"response\": \"The tool returns a structured classification result for a given input text sequence in a zero-shot setting. The response typically contains the following components:  \\n\\n1. **`sequence`** \\u2013 The original input text that was classified. This allows the user to verify which text the labels and scores correspond to.  \\n\\n2. **`labels`** \\u2013 A list of the candidate labels provided by the user, usually sorted in descending order of likelihood. These are the categories that the model considered for the given sequence.  \\n\\n3. **`scores`** \\u2013 A list of confidence scores corresponding to each label, indicating the model\\u2019s estimated probability that the input sequence belongs to that label. The scores are numeric values, typically between 0 and 1, and align positionally with the `labels` list.  \\n\\n4. **Optional `hypothesis_template`** \\u2013 If provided or overridden, the template used to convert the candidate labels into hypothesis sentences for classification. This may appear in the response metadata depending on the API or implementation.  \\n\\nOverall, the response provides a mapping from the input text to each candidate label along with the model\\u2019s confidence, enabling users to interpret which category the text most likely belongs to in a zero-shot manner.\", \"examples\": [{\"user_intent\": \"Classify this German news sentence into the correct category: 'Die Polizei hat gestern einen Betr\\u00fcger verhaftet.'\", \"tool_call\": {\"name\": \"classifier\", \"arguments\": {\"sequence\": \"Die Polizei hat gestern einen Betr\\u00fcger verhaftet.\", \"candidate_labels\": [\"Verbrechen\", \"Nachrichten\", \"Politik\"], \"hypothesis_template\": \"In deisem geht es um {}.\"}}}, {\"user_intent\": \"Determine whether the following German text is about a tragedy, crime, or theft: 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie.'\", \"tool_call\": {\"name\": \"classifier\", \"arguments\": {\"sequence\": \"Letzte Woche gab es einen Selbstmord in einer nahe gelegenen Kolonie.\", \"candidate_labels\": [\"Verbrechen\", \"Trag\\u00f6die\", \"Stehlen\"], \"hypothesis_template\": \"In deisem geht es um {}.\"}}}, {\"user_intent\": \"Identify the topic of this German sentence: 'Die Bundesregierung plant neue Umweltgesetze zur Reduzierung von CO2.'\", \"tool_call\": {\"name\": \"classifier\", \"arguments\": {\"sequence\": \"Die Bundesregierung plant neue Umweltgesetze zur Reduzierung von CO2.\", \"candidate_labels\": [\"Umwelt\", \"Politik\", \"Wirtschaft\"], \"hypothesis_template\": \"In deisem geht es um {}.\"}}}]}\n\n# User Query\nWe need a classifier that can sort movie synopses in German into three categories: crime, tragedy, and theft.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify a German movie synopsis into one of three categories: crime, tragedy, or theft.",
                    "tool_description": "Classify a German text (movie synopsis) into one or more candidate labels using a zero-shot classification model, without requiring training data specific to the labels.",
                    "expected_response": "A structured classification result with labels (crime, tragedy, or theft) and corresponding confidence scores for the input movie synopsis."
                }
            ],
            "extracted_arguments": {
                "sequence": "The German movie synopsis to be classified.",
                "candidate_labels": "A list of possible categories (crime, tragedy, theft).",
                "hypothesis_template": "A template string to form a hypothesis for classification (e.g., 'In diesem geht es um {}')."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_191",
        "query": "The company needs to separate voice from background noise in a recorded podcast episode.",
        "instruction": "Given an `audio processing` task to separate voice from background noise, retrieve tools that enhance audio clarity by isolating speech from noise in recorded audio files, ensuring improved sound quality and intelligibility.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "hf_hub_download(repo_id='JorisCos/ConvTasNet_Libri2Mix_sepclean_8k')",
                    "api_arguments": [
                        "repo_id",
                        "filename"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_797",
                    "name": "ConvTasNet_Libri2Mix_sepclean_8k"
                },
                "id": "gorilla_huggingface_tool_792",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 1: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.",
            "Tool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 7: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 8: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 9: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 10: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 11: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 12: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 13: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 16: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 17: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 19: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio."
        ],
        "tool_view": {
            "description": "This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that needs to be enhanced.",
                    "required": true,
                    "example": "speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav"
                }
            },
            "response": "The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \n\n1. **Enhanced Audio Tensor**:  \n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \n   - The values are floating-point numbers representing the audio amplitude.  \n\n2. **Sample Rate**:  \n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \n\n3. **Optional Metadata**:  \n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \n\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of a meeting and want to clean it up for transcription.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/audio/meeting_noisy.wav"
                        }
                    }
                },
                {
                    "user_intent": "Enhance the audio quality of my podcast episode recorded in a cafe.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/podcasts/episode5_cafe.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to denoise a voice memo recorded with background traffic noise.",
                    "tool_call": {
                        "name": "speechbrain/sepformer-wham16k-enhancement",
                        "arguments": {
                            "path": "/home/user/voice_memos/traffic_memo.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 1: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 2: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 3: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: This tool provides a Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality images from random noise. Trained on the CIFAR-10 dataset, it enables unconditional image synthesis and supports multiple noise scheduling strategies to produce diverse and realistic images, making it useful for computer vision tasks that require sample generation or creative image modeling.\nTool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 7: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 8: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 9: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 10: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 11: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 12: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 13: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 14: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 15: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 16: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 17: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 18: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 19: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that needs to be enhanced.\", \"required\": true, \"example\": \"speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav\"}}, \"response\": \"The tool produces an enhanced (denoised) version of an input audio signal. The typical response is an audio data structure representing the separated or enhanced speech. Specifically, the response includes the following components:  \\n\\n1. **Enhanced Audio Tensor**:  \\n   - A multi-dimensional array (tensor) representing the waveform of the enhanced speech signal.  \\n   - Dimensions usually correspond to `[channels, samples]` or `[batch, channels, samples]` depending on the model configuration.  \\n   - The values are floating-point numbers representing the audio amplitude.  \\n\\n2. **Sample Rate**:  \\n   - The audio sampling frequency used to interpret the waveform correctly. For this model, it is 16 kHz.  \\n\\n3. **Optional Metadata**:  \\n   - Some implementations may include additional metadata such as the number of sources separated, channel layout, or processing parameters.  \\n\\nIn practice, the response is used to save or play back the enhanced audio. For example, one would typically pass the tensor to an audio library (like `torchaudio`) along with the sample rate to store it as a WAV file. The response does **not** include textual output; its primary content is the denoised audio waveform.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of a meeting and want to clean it up for transcription.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/audio/meeting_noisy.wav\"}}}, {\"user_intent\": \"Enhance the audio quality of my podcast episode recorded in a cafe.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/podcasts/episode5_cafe.wav\"}}}, {\"user_intent\": \"I want to denoise a voice memo recorded with background traffic noise.\", \"tool_call\": {\"name\": \"speechbrain/sepformer-wham16k-enhancement\", \"arguments\": {\"path\": \"/home/user/voice_memos/traffic_memo.wav\"}}}]}\n\n# User Query\nThe company needs to separate voice from background noise in a recorded podcast episode.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Separate the speech from background noise in a recorded podcast episode.",
                    "tool_description": "Perform audio source separation using a pretrained SepFormer model to isolate the voice from other background sounds in a recording, enhancing clarity and intelligibility.",
                    "expected_response": "An audio data structure containing the isolated speech signal and optionally separated background noise, along with the sample rate and any relevant metadata."
                }
            ],
            "extracted_arguments": {
                "audio_path": "The file path to the input podcast audio recording that needs voice separation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_460",
        "query": "Develop an automated podcast recommender system to suggest content that matches user's taste based on their favorite podcast speakers.",
        "instruction": "Given a `podcast recommendation` task, retrieve tools that facilitate the development of automated recommender systems by processing user preferences and audio features, such as speaker identification, to suggest personalized content based on a user's favorite podcast speakers.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speaker Verification",
                    "api_call": "EncoderClassifier.from_hparams(source='speechbrain/spkrec-xvect-voxceleb')",
                    "api_arguments": [
                        "source",
                        "savedir"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "import torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)",
                    "performance": {
                        "dataset": "Voxceleb1-test set (Cleaned)",
                        "accuracy": "EER(%) 3.2"
                    },
                    "description": "This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.",
                    "id": "huggingface_api_815",
                    "name": "speechbrain/spkrec-xvect-voxceleb"
                },
                "id": "gorilla_huggingface_tool_810",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 2: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 3: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 4: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.",
            "Tool 5: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 6: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 7: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 8: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 9: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 10: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 11: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 12: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 13: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.",
            "Tool 14: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 15: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 16: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 17: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 18: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios."
        ],
        "tool_view": {
            "description": "This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name of the pre-trained model to load for emotion recognition. This specifies which model from the Hugging Face library should be used.",
                    "required": true,
                    "example": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
                }
            },
            "response": "The tool returns a structured response containing two primary components derived from a raw audio input:\n\n1. **Pooled Transformer States**  \n   - This is a tensor representing the aggregated (pooled) hidden states from the last layer of the Wav2Vec 2.0 transformer model.  \n   - The pooling is typically performed across the time dimension of the audio sequence, resulting in a fixed-size representation for the entire input.  \n   - These hidden states can be used for downstream tasks or further analysis beyond the emotion predictions.\n\n2. **Emotion Predictions (Logits)**  \n   - This is a numeric vector corresponding to the dimensional predictions for **arousal**, **dominance**, and **valence**.  \n   - Each value is approximately within the range 0 to 1, indicating the intensity or degree of the respective emotional dimension in the input speech.  \n   - These predictions are generated by a regression head applied to the pooled transformer states.\n\nIn summary, the response consists of a tuple or structured object with:\n\n- **`hidden_states`**: the pooled representation of the audio from the last transformer layer.  \n- **`logits`**: the predicted continuous emotion values for arousal, dominance, and valence.  \n\nThis output enables both direct emotion analysis and further embedding-based processing for audio classification or multimodal tasks.",
            "examples": [
                {
                    "user_intent": "Analyze the emotional state in a podcast audio clip to understand the speaker's arousal, dominance, and valence.",
                    "tool_call": {
                        "name": "EmotionModel.from_pretrained",
                        "arguments": {
                            "model_name": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
                        }
                    }
                },
                {
                    "user_intent": "I want to detect emotions from a recorded customer support call to evaluate agent performance.",
                    "tool_call": {
                        "name": "EmotionModel.from_pretrained",
                        "arguments": {
                            "model_name": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
                        }
                    }
                },
                {
                    "user_intent": "Process a dataset of raw audio files from interviews to extract emotion features for research purposes.",
                    "tool_call": {
                        "name": "EmotionModel.from_pretrained",
                        "arguments": {
                            "model_name": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 1: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 2: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 3: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 4: A transformer-based image captioning tool that generates descriptive text for input images, enabling automated understanding and annotation of visual content.\nTool 5: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 6: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 7: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 8: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 9: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 10: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 11: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 12: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 13: ConvNeXT is a convolutional neural network model inspired by Vision Transformer designs, optimized for image classification tasks. It can process images to predict their class labels and is useful for applications that require automated recognition or categorization of visual content.\nTool 14: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 15: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 16: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 17: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 18: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 19: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name of the pre-trained model to load for emotion recognition. This specifies which model from the Hugging Face library should be used.\", \"required\": true, \"example\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"}}, \"response\": \"The tool returns a structured response containing two primary components derived from a raw audio input:\\n\\n1. **Pooled Transformer States**  \\n   - This is a tensor representing the aggregated (pooled) hidden states from the last layer of the Wav2Vec 2.0 transformer model.  \\n   - The pooling is typically performed across the time dimension of the audio sequence, resulting in a fixed-size representation for the entire input.  \\n   - These hidden states can be used for downstream tasks or further analysis beyond the emotion predictions.\\n\\n2. **Emotion Predictions (Logits)**  \\n   - This is a numeric vector corresponding to the dimensional predictions for **arousal**, **dominance**, and **valence**.  \\n   - Each value is approximately within the range 0 to 1, indicating the intensity or degree of the respective emotional dimension in the input speech.  \\n   - These predictions are generated by a regression head applied to the pooled transformer states.\\n\\nIn summary, the response consists of a tuple or structured object with:\\n\\n- **`hidden_states`**: the pooled representation of the audio from the last transformer layer.  \\n- **`logits`**: the predicted continuous emotion values for arousal, dominance, and valence.  \\n\\nThis output enables both direct emotion analysis and further embedding-based processing for audio classification or multimodal tasks.\", \"examples\": [{\"user_intent\": \"Analyze the emotional state in a podcast audio clip to understand the speaker's arousal, dominance, and valence.\", \"tool_call\": {\"name\": \"EmotionModel.from_pretrained\", \"arguments\": {\"model_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"}}}, {\"user_intent\": \"I want to detect emotions from a recorded customer support call to evaluate agent performance.\", \"tool_call\": {\"name\": \"EmotionModel.from_pretrained\", \"arguments\": {\"model_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"}}}, {\"user_intent\": \"Process a dataset of raw audio files from interviews to extract emotion features for research purposes.\", \"tool_call\": {\"name\": \"EmotionModel.from_pretrained\", \"arguments\": {\"model_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"}}}]}\n\n# User Query\nDevelop an automated podcast recommender system to suggest content that matches user's taste based on their favorite podcast speakers.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze the emotional characteristics of a user's favorite podcast speakers.",
                    "tool_description": "Use a speech emotion recognition model to process audio from selected speakers and extract dimensional emotion features such as arousal, dominance, and valence.",
                    "expected_response": "Structured emotion features for each speaker, including pooled transformer states and continuous emotion predictions."
                },
                {
                    "user_intent": "Match podcasts to the user's emotional preferences based on speaker profiles.",
                    "tool_description": "Compare emotion features from available podcasts with the user's preferred speaker emotion profiles to generate a relevance score or ranking.",
                    "expected_response": "A ranked list of podcast recommendations aligned with the user's emotional taste."
                }
            ],
            "extracted_arguments": {
                "favorite_speakers_audio": "Audio files or streams of the user's preferred podcast speakers for emotion analysis.",
                "podcast_library_audio": "A collection of podcast audio files to be analyzed and recommended based on emotional similarity.",
                "emotion_model_name": "The identifier of the pretrained speech emotion recognition model to extract features from audio."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_101",
        "query": "My company is working on a children's storybook. I need to generate images based on the text descriptions of scenes in the story.",
        "instruction": "Given an `image generation` task, retrieve tools that facilitate the creation of visual content by processing text descriptions and generating corresponding images, aligning the outputs with the narrative themes and details provided in the story.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Generation",
                    "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')",
                    "api_arguments": {
                        "prompt": "a photo of an astronaut riding a horse on mars"
                    },
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)",
                    "performance": {
                        "dataset": "COCO2017",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.",
                    "id": "huggingface_api_36",
                    "name": "stabilityai/stable-diffusion-2-1"
                },
                "id": "gorilla_huggingface_tool_36",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 1: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 4: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 5: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.",
            "Tool 6: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 7: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 9: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.",
            "Tool 10: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 12: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 13: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 14: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 15: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 17: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.",
            "Tool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content."
        ],
        "tool_view": {
            "description": "Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description in English that specifies the content and style of the image to be generated by the model.",
                    "required": true,
                    "example": "a photo of an astronaut riding a horse on mars"
                }
            },
            "response": "The tool produces a response consisting of one or more generated images corresponding to the input text prompt. The typical structure and meaning of the response are as follows:\n\n- **images**: A collection (usually a list or array) of image objects or image data generated by the model. Each element represents one rendered image corresponding to the prompt. These images are typically in a standard image format (e.g., RGB arrays or PIL Image objects) suitable for display, saving, or further processing.  \n- **metadata (optional)**: Depending on the pipeline implementation, the response may include additional information such as:\n  - **prompt**: The original text input used to generate the images.\n  - **seed or randomness info**: Data about the random seed or latent sampling used for reproducibility.\n  - **scheduler or step info**: Details about the diffusion process steps, scheduler type, or configuration used during generation.\n  - **model info**: Reference to the model version used for generation.\n  \nIn essence, the primary content of the response is the generated image(s), while any accompanying metadata provides context about the generation parameters and configuration.",
            "examples": [
                {
                    "user_intent": "Generate a futuristic cityscape at sunset with flying cars and neon lights.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-1",
                        "arguments": {
                            "prompt": "a futuristic cityscape at sunset, flying cars, neon lights, cinematic lighting"
                        }
                    }
                },
                {
                    "user_intent": "Create an image of a cute baby panda eating bamboo in a misty forest.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-1",
                        "arguments": {
                            "prompt": "a cute baby panda eating bamboo in a misty forest, photorealistic, soft lighting"
                        }
                    }
                },
                {
                    "user_intent": "Produce an illustration of a medieval knight fighting a dragon on a mountain cliff.",
                    "tool_call": {
                        "name": "stabilityai/stable-diffusion-2-1",
                        "arguments": {
                            "prompt": "medieval knight fighting a dragon on a mountain cliff, dramatic, high detail, fantasy art"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 1: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 2: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 3: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 4: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 5: MGP-STR is a vision-based Scene Text Recognition (STR) model that uses a ViT backbone with specialized A³ modules to perform optical character recognition (OCR) on images containing text. It is trained on the MJSynth and SynthText datasets and is designed to accurately extract text from visual scenes.\nTool 6: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 7: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 8: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 9: This tool uses a pretrained multimodal Vision-Encoder-Decoder model to automatically generate captions for images. It takes a list of image files as input and produces concise text descriptions, making it useful for tasks like image annotation, accessibility, or content summarization.\nTool 10: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 12: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 13: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 14: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 15: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 16: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 17: Mask2Former is a computer vision model designed for semantic, instance, and panoptic segmentation, capable of predicting masks and corresponding labels for images. Trained on the Cityscapes dataset with a Swin-based large architecture, it improves on previous approaches like MaskFormer in both performance and efficiency, making it suitable for accurately segmenting complex urban scenes.\nTool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 19: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description in English that specifies the content and style of the image to be generated by the model.\", \"required\": true, \"example\": \"a photo of an astronaut riding a horse on mars\"}}, \"response\": \"The tool produces a response consisting of one or more generated images corresponding to the input text prompt. The typical structure and meaning of the response are as follows:\\n\\n- **images**: A collection (usually a list or array) of image objects or image data generated by the model. Each element represents one rendered image corresponding to the prompt. These images are typically in a standard image format (e.g., RGB arrays or PIL Image objects) suitable for display, saving, or further processing.  \\n- **metadata (optional)**: Depending on the pipeline implementation, the response may include additional information such as:\\n  - **prompt**: The original text input used to generate the images.\\n  - **seed or randomness info**: Data about the random seed or latent sampling used for reproducibility.\\n  - **scheduler or step info**: Details about the diffusion process steps, scheduler type, or configuration used during generation.\\n  - **model info**: Reference to the model version used for generation.\\n  \\nIn essence, the primary content of the response is the generated image(s), while any accompanying metadata provides context about the generation parameters and configuration.\", \"examples\": [{\"user_intent\": \"Generate a futuristic cityscape at sunset with flying cars and neon lights.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-1\", \"arguments\": {\"prompt\": \"a futuristic cityscape at sunset, flying cars, neon lights, cinematic lighting\"}}}, {\"user_intent\": \"Create an image of a cute baby panda eating bamboo in a misty forest.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-1\", \"arguments\": {\"prompt\": \"a cute baby panda eating bamboo in a misty forest, photorealistic, soft lighting\"}}}, {\"user_intent\": \"Produce an illustration of a medieval knight fighting a dragon on a mountain cliff.\", \"tool_call\": {\"name\": \"stabilityai/stable-diffusion-2-1\", \"arguments\": {\"prompt\": \"medieval knight fighting a dragon on a mountain cliff, dramatic, high detail, fantasy art\"}}}]}\n\n# User Query\nMy company is working on a children's storybook. I need to generate images based on the text descriptions of scenes in the story.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate images based on the text descriptions for the children's storybook.",
                    "tool_description": "Generate high-quality images from text descriptions provided as prompts. The model will produce creative visual content based on scene descriptions from a storybook.",
                    "expected_response": "A collection of image objects corresponding to the generated scenes based on the input text descriptions."
                }
            ],
            "extracted_arguments": {
                "prompt": "A text description in English specifying the content and style of the image to be generated by the model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_311",
        "query": "A podcast company reached out to us wanting to change their speaker's voice in a recorded podcast.",
        "instruction": "Given a `voice conversion` task, retrieve tools that enable transformation of speaker's voice in audio recordings by processing speech inputs and applying conversion models to match the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')",
                    "api_arguments": {
                        "audio": "example_speech",
                        "sampling_rate": "sampling_rate",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets",
                        "numpy",
                        "torch",
                        "soundfile"
                    ],
                    "example_code": "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\ndataset = load_dataset('hf-internal-testing/librispeech_asr_demo', 'clean', split='validation')\ndataset = dataset.sort('id')\nsampling_rate = dataset.features['audio'].sampling_rate\nexample_speech = dataset[0]['audio']['array']\nprocessor = SpeechT5Processor.from_pretrained('microsoft/speecht5_vc')\nmodel = SpeechT5ForSpeechToSpeech.from_pretrained('microsoft/speecht5_vc')\nvocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\ninputs = processor(audio=example_speech, sampling_rate=sampling_rate, return_tensors='pt')\nimport numpy as np\nimport torch\nspeaker_embeddings = np.load('xvector_speaker_embedding.npy')\nspeaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\nspeech = model.generate_speech(inputs['input_values'], speaker_embeddings, vocoder=vocoder)\nimport soundfile as sf\nsf.write('speech.wav', speech.numpy(), samplerate=16000)",
                    "performance": {
                        "dataset": "CMU ARCTIC",
                        "accuracy": "Not specified"
                    },
                    "description": "SpeechT5 model fine-tuned for voice conversion (speech-to-speech) on CMU ARCTIC. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. It is designed to improve the modeling capability for both speech and text. This model can be used for speech conversion tasks.",
                    "id": "huggingface_api_784",
                    "name": "microsoft/speecht5_vc"
                },
                "id": "gorilla_huggingface_tool_779",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 1: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.",
            "Tool 2: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 3: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 4: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 6: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 7: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 8: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 9: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 10: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 12: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 13: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.",
            "Tool 14: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 18: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream."
        ],
        "tool_view": {
            "description": "A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "parameters": {
                "audio": {
                    "type": "array",
                    "description": "An array representing the input speech waveform that will be used for speech-to-speech conversion.",
                    "required": true,
                    "example": [
                        0.0,
                        0.1,
                        -0.05,
                        0.02
                    ]
                },
                "sampling_rate": {
                    "type": "number",
                    "description": "The sampling rate (in Hz) of the input audio waveform, which ensures the audio is processed correctly by the model.",
                    "required": true,
                    "example": 16000
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after processing the audio. Typically 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool does not explicitly define the response format. Based on the functionality described, we can infer a plausible response structure for the tool's operation.\n\nWhen the tool is invoked, it processes input speech data using the SpeechT5 model, and the expected response would most likely be the transformed speech output. Here’s a description of the response structure:\n\n1. **Transformed Speech Output**:  \n   The response will likely contain the generated transformed speech in the form of an audio waveform. This audio data would be represented as a numerical array, typically using a tensor format (e.g., PyTorch tensor). The array represents the audio signal generated by the model after processing the input speech with a speaker embedding and vocoder.\n\n   - **Audio Data**: This would be the generated speech waveform, possibly in the form of a 1D array of audio samples.\n   - **Sampling Rate**: The response may include the sampling rate at which the audio was generated, which is typically required for further audio processing or saving the file.\n\n2. **Optional Metadata**:  \n   Depending on the system design, the response could also include metadata related to the transformation, such as:\n   - **Speaker Embeddings**: The embedding used for voice conversion might be included as part of the response for traceability or further processing.\n   - **Additional Parameters**: Any relevant details like the model's specific configuration used during the transformation, or a reference to the vocoder model, could be returned.\n\nThus, the expected response will consist primarily of the generated speech audio data (as a numerical array or tensor) and possibly include additional metadata such as sampling rate and speaker embeddings.",
            "examples": [
                {
                    "user_intent": "Convert my recorded English speech to sound like a different speaker's voice.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "my_recording.wav",
                            "sampling_rate": 16000,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to modify the voice in this audiobook sample to match a target speaker.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "audiobook_sample.wav",
                            "sampling_rate": 22050,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Transform a short speech clip to mimic another person's voice for testing a voice assistant.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "test_clip.wav",
                            "sampling_rate": 16000,
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 1: This tool provides a speech emotion recognition model based on Wav2Vec 2.0, which takes raw audio as input and predicts emotions in terms of arousal, dominance, and valence within a range of 0 to 1. The model was fine-tuned on the MSP-Podcast dataset and pruned to 12 transformer layers. It also outputs the pooled states of the last transformer layer, useful for further analysis. This model can be integrated using Hugging Face Transformers and is designed for dimensional emotion recognition tasks.\nTool 2: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 3: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 4: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 5: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 6: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 7: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 8: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 9: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 10: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 11: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 12: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 13: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\nTool 14: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 16: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 17: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 18: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 19: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker\\u2019s voice into another speaker\\u2019s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\", \"parameters\": {\"audio\": {\"type\": \"array\", \"description\": \"An array representing the input speech waveform that will be used for speech-to-speech conversion.\", \"required\": true, \"example\": [0.0, 0.1, -0.05, 0.02]}, \"sampling_rate\": {\"type\": \"number\", \"description\": \"The sampling rate (in Hz) of the input audio waveform, which ensures the audio is processed correctly by the model.\", \"required\": true, \"example\": 16000}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after processing the audio. Typically 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool does not explicitly define the response format. Based on the functionality described, we can infer a plausible response structure for the tool's operation.\\n\\nWhen the tool is invoked, it processes input speech data using the SpeechT5 model, and the expected response would most likely be the transformed speech output. Here\\u2019s a description of the response structure:\\n\\n1. **Transformed Speech Output**:  \\n   The response will likely contain the generated transformed speech in the form of an audio waveform. This audio data would be represented as a numerical array, typically using a tensor format (e.g., PyTorch tensor). The array represents the audio signal generated by the model after processing the input speech with a speaker embedding and vocoder.\\n\\n   - **Audio Data**: This would be the generated speech waveform, possibly in the form of a 1D array of audio samples.\\n   - **Sampling Rate**: The response may include the sampling rate at which the audio was generated, which is typically required for further audio processing or saving the file.\\n\\n2. **Optional Metadata**:  \\n   Depending on the system design, the response could also include metadata related to the transformation, such as:\\n   - **Speaker Embeddings**: The embedding used for voice conversion might be included as part of the response for traceability or further processing.\\n   - **Additional Parameters**: Any relevant details like the model's specific configuration used during the transformation, or a reference to the vocoder model, could be returned.\\n\\nThus, the expected response will consist primarily of the generated speech audio data (as a numerical array or tensor) and possibly include additional metadata such as sampling rate and speaker embeddings.\", \"examples\": [{\"user_intent\": \"Convert my recorded English speech to sound like a different speaker's voice.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"my_recording.wav\", \"sampling_rate\": 16000, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to modify the voice in this audiobook sample to match a target speaker.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"audiobook_sample.wav\", \"sampling_rate\": 22050, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Transform a short speech clip to mimic another person's voice for testing a voice assistant.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"test_clip.wav\", \"sampling_rate\": 16000, \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nA podcast company reached out to us wanting to change their speaker's voice in a recorded podcast.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert the recorded podcast speaker's voice to sound like a different target speaker.",
                    "tool_description": "Transform an input speech audio waveform into another speaker's voice while preserving the original linguistic content using a speech-to-speech voice conversion model based on SpeechT5.",
                    "expected_response": "The transformed speech audio waveform as a numerical array or tensor, optionally including sampling rate and speaker embedding metadata."
                }
            ],
            "extracted_arguments": {
                "audio": "The recorded podcast speech waveform to be converted, represented as a numerical array.",
                "sampling_rate": "The sampling rate of the input audio waveform in Hz.",
                "return_tensors": "The type of tensor format to return for the generated audio, e.g., PyTorch tensor."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_421",
        "query": "I have a dataset with CO2 emissions in a CSV file, and I want to classify which sources have high or low emissions.",
        "instruction": "Given a `data classification` task, retrieve tools that use tabular data processing to classify dataset entries, such as determining high or low CO2 emissions, by utilizing machine learning models and frameworks like Hugging Face Transformers.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Tabular Classification",
                    "api_call": "AutoModel.from_pretrained('datadmg/autotrain-test-news-44534112235')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "datadmg/autotrain-data-test-news",
                        "accuracy": 0.333
                    },
                    "description": "This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.",
                    "id": "huggingface_api_857",
                    "name": "datadmg/autotrain-test-news-44534112235"
                },
                "id": "gorilla_huggingface_tool_850",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 1: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 2: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 4: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.",
            "Tool 5: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.",
            "Tool 6: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 7: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.",
            "Tool 8: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 10: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.",
            "Tool 11: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 13: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 14: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.",
            "Tool 15: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.",
            "Tool 16: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.",
            "Tool 17: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 18: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 19: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions."
        ],
        "tool_view": {
            "description": "This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "parameters": {
                "data": {
                    "type": "object",
                    "description": "The input data for making predictions with the trained model. This should be a structured dataset, typically in the form of a DataFrame containing features selected for prediction.",
                    "required": true,
                    "example": {
                        "feat_1": 1.5,
                        "feat_2": 2.3,
                        "feat_3": 4.1
                    }
                }
            },
            "response": "The tool response provides the predicted carbon emissions (in grams) based on input data passed to the model. The structure and meaning of the response components are as follows:\n\n1. **Predictions**: \n   - The tool will return a set of predicted values corresponding to the carbon emissions (CO2 in grams) for each data point provided as input.\n   - These predictions are derived from applying the trained model (loaded using `joblib.load('model.joblib')`) to the input features, which are typically tabular data formatted in a specific way (with columns corresponding to features of the model).\n\n2. **Metadata (Optional)**: \n   - While the response itself primarily consists of the predictions, additional metadata can be included that provides context on the model's performance and usage. This can include:\n     - **Accuracy Metrics**: These might include various performance metrics like Loss, R2 (R-squared), MSE (Mean Squared Error), MAE (Mean Absolute Error), and RMSLE (Root Mean Squared Logarithmic Error), as indicated in the performance section. These metrics help evaluate the model's prediction quality on the provided dataset.\n     - **Model Information**: Details about the model, such as its name (`pcoloc/autotrain-600-dragino-1839063122`) and the dataset it was trained on (`pcoloc/autotrain-data-600-dragino`), might be included for reference.\n\n3. **Structure**:\n   - **Predictions**: This will likely be a list or array of numerical values, where each element corresponds to the predicted carbon emission for a single row of input data.\n   - **Optional Metadata (Accuracy Metrics)**: A dictionary or object containing performance evaluation metrics like:\n     - **Loss**: A numerical value representing the loss function used in model training (lower is better).\n     - **R2**: A measure of how well the model explains the variance of the data (values closer to 1 are better).\n     - **MSE**: Mean Squared Error, which quantifies the average squared difference between predicted and actual values.\n     - **MAE**: Mean Absolute Error, which quantifies the average absolute difference between predicted and actual values.\n     - **RMSLE**: Root Mean Squared Logarithmic Error, which focuses on the relative error in the logarithmic scale, useful for data with wide ranges.\n\n4. **Additional Information**:\n   - The response may also include an indication of any issues or errors if the model or data is incompatible or fails during the prediction process, although this is not explicitly outlined in the documentation.\n\nIn summary, the response contains:\n- A list or array of predicted carbon emissions values (in grams) for each input sample.\n- Optionally, a set of performance metrics summarizing the model's accuracy and predictive power.",
            "examples": [
                {
                    "user_intent": "Predict CO2 emissions for a new batch of industrial machines using the trained model.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "new_machines_data.csv"
                        }
                    }
                },
                {
                    "user_intent": "Estimate carbon emissions for a factory dataset I have in a CSV file.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "factory_emissions.csv"
                        }
                    }
                },
                {
                    "user_intent": "Run the carbon emissions regression on the latest transportation fleet data.",
                    "tool_call": {
                        "name": "pcoloc/autotrain-600-dragino-1839063122",
                        "arguments": {
                            "data": "fleet_transport_data.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 1: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 2: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 3: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 4: A tabular regression model trained using AutoTrain to predict CO2 emissions in grams, allowing users to input structured data and obtain emission predictions using a pre-trained model.\nTool 5: This tool provides a binary classification model for predicting carbon emissions, trained using AutoTrain. It leverages the Joblib framework to load the model and uses features specified in a configuration file to process input data for prediction. The model is designed to achieve high accuracy, as demonstrated in its performance on a specific dataset.\nTool 6: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 7: A binary classification model that predicts CO2 emissions from tabular data, enabling users to estimate carbon output based on selected features. It is implemented in Python using Joblib for model loading and supports data preprocessing with Pandas.\nTool 8: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 10: A tabular regression model trained to predict carbon emissions using AutoTrain, capable of making predictions on structured data and requiring Joblib and Pandas for loading and processing. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and achieves an R² score of 0.54, making it suitable for estimating carbon emissions from tabular feature data.\nTool 11: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 13: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 14: A binary classification tool for predicting carbon emissions from tabular data, designed to load a pre-trained model using Joblib and generate predictions based on selected features from a dataset.\nTool 15: A tabular regression model trained with AutoTrain on Hugging Face, designed to predict carbon emissions from structured data inputs.\nTool 16: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.\nTool 17: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 18: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 19: A multi-class classification model for predicting carbon emissions from tabular data, designed to load with Joblib and process structured datasets for accurate emission predictions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\", \"parameters\": {\"data\": {\"type\": \"object\", \"description\": \"The input data for making predictions with the trained model. This should be a structured dataset, typically in the form of a DataFrame containing features selected for prediction.\", \"required\": true, \"example\": {\"feat_1\": 1.5, \"feat_2\": 2.3, \"feat_3\": 4.1}}}, \"response\": \"The tool response provides the predicted carbon emissions (in grams) based on input data passed to the model. The structure and meaning of the response components are as follows:\\n\\n1. **Predictions**: \\n   - The tool will return a set of predicted values corresponding to the carbon emissions (CO2 in grams) for each data point provided as input.\\n   - These predictions are derived from applying the trained model (loaded using `joblib.load('model.joblib')`) to the input features, which are typically tabular data formatted in a specific way (with columns corresponding to features of the model).\\n\\n2. **Metadata (Optional)**: \\n   - While the response itself primarily consists of the predictions, additional metadata can be included that provides context on the model's performance and usage. This can include:\\n     - **Accuracy Metrics**: These might include various performance metrics like Loss, R2 (R-squared), MSE (Mean Squared Error), MAE (Mean Absolute Error), and RMSLE (Root Mean Squared Logarithmic Error), as indicated in the performance section. These metrics help evaluate the model's prediction quality on the provided dataset.\\n     - **Model Information**: Details about the model, such as its name (`pcoloc/autotrain-600-dragino-1839063122`) and the dataset it was trained on (`pcoloc/autotrain-data-600-dragino`), might be included for reference.\\n\\n3. **Structure**:\\n   - **Predictions**: This will likely be a list or array of numerical values, where each element corresponds to the predicted carbon emission for a single row of input data.\\n   - **Optional Metadata (Accuracy Metrics)**: A dictionary or object containing performance evaluation metrics like:\\n     - **Loss**: A numerical value representing the loss function used in model training (lower is better).\\n     - **R2**: A measure of how well the model explains the variance of the data (values closer to 1 are better).\\n     - **MSE**: Mean Squared Error, which quantifies the average squared difference between predicted and actual values.\\n     - **MAE**: Mean Absolute Error, which quantifies the average absolute difference between predicted and actual values.\\n     - **RMSLE**: Root Mean Squared Logarithmic Error, which focuses on the relative error in the logarithmic scale, useful for data with wide ranges.\\n\\n4. **Additional Information**:\\n   - The response may also include an indication of any issues or errors if the model or data is incompatible or fails during the prediction process, although this is not explicitly outlined in the documentation.\\n\\nIn summary, the response contains:\\n- A list or array of predicted carbon emissions values (in grams) for each input sample.\\n- Optionally, a set of performance metrics summarizing the model's accuracy and predictive power.\", \"examples\": [{\"user_intent\": \"Predict CO2 emissions for a new batch of industrial machines using the trained model.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"new_machines_data.csv\"}}}, {\"user_intent\": \"Estimate carbon emissions for a factory dataset I have in a CSV file.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"factory_emissions.csv\"}}}, {\"user_intent\": \"Run the carbon emissions regression on the latest transportation fleet data.\", \"tool_call\": {\"name\": \"pcoloc/autotrain-600-dragino-1839063122\", \"arguments\": {\"data\": \"fleet_transport_data.csv\"}}}]}\n\n# User Query\nI have a dataset with CO2 emissions in a CSV file, and I want to classify which sources have high or low emissions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify sources in my dataset as having high or low CO2 emissions.",
                    "tool_description": "Use a pre-trained binary classification model to predict whether each data point in a structured CSV dataset corresponds to a high or low CO2 emission source.",
                    "expected_response": "A list of predicted classes (e.g., 'high' or 'low') for each row in the input dataset, optionally including confidence scores or probabilities for each prediction."
                }
            ],
            "extracted_arguments": {
                "data": "The input CSV file containing structured features for each source to be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_327",
        "query": "I want to automate the process of answering questions about historical facts. When given a question and a surrounding context, it should provide an accurate response.",
        "instruction": "Given an `automated question answering` task, retrieve tools that process questions and context information using natural language processing models to deliver precise and accurate responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')",
                    "api_arguments": {
                        "model_name": "deepset/roberta-base-squad2",
                        "tokenizer": "deepset/roberta-base-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "code": "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = deepset/roberta-base-squad2\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 79.87029394424324,
                            "f1": 82.91251169582613
                        }
                    },
                    "description": "This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions.",
                    "id": "huggingface_api_461",
                    "name": "deepset/roberta-base-squad2"
                },
                "id": "gorilla_huggingface_tool_457",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 1: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 2: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 6: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 7: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 10: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 11: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 12: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 13: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 17: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 18: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 19: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context."
        ],
        "tool_view": {
            "description": "A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The question you want the model to answer, expressed in natural language.",
                    "required": true,
                    "example": "Why is model conversion important?"
                },
                "context": {
                    "type": "string",
                    "description": "The text passage or context in which the answer to the question can be found.",
                    "required": true,
                    "example": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                }
            },
            "response": "The tool performs **question answering** using a **BERT-based model** that has been fine-tuned on the **SQuAD2.0** dataset. The typical response from this tool would contain the following components:\n\n1. **Answer**: A string containing the extractive answer to the provided question, derived from the context. This is the primary output of the question-answering task.\n\n2. **Start**: The index (position) within the context where the answer begins.\n\n3. **End**: The index (position) within the context where the answer ends.\n\nThese components describe the span of the context that provides the answer to the question, along with the actual answer text.\n\nIn summary, the response is structured to give the model's best guess at an answer, alongside the span of text in the context that corresponds to the answer.",
            "examples": [
                {
                    "user_intent": "I want to know who developed the theory of relativity from this passage.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "Who developed the theory of relativity?",
                            "context": "Albert Einstein, a German-born theoretical physicist, developed the theory of relativity, one of the two pillars of modern physics."
                        }
                    }
                },
                {
                    "user_intent": "Find out why model conversion between frameworks is useful.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "Why is model conversion between frameworks important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                        }
                    }
                },
                {
                    "user_intent": "Determine the capital city mentioned in this travel guide excerpt.",
                    "tool_call": {
                        "name": "bert-large-uncased-whole-word-masking-squad2",
                        "arguments": {
                            "question": "What is the capital city of France?",
                            "context": "France is known for its rich history, cuisine, and iconic landmarks like the Eiffel Tower in Paris, which is the capital city."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 1: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 2: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 4: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 5: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 6: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 7: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 10: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 11: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 12: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 13: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 14: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 15: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 17: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 18: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 19: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The question you want the model to answer, expressed in natural language.\", \"required\": true, \"example\": \"Why is model conversion important?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text passage or context in which the answer to the question can be found.\", \"required\": true, \"example\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}, \"response\": \"The tool performs **question answering** using a **BERT-based model** that has been fine-tuned on the **SQuAD2.0** dataset. The typical response from this tool would contain the following components:\\n\\n1. **Answer**: A string containing the extractive answer to the provided question, derived from the context. This is the primary output of the question-answering task.\\n\\n2. **Start**: The index (position) within the context where the answer begins.\\n\\n3. **End**: The index (position) within the context where the answer ends.\\n\\nThese components describe the span of the context that provides the answer to the question, along with the actual answer text.\\n\\nIn summary, the response is structured to give the model's best guess at an answer, alongside the span of text in the context that corresponds to the answer.\", \"examples\": [{\"user_intent\": \"I want to know who developed the theory of relativity from this passage.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"Who developed the theory of relativity?\", \"context\": \"Albert Einstein, a German-born theoretical physicist, developed the theory of relativity, one of the two pillars of modern physics.\"}}}, {\"user_intent\": \"Find out why model conversion between frameworks is useful.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"Why is model conversion between frameworks important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}}, {\"user_intent\": \"Determine the capital city mentioned in this travel guide excerpt.\", \"tool_call\": {\"name\": \"bert-large-uncased-whole-word-masking-squad2\", \"arguments\": {\"question\": \"What is the capital city of France?\", \"context\": \"France is known for its rich history, cuisine, and iconic landmarks like the Eiffel Tower in Paris, which is the capital city.\"}}}]}\n\n# User Query\nI want to automate the process of answering questions about historical facts. When given a question and a surrounding context, it should provide an accurate response.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically answer questions about historical facts given a context.",
                    "tool_description": "Use a BERT-based extractive question answering model fine-tuned on SQuAD 2.0 to identify precise answers to questions within a provided text context.",
                    "expected_response": "A structured answer object containing the extracted answer text and its start and end positions within the context."
                }
            ],
            "extracted_arguments": {
                "question": "The natural language question about a historical fact.",
                "context": "The text passage containing information relevant to the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_152",
        "query": "A Russia-based online lesson website needs an English subtitle for the recorded lessons of the students.",
        "instruction": "Given an `audio-to-text subtitle generation` task, retrieve tools that facilitate the creation of English subtitles for Russian audio content by employing speech recognition to transcribe recordings, ensuring the outputs match the linguistic and contextual requirements of the lessons.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Recognition",
                    "api_call": "SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')",
                    "api_arguments": [
                        "audio_paths"
                    ],
                    "python_environment_requirements": [
                        "huggingsound",
                        "torch",
                        "librosa",
                        "datasets",
                        "transformers"
                    ],
                    "example_code": "from huggingsound import SpeechRecognitionModel\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-russian')\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\ntranscriptions = model.transcribe(audio_paths)",
                    "performance": {
                        "dataset": "mozilla-foundation/common_voice_6_0",
                        "accuracy": {
                            "Test WER": 13.3,
                            "Test CER": 2.88,
                            "Test WER (+LM)": 9.57,
                            "Test CER (+LM)": 2.24
                        }
                    },
                    "description": "Fine-tuned XLSR-53 large model for speech recognition in Russian. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.",
                    "id": "huggingface_api_758",
                    "name": "jonatasgrosman/wav2vec2-large-xlsr-53-russian"
                },
                "id": "gorilla_huggingface_tool_753",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 1: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 2: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 3: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 4: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 5: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 6: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 7: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.",
            "Tool 8: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 9: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 10: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 11: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 12: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.",
            "Tool 13: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.",
            "Tool 14: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 15: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 16: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 17: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 19: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks."
        ],
        "tool_view": {
            "description": "A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "parameters": {
                "audio file or recording": {
                    "type": "string",
                    "description": "The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.",
                    "required": true,
                    "example": "path/to/input_audio.wav"
                }
            },
            "response": "The tool’s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \n\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \n\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.",
            "examples": [
                {
                    "user_intent": "Translate this Romanian audio recording of a short story into English speech.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "story_ro.wav"
                        }
                    }
                },
                {
                    "user_intent": "I have a 2-minute interview in Romanian. Convert it to English audio.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "interview_ro.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Translate this Romanian podcast episode into English for my listeners.",
                    "tool_call": {
                        "name": "facebook/textless_sm_ro_en",
                        "arguments": {
                            "audio_file": "podcast_episode_ro.flac"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 1: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 2: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 3: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 4: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 5: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 6: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 7: This tool provides a Dutch-to-English translation model using the Helsinki-NLP/opus-mt-nl-en model, based on the OPUS dataset. It leverages a transformer-align architecture with normalization and SentencePiece pre-processing for accurate translations, and is designed to work within the Hugging Face Transformers framework. It is useful for translating Dutch text into English in NLP applications.\nTool 8: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 9: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 10: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 11: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 12: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.\nTool 13: The Helsinki-NLP/opus-mt-en-de model is a machine translation tool that converts English text into German using the Hugging Face Transformers library. It is trained on the OPUS dataset and provides high-quality translations, achieving a BLEU score of 45.2 on the newstest2018 English-German benchmark, making it suitable for natural language processing applications that require accurate English-to-German translation.\nTool 14: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 15: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 16: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 17: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 18: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 19: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\", \"parameters\": {\"audio file or recording\": {\"type\": \"string\", \"description\": \"The path to an audio file or a recording input that will be used as input for the speech-to-speech translation model.\", \"required\": true, \"example\": \"path/to/input_audio.wav\"}}, \"response\": \"The tool\\u2019s response is expected to be an audio representation of the input speech translated from Romanian to English. Specifically, the response would typically include:  \\n\\n1. **Translated audio content**: A waveform or audio file containing the spoken translation in English, preserving natural prosody and intonation.  \\n2. **Metadata (optional, depending on implementation)**: This may include information about the audio format (e.g., sample rate, encoding), duration of the generated audio, and possibly confidence scores or timing alignment if the system provides them.  \\n\\nIn essence, the primary output is a playable audio signal representing the speech-to-speech translation, produced from the input audio in Romanian.\", \"examples\": [{\"user_intent\": \"Translate this Romanian audio recording of a short story into English speech.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"story_ro.wav\"}}}, {\"user_intent\": \"I have a 2-minute interview in Romanian. Convert it to English audio.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"interview_ro.mp3\"}}}, {\"user_intent\": \"Translate this Romanian podcast episode into English for my listeners.\", \"tool_call\": {\"name\": \"facebook/textless_sm_ro_en\", \"arguments\": {\"audio_file\": \"podcast_episode_ro.flac\"}}}]}\n\n# User Query\nA Russia-based online lesson website needs an English subtitle for the recorded lessons of the students.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate English subtitles for a recorded audio lesson.",
                    "tool_description": "Generate English subtitles by processing a recorded audio file and performing speech-to-text transcription and translation.",
                    "expected_response": "A collection of text subtitle segments with timestamp alignment to the audio file."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path to the recorded audio lesson that will be processed for subtitle generation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_320",
        "query": "The company needs to summarize articles for its news application. Provide guidelines to use PEGASUS for this purpose.",
        "instruction": "Given a `text summarization` task, retrieve tools that are designed to condense and simplify articles or text content using natural language processing models, specifically tailored for abstractive summarization tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Summarization",
                    "api_call": "pipeline('summarization', model='google/pegasus-large')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline('summarization', model='google/pegasus-large')\nsummary = summarizer('your_text_here')",
                    "performance": {
                        "dataset": [
                            {
                                "name": "xsum",
                                "accuracy": "47.60/24.83/39.64"
                            },
                            {
                                "name": "cnn_dailymail",
                                "accuracy": "44.16/21.56/41.30"
                            },
                            {
                                "name": "newsroom",
                                "accuracy": "45.98/34.20/42.18"
                            },
                            {
                                "name": "multi_news",
                                "accuracy": "47.65/18.75/24.95"
                            },
                            {
                                "name": "gigaword",
                                "accuracy": "39.65/20.47/36.76"
                            },
                            {
                                "name": "wikihow",
                                "accuracy": "46.39/22.12/38.41"
                            },
                            {
                                "name": "reddit_tifu",
                                "accuracy": "27.99/9.81/22.94"
                            },
                            {
                                "name": "big_patent",
                                "accuracy": "52.29/33.08/41.66"
                            },
                            {
                                "name": "arxiv",
                                "accuracy": "44.21/16.95/25.67"
                            },
                            {
                                "name": "pubmed",
                                "accuracy": "45.97/20.15/28.25"
                            },
                            {
                                "name": "aeslc",
                                "accuracy": "37.68/21.25/36.51"
                            },
                            {
                                "name": "billsum",
                                "accuracy": "59.67/41.58/47.59"
                            }
                        ]
                    },
                    "description": "google/pegasus-large is a pre-trained model for abstractive text summarization based on the PEGASUS architecture. It is trained on a mixture of C4 and HugeNews datasets and uses a sentencepiece tokenizer that can encode newline characters. The model has been fine-tuned for various summarization tasks and achieves state-of-the-art performance on multiple benchmarks.",
                    "id": "huggingface_api_558",
                    "name": "google/pegasus-large"
                },
                "id": "gorilla_huggingface_tool_554",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 1: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 3: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 5: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 6: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 7: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 8: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 11: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 12: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 13: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset.",
            "Tool 14: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 15: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 17: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 18: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.",
            "Tool 19: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data."
        ],
        "tool_view": {
            "description": "The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "parameters": {},
            "response": "The tool produces a text summarization response in the form of a structured output, typically a list of one or more generated summaries. Each element in the list represents a single summarization result and usually contains at least the following components:\n\n- **`summary_text`** (or similarly named field): A string containing the abstractive summary generated by the PEGASUS model. This is the main output and represents the condensed version of the input text.  \n\nThe list may contain multiple summaries if the model is configured to produce several outputs (for example, when specifying `num_return_sequences` or similar parameters in the Hugging Face pipeline). Each summary in the list corresponds to one possible abstractive summary of the input text.\n\nIn essence, the response encapsulates the model’s attempt to generate concise, coherent, and contextually accurate summaries of the provided input document or text.",
            "examples": [
                {
                    "user_intent": "Summarize this news article about the recent climate summit into a concise paragraph.",
                    "tool_call": {
                        "name": "google/pegasus-newsroom",
                        "arguments": {
                            "text": "World leaders met at the annual climate summit to discuss global strategies for reducing carbon emissions. Various initiatives were proposed to accelerate the adoption of renewable energy, and financial commitments were made to support developing countries in their climate adaptation efforts..."
                        }
                    }
                },
                {
                    "user_intent": "Create a short summary for this long investigative report on tech company acquisitions.",
                    "tool_call": {
                        "name": "google/pegasus-newsroom",
                        "arguments": {
                            "text": "In the past year, several major tech companies have acquired smaller startups to expand their AI capabilities. Analysts suggest that this consolidation trend may lead to less competition and higher prices for consumers. Detailed case studies of the acquisitions reveal strategic patterns in corporate growth..."
                        }
                    }
                },
                {
                    "user_intent": "I want a brief summary of this financial news article for a newsletter.",
                    "tool_call": {
                        "name": "google/pegasus-newsroom",
                        "arguments": {
                            "text": "The stock market experienced volatility this week as investors reacted to inflation data and central bank policy announcements. Tech stocks saw significant gains, while energy stocks dipped slightly. Market analysts advise caution as economic indicators remain mixed..."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 1: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 2: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 3: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 4: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 5: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 6: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 7: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 8: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 11: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 12: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 13: A pretrained PEGASUS model for abstractive text summarization, designed to generate concise summaries of input text using the CNN/DailyMail dataset.\nTool 14: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 15: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 17: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 18: The \"google/pegasus-pubmed\" model is a pre-trained transformer designed for abstractive text summarization. It leverages the PEGASUS architecture, which samples important sentences from a source text, and is fine-tuned on medical and scientific datasets like PubMed. This model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that handles newline characters. It is commonly used for generating concise summaries from large text inputs, particularly in the context of scientific and medical literature.\nTool 19: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"google/pegasus-newsroom\\\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\", \"parameters\": {}, \"response\": \"The tool produces a text summarization response in the form of a structured output, typically a list of one or more generated summaries. Each element in the list represents a single summarization result and usually contains at least the following components:\\n\\n- **`summary_text`** (or similarly named field): A string containing the abstractive summary generated by the PEGASUS model. This is the main output and represents the condensed version of the input text.  \\n\\nThe list may contain multiple summaries if the model is configured to produce several outputs (for example, when specifying `num_return_sequences` or similar parameters in the Hugging Face pipeline). Each summary in the list corresponds to one possible abstractive summary of the input text.\\n\\nIn essence, the response encapsulates the model\\u2019s attempt to generate concise, coherent, and contextually accurate summaries of the provided input document or text.\", \"examples\": [{\"user_intent\": \"Summarize this news article about the recent climate summit into a concise paragraph.\", \"tool_call\": {\"name\": \"google/pegasus-newsroom\", \"arguments\": {\"text\": \"World leaders met at the annual climate summit to discuss global strategies for reducing carbon emissions. Various initiatives were proposed to accelerate the adoption of renewable energy, and financial commitments were made to support developing countries in their climate adaptation efforts...\"}}}, {\"user_intent\": \"Create a short summary for this long investigative report on tech company acquisitions.\", \"tool_call\": {\"name\": \"google/pegasus-newsroom\", \"arguments\": {\"text\": \"In the past year, several major tech companies have acquired smaller startups to expand their AI capabilities. Analysts suggest that this consolidation trend may lead to less competition and higher prices for consumers. Detailed case studies of the acquisitions reveal strategic patterns in corporate growth...\"}}}, {\"user_intent\": \"I want a brief summary of this financial news article for a newsletter.\", \"tool_call\": {\"name\": \"google/pegasus-newsroom\", \"arguments\": {\"text\": \"The stock market experienced volatility this week as investors reacted to inflation data and central bank policy announcements. Tech stocks saw significant gains, while energy stocks dipped slightly. Market analysts advise caution as economic indicators remain mixed...\"}}}]}\n\n# User Query\nThe company needs to summarize articles for its news application. Provide guidelines to use PEGASUS for this purpose.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to summarize news articles for a news application using PEGASUS.",
                    "tool_description": "Use the PEGASUS-based model for generating concise summaries from news articles or long-form text. The model will process the input and produce an abstractive summary, capturing the key points of the article in a more compact form.",
                    "expected_response": "A list of generated summaries, each consisting of a string containing the condensed version of the input text."
                }
            ],
            "extracted_arguments": {
                "text": "The full text of the article or document to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_54",
        "query": "I am an interior designer and want to showcase a modern living room with a fireplace and a large window overlooking a forest. Create an image according to this description.",
        "instruction": "Given an `image generation` task, retrieve tools that create images based on detailed text descriptions by processing the textual inputs to generate visuals that align with the specified features and style in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image generation and modification based on text prompts",
                    "api_call": "StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')",
                    "api_arguments": [
                        "prompt",
                        "image",
                        "mask_image"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "scipy",
                        "safetensors"
                    ],
                    "example_code": "from diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\n\npipe.to(cuda)\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(./yellow_cat_on_park_bench.png)",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.",
                    "id": "huggingface_api_38",
                    "name": "stabilityai/stable-diffusion-2-inpainting"
                },
                "id": "gorilla_huggingface_tool_38",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 1: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 3: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 4: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 5: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 6: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 7: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 8: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 10: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 12: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 13: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 14: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 17: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.",
            "Tool 18: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 19: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text."
        ],
        "tool_view": {
            "description": "GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The path to the image file that will be processed by the model.",
                    "required": true,
                    "example": "path/to/image.jpg"
                },
                "text_guidance": {
                    "type": "string",
                    "description": "Optional text guidance to provide additional context or instructions to the model for generating the output image.",
                    "required": false,
                    "example": "A serene landscape with rolling hills and sheep grazing"
                }
            },
            "response": "The tool provides an image-to-image transformation service, using the model \"GreeneryScenery/SheepsControlV3\" to generate an output image based on an input image and optional text guidance. The response of the tool is expected to contain the following components:\n\n1. **Output Image**: The primary content of the response is the generated image resulting from the transformation process. This is the output based on the input image and any provided text guidance.\n\n2. **Status or Error Information (if applicable)**: If the process completes successfully, the response will simply return the transformed image. If there are errors, the response may contain error messages or codes indicating the nature of the issue, such as incorrect image format, issues with the input, or processing failures.\n\n3. **Metadata** (Optional): The response may also contain metadata related to the output image, such as processing time or model-specific information about the transformation. However, this is not explicitly defined in the provided tool documentation and may vary depending on the model's design or the framework in use.\n\nIn essence, the response is primarily focused on delivering the transformed image, potentially with additional details about the processing or errors encountered.",
            "examples": [
                {
                    "user_intent": "Enhance this landscape photo by adding more sheep grazing in the meadow.",
                    "tool_call": {
                        "name": "GreeneryScenery/SheepsControlV3",
                        "arguments": {
                            "image": "images/landscape_meadow.jpg",
                            "text_guidance": "Add several sheep grazing peacefully in the meadow"
                        }
                    }
                },
                {
                    "user_intent": "Transform my garden photo into a more vibrant spring scene.",
                    "tool_call": {
                        "name": "GreeneryScenery/SheepsControlV3",
                        "arguments": {
                            "image": "uploads/garden_photo.png",
                            "text_guidance": "Make the garden lush with colorful flowers and bright greenery"
                        }
                    }
                },
                {
                    "user_intent": "Apply the model to my farm image without any specific instructions, just see the default output.",
                    "tool_call": {
                        "name": "GreeneryScenery/SheepsControlV3",
                        "arguments": {
                            "image": "images/farm_scene.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 1: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 2: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 3: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 4: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 5: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 6: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 7: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 8: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 10: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 12: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 13: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 14: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 16: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 17: The `git-large-textcaps` tool is a large Generative Image-to-Text (GIT) model fine-tuned on the TextCaps dataset to generate descriptive text from images. It can produce image captions, support visual question answering, and perform image classification by generating textual outputs based on image inputs, making it useful for a variety of vision-and-language tasks.\nTool 18: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 19: DialogLED-base-16384 is a pre-trained model designed for understanding and summarizing long dialogues. Built on the Longformer-Encoder-Decoder (LED) architecture, it leverages window-based denoising during pre-training on extensive long-dialogue data, enabling it to handle inputs of up to 16,384 tokens. It is used to generate concise, context-aware summaries or responses for lengthy conversational text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The path to the image file that will be processed by the model.\", \"required\": true, \"example\": \"path/to/image.jpg\"}, \"text_guidance\": {\"type\": \"string\", \"description\": \"Optional text guidance to provide additional context or instructions to the model for generating the output image.\", \"required\": false, \"example\": \"A serene landscape with rolling hills and sheep grazing\"}}, \"response\": \"The tool provides an image-to-image transformation service, using the model \\\"GreeneryScenery/SheepsControlV3\\\" to generate an output image based on an input image and optional text guidance. The response of the tool is expected to contain the following components:\\n\\n1. **Output Image**: The primary content of the response is the generated image resulting from the transformation process. This is the output based on the input image and any provided text guidance.\\n\\n2. **Status or Error Information (if applicable)**: If the process completes successfully, the response will simply return the transformed image. If there are errors, the response may contain error messages or codes indicating the nature of the issue, such as incorrect image format, issues with the input, or processing failures.\\n\\n3. **Metadata** (Optional): The response may also contain metadata related to the output image, such as processing time or model-specific information about the transformation. However, this is not explicitly defined in the provided tool documentation and may vary depending on the model's design or the framework in use.\\n\\nIn essence, the response is primarily focused on delivering the transformed image, potentially with additional details about the processing or errors encountered.\", \"examples\": [{\"user_intent\": \"Enhance this landscape photo by adding more sheep grazing in the meadow.\", \"tool_call\": {\"name\": \"GreeneryScenery/SheepsControlV3\", \"arguments\": {\"image\": \"images/landscape_meadow.jpg\", \"text_guidance\": \"Add several sheep grazing peacefully in the meadow\"}}}, {\"user_intent\": \"Transform my garden photo into a more vibrant spring scene.\", \"tool_call\": {\"name\": \"GreeneryScenery/SheepsControlV3\", \"arguments\": {\"image\": \"uploads/garden_photo.png\", \"text_guidance\": \"Make the garden lush with colorful flowers and bright greenery\"}}}, {\"user_intent\": \"Apply the model to my farm image without any specific instructions, just see the default output.\", \"tool_call\": {\"name\": \"GreeneryScenery/SheepsControlV3\", \"arguments\": {\"image\": \"images/farm_scene.jpg\"}}}]}\n\n# User Query\nI am an interior designer and want to showcase a modern living room with a fireplace and a large window overlooking a forest. Create an image according to this description.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an image of a modern living room with a fireplace and a large window overlooking a forest.",
                    "tool_description": "Use an image-to-image or text-to-image model to create a visual representation based on the provided textual description, allowing the generation of detailed interior design imagery.",
                    "expected_response": "A high-quality image depicting a modern living room with the specified architectural and scenic elements."
                }
            ],
            "extracted_arguments": {
                "text_description": "A textual prompt describing the desired scene, including room style, key furniture or architectural elements, and surrounding scenery."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_259",
        "query": "To enhance our FAQ bot, we need to extract answers from a given knowledge base text.",
        "instruction": "Given a `knowledge extraction` task, retrieve tools that perform question answering by leveraging a given text to extract accurate and relevant answers, aligning with the FAQ enhancement objectives.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.",
            "Tool 2: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.",
            "Tool 3: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 4: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 5: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 7: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 8: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.",
            "Tool 9: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 10: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.",
            "Tool 11: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 12: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.",
            "Tool 13: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 16: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 17: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 18: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 19: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\nTool 2: The deepset/roberta-base-squad2 model is a fine-tuned version of the RoBERTa architecture, specifically trained for Question Answering using the SQuAD2.0 dataset, which includes both answerable and unanswerable questions. It is designed to extract answers from a given context and is available through the Hugging Face Transformers library.\nTool 3: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 4: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 5: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 6: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 7: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 8: A BERT large uncased model with whole word masking, fine-tuned on the SQuAD dataset for question-answering tasks. It leverages pretraining on BookCorpus and English Wikipedia using masked language modeling and next sentence prediction, enabling it to accurately extract answers from a given context. This model is suitable for applications that require precise question answering from text, achieving high performance on SQuAD benchmarks.\nTool 9: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 10: This tool provides a BERT-based model for question answering, trained on the SQuAD v2 dataset, and is used to extract answers from a given context based on a user-provided question. It leverages Hugging Face Transformers to perform the task, offering an easy way to load the model and apply it for NLP-based question answering tasks.\nTool 11: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 12: A BERT large, cased model trained with whole word masking and fine-tuned on the SQuAD dataset for question answering, enabling accurate extraction of answers from given text contexts.\nTool 13: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 14: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 16: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 17: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 18: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 19: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nTo enhance our FAQ bot, we need to extract answers from a given knowledge base text.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract precise answers from a knowledge base text to enhance the FAQ bot.",
                    "tool_description": "Use a pre-trained question-answering model to identify and extract answers from a given text context based on user-provided questions.",
                    "expected_response": "A structured answer object containing the extracted answer text, its confidence score, and the start and end character indices within the context."
                }
            ],
            "extracted_arguments": {
                "question": "The natural language question to be answered using the knowledge base text.",
                "context": "The knowledge base text or passage that contains the information needed to answer the question."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_91",
        "query": "Our travel agency needs to build a chatbot that helps tourists find the best attractions in various destinations. The chatbot should answer questions related to tourist attractions.",
        "instruction": "Given a `chatbot development` task, retrieve tools that facilitate the creation of question-answering systems by leveraging natural language processing techniques to address inquiries about tourist attractions, ensuring relevant and precise responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')",
                    "api_arguments": [
                        "input_ids"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\nembeddings = model(input_ids).pooler_output",
                    "performance": {
                        "dataset": [
                            {
                                "name": "NQ",
                                "accuracy": {
                                    "top_20": 78.4,
                                    "top_100": 85.4
                                }
                            },
                            {
                                "name": "TriviaQA",
                                "accuracy": {
                                    "top_20": 79.4,
                                    "top_100": 85.0
                                }
                            },
                            {
                                "name": "WQ",
                                "accuracy": {
                                    "top_20": 73.2,
                                    "top_100": 81.4
                                }
                            },
                            {
                                "name": "TREC",
                                "accuracy": {
                                    "top_20": 79.8,
                                    "top_100": 89.1
                                }
                            },
                            {
                                "name": "SQuAD",
                                "accuracy": {
                                    "top_20": 63.2,
                                    "top_100": 77.2
                                }
                            }
                        ]
                    },
                    "description": "Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).",
                    "id": "huggingface_api_19",
                    "name": "facebook/dpr-question_encoder-single-nq-base"
                },
                "id": "gorilla_huggingface_tool_19",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 1: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 2: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 3: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 4: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 5: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 6: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 7: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 8: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 9: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.",
            "Tool 10: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 13: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 14: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 15: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 16: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 17: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 18: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model."
        ],
        "tool_view": {
            "description": "BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that will initiate the conversation with the Blenderbot model.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \n\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model’s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \n\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \n\nIn practice, the tool accepts a single input, `\"message\"`, and outputs a single text string representing the chatbot’s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.",
            "examples": [
                {
                    "user_intent": "I want to have a friendly chat about movies and get some recommendations.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hi! Can you suggest some good movies to watch this weekend?"
                        }
                    }
                },
                {
                    "user_intent": "I'm feeling a bit down and need someone to talk to for comfort.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "I'm feeling sad today. Can we talk for a bit?"
                        }
                    }
                },
                {
                    "user_intent": "I want to have a casual conversation about hobbies and interests.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hey! What are some fun hobbies you think I should try?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 1: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 2: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 3: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 4: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 5: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 6: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 7: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 8: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 9: The \"google/tapas-mini-finetuned-wtq\" model is a TAPAS mini variant fine-tuned on the WikiTable Questions (WTQ) dataset, designed for answering questions related to tables. It leverages a pretrained model from a large corpus of English data from Wikipedia, making it suitable for natural language processing tasks that require understanding and interpreting tabular data.\nTool 10: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 11: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 12: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 13: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 14: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 15: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 16: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 17: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 18: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that will initiate the conversation with the Blenderbot model.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \\n\\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model\\u2019s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \\n\\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \\n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \\n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \\n\\nIn practice, the tool accepts a single input, `\\\"message\\\"`, and outputs a single text string representing the chatbot\\u2019s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.\", \"examples\": [{\"user_intent\": \"I want to have a friendly chat about movies and get some recommendations.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hi! Can you suggest some good movies to watch this weekend?\"}}}, {\"user_intent\": \"I'm feeling a bit down and need someone to talk to for comfort.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"I'm feeling sad today. Can we talk for a bit?\"}}}, {\"user_intent\": \"I want to have a casual conversation about hobbies and interests.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hey! What are some fun hobbies you think I should try?\"}}}]}\n\n# User Query\nOur travel agency needs to build a chatbot that helps tourists find the best attractions in various destinations. The chatbot should answer questions related to tourist attractions.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a chatbot that can answer questions about tourist attractions in different destinations.",
                    "tool_description": "Deploy a conversational AI model capable of engaging in multi-turn dialogue, providing contextually relevant information about tourist attractions, and handling general travel-related queries.",
                    "expected_response": "Textual responses from the chatbot containing information about tourist attractions, recommendations, and related travel advice."
                }
            ],
            "extracted_arguments": {
                "message": "The input message or question from a user inquiring about tourist attractions or travel information."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_390",
        "query": "We're creating a promotional image for a wildlife-themed event. We need to display two tigers in a natural setting.",
        "instruction": "Given an `image generation` task, retrieve tools that process text prompts to create or modify images in accordance with the specified theme and requirements of the promotional content.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Generate and modify images based on text prompts",
                    "api_call": "StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')",
                    "api_arguments": {
                        "prompt": "Text prompt to generate image",
                        "image": "Initial image (optional)",
                        "negative_prompt": "Negative text prompt to avoid certain features",
                        "strength": "Strength of the prompt effect on the generated image"
                    },
                    "python_environment_requirements": [
                        "pip install -U git+https://github.com/huggingface/transformers.git",
                        "pip install diffusers transformers accelerate scipy safetensors"
                    ],
                    "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom diffusers import StableDiffusionDepth2ImgPipeline\n\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n stabilityai/stable-diffusion-2-depth,\n torch_dtype=torch.float16,\n).to(cuda)\n\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\ninit_image = Image.open(requests.get(url, stream=True).raw)\nprompt = two tigers\nn_propmt = bad, deformed, ugly, bad anotomy\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]",
                    "performance": {
                        "dataset": "COCO2017 validation set",
                        "accuracy": "Not optimized for FID scores"
                    },
                    "description": "Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.",
                    "id": "huggingface_api_44",
                    "name": "stabilityai/stable-diffusion-2-depth"
                },
                "id": "gorilla_huggingface_tool_44",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 2: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 4: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 7: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 8: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.",
            "Tool 9: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 10: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 11: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 13: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 14: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 15: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 16: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 18: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models."
        ],
        "tool_view": {
            "description": "ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that the user sends to the chatbot for generating a response.",
                    "required": true,
                    "example": "What is your name?"
                }
            },
            "response": "The tool produces a response representing the chatbot’s generated reply to a user-provided message. The response typically includes the following components:\n\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \n\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot’s attempt to provide a coherent, contextually relevant, and human-like answer.  \n\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \n\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \n\nIn essence, the response is a structured object that contains the user’s message, the chatbot’s reply, and optionally any historical context to support multi-turn conversation continuity.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat and ask the bot how its day is going.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Hi ShuanBot, how's your day going?"
                        }
                    }
                },
                {
                    "user_intent": "Ask the chatbot for advice on improving my productivity.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "Can you give me some tips to be more productive at work?"
                        }
                    }
                },
                {
                    "user_intent": "Start a conversation with ShuanBot about a fun topic like movies.",
                    "tool_call": {
                        "name": "mywateriswet/ShuanBot",
                        "arguments": {
                            "message": "What's your favorite movie genre and why?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 1: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 2: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 3: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 4: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 6: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 7: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 8: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.\nTool 9: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 10: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 11: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 12: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 13: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 14: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 15: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 16: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 17: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 18: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that the user sends to the chatbot for generating a response.\", \"required\": true, \"example\": \"What is your name?\"}}, \"response\": \"The tool produces a response representing the chatbot\\u2019s generated reply to a user-provided message. The response typically includes the following components:\\n\\n1. **User Input / Message**: The original text or query provided to the chatbot. This represents the conversational context or prompt that the model is responding to.  \\n\\n2. **Generated Response / Reply**: The text output produced by the model in response to the user input. This is the main content of the response and represents the chatbot\\u2019s attempt to provide a coherent, contextually relevant, and human-like answer.  \\n\\n3. **Conversation History (Optional)**: Depending on the implementation of the conversational pipeline, the response may include a record of previous exchanges in the chat session. This allows the model to maintain context across multiple turns of dialogue.  \\n\\n4. **Metadata (Optional)**: Some implementations may include additional information such as identifiers for the conversation, timestamps, or model-specific details (e.g., confidence scores or token-level information), though these are not explicitly required.  \\n\\nIn essence, the response is a structured object that contains the user\\u2019s message, the chatbot\\u2019s reply, and optionally any historical context to support multi-turn conversation continuity.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat and ask the bot how its day is going.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Hi ShuanBot, how's your day going?\"}}}, {\"user_intent\": \"Ask the chatbot for advice on improving my productivity.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"Can you give me some tips to be more productive at work?\"}}}, {\"user_intent\": \"Start a conversation with ShuanBot about a fun topic like movies.\", \"tool_call\": {\"name\": \"mywateriswet/ShuanBot\", \"arguments\": {\"message\": \"What's your favorite movie genre and why?\"}}}]}\n\n# User Query\nWe're creating a promotional image for a wildlife-themed event. We need to display two tigers in a natural setting.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an image of two tigers in a natural wildlife setting for a promotional event.",
                    "tool_description": "Create a high-quality, photorealistic or artistic image based on a text prompt describing the desired scene, including specific elements such as animals and environment.",
                    "expected_response": "An image file or image object representing the scene described in the prompt, suitable for use in promotional materials."
                }
            ],
            "extracted_arguments": {
                "scene_description": "A textual description of the image content, specifying the number of tigers and their natural setting.",
                "image_style": "The desired visual style or realism level for the generated image (e.g., photorealistic, artistic)."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_256",
        "query": "We have a new platform that offers various services related to digital art, and we want it to be able to produce creative story ideas based on a short description. Can you help me providing suggestions?",
        "instruction": "Given a `creative story generation` task, retrieve tools that leverage text generation models to produce creative and coherent story ideas by processing short descriptive inputs and aligning with the query's thematic requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "PyTorch Transformers",
                    "functionality": "Text Generation",
                    "api_call": "AutoModel.from_pretrained('decapoda-research/llama-7b-hf')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\n\ngen = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\n\nresult = gen('Once upon a time')\nprint(result[0]['generated_text'])",
                    "performance": {
                        "dataset": [
                            {
                                "name": "BoolQ",
                                "accuracy": 76.5
                            },
                            {
                                "name": "PIQA",
                                "accuracy": 79.8
                            },
                            {
                                "name": "SIQA",
                                "accuracy": 48.9
                            },
                            {
                                "name": "HellaSwag",
                                "accuracy": 76.1
                            },
                            {
                                "name": "WinoGrande",
                                "accuracy": 70.1
                            },
                            {
                                "name": "ARC-e",
                                "accuracy": 76.7
                            },
                            {
                                "name": "ARC-c",
                                "accuracy": 47.6
                            },
                            {
                                "name": "OBQAC",
                                "accuracy": 57.2
                            },
                            {
                                "name": "COPA",
                                "accuracy": 93
                            }
                        ]
                    },
                    "description": "LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English.",
                    "id": "huggingface_api_608",
                    "name": "decapoda-research/llama-7b-hf"
                },
                "id": "gorilla_huggingface_tool_603",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 3: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 4: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 5: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 6: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 7: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 8: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 9: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 10: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 11: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.",
            "Tool 12: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 13: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 15: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 16: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.",
            "Tool 17: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 18: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.",
            "Tool 19: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval."
        ],
        "tool_view": {
            "description": "GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text prompt that the model will continue or generate text from.",
                    "required": true,
                    "example": "Once upon a time"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "example": 50
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of independent text sequences to generate for the given input prompt.",
                    "required": true,
                    "example": 3
                }
            },
            "response": "The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\n\nHere’s a description of the response structure:\n\n1. **Generated Texts**: \n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \n   - Each sequence will represent a continuation or completion of the input text (e.g., \"Hello, I'm a language model,\").\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\n\n2. **Content of Each Text Generation**: \n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\n\n### Key Components:\n\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\n   - Each element in the array is a string, representing a complete generated text.\n\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\n\n### Example of Expected Output Content (Abstract):\n\n- A list of 5 strings, each one being a text sequence that continues from the input (\"Hello, I'm a language model,\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\n\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a distant galaxy,'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "Once upon a time in a distant galaxy,",
                            "max_length": 50,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Continue this sentence realistically: 'The stock market crashed yesterday because'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "The stock market crashed yesterday because",
                            "max_length": 40,
                            "num_return_sequences": 2
                        }
                    }
                },
                {
                    "user_intent": "Write multiple humorous completions for: 'I never expected my cat to'",
                    "tool_call": {
                        "name": "gpt2-large",
                        "arguments": {
                            "text": "I never expected my cat to",
                            "max_length": 30,
                            "num_return_sequences": 5
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 1: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 2: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 3: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 4: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 5: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 6: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 7: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 8: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 9: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 10: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 11: A YOLOv8 model designed for detecting and segmenting defects in printed circuit board (PCB) images. Trained on the pcb-defect-segmentation dataset, it can identify issues such as dry joints, incorrect installation, PCB damage, and short circuits, providing both bounding boxes and segmentation masks for precise defect localization. This tool is used to automate PCB quality inspection and defect analysis in manufacturing or testing workflows.\nTool 12: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 13: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 15: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 16: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.\nTool 17: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 18: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.\nTool 19: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text prompt that the model will continue or generate text from.\", \"required\": true, \"example\": \"Once upon a time\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"example\": 50}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of independent text sequences to generate for the given input prompt.\", \"required\": true, \"example\": 3}}, \"response\": \"The tool is designed to perform text generation using the GPT-2 Large model within a transformer-based framework. When this tool is invoked, the expected response structure will typically consist of a set of text generations based on the input prompt provided.\\n\\nHere\\u2019s a description of the response structure:\\n\\n1. **Generated Texts**: \\n   - This is the core of the response, consisting of multiple generated sequences of text. The tool generates these sequences based on the input prompt provided by the user. \\n   - Each sequence will represent a continuation or completion of the input text (e.g., \\\"Hello, I'm a language model,\\\").\\n   - The number of generated sequences is determined by the `num_return_sequences` argument, which in this case is set to 5. Therefore, the response will contain five distinct text completions or continuations.\\n\\n2. **Content of Each Text Generation**: \\n   - Each generated text sequence will be a string of text. It represents one possible output continuation based on the provided prompt.\\n   - The length of each generated text is constrained by the `max_length` parameter (30 tokens in this case). This defines the upper limit of tokens (words or subwords) in each generated sequence.\\n\\n### Key Components:\\n\\n- **Generated Texts Array**: A list of text sequences, each corresponding to one generated response based on the input prompt. \\n   - The array size corresponds to `num_return_sequences`, which is 5 in this case.\\n   - Each element in the array is a string, representing a complete generated text.\\n\\n- **Token Length**: The length of each generated text will not exceed the `max_length` parameter, which is set to 30 tokens.\\n\\n### Example of Expected Output Content (Abstract):\\n\\n- A list of 5 strings, each one being a text sequence that continues from the input (\\\"Hello, I'm a language model,\\\") and does so for up to 30 tokens, providing diverse completions based on the same prompt.\\n\\nThis response structure aligns with the described functionality and parameters of the tool, focusing on text generation using the GPT-2 Large model.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a distant galaxy,'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"Once upon a time in a distant galaxy,\", \"max_length\": 50, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Continue this sentence realistically: 'The stock market crashed yesterday because'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"The stock market crashed yesterday because\", \"max_length\": 40, \"num_return_sequences\": 2}}}, {\"user_intent\": \"Write multiple humorous completions for: 'I never expected my cat to'\", \"tool_call\": {\"name\": \"gpt2-large\", \"arguments\": {\"text\": \"I never expected my cat to\", \"max_length\": 30, \"num_return_sequences\": 5}}}]}\n\n# User Query\nWe have a new platform that offers various services related to digital art, and we want it to be able to produce creative story ideas based on a short description. Can you help me providing suggestions?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate creative story ideas based on a short text description.",
                    "tool_description": "Use a transformer-based language model to generate multiple coherent and imaginative story continuations from a given prompt.",
                    "expected_response": "A list of text sequences, each representing a distinct story idea or continuation based on the input description."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "A brief textual description or seed idea for the story, provided as a string.",
                "max_length": "The maximum number of tokens for each generated story idea, specified as a number.",
                "num_return_sequences": "The number of independent story ideas to generate, specified as a number."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_477",
        "query": "Our company wants to predict housing prices in the US based on given features. Help us use the trained model to predict the prices.",
        "instruction": "Given a `housing price prediction` task, retrieve tools that utilize trained regression models to process tabular data inputs, specifically housing features, and output predicted housing prices aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Joblib",
                    "functionality": "Single Column Regression",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": {
                        "data": "pandas.DataFrame"
                    },
                    "python_environment_requirements": {
                        "joblib": "latest",
                        "pandas": "latest"
                    },
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "jwan2021/autotrain-data-us-housing-prices",
                        "accuracy": {
                            "Loss": 134406.507,
                            "R2": 0.861,
                            "MSE": 18065109105.27,
                            "MAE": 103271.843,
                            "RMSLE": 0.139
                        }
                    },
                    "description": "A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.",
                    "id": "huggingface_api_871",
                    "name": "jwan2021/autotrain-us-housing-prices-1771761511"
                },
                "id": "gorilla_huggingface_tool_864",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 1: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 2: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.",
            "Tool 3: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.",
            "Tool 4: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.",
            "Tool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 6: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.",
            "Tool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 8: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.",
            "Tool 10: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.",
            "Tool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.",
            "Tool 12: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 13: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.",
            "Tool 14: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.",
            "Tool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 16: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.",
            "Tool 17: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 18: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 19: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features."
        ],
        "tool_view": {
            "description": "This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "parameters": {
                "data": {
                    "type": "pandas.DataFrame",
                    "description": "A pandas DataFrame containing the input data for the model. This data will be used for making predictions.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_1': [1, 2, 3], 'feat_2': [4, 5, 6]})"
                }
            },
            "response": "The response from this tool would contain the following key components:\n\n1. **Model Predictions**:\n   - **predictions**: This would be the primary output, containing the predicted values for the input data. It represents the model’s predicted values based on the input features from the provided data (typically in the form of a pandas DataFrame).\n   \n2. **Model Information**:\n   - **name**: The identifier or name of the model, in this case, \"jwan2021/autotrain-us-housing-prices-1771761511\".\n   - **description**: A textual description of the model, explaining that it was trained using AutoTrain on the US housing prices dataset, with the dataset ID and the model being a single column regression model.\n   - **framework**: Specifies the framework used to train the model, which is \"Joblib\" in this case.\n   \n3. **Performance Metrics**:\n   - **accuracy**: A dictionary containing various performance metrics that evaluate the model's predictive accuracy. These metrics might include:\n     - **Loss**: The loss value (such as Mean Squared Error, or MSE).\n     - **R2**: The R-squared value, which measures the proportion of variance in the dependent variable explained by the model.\n     - **MSE**: The Mean Squared Error, representing the average squared difference between actual and predicted values.\n     - **MAE**: The Mean Absolute Error, indicating the average absolute differences between predicted and actual values.\n     - **RMSLE**: The Root Mean Squared Logarithmic Error, which is a measure of prediction error for regression tasks where scale is important.\n\n4. **Python Environment Requirements**:\n   - **joblib**: Specifies the version of the joblib library required (the latest version).\n   - **pandas**: Specifies the version of the pandas library required (the latest version).\n\n5. **Example Code**:\n   - An example of how to use the tool, including:\n     - Loading the model using `joblib.load('model.joblib')`.\n     - Loading the configuration file (`config.json`) and selecting features.\n     - Reading input data from a CSV file using pandas and applying the necessary feature transformations (renaming columns).\n     - Using the model to make predictions on the transformed data.\n\nThe response is structured around providing not only the predictions but also key details about the model's configuration, performance metrics, and the environment needed to run it effectively.",
            "examples": [
                {
                    "user_intent": "Predict the prices of houses in my CSV file using the pre-trained US housing model.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "pd.read_csv('my_housing_data.csv')"
                        }
                    }
                },
                {
                    "user_intent": "I have a dataset with selected features and I want to get the model's predicted housing prices.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "data[['num_bedrooms','num_bathrooms','square_feet','zipcode']]"
                        }
                    }
                },
                {
                    "user_intent": "Run the single column regression model on a small DataFrame to estimate housing prices for testing.",
                    "tool_call": {
                        "name": "jwan2021/autotrain-us-housing-prices-1771761511",
                        "arguments": {
                            "data": "pd.DataFrame({'num_bedrooms':[3,4], 'num_bathrooms':[2,3], 'square_feet':[1500, 2200], 'zipcode':[94103, 94016]})"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 1: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 2: This tool is a single-column regression model, trained to predict US housing prices using the Joblib framework. It loads the trained model from a file and uses input data to generate predictions based on specified features. The model was trained with AutoTrain and can be applied in a Python environment with dependencies on Joblib and Pandas.\nTool 3: A Random Forest regression model implemented in Scikit-learn, designed to predict housing prices using tabular data, suitable for capturing complex relationships in datasets like the California Housing dataset.\nTool 4: A tabular regression model trained with AutoTrain to predict carbon emissions in grams from structured data, designed for use in analyzing environmental impact based on input features.\nTool 5: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 6: A tabular regression model designed to predict carbon emissions from input features, trained with AutoTrain and optimized for high accuracy in regression tasks. It can be loaded with Joblib and applied to structured data for generating predictions based on selected features.\nTool 7: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 8: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 9: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, designed to load with Joblib and make predictions on selected features from a dataset.\nTool 10: This tool is a binary classification model trained on the Adult dataset using AutoTrain, designed to predict CO2 emissions based on input features. It utilizes the Joblib framework for loading the model and requires libraries such as Joblib, Pandas, and JSON in the Python environment. The model can be used by loading the pre-trained model with `joblib.load('model.joblib')` and providing input data in CSV format. The tool is optimized to predict CO2 emissions with an accuracy of approximately 86%.\nTool 11: This tool provides a tabular regression model trained to predict carbon emissions based on input features. It is implemented using Joblib for model loading and relies on Python libraries such as Pandas and JSON for data processing. The model is loaded from a `.joblib` file and used to make predictions on specified features from a CSV input dataset. It is useful for applications that require predicting carbon emissions from structured data.\nTool 12: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 13: This tool provides a Decision Transformer model trained on expert trajectories from the Gym HalfCheetah environment, enabling reinforcement learning tasks that leverage sequence modeling to predict optimal actions. It is designed for use with the Hugging Face Transformers framework to facilitate decision-making in simulated control environments.\nTool 14: This tool uses a trained regression model to predict CO2 emissions in grams based on input data. It leverages the AutoTrain framework and is implemented using Joblib for model loading. The model requires a CSV file with specific features, which are processed and predicted using the loaded model. It is designed to work in a Python environment with the Joblib and Pandas libraries.\nTool 15: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 16: The \"lewtun/tiny-random-mt5\" is a small, pretrained text generation model based on the mT5 architecture, available for use with the Hugging Face Transformers framework. It is designed to generate text based on a given input.\nTool 17: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 18: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 19: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\", \"parameters\": {\"data\": {\"type\": \"pandas.DataFrame\", \"description\": \"A pandas DataFrame containing the input data for the model. This data will be used for making predictions.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_1': [1, 2, 3], 'feat_2': [4, 5, 6]})\"}}, \"response\": \"The response from this tool would contain the following key components:\\n\\n1. **Model Predictions**:\\n   - **predictions**: This would be the primary output, containing the predicted values for the input data. It represents the model\\u2019s predicted values based on the input features from the provided data (typically in the form of a pandas DataFrame).\\n   \\n2. **Model Information**:\\n   - **name**: The identifier or name of the model, in this case, \\\"jwan2021/autotrain-us-housing-prices-1771761511\\\".\\n   - **description**: A textual description of the model, explaining that it was trained using AutoTrain on the US housing prices dataset, with the dataset ID and the model being a single column regression model.\\n   - **framework**: Specifies the framework used to train the model, which is \\\"Joblib\\\" in this case.\\n   \\n3. **Performance Metrics**:\\n   - **accuracy**: A dictionary containing various performance metrics that evaluate the model's predictive accuracy. These metrics might include:\\n     - **Loss**: The loss value (such as Mean Squared Error, or MSE).\\n     - **R2**: The R-squared value, which measures the proportion of variance in the dependent variable explained by the model.\\n     - **MSE**: The Mean Squared Error, representing the average squared difference between actual and predicted values.\\n     - **MAE**: The Mean Absolute Error, indicating the average absolute differences between predicted and actual values.\\n     - **RMSLE**: The Root Mean Squared Logarithmic Error, which is a measure of prediction error for regression tasks where scale is important.\\n\\n4. **Python Environment Requirements**:\\n   - **joblib**: Specifies the version of the joblib library required (the latest version).\\n   - **pandas**: Specifies the version of the pandas library required (the latest version).\\n\\n5. **Example Code**:\\n   - An example of how to use the tool, including:\\n     - Loading the model using `joblib.load('model.joblib')`.\\n     - Loading the configuration file (`config.json`) and selecting features.\\n     - Reading input data from a CSV file using pandas and applying the necessary feature transformations (renaming columns).\\n     - Using the model to make predictions on the transformed data.\\n\\nThe response is structured around providing not only the predictions but also key details about the model's configuration, performance metrics, and the environment needed to run it effectively.\", \"examples\": [{\"user_intent\": \"Predict the prices of houses in my CSV file using the pre-trained US housing model.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"pd.read_csv('my_housing_data.csv')\"}}}, {\"user_intent\": \"I have a dataset with selected features and I want to get the model's predicted housing prices.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"data[['num_bedrooms','num_bathrooms','square_feet','zipcode']]\"}}}, {\"user_intent\": \"Run the single column regression model on a small DataFrame to estimate housing prices for testing.\", \"tool_call\": {\"name\": \"jwan2021/autotrain-us-housing-prices-1771761511\", \"arguments\": {\"data\": \"pd.DataFrame({'num_bedrooms':[3,4], 'num_bathrooms':[2,3], 'square_feet':[1500, 2200], 'zipcode':[94103, 94016]})\"}}}]}\n\n# User Query\nOur company wants to predict housing prices in the US based on given features. Help us use the trained model to predict the prices.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Use the pre-trained US housing price model to predict housing prices from our dataset.",
                    "tool_description": "Load a pre-trained single-column regression model trained on US housing prices and apply it to input data with specified features to generate predicted prices.",
                    "expected_response": "A structured output containing predicted housing prices, model information including name and description, performance metrics such as R2 and MAE, and environment requirements for using the model."
                }
            ],
            "extracted_arguments": {
                "data": "A pandas DataFrame containing the input features for which housing prices need to be predicted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_368",
        "query": "A photographer wants to create artistic interpretations of some of her pictures. Let's help her to get variated styles of her image.",
        "instruction": "Given an `artistic image transformation` task, retrieve tools that process input images to create artistic variations by applying different styles or representations, aligned with the task's creative requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Image-to-Image",
                    "api_call": "pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')",
                    "api_arguments": {
                        "input_image": "path/to/image/file"
                    },
                    "python_environment_requirements": {
                        "huggingface_hub": ">=0.0.17",
                        "transformers": ">=4.13.0",
                        "torch": ">=1.10.0"
                    },
                    "example_code": "",
                    "performance": {
                        "dataset": "poloclub/diffusiondb",
                        "accuracy": "Not provided"
                    },
                    "description": "SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.",
                    "id": "huggingface_api_282",
                    "name": "GreeneryScenery/SheepsControlV5"
                },
                "id": "gorilla_huggingface_tool_278",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 1: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.",
            "Tool 2: Lykon/DreamShaper is a text-to-image model that generates artistic images based on input text. It leverages the Hugging Face framework and is implemented using the transformers and torch libraries.",
            "Tool 3: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 4: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 7: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 8: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 9: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 10: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 11: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 12: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 13: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 16: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 17: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 18: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts."
        ],
        "tool_view": {
            "description": "A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "parameters": {},
            "response": "The tool produces a generated image as its output. The response structure is as follows:  \n\n- **Primary output**: An image object representing a newly generated butterfly image. This image is created unconditionally, meaning it is generated without any specific text prompt or input conditioning.  \n- **Data format**: The image is typically returned as an object compatible with standard Python imaging libraries (e.g., a PIL Image object). Users can directly manipulate, display, or save this image.  \n- **Semantic meaning**: Each output image is a unique sample produced by the diffusion model, reflecting the model’s learned distribution of “cute butterflies.” There is no deterministic mapping from input to output; every invocation can produce a different visual result.  \n\nNo additional metadata, accuracy scores, or auxiliary information are explicitly returned by the pipeline in its basic form. The core response is the generated image itself.",
            "examples": [
                {
                    "user_intent": "Generate a random image of a cute butterfly for my desktop wallpaper.",
                    "tool_call": {
                        "name": "ntrant7/sd-class-butterflies-32",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "I want to create multiple unique butterfly images for a children's book illustration.",
                    "tool_call": {
                        "name": "ntrant7/sd-class-butterflies-32",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Produce a high-resolution butterfly image for use in a digital art project.",
                    "tool_call": {
                        "name": "ntrant7/sd-class-butterflies-32",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 1: SheepsControlV5 is an image-to-image model that transforms input images into different styles or representations, allowing users to modify or stylize images based on the patterns it learned from the poloclub/diffusiondb dataset.\nTool 2: Lykon/DreamShaper is a text-to-image model that generates artistic images based on input text. It leverages the Hugging Face framework and is implemented using the transformers and torch libraries.\nTool 3: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 4: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 5: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 6: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 7: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 8: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 9: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 10: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 11: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 12: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 13: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 14: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 16: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 17: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 18: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 19: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\", \"parameters\": {}, \"response\": \"The tool produces a generated image as its output. The response structure is as follows:  \\n\\n- **Primary output**: An image object representing a newly generated butterfly image. This image is created unconditionally, meaning it is generated without any specific text prompt or input conditioning.  \\n- **Data format**: The image is typically returned as an object compatible with standard Python imaging libraries (e.g., a PIL Image object). Users can directly manipulate, display, or save this image.  \\n- **Semantic meaning**: Each output image is a unique sample produced by the diffusion model, reflecting the model\\u2019s learned distribution of \\u201ccute butterflies.\\u201d There is no deterministic mapping from input to output; every invocation can produce a different visual result.  \\n\\nNo additional metadata, accuracy scores, or auxiliary information are explicitly returned by the pipeline in its basic form. The core response is the generated image itself.\", \"examples\": [{\"user_intent\": \"Generate a random image of a cute butterfly for my desktop wallpaper.\", \"tool_call\": {\"name\": \"ntrant7/sd-class-butterflies-32\", \"arguments\": {}}}, {\"user_intent\": \"I want to create multiple unique butterfly images for a children's book illustration.\", \"tool_call\": {\"name\": \"ntrant7/sd-class-butterflies-32\", \"arguments\": {}}}, {\"user_intent\": \"Produce a high-resolution butterfly image for use in a digital art project.\", \"tool_call\": {\"name\": \"ntrant7/sd-class-butterflies-32\", \"arguments\": {}}}]}\n\n# User Query\nA photographer wants to create artistic interpretations of some of her pictures. Let's help her to get variated styles of her image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to transform an image into a different artistic style.",
                    "tool_description": "Use an image-to-image model to transform an input image into a different artistic style or representation, based on the model's learned patterns.",
                    "expected_response": "A transformed version of the input image in the requested style."
                }
            ],
            "extracted_arguments": {
                "input_image": "The original image to be transformed.",
                "style": "The desired artistic style or representation to be applied to the input image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_442",
        "query": "Our energy company is trying to identify anomalies in the energy consumption data. Could you perform anomaly detection on the time series data?",
        "instruction": "Given an `anomaly detection` task, retrieve tools designed to identify irregular patterns in time series data by leveraging advanced techniques such as reconstruction convolutional autoencoder models to analyze and detect deviations in ordered, timestamped metrics.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Regression",
                    "framework": "Keras",
                    "functionality": "anomaly-detection",
                    "api_call": "TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')",
                    "api_arguments": {
                        "optimizer": {
                            "name": "Adam",
                            "learning_rate": 0.001,
                            "decay": 0.0,
                            "beta_1": 0.9,
                            "beta_2": 0.999,
                            "epsilon": 1e-07,
                            "amsgrad": false
                        },
                        "training_precision": "float32"
                    },
                    "python_environment_requirements": [
                        "tensorflow",
                        "keras"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "Numenta Anomaly Benchmark(NAB)",
                        "accuracy": {
                            "Train Loss": 0.006,
                            "Validation Loss": 0.008
                        }
                    },
                    "description": "This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.",
                    "id": "huggingface_api_869",
                    "name": "keras-io/timeseries-anomaly-detection"
                },
                "id": "gorilla_huggingface_tool_862",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.",
            "Tool 3: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 4: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 7: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.",
            "Tool 9: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.",
            "Tool 10: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 11: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 12: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 13: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.",
            "Tool 14: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 15: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 16: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.",
            "Tool 18: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 19: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data."
        ],
        "tool_view": {
            "description": "This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "parameters": {
                "optimizer": {
                    "type": "object",
                    "description": "Configuration object for the optimizer used during model training, including type and hyperparameters.",
                    "required": true,
                    "example": {
                        "name": "Adam",
                        "learning_rate": 0.001,
                        "decay": 0.0,
                        "beta_1": 0.9,
                        "beta_2": 0.999,
                        "epsilon": 1e-07,
                        "amsgrad": false
                    }
                },
                "training_precision": {
                    "type": "string",
                    "description": "The numerical precision to be used during training for model computations.",
                    "required": true,
                    "example": "float32"
                }
            },
            "response": "The tool provides a model for anomaly detection in time-series data using a reconstruction convolutional autoencoder. The response structure would typically contain the following components:\n\n1. **Model Information**: \n   - **Framework**: Indicates the underlying framework used for this tool, which is Keras in this case.\n   - **Functionality**: Describes the purpose of the model, which is anomaly detection in time-series data.\n\n2. **API Call and Arguments**: \n   - **API Call**: This specifies the function that loads the pre-trained model (`TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')`).\n   - **API Arguments**: Lists the configuration details for the model:\n     - **Optimizer**: Parameters of the Adam optimizer, including `learning_rate`, `decay`, `beta_1`, `beta_2`, `epsilon`, and `amsgrad`.\n     - **Training Precision**: Specifies the numerical precision used in training (e.g., `float32`).\n\n3. **Python Environment Requirements**: \n   - Lists the necessary libraries for running the tool, specifically `tensorflow` and `keras`.\n\n4. **Performance Metrics**: \n   - **Dataset**: The dataset used for evaluation, in this case, the \"Numenta Anomaly Benchmark (NAB)\".\n   - **Accuracy**: Contains performance details, such as training and validation loss:\n     - **Train Loss**: The loss value on the training set.\n     - **Validation Loss**: The loss value on the validation set.\n\n5. **Description**: \n   - Provides a detailed explanation of what the model does. It emphasizes that the tool uses a convolutional autoencoder for anomaly detection on time-series data, with a specific focus on the NAB dataset, which includes artificial time-series data with labeled anomalies.\n\n6. **Model Name**: \n   - Specifies the identifier of the pre-trained model (`keras-io/timeseries-anomaly-detection`). \n\nThe response would primarily be focused on providing this structured information to explain the model's functionality, setup, performance, and requirements for users interested in anomaly detection tasks using time-series data.",
            "examples": [
                {
                    "user_intent": "I want to use the pre-trained model for anomaly detection on my timeseries data.",
                    "tool_call": {
                        "api_call": "TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')",
                        "api_arguments": {
                            "optimizer": {
                                "name": "Adam",
                                "learning_rate": 0.001,
                                "decay": 0.0,
                                "beta_1": 0.9,
                                "beta_2": 0.999,
                                "epsilon": 1e-07,
                                "amsgrad": false
                            },
                            "training_precision": "float32"
                        }
                    }
                },
                {
                    "user_intent": "I need to train an anomaly detection model on the NAB dataset using the Keras framework.",
                    "tool_call": {
                        "api_call": "TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')",
                        "api_arguments": {
                            "optimizer": {
                                "name": "Adam",
                                "learning_rate": 0.001,
                                "decay": 0.0,
                                "beta_1": 0.9,
                                "beta_2": 0.999,
                                "epsilon": 1e-07,
                                "amsgrad": false
                            },
                            "training_precision": "float32"
                        }
                    }
                },
                {
                    "user_intent": "Can I use this model to detect anomalies in my time series dataset with TensorFlow and Keras?",
                    "tool_call": {
                        "api_call": "TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')",
                        "api_arguments": {
                            "optimizer": {
                                "name": "Adam",
                                "learning_rate": 0.001,
                                "decay": 0.0,
                                "beta_1": 0.9,
                                "beta_2": 0.999,
                                "epsilon": 1e-07,
                                "amsgrad": false
                            },
                            "training_precision": "float32"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: A Scikit-learn RandomForestRegressor model designed to predict electricity consumption, using an ensemble of decision trees to improve regression accuracy and handle complex patterns in tabular data.\nTool 3: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 4: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 5: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 6: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 7: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 8: The \"xm_transformer_s2ut_en-hk\" is a speech-to-speech translation model from Fairseq, designed to convert English speech into Hokkien. It uses a single-pass decoder (S2UT) and is trained with both supervised data in the TED domain and weakly supervised data from TED and Audiobook domains. This model is ideal for applications requiring real-time or batch translation of spoken English into Hokkien.\nTool 9: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.\nTool 10: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 11: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 12: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 13: A RoBERTa-based model fine-tuned on SQuAD-style CORD-19 data for extractive question answering in the context of COVID-19, designed to identify precise answers from text using the Hugging Face Transformers library.\nTool 14: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 15: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 16: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 17: A text classification tool based on a distilled DistilBERT model trained on the GoEmotions dataset, designed to efficiently identify emotions in text. It provides a lightweight alternative to full zero-shot NLI models, enabling accurate emotion classification using only unlabeled data.\nTool 18: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 19: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\", \"parameters\": {\"optimizer\": {\"type\": \"object\", \"description\": \"Configuration object for the optimizer used during model training, including type and hyperparameters.\", \"required\": true, \"example\": {\"name\": \"Adam\", \"learning_rate\": 0.001, \"decay\": 0.0, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": false}}, \"training_precision\": {\"type\": \"string\", \"description\": \"The numerical precision to be used during training for model computations.\", \"required\": true, \"example\": \"float32\"}}, \"response\": \"The tool provides a model for anomaly detection in time-series data using a reconstruction convolutional autoencoder. The response structure would typically contain the following components:\\n\\n1. **Model Information**: \\n   - **Framework**: Indicates the underlying framework used for this tool, which is Keras in this case.\\n   - **Functionality**: Describes the purpose of the model, which is anomaly detection in time-series data.\\n\\n2. **API Call and Arguments**: \\n   - **API Call**: This specifies the function that loads the pre-trained model (`TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')`).\\n   - **API Arguments**: Lists the configuration details for the model:\\n     - **Optimizer**: Parameters of the Adam optimizer, including `learning_rate`, `decay`, `beta_1`, `beta_2`, `epsilon`, and `amsgrad`.\\n     - **Training Precision**: Specifies the numerical precision used in training (e.g., `float32`).\\n\\n3. **Python Environment Requirements**: \\n   - Lists the necessary libraries for running the tool, specifically `tensorflow` and `keras`.\\n\\n4. **Performance Metrics**: \\n   - **Dataset**: The dataset used for evaluation, in this case, the \\\"Numenta Anomaly Benchmark (NAB)\\\".\\n   - **Accuracy**: Contains performance details, such as training and validation loss:\\n     - **Train Loss**: The loss value on the training set.\\n     - **Validation Loss**: The loss value on the validation set.\\n\\n5. **Description**: \\n   - Provides a detailed explanation of what the model does. It emphasizes that the tool uses a convolutional autoencoder for anomaly detection on time-series data, with a specific focus on the NAB dataset, which includes artificial time-series data with labeled anomalies.\\n\\n6. **Model Name**: \\n   - Specifies the identifier of the pre-trained model (`keras-io/timeseries-anomaly-detection`). \\n\\nThe response would primarily be focused on providing this structured information to explain the model's functionality, setup, performance, and requirements for users interested in anomaly detection tasks using time-series data.\", \"examples\": [{\"user_intent\": \"I want to use the pre-trained model for anomaly detection on my timeseries data.\", \"tool_call\": {\"api_call\": \"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\", \"api_arguments\": {\"optimizer\": {\"name\": \"Adam\", \"learning_rate\": 0.001, \"decay\": 0.0, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": false}, \"training_precision\": \"float32\"}}}, {\"user_intent\": \"I need to train an anomaly detection model on the NAB dataset using the Keras framework.\", \"tool_call\": {\"api_call\": \"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\", \"api_arguments\": {\"optimizer\": {\"name\": \"Adam\", \"learning_rate\": 0.001, \"decay\": 0.0, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": false}, \"training_precision\": \"float32\"}}}, {\"user_intent\": \"Can I use this model to detect anomalies in my time series dataset with TensorFlow and Keras?\", \"tool_call\": {\"api_call\": \"TFAutoModelForSequenceClassification.from_pretrained('keras-io/timeseries-anomaly-detection')\", \"api_arguments\": {\"optimizer\": {\"name\": \"Adam\", \"learning_rate\": 0.001, \"decay\": 0.0, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": false}, \"training_precision\": \"float32\"}}}]}\n\n# User Query\nOur energy company is trying to identify anomalies in the energy consumption data. Could you perform anomaly detection on the time series data?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect anomalies in our energy consumption time series data.",
                    "tool_description": "Use a Keras-based convolutional autoencoder model to analyze ordered, timestamped energy consumption metrics and identify unusual patterns or deviations.",
                    "expected_response": "A structured output indicating detected anomalies in the time series data, including anomaly scores or flagged timestamps."
                }
            ],
            "extracted_arguments": {
                "time_series_data": "A sequential dataset representing energy consumption measurements over time, suitable for input into a time series anomaly detection model.",
                "optimizer": "Configuration object specifying the optimization algorithm and hyperparameters for the model training.",
                "training_precision": "The numerical precision (e.g., float32) to be used during model computations."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_1",
        "query": "We want to implement a customer service chatbot to engage with website visitors and provide support.",
        "instruction": "Given a `chatbot implementation` task, retrieve tools that facilitate the creation of a conversational agent by processing text-based interactions and generating suitable responses to engage with users on a website, while ensuring support and continuous engagement.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Transformers",
                    "functionality": "Conversational",
                    "api_call": "BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')",
                    "api_arguments": "['message']",
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "Input a message to start chatting with facebook/blenderbot-400M-distill.",
                    "performance": {
                        "dataset": "blended_skill_talk",
                        "accuracy": "Not specified"
                    },
                    "description": "BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.",
                    "id": "huggingface_api_572",
                    "name": "facebook/blenderbot-400M-distill"
                },
                "id": "gorilla_huggingface_tool_568",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 1: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 2: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 6: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 7: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 9: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 11: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 13: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 14: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 15: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 16: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 17: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 18: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 19: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text."
        ],
        "tool_view": {
            "description": "BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input message or query that will initiate the conversation with the Blenderbot model.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \n\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model’s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \n\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \n\nIn practice, the tool accepts a single input, `\"message\"`, and outputs a single text string representing the chatbot’s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.",
            "examples": [
                {
                    "user_intent": "I want to have a friendly chat about movies and get some recommendations.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hi! Can you suggest some good movies to watch this weekend?"
                        }
                    }
                },
                {
                    "user_intent": "I'm feeling a bit down and need someone to talk to for comfort.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "I'm feeling sad today. Can we talk for a bit?"
                        }
                    }
                },
                {
                    "user_intent": "I want to have a casual conversation about hobbies and interests.",
                    "tool_call": {
                        "name": "facebook/blenderbot_small-90M",
                        "arguments": {
                            "message": "Hey! What are some fun hobbies you think I should try?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 1: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 2: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 3: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 6: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 7: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 8: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 9: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 11: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 12: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 13: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 14: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 15: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 16: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 17: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 18: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 19: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input message or query that will initiate the conversation with the Blenderbot model.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a conversational response generated by the BlenderBot chatbot model. The response structure can be described as follows:  \\n\\n- **Text content**: The primary component is the textual reply from the chatbot, which is intended to be contextually relevant to the input message. This text reflects the model\\u2019s attempt to engage naturally, incorporating knowledge, empathy, personality, and appropriate conversational cues.  \\n\\n- **Optional metadata (typical in text-generation frameworks)**: While not explicitly defined in the JSON, responses from Hugging Face text-to-text models often include additional fields such as:  \\n  - **generated tokens or IDs**: The underlying token sequence used to produce the text.  \\n  - **generation info**: Parameters or scores related to the generation process, such as probabilities, beam search details, or sequence length.  \\n\\nIn practice, the tool accepts a single input, `\\\"message\\\"`, and outputs a single text string representing the chatbot\\u2019s reply. This reply is the main actionable content for downstream use, such as displaying to a user in a conversational interface.\", \"examples\": [{\"user_intent\": \"I want to have a friendly chat about movies and get some recommendations.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hi! Can you suggest some good movies to watch this weekend?\"}}}, {\"user_intent\": \"I'm feeling a bit down and need someone to talk to for comfort.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"I'm feeling sad today. Can we talk for a bit?\"}}}, {\"user_intent\": \"I want to have a casual conversation about hobbies and interests.\", \"tool_call\": {\"name\": \"facebook/blenderbot_small-90M\", \"arguments\": {\"message\": \"Hey! What are some fun hobbies you think I should try?\"}}}]}\n\n# User Query\nWe want to implement a customer service chatbot to engage with website visitors and provide support.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to set up a conversational AI to handle customer inquiries on my website.",
                    "tool_description": "Deploy a BlenderBot-based conversational chatbot that can engage website visitors in natural, multi-turn dialogue, providing support, answering questions, and responding appropriately with knowledge, empathy, and personality.",
                    "expected_response": "A conversational interface capable of generating context-aware text responses to visitor messages, with each response including the textual reply and optional generation metadata."
                }
            ],
            "extracted_arguments": {
                "message": "The input message from a website visitor that initiates the conversation with the chatbot."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_232",
        "query": "### Instruction: I teach at a school, in a natural language processing subject and I want to improve the readability and grammaticality of the provided sentence by suggesting the best replacement for the masked part.",
        "instruction": "Given a `sentence enhancement` task, retrieve tools focused on natural language processing that can improve sentence readability and grammaticality by suggesting contextually appropriate replacements for masked words or phrases based on high accuracy language models.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Transformers",
                    "functionality": "Fill-Mask",
                    "api_call": "DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')",
                    "api_arguments": "Mask token: [MASK]",
                    "python_environment_requirements": "PyTorch, TensorFlow",
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQuAD 1.1",
                                "accuracy": "95.8/90.8"
                            },
                            {
                                "name": "SQuAD 2.0",
                                "accuracy": "91.4/88.9"
                            },
                            {
                                "name": "MNLI-m/mm",
                                "accuracy": "91.7/91.6"
                            },
                            {
                                "name": "SST-2",
                                "accuracy": "97.5"
                            },
                            {
                                "name": "QNLI",
                                "accuracy": "95.8"
                            },
                            {
                                "name": "CoLA",
                                "accuracy": "71.1"
                            },
                            {
                                "name": "RTE",
                                "accuracy": "93.9"
                            },
                            {
                                "name": "MRPC",
                                "accuracy": "92.0/94.2"
                            },
                            {
                                "name": "QQP",
                                "accuracy": "92.3/89.8"
                            },
                            {
                                "name": "STS-B",
                                "accuracy": "92.9/92.9"
                            }
                        ]
                    },
                    "description": "DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.",
                    "id": "huggingface_api_683",
                    "name": "microsoft/deberta-v2-xlarge"
                },
                "id": "gorilla_huggingface_tool_678",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 2: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 3: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 4: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 5: IndoBERT is a pretrained BERT-based language model for Indonesian that generates contextualized representations of text. It is trained using masked language modeling and next sentence prediction, making it useful for natural language understanding tasks, feature extraction, and embedding Indonesian text in downstream NLP applications.",
            "Tool 6: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.",
            "Tool 7: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 8: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 9: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 10: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.",
            "Tool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 12: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.",
            "Tool 13: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 14: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.",
            "Tool 15: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.",
            "Tool 16: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 17: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 18: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.",
            "Tool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models."
        ],
        "tool_view": {
            "description": "This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer to be used with the model, typically initialized from a pretrained model checkpoint. This ensures the input text is properly tokenized for the model.",
                    "required": true,
                    "example": "BertTokenizerFast.from_pretrained('bert-base-chinese')"
                }
            },
            "response": "The tool produces a response that consists of token-level part-of-speech (POS) annotations for a given input text. The response is typically structured as a sequence where each element corresponds to a single token from the input text. Each element in the sequence includes at least the following components:  \n\n1. **Token**: The individual word or subword unit from the input text as processed by the tokenizer.  \n2. **POS tag**: The part-of-speech label assigned to the token, indicating its grammatical category (e.g., noun, verb, adjective) according to the tagging scheme used by the model.  \n\nOptionally, the response may also include:  \n\n- **Confidence score**: A probability or score representing the model’s confidence in the predicted POS tag for each token.  \n- **Token indices**: The start and end positions of the token in the original input text, useful for alignment or downstream processing.  \n\nOverall, the response provides a detailed mapping of the input text into token-level grammatical categories, suitable for tasks such as syntactic analysis, linguistic feature extraction, or preprocessing for downstream NLP applications.",
            "examples": [
                {
                    "user_intent": "I want to tag each word in a traditional Chinese sentence with its part of speech.",
                    "tool_call": {
                        "name": "ckiplab/bert-base-chinese-pos",
                        "arguments": {
                            "tokenizer": "BertTokenizerFast.from_pretrained('bert-base-chinese')",
                            "sentence": "今天天氣很好，我想去公園散步。"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the part-of-speech structure of a Chinese news article paragraph.",
                    "tool_call": {
                        "name": "ckiplab/bert-base-chinese-pos",
                        "arguments": {
                            "tokenizer": "BertTokenizerFast.from_pretrained('bert-base-chinese')",
                            "sentence": "台北市政府今天宣布新的交通管制措施，以減少市區擁堵。"
                        }
                    }
                },
                {
                    "user_intent": "Check the grammatical components of multiple Chinese sentences for NLP preprocessing.",
                    "tool_call": {
                        "name": "ckiplab/bert-base-chinese-pos",
                        "arguments": {
                            "tokenizer": "BertTokenizerFast.from_pretrained('bert-base-chinese')",
                            "sentence": "小明在圖書館讀書，他非常專心。老師表揚了他的努力。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 2: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 3: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 4: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 5: IndoBERT is a pretrained BERT-based language model for Indonesian that generates contextualized representations of text. It is trained using masked language modeling and next sentence prediction, making it useful for natural language understanding tasks, feature extraction, and embedding Indonesian text in downstream NLP applications.\nTool 6: LaBSE (Language-agnostic BERT Sentence Encoder) is a BERT-based model designed to generate high-quality sentence embeddings across 109 languages. It combines masked language modeling and translation language modeling during pre-training, making it suitable for multilingual sentence similarity, cross-lingual retrieval, and other tasks that require language-independent sentence representations.\nTool 7: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 8: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 9: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 10: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.\nTool 11: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 12: This tool provides a pre-trained BERT-based model for generating sentence embeddings in Russian, enabling the extraction of meaningful vector representations from text for use in natural language processing tasks such as semantic similarity, clustering, or classification.\nTool 13: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 14: A pretrained BERT base (uncased) transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and analyze sentence relationships, and serving as a foundation for fine-tuning on tasks like sequence classification, token classification, or question answering.\nTool 15: This tool uses the CodeTrans T5-base model to generate natural-language documentation for Python functions from tokenized code. It is designed to produce clear descriptions of code functionality and can also be fine-tuned for other Python code-related tasks, making it useful for automating code documentation and improving code readability.\nTool 16: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 17: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 18: A BERT large uncased transformer model designed for masked language modeling and next sentence prediction, allowing users to predict missing words in text and understand sentence relationships. It is pretrained on a large English corpus and can be fine-tuned for downstream NLP tasks such as sequence classification, token classification, or question answering.\nTool 19: FLAN-T5 large is a language model fine-tuned on over 1,000 tasks and multiple languages, achieving state-of-the-art performance on benchmarks like the MMLU with 75.2% accuracy in a few-shot setting. Based on the pretrained T5 architecture, it has been enhanced with instructions to improve zero-shot and few-shot performance, making it suitable for a wide range of natural language processing tasks, including reasoning, question answering, and research focused on fairness and safety in language models.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \\\"ckiplab/bert-base-chinese-pos.\\\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer to be used with the model, typically initialized from a pretrained model checkpoint. This ensures the input text is properly tokenized for the model.\", \"required\": true, \"example\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\"}}, \"response\": \"The tool produces a response that consists of token-level part-of-speech (POS) annotations for a given input text. The response is typically structured as a sequence where each element corresponds to a single token from the input text. Each element in the sequence includes at least the following components:  \\n\\n1. **Token**: The individual word or subword unit from the input text as processed by the tokenizer.  \\n2. **POS tag**: The part-of-speech label assigned to the token, indicating its grammatical category (e.g., noun, verb, adjective) according to the tagging scheme used by the model.  \\n\\nOptionally, the response may also include:  \\n\\n- **Confidence score**: A probability or score representing the model\\u2019s confidence in the predicted POS tag for each token.  \\n- **Token indices**: The start and end positions of the token in the original input text, useful for alignment or downstream processing.  \\n\\nOverall, the response provides a detailed mapping of the input text into token-level grammatical categories, suitable for tasks such as syntactic analysis, linguistic feature extraction, or preprocessing for downstream NLP applications.\", \"examples\": [{\"user_intent\": \"I want to tag each word in a traditional Chinese sentence with its part of speech.\", \"tool_call\": {\"name\": \"ckiplab/bert-base-chinese-pos\", \"arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\", \"sentence\": \"\\u4eca\\u5929\\u5929\\u6c23\\u5f88\\u597d\\uff0c\\u6211\\u60f3\\u53bb\\u516c\\u5712\\u6563\\u6b65\\u3002\"}}}, {\"user_intent\": \"Analyze the part-of-speech structure of a Chinese news article paragraph.\", \"tool_call\": {\"name\": \"ckiplab/bert-base-chinese-pos\", \"arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\", \"sentence\": \"\\u53f0\\u5317\\u5e02\\u653f\\u5e9c\\u4eca\\u5929\\u5ba3\\u5e03\\u65b0\\u7684\\u4ea4\\u901a\\u7ba1\\u5236\\u63aa\\u65bd\\uff0c\\u4ee5\\u6e1b\\u5c11\\u5e02\\u5340\\u64c1\\u5835\\u3002\"}}}, {\"user_intent\": \"Check the grammatical components of multiple Chinese sentences for NLP preprocessing.\", \"tool_call\": {\"name\": \"ckiplab/bert-base-chinese-pos\", \"arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\", \"sentence\": \"\\u5c0f\\u660e\\u5728\\u5716\\u66f8\\u9928\\u8b80\\u66f8\\uff0c\\u4ed6\\u975e\\u5e38\\u5c08\\u5fc3\\u3002\\u8001\\u5e2b\\u8868\\u63da\\u4e86\\u4ed6\\u7684\\u52aa\\u529b\\u3002\"}}}]}\n\n# User Query\n### Instruction: I teach at a school, in a natural language processing subject and I want to improve the readability and grammaticality of the provided sentence by suggesting the best replacement for the masked part.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict the best word or phrase to replace a masked part in a sentence to improve readability and grammar.",
                    "tool_description": "Use a pre-trained masked language model to analyze a sentence with a masked token and suggest the most contextually appropriate replacements.",
                    "expected_response": "A ranked list of candidate words or phrases for the masked position, each with a probability or confidence score indicating model preference."
                }
            ],
            "extracted_arguments": {
                "masked_sentence": "A string containing the input sentence with one or more masked tokens indicating the part to be replaced."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_80",
        "query": "I am working for the review section of a book company. I want to convert a book summary into a positive book review.",
        "instruction": "Given a `text conversion` task, retrieve tools that perform sentiment analysis and text generation by processing a book summary to transform it into a positively embellished book review.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation, Summarization, Question Answering, Sentiment Analysis",
                    "api_call": "T5ForConditionalGeneration.from_pretrained('t5-3b')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "input_text = 'translate English to French: The quick brown fox jumps over the lazy dog'; inputs = tokenizer.encode(input_text, return_tensors='pt'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "See research paper, Table 14"
                    },
                    "description": "T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.",
                    "id": "huggingface_api_535",
                    "name": "t5-3b"
                },
                "id": "gorilla_huggingface_tool_531",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 3: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 4: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 5: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 6: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 7: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 8: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 9: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.",
            "Tool 10: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.",
            "Tool 12: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 13: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 14: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.",
            "Tool 15: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 16: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.",
            "Tool 17: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.",
            "Tool 18: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 19: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications."
        ],
        "tool_view": {
            "description": "A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "parameters": {
                "data": {
                    "type": "dataframe",
                    "description": "A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.",
                    "required": true,
                    "example": "pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})"
                }
            },
            "response": "The tool response is expected to contain the following elements:\n\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \"negative\" or \"positive\" sentiment in the context of sentiment analysis).\n\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\n\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\n\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\n\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.",
            "examples": [
                {
                    "user_intent": "I want to predict whether the movie reviews in my CSV file are positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "movie_reviews.csv"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a batch of new reviews we just collected from our website.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "new_reviews_batch.csv"
                        }
                    }
                },
                {
                    "user_intent": "I have a CSV file with user comments, and I need the model to classify each as positive or negative.",
                    "tool_call": {
                        "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085",
                        "arguments": {
                            "data": "user_comments.csv"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 3: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 4: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 5: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 6: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 7: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 8: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 9: A Marathi Male Text-to-Speech model based on the ESPnet framework, designed to convert text into spoken Marathi language.\nTool 10: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 11: A Korean text-to-speech model trained using the ESPnet Novelspeech recipe, designed to convert Korean text into natural-sounding spoken audio.\nTool 12: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 13: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 14: Convert written text into spoken audio using the ESPnet-based \"imdanboy/jets\" model, which was trained on the LJSpeech dataset, allowing developers to generate realistic speech from text.\nTool 15: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 16: This tool uses the `sentence-transformers/paraphrase-distilroberta-base-v2` model to convert sentences or paragraphs into 768-dimensional dense vector embeddings, enabling applications such as semantic similarity comparison, clustering, and semantic search.\nTool 17: This tool provides a speech-to-speech translation model, allowing on-demand translation of spoken language using the Fairseq framework. It utilizes the \"facebook/xm_transformer_sm_all-en\" model to convert speech input into translated speech output.\nTool 18: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 19: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\", \"parameters\": {\"data\": {\"type\": \"dataframe\", \"description\": \"A pandas DataFrame containing the input features for the model. Each column should correspond to a feature used during training, and the columns must be preprocessed as expected by the model.\", \"required\": true, \"example\": \"pd.DataFrame({'feat_0': [0.1, 0.5], 'feat_1': [1.2, 3.4]})\"}}, \"response\": \"The tool response is expected to contain the following elements:\\n\\n1. **Predictions**: This will be the main output of the model's inference. It is expected to include the predicted labels for the input data provided in the `data` argument. Each label corresponds to a binary classification result (e.g., 0 or 1, representing categories such as \\\"negative\\\" or \\\"positive\\\" sentiment in the context of sentiment analysis).\\n\\n2. **Confidence Scores (optional)**: While not explicitly required by the documentation, a typical binary classification model may return confidence scores or probabilities associated with each prediction. These would reflect the model's certainty about each classification decision, usually in the form of probabilities between 0 and 1 for each class (e.g., the likelihood that a given review is positive or negative).\\n\\n3. **Metadata (optional)**: The response may include additional metadata about the prediction process. This could encompass things like the model version, processing time, or other relevant information that may assist with interpreting the results. However, this is not explicitly mentioned in the documentation, so it would be inferred as a common practice.\\n\\n4. **Model Information (optional)**: Although not required, there might be an inclusion of model-related information like the model's name (`desertdev/autotrain-imdb-sentiment-analysis-44994113085`), accuracy (`0.565`), and possibly any other relevant details about the training dataset (e.g., `desertdev/autotrain-data-imdb-sentiment-analysis`) to provide context for the predictions made.\\n\\nIn summary, the response would contain the binary classification results (predicted labels for the input data), potentially accompanied by confidence scores and additional metadata for the user to understand the predictions and their context.\", \"examples\": [{\"user_intent\": \"I want to predict whether the movie reviews in my CSV file are positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"movie_reviews.csv\"}}}, {\"user_intent\": \"Check the sentiment of a batch of new reviews we just collected from our website.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"new_reviews_batch.csv\"}}}, {\"user_intent\": \"I have a CSV file with user comments, and I need the model to classify each as positive or negative.\", \"tool_call\": {\"name\": \"desertdev/autotrain-imdb-sentiment-analysis-44994113085\", \"arguments\": {\"data\": \"user_comments.csv\"}}}]}\n\n# User Query\nI am working for the review section of a book company. I want to convert a book summary into a positive book review.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a book review from a book summary.",
                    "tool_description": "Transform the provided book summary text into a coherent, human-like review suitable for publication, emphasizing positive aspects of the book.",
                    "expected_response": "A text string containing a positive book review generated from the input summary."
                },
                {
                    "user_intent": "Analyze the sentiment of the generated book review to ensure it is positive.",
                    "tool_description": "Classify the sentiment of the generated review text to verify it expresses a positive opinion using a sentiment analysis model.",
                    "expected_response": "A sentiment classification label (positive, negative, or neutral) with optional confidence scores."
                }
            ],
            "extracted_arguments": {
                "book_summary": "The textual summary of the book provided as input for generating a review.",
                "generated_review_text": "The text output of the book review generated from the summary for sentiment evaluation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_222",
        "query": "I'm a nutritionist and want to help my clients by answering questions about their meals. They will send me an image of their food and ask me a question about it, like \"Is this vegan?\" or \"How many calories do you think it contains?\"",
        "instruction": "Given a `visual question answering task`, retrieve tools that handle image inputs and answer questions related to the content of the images. These tools should be capable of processing visual data and providing relevant answers to queries, such as dietary information or content analysis, based on the depicted items.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')",
                    "api_arguments": {
                        "model": "Bingsu/temp_vilt_vqa",
                        "tokenizer": "Bingsu/temp_vilt_vqa"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A visual question answering model for answering questions related to images using the Hugging Face Transformers library.",
                    "id": "huggingface_api_112",
                    "name": "temp_vilt_vqa"
                },
                "id": "gorilla_huggingface_tool_111",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 5: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 7: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 9: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 10: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 12: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 13: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 14: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 15: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.",
            "Tool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 17: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 18: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 19: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language."
        ],
        "tool_view": {
            "description": "JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "parameters": {},
            "response": "The tool is designed for **Visual Question Answering (VQA)**, which involves answering questions about images. The response to a VQA request typically consists of the answer to the question posed, based on the contents of the image provided.\n\nThe expected response from this tool would likely include the following components:\n\n1. **Answer**: A textual response that directly answers the question asked, based on the visual content of the image. This is the primary output of the tool.\n   \n2. **Confidence/Probability (optional)**: A numerical value indicating the model's confidence in the provided answer. This could be expressed as a probability score or a similar metric, although it is not explicitly defined in the documentation.\n\n3. **Language**: Since the model supports multiple languages (English, Chinese, Japanese, and German), the answer will likely be returned in the same language as the question posed.\n\n4. **Additional Context (optional)**: In some VQA systems, additional context or metadata may be included, such as the specific region of the image that contributed to the answer. This could be part of the response, though it is not specified in the documentation.\n\nThe response would focus on providing the most relevant answer based on the combination of the image input and the question posed.",
            "examples": [
                {
                    "user_intent": "I want to ask a question about an image in English, like 'What color is the car?'",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "What color is the car?"
                        }
                    }
                },
                {
                    "user_intent": "Can I ask about an image in Japanese? For example, 'この動物は何ですか？' (What animal is this?)",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "この動物は何ですか？"
                        }
                    }
                },
                {
                    "user_intent": "I have an image and I want to ask in German, 'Wie viele Menschen sind auf diesem Bild?' (How many people are in this image?)",
                    "tool_call": {
                        "name": "JosephusCheung/GuanacoVQA",
                        "arguments": {
                            "image": "<image_path>",
                            "question": "Wie viele Menschen sind auf diesem Bild?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 1: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 3: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 5: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 6: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 7: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 8: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 9: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 10: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 11: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 12: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 13: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 14: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 15: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.\nTool 16: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 17: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 18: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 19: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\", \"parameters\": {}, \"response\": \"The tool is designed for **Visual Question Answering (VQA)**, which involves answering questions about images. The response to a VQA request typically consists of the answer to the question posed, based on the contents of the image provided.\\n\\nThe expected response from this tool would likely include the following components:\\n\\n1. **Answer**: A textual response that directly answers the question asked, based on the visual content of the image. This is the primary output of the tool.\\n   \\n2. **Confidence/Probability (optional)**: A numerical value indicating the model's confidence in the provided answer. This could be expressed as a probability score or a similar metric, although it is not explicitly defined in the documentation.\\n\\n3. **Language**: Since the model supports multiple languages (English, Chinese, Japanese, and German), the answer will likely be returned in the same language as the question posed.\\n\\n4. **Additional Context (optional)**: In some VQA systems, additional context or metadata may be included, such as the specific region of the image that contributed to the answer. This could be part of the response, though it is not specified in the documentation.\\n\\nThe response would focus on providing the most relevant answer based on the combination of the image input and the question posed.\", \"examples\": [{\"user_intent\": \"I want to ask a question about an image in English, like 'What color is the car?'\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"What color is the car?\"}}}, {\"user_intent\": \"Can I ask about an image in Japanese? For example, '\\u3053\\u306e\\u52d5\\u7269\\u306f\\u4f55\\u3067\\u3059\\u304b\\uff1f' (What animal is this?)\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"\\u3053\\u306e\\u52d5\\u7269\\u306f\\u4f55\\u3067\\u3059\\u304b\\uff1f\"}}}, {\"user_intent\": \"I have an image and I want to ask in German, 'Wie viele Menschen sind auf diesem Bild?' (How many people are in this image?)\", \"tool_call\": {\"name\": \"JosephusCheung/GuanacoVQA\", \"arguments\": {\"image\": \"<image_path>\", \"question\": \"Wie viele Menschen sind auf diesem Bild?\"}}}]}\n\n# User Query\nI'm a nutritionist and want to help my clients by answering questions about their meals. They will send me an image of their food and ask me a question about it, like \"Is this vegan?\" or \"How many calories do you think it contains?\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to answer questions about images of food sent by my clients, such as 'Is this vegan?' or 'How many calories does this meal contain?'",
                    "tool_description": "Perform Visual Question Answering (VQA) on images of meals by interpreting both the visual content of the food and the natural language question provided, to generate informative and relevant responses.",
                    "expected_response": "A textual answer addressing the client's question about the meal, potentially including dietary classification, calorie estimation, or other nutritional insights based on the image."
                }
            ],
            "extracted_arguments": {
                "image": "The image file of the client's meal to be analyzed.",
                "question": "A natural language question from the client regarding the meal, such as dietary restrictions or nutritional content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_150",
        "query": "I want to identify when people are speaking in an audio file.",
        "instruction": "Given an `audio voice activity detection` task, retrieve tools that analyze audio files to identify periods of active speech by processing audio inputs and providing timestamps or segments where speech activity is detected.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Automatic Speech Recognition",
                    "api_call": "Pipeline.from_pretrained('pyannote/voice-activity-detection')",
                    "api_arguments": [
                        "audio.wav"
                    ],
                    "python_environment_requirements": [
                        "pyannote.audio 2.1"
                    ],
                    "example_code": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\noutput = pipeline(audio.wav)\nfor speech in output.get_timeline().support():\n  # active speech between speech.start and speech.end",
                    "performance": {
                        "dataset": "ami",
                        "accuracy": "Not specified"
                    },
                    "description": "A pretrained voice activity detection pipeline that detects active speech in audio files.",
                    "id": "huggingface_api_745",
                    "name": "pyannote/voice-activity-detection"
                },
                "id": "gorilla_huggingface_tool_740",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 2: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 3: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 6: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 7: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 8: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 9: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 10: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.",
            "Tool 11: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 12: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.",
            "Tool 13: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 14: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.",
            "Tool 15: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 16: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.",
            "Tool 17: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.",
            "Tool 18: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 19: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage."
        ],
        "tool_view": {
            "description": "Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "parameters": {},
            "response": "The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\n\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \n- **end_time**: The timestamp indicating when the speech segment ends.  \n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \n\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.",
            "examples": [
                {
                    "user_intent": "I have an audio file in Hindi, and I want to detect when someone is speaking in it.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "audio/hindi_conversation.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze my Tamil podcast recording to find all segments where there is active speech.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "podcasts/tamil_episode1.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.",
                    "tool_call": {
                        "name": "d4data/Indian-voice-cloning",
                        "arguments": {
                            "audio_file_path": "interviews/telugu_batch1.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 1: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 2: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 3: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 6: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 7: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 8: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 9: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 10: A Hugging Face Transformers-based tool for classifying speech audio using the SUPERB Keyword Spotting model. It processes 16 kHz audio files to identify keywords, providing the top predicted labels, and is useful for speech recognition tasks such as command detection.\nTool 11: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 12: This tool provides a Hugging Face Transformers pipeline for audio classification using the fine-tuned model `mazkooleg/0-9up-wavlm-base-plus-ft`. It is designed to accurately identify spoken commands from audio inputs, achieving high performance with an accuracy of 99.73% on the evaluation dataset. It is useful for applications that require precise speech recognition or audio-based command detection.\nTool 13: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 14: This tool is an audio classification pipeline for recognizing emotions in speech using the Hugging Face Transformers framework. It leverages the Hubert-large model pretrained on 16kHz speech and is designed for the SUPERB Emotion Recognition task, allowing users to identify emotional states from audio inputs with reasonable accuracy. It is useful for applications in speech analysis, affective computing, and emotion-aware systems.\nTool 15: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 16: The \"lang-id-voxlingua107-ecapa\" tool is a spoken language recognition model trained on the VoxLingua107 dataset using the ECAPA-TDNN architecture, originally designed for speaker recognition. It can identify 107 different languages with 93.3% accuracy and is implemented using the SpeechBrain framework. This tool is useful for classifying languages from audio signals, enabling language identification in speech processing applications.\nTool 17: This tool uses a fine-tuned Wav2Vec2 model to perform speech emotion recognition, analyzing audio recordings to identify emotions such as anger, happiness, sadness, fear, and others. It is designed for applications that require automatic detection of emotional states from English speech and achieves strong performance on the RAVDESS dataset, making it suitable for research, human-computer interaction, and affective computing tasks.\nTool 18: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 19: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\", \"parameters\": {}, \"response\": \"The tool performs voice activity detection (VAD) on an audio input, identifying segments where speech is present. The typical response is a structured collection of speech segments. Each segment in the response generally includes:\\n\\n- **start_time**: The timestamp indicating when the speech segment begins within the audio.  \\n- **end_time**: The timestamp indicating when the speech segment ends.  \\n- **score** (optional): A confidence score or probability indicating the likelihood that the segment contains speech.  \\n\\nThe overall response is a list or array of these segment objects, ordered chronologically, covering all detected speech regions in the audio. This structure allows downstream processing to identify, extract, or analyze spoken portions of the audio efficiently.\", \"examples\": [{\"user_intent\": \"I have an audio file in Hindi, and I want to detect when someone is speaking in it.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"audio/hindi_conversation.wav\"}}}, {\"user_intent\": \"Analyze my Tamil podcast recording to find all segments where there is active speech.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"podcasts/tamil_episode1.mp3\"}}}, {\"user_intent\": \"I need to process a batch of Telugu interview recordings to mark the speech intervals automatically.\", \"tool_call\": {\"name\": \"d4data/Indian-voice-cloning\", \"arguments\": {\"audio_file_path\": \"interviews/telugu_batch1.wav\"}}}]}\n\n# User Query\nI want to identify when people are speaking in an audio file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect all segments in an audio file where people are speaking.",
                    "tool_description": "Perform voice activity detection (VAD) on an audio recording to identify time intervals when speech is present, distinguishing speech from silence or background noise.",
                    "expected_response": "A chronological list of speech segments, each including start_time, end_time, and optionally a confidence score, marking all intervals of detected speech in the audio."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The path or reference to the audio file to analyze for speech activity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_57",
        "query": "As the project manager of a company who receives long emails, my goal is to summarize them and extract the most important information.",
        "instruction": "Given a `summarization` task, retrieve tools that can process lengthy text inputs to produce concise summaries by extracting the most significant information, effectively meeting the query's objective of information distillation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "text2text-generation",
                    "api_call": "PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_summarizer')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "pip install sentencepiece"
                    ],
                    "example_code": "context = \nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day's play.You can say whatever you want to chant, but don't throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England's score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\n\nget_response(context)",
                    "performance": {
                        "dataset": "cnn_dailymail",
                        "accuracy": {
                            "ROUGE-1": 36.604,
                            "ROUGE-2": 14.64,
                            "ROUGE-L": 23.884,
                            "ROUGE-LSUM": 32.902,
                            "loss": 2.576,
                            "gen_len": 76.398
                        }
                    },
                    "description": "PEGASUS fine-tuned for summarization",
                    "id": "huggingface_api_574",
                    "name": "tuner007/pegasus_summarizer"
                },
                "id": "gorilla_huggingface_tool_570",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 1: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 4: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.",
            "Tool 5: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 6: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 7: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.",
            "Tool 8: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 10: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 12: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.",
            "Tool 13: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 14: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 16: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.",
            "Tool 17: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.",
            "Tool 18: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model."
        ],
        "tool_view": {
            "description": "A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path to the pretrained Vision Transformer (ViT) model to be loaded. This specifies which model configuration and weights to use for feature extraction.",
                    "required": true,
                    "example": "facebook/dino-vitb16"
                }
            },
            "response": "The tool is designed for feature extraction using the Vision Transformer (ViT) model, specifically the 'facebook/dino-vitb16' variant, which has been pretrained using the DINO method. The response to an API call involving this tool will contain the following elements:\n\n1. **Last Hidden State**:  \n   This is the key output from the model. It represents the final hidden states of the transformer after processing the input image. The last hidden state typically contains the transformed feature representations of the input image at the model's final layer. These representations can be used for various downstream tasks, such as image classification, clustering, or similarity search.\n\n   - **Structure**: The last hidden state is likely represented as a tensor or array, containing features for each image patch (16x16 pixel segments) and potentially an additional feature vector for the [CLS] token, used for classification or summarization of the image.\n   - **Meaning**: This tensor encapsulates the learned representation of the image, capturing high-level features from the input image.\n\n2. **Model Configuration Information**:  \n   The response may optionally include information regarding the specific model configuration, like:\n   \n   - The architecture (ViT model) and its variant ('facebook/dino-vitb16').\n   - The pretrained nature of the model, indicating it was trained on ImageNet-1k with a specific resolution of 224x224 pixels and patch size of 16x16 pixels.\n   \n3. **Potential Error Messages or Status Information**:  \n   If there is an issue with the image processing, model loading, or feature extraction, the response might include error details or status messages to indicate what went wrong. This would help users troubleshoot issues like incorrect input format or model loading failures.\n\n4. **Additional Outputs**:  \n   Depending on the use case, there might be other outputs like:\n   - Intermediate features from earlier layers (if requested).\n   - Attention maps or visualizations related to the Transformer’s attention mechanism.\n\nThus, the response from this tool would primarily contain the **last hidden states** of the input image processed by the Vision Transformer model, which are the primary features to be used for further analysis or tasks. If any errors occur during the process, corresponding error messages would be included to assist in diagnosing the issue.",
            "examples": [
                {
                    "user_intent": "Extract feature embeddings from an online image of a cat for similarity search.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                },
                {
                    "user_intent": "I want to get the image features of a photo I uploaded locally to use in a clustering algorithm.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                },
                {
                    "user_intent": "Generate embeddings for multiple images from a URL dataset to use for content-based image retrieval.",
                    "tool_call": {
                        "name": "ViTModel.from_pretrained",
                        "arguments": {
                            "pretrained_model_name_or_path": "facebook/dino-vitb16"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 1: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 2: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 4: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\nTool 5: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 6: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 7: The \"philschmid/bart-large-cnn-samsum\" tool is a BART-based model designed for generating abstractive summaries of conversations. It is trained on the SAMSum dataset and can be used within the Hugging Face Transformers framework to process and summarize conversational text.\nTool 8: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 9: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 10: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 11: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 12: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\nTool 13: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 14: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 15: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 16: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.\nTool 17: The \"cross-encoder/ms-marco-MiniLM-L-6-v2\" model is designed for information retrieval tasks. It encodes a query alongside multiple candidate passages, ranking them based on relevance. Trained on the MS Marco Passage Ranking dataset, the model is used to sort passages in descending order of their relevance to the query, making it ideal for enhancing search systems and improving document retrieval accuracy.\nTool 18: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path to the pretrained Vision Transformer (ViT) model to be loaded. This specifies which model configuration and weights to use for feature extraction.\", \"required\": true, \"example\": \"facebook/dino-vitb16\"}}, \"response\": \"The tool is designed for feature extraction using the Vision Transformer (ViT) model, specifically the 'facebook/dino-vitb16' variant, which has been pretrained using the DINO method. The response to an API call involving this tool will contain the following elements:\\n\\n1. **Last Hidden State**:  \\n   This is the key output from the model. It represents the final hidden states of the transformer after processing the input image. The last hidden state typically contains the transformed feature representations of the input image at the model's final layer. These representations can be used for various downstream tasks, such as image classification, clustering, or similarity search.\\n\\n   - **Structure**: The last hidden state is likely represented as a tensor or array, containing features for each image patch (16x16 pixel segments) and potentially an additional feature vector for the [CLS] token, used for classification or summarization of the image.\\n   - **Meaning**: This tensor encapsulates the learned representation of the image, capturing high-level features from the input image.\\n\\n2. **Model Configuration Information**:  \\n   The response may optionally include information regarding the specific model configuration, like:\\n   \\n   - The architecture (ViT model) and its variant ('facebook/dino-vitb16').\\n   - The pretrained nature of the model, indicating it was trained on ImageNet-1k with a specific resolution of 224x224 pixels and patch size of 16x16 pixels.\\n   \\n3. **Potential Error Messages or Status Information**:  \\n   If there is an issue with the image processing, model loading, or feature extraction, the response might include error details or status messages to indicate what went wrong. This would help users troubleshoot issues like incorrect input format or model loading failures.\\n\\n4. **Additional Outputs**:  \\n   Depending on the use case, there might be other outputs like:\\n   - Intermediate features from earlier layers (if requested).\\n   - Attention maps or visualizations related to the Transformer\\u2019s attention mechanism.\\n\\nThus, the response from this tool would primarily contain the **last hidden states** of the input image processed by the Vision Transformer model, which are the primary features to be used for further analysis or tasks. If any errors occur during the process, corresponding error messages would be included to assist in diagnosing the issue.\", \"examples\": [{\"user_intent\": \"Extract feature embeddings from an online image of a cat for similarity search.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}, {\"user_intent\": \"I want to get the image features of a photo I uploaded locally to use in a clustering algorithm.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}, {\"user_intent\": \"Generate embeddings for multiple images from a URL dataset to use for content-based image retrieval.\", \"tool_call\": {\"name\": \"ViTModel.from_pretrained\", \"arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}}}]}\n\n# User Query\nAs the project manager of a company who receives long emails, my goal is to summarize them and extract the most important information.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Summarize long emails to extract the main points efficiently.",
                    "tool_description": "Use an abstractive text summarization model to condense lengthy email content into a concise summary while preserving key information.",
                    "expected_response": "A concise text summary highlighting the main points of the email."
                },
                {
                    "user_intent": "Identify and extract key entities and information from emails after summarization.",
                    "tool_description": "Apply a named entity recognition or token classification model to detect and extract important entities such as people, companies, dates, or actions from the summarized email content.",
                    "expected_response": "A structured list of extracted entities and their types from the email content."
                }
            ],
            "extracted_arguments": {
                "email_text": "The full content of the email(s) to be summarized, provided as text.",
                "summary_length": "Optional parameter defining the desired length or level of detail for the summary.",
                "entity_types": "Optional list of entity categories to extract, such as person names, organizations, dates, or key actions."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_174",
        "query": "I would like to give a brief overview of our team meeting to my supervisor, so I need a summary of the conversation.",
        "instruction": "Given a `text summarization` task, retrieve tools that can process conversational inputs to generate concise summaries, effectively capturing the key points of the meeting as outlined in the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Summarization",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Summarization",
                    "api_call": "pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum')",
                    "api_arguments": {
                        "model": "philschmid/distilbart-cnn-12-6-samsum"
                    },
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\nconversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation? \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\n'''\nsummarizer(conversation)",
                    "performance": {
                        "dataset": "samsum",
                        "accuracy": {
                            "ROUGE-1": 41.09,
                            "ROUGE-2": 20.746,
                            "ROUGE-L": 31.595,
                            "ROUGE-LSUM": 38.339
                        }
                    },
                    "description": "This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.",
                    "id": "huggingface_api_552",
                    "name": "distilbart-cnn-12-6-samsum"
                },
                "id": "gorilla_huggingface_tool_548",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 1: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 2: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.",
            "Tool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 5: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 8: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 11: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 12: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 13: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 14: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 15: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 16: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.",
            "Tool 17: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.",
            "Tool 18: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 19: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`."
        ],
        "tool_view": {
            "description": "`tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained model to load. This can be a model identifier from the Hugging Face Hub or a local path to a saved model.",
                    "required": true,
                    "example": "tinkoff-ai/ruDialoGPT-medium"
                }
            },
            "response": "The tool produces generated conversational text responses based on a given input dialogue context. The structure and components of its response are as follows:\n\n1. **Type of Response**:  \n   The output is a sequence or list of text strings, where each string represents a complete generated continuation of the input conversation.\n\n2. **Content of Each Element**:  \n   - Each string contains the dialogue context provided as input, followed by one possible model-generated response.  \n   - The response text is coherent with the preceding dialogue and is intended to be contextually appropriate, sensical, and relevant.\n\n3. **Multiplicity / Variants**:  \n   - The model can produce multiple alternative responses for the same input.  \n   - The number of returned alternatives corresponds to the `num_return_sequences` parameter specified during generation. Each alternative is independent, reflecting different plausible continuations.\n\n4. **Text Encoding and Decoding**:  \n   - Internally, the model outputs token IDs, which are decoded into human-readable text using the tokenizer.  \n   - Special tokens (like end-of-sequence) may be included in the tokenization process but are typically removed or interpreted in the final decoded string.\n\n5. **Semantic Properties**:  \n   - Each response is generated to balance sensibleness and specificity, reflecting the model’s training on dialogue data.  \n   - Responses may vary in style, length, and content based on sampling parameters such as `top_k`, `top_p`, `temperature`, and repetition penalties.\n\n6. **Overall Structure**:  \n   - Conceptually, the tool returns a **list of dialogue strings**, where each string is the input context plus a generated continuation.  \n   - No additional metadata is returned by default, although the model internally computes probabilities for token selection.\n\nIn summary, the tool’s response is a set of text continuations of the input dialogue, each representing a plausible conversational reply generated by the model.",
            "examples": [
                {
                    "user_intent": "I want to generate a conversational response using the ruDialoGPT model.",
                    "tool_call": {
                        "name": "tinkoff-ai/ruDialoGPT-medium",
                        "arguments": {
                            "pretrained_model_name_or_path": "tinkoff-ai/ruDialoGPT-medium",
                            "inputs": {
                                "text": "@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ как дела? @@ВТОРОЙ@@",
                                "return_tensors": "pt"
                            },
                            "top_k": 10,
                            "top_p": 0.95,
                            "num_beams": 3,
                            "num_return_sequences": 3,
                            "do_sample": true,
                            "no_repeat_ngram_size": 2,
                            "temperature": 1.2,
                            "repetition_penalty": 1.2,
                            "length_penalty": 1.0,
                            "eos_token_id": 50257,
                            "max_new_tokens": 40
                        }
                    }
                },
                {
                    "user_intent": "Can you help me create a chatbot response for a greeting?",
                    "tool_call": {
                        "name": "tinkoff-ai/ruDialoGPT-medium",
                        "arguments": {
                            "pretrained_model_name_or_path": "tinkoff-ai/ruDialoGPT-medium",
                            "inputs": {
                                "text": "@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@, как ты?",
                                "return_tensors": "pt"
                            },
                            "top_k": 10,
                            "top_p": 0.95,
                            "num_beams": 3,
                            "num_return_sequences": 3,
                            "do_sample": true,
                            "no_repeat_ngram_size": 2,
                            "temperature": 1.2,
                            "repetition_penalty": 1.2,
                            "length_penalty": 1.0,
                            "eos_token_id": 50257,
                            "max_new_tokens": 40
                        }
                    }
                },
                {
                    "user_intent": "Generate a response for a user asking about the weather.",
                    "tool_call": {
                        "name": "tinkoff-ai/ruDialoGPT-medium",
                        "arguments": {
                            "pretrained_model_name_or_path": "tinkoff-ai/ruDialoGPT-medium",
                            "inputs": {
                                "text": "@@ПЕРВЫЙ@@ Как погода сегодня? @@ВТОРОЙ@@",
                                "return_tensors": "pt"
                            },
                            "top_k": 10,
                            "top_p": 0.95,
                            "num_beams": 3,
                            "num_return_sequences": 3,
                            "do_sample": true,
                            "no_repeat_ngram_size": 2,
                            "temperature": 1.2,
                            "repetition_penalty": 1.2,
                            "length_penalty": 1.0,
                            "eos_token_id": 50257,
                            "max_new_tokens": 40
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 1: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 2: microsoft/DialoGPT-large is a state-of-the-art dialogue response generation model designed for multi-turn conversations. Trained on 147 million dialogues from Reddit discussion threads, it is capable of producing human-like responses, achieving performance comparable to human-level quality in single-turn conversation Turing tests. It is used for generating conversational text in applications requiring natural, context-aware dialogue.\nTool 3: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 5: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 8: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 9: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 11: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 12: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 13: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 14: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 15: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 16: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\nTool 17: A conversational AI tool that uses the DialoGPT-medium-PALPATINE2 model to generate human-like responses, enabling interactive and natural text-based dialogue.\nTool 18: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 19: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"`tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained model to load. This can be a model identifier from the Hugging Face Hub or a local path to a saved model.\", \"required\": true, \"example\": \"tinkoff-ai/ruDialoGPT-medium\"}}, \"response\": \"The tool produces generated conversational text responses based on a given input dialogue context. The structure and components of its response are as follows:\\n\\n1. **Type of Response**:  \\n   The output is a sequence or list of text strings, where each string represents a complete generated continuation of the input conversation.\\n\\n2. **Content of Each Element**:  \\n   - Each string contains the dialogue context provided as input, followed by one possible model-generated response.  \\n   - The response text is coherent with the preceding dialogue and is intended to be contextually appropriate, sensical, and relevant.\\n\\n3. **Multiplicity / Variants**:  \\n   - The model can produce multiple alternative responses for the same input.  \\n   - The number of returned alternatives corresponds to the `num_return_sequences` parameter specified during generation. Each alternative is independent, reflecting different plausible continuations.\\n\\n4. **Text Encoding and Decoding**:  \\n   - Internally, the model outputs token IDs, which are decoded into human-readable text using the tokenizer.  \\n   - Special tokens (like end-of-sequence) may be included in the tokenization process but are typically removed or interpreted in the final decoded string.\\n\\n5. **Semantic Properties**:  \\n   - Each response is generated to balance sensibleness and specificity, reflecting the model\\u2019s training on dialogue data.  \\n   - Responses may vary in style, length, and content based on sampling parameters such as `top_k`, `top_p`, `temperature`, and repetition penalties.\\n\\n6. **Overall Structure**:  \\n   - Conceptually, the tool returns a **list of dialogue strings**, where each string is the input context plus a generated continuation.  \\n   - No additional metadata is returned by default, although the model internally computes probabilities for token selection.\\n\\nIn summary, the tool\\u2019s response is a set of text continuations of the input dialogue, each representing a plausible conversational reply generated by the model.\", \"examples\": [{\"user_intent\": \"I want to generate a conversational response using the ruDialoGPT model.\", \"tool_call\": {\"name\": \"tinkoff-ai/ruDialoGPT-medium\", \"arguments\": {\"pretrained_model_name_or_path\": \"tinkoff-ai/ruDialoGPT-medium\", \"inputs\": {\"text\": \"@@\\u041f\\u0415\\u0420\\u0412\\u042b\\u0419@@ \\u043f\\u0440\\u0438\\u0432\\u0435\\u0442 @@\\u0412\\u0422\\u041e\\u0420\\u041e\\u0419@@ \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430? @@\\u0412\\u0422\\u041e\\u0420\\u041e\\u0419@@\", \"return_tensors\": \"pt\"}, \"top_k\": 10, \"top_p\": 0.95, \"num_beams\": 3, \"num_return_sequences\": 3, \"do_sample\": true, \"no_repeat_ngram_size\": 2, \"temperature\": 1.2, \"repetition_penalty\": 1.2, \"length_penalty\": 1.0, \"eos_token_id\": 50257, \"max_new_tokens\": 40}}}, {\"user_intent\": \"Can you help me create a chatbot response for a greeting?\", \"tool_call\": {\"name\": \"tinkoff-ai/ruDialoGPT-medium\", \"arguments\": {\"pretrained_model_name_or_path\": \"tinkoff-ai/ruDialoGPT-medium\", \"inputs\": {\"text\": \"@@\\u041f\\u0415\\u0420\\u0412\\u042b\\u0419@@ \\u043f\\u0440\\u0438\\u0432\\u0435\\u0442 @@\\u0412\\u0422\\u041e\\u0420\\u041e\\u0419@@, \\u043a\\u0430\\u043a \\u0442\\u044b?\", \"return_tensors\": \"pt\"}, \"top_k\": 10, \"top_p\": 0.95, \"num_beams\": 3, \"num_return_sequences\": 3, \"do_sample\": true, \"no_repeat_ngram_size\": 2, \"temperature\": 1.2, \"repetition_penalty\": 1.2, \"length_penalty\": 1.0, \"eos_token_id\": 50257, \"max_new_tokens\": 40}}}, {\"user_intent\": \"Generate a response for a user asking about the weather.\", \"tool_call\": {\"name\": \"tinkoff-ai/ruDialoGPT-medium\", \"arguments\": {\"pretrained_model_name_or_path\": \"tinkoff-ai/ruDialoGPT-medium\", \"inputs\": {\"text\": \"@@\\u041f\\u0415\\u0420\\u0412\\u042b\\u0419@@ \\u041a\\u0430\\u043a \\u043f\\u043e\\u0433\\u043e\\u0434\\u0430 \\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f? @@\\u0412\\u0422\\u041e\\u0420\\u041e\\u0419@@\", \"return_tensors\": \"pt\"}, \"top_k\": 10, \"top_p\": 0.95, \"num_beams\": 3, \"num_return_sequences\": 3, \"do_sample\": true, \"no_repeat_ngram_size\": 2, \"temperature\": 1.2, \"repetition_penalty\": 1.2, \"length_penalty\": 1.0, \"eos_token_id\": 50257, \"max_new_tokens\": 40}}}]}\n\n# User Query\nI would like to give a brief overview of our team meeting to my supervisor, so I need a summary of the conversation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a summary of a conversation.",
                    "tool_description": "Summarize the content of a conversation or meeting to provide a concise overview.",
                    "expected_response": "A coherent summary of the conversation's main points and topics."
                }
            ],
            "extracted_arguments": {
                "conversation_text": "The content of the conversation or meeting to be summarized."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_328",
        "query": "We are building AI glasses that should tell us about things that we are seeing with classifications. We want to use a visual transformer architecture.",
        "instruction": "Given a `visual classification with transformer` task, retrieve tools that enable AI applications to label and categorize visual inputs through sophisticated image classification models, utilizing visual transformer architectures to deliver accurate results aligned with the specified needs.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')",
                    "api_arguments": {
                        "images": "image",
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoFeatureExtractor",
                        "PIL": "Image",
                        "requests": "requests"
                    },
                    "example_code": "from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not specified"
                    },
                    "description": "Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.",
                    "id": "huggingface_api_193",
                    "name": "microsoft/swin-tiny-patch4-window7-224"
                },
                "id": "gorilla_huggingface_tool_189",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 3: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 5: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 6: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 7: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.",
            "Tool 8: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.",
            "Tool 9: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 13: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 14: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 16: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 17: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 18: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.",
            "Tool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use."
        ],
        "tool_view": {
            "description": "BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "parameters": {
                "raw_image": {
                    "type": "Image",
                    "description": "The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                },
                "question": {
                    "type": "string",
                    "description": "The natural language question about the content of the image that the model should answer.",
                    "required": true,
                    "example": "How many dogs are in the picture?"
                }
            },
            "response": "The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model’s predicted answer. Semantically, this string conveys the model’s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model’s internal generation.",
            "examples": [
                {
                    "user_intent": "How many people are standing in this photo?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1607746882042-944635dfe10e",
                            "question": "How many people are standing in this photo?"
                        }
                    }
                },
                {
                    "user_intent": "What color is the car in the image?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b",
                            "question": "What color is the car in the image?"
                        }
                    }
                },
                {
                    "user_intent": "Is there any food on the table in this picture?",
                    "tool_call": {
                        "name": "blip-vqa-base",
                        "arguments": {
                            "raw_image": "https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c",
                            "question": "Is there any food on the table in this picture?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 1: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 3: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 4: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 5: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 6: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 7: The \"dandelin/vilt-b32-finetuned-vqa\" is a Vision-and-Language Transformer (ViLT) model fine-tuned for visual question answering tasks. It can be used to answer questions about images by processing both the image and the corresponding text query. The model was trained on the VQAv2 dataset and is based on the ViLT architecture, which integrates vision and language processing without convolution or region supervision. This tool is useful for tasks requiring the extraction of information from images based on natural language questions.\nTool 8: The Intel/dpt-hybrid-midas is a depth estimation model based on the Dense Prediction Transformer (DPT) architecture, which uses Vision Transformer (ViT) as a backbone. It is specifically trained for monocular depth estimation and has been optimized with a hybrid approach that integrates activations from the ViT-hybrid backbone. The model is capable of predicting depth from a single image and is designed for use with the Hugging Face Transformers framework. It is particularly useful for tasks requiring accurate depth prediction from visual data, such as computer vision applications in robotics or augmented reality.\nTool 9: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 10: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 11: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 13: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 14: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 15: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 16: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 17: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 18: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\nTool 19: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\", \"parameters\": {\"raw_image\": {\"type\": \"Image\", \"description\": \"The input image on which the visual question answering model will perform analysis. This should be provided as a PIL Image object.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}, \"question\": {\"type\": \"string\", \"description\": \"The natural language question about the content of the image that the model should answer.\", \"required\": true, \"example\": \"How many dogs are in the picture?\"}}, \"response\": \"The tool produces a textual answer to a visual question posed about an input image. The response is structured as a single string representing the model\\u2019s predicted answer. Semantically, this string conveys the model\\u2019s understanding of the visual content of the image in relation to the provided question. The response does not include additional metadata by default, such as confidence scores or token-level outputs; it is primarily the decoded natural language answer derived from the model\\u2019s internal generation.\", \"examples\": [{\"user_intent\": \"How many people are standing in this photo?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1607746882042-944635dfe10e\", \"question\": \"How many people are standing in this photo?\"}}}, {\"user_intent\": \"What color is the car in the image?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1612392061781-3f3b9a0f6c6b\", \"question\": \"What color is the car in the image?\"}}}, {\"user_intent\": \"Is there any food on the table in this picture?\", \"tool_call\": {\"name\": \"blip-vqa-base\", \"arguments\": {\"raw_image\": \"https://images.unsplash.com/photo-1556910103-1b1b3c0c4f4c\", \"question\": \"Is there any food on the table in this picture?\"}}}]}\n\n# User Query\nWe are building AI glasses that should tell us about things that we are seeing with classifications. We want to use a visual transformer architecture.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify objects and scenes seen through AI glasses using a visual transformer model.",
                    "tool_description": "Use a vision-language or vision-only transformer-based model to process real-time visual input and provide classifications or descriptions of the observed content.",
                    "expected_response": "A textual or categorical output describing the objects, scenes, or attributes detected in the visual input."
                }
            ],
            "extracted_arguments": {
                "input_image": "The visual frame or image captured by the AI glasses, suitable for processing by a transformer-based vision model."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_393",
        "query": "We are developing a virtual assistant and need to integrate a speech enhancement feature in it.",
        "instruction": "Given a `speech enhancement feature integration` task, retrieve tools that enhance audio quality by processing audio inputs to reduce noise and reverberation, utilizing pre-trained models or specific frameworks designed for optimizing speech clarity in virtual assistant applications.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source='speechbrain/sepformer-wham-enhancement')",
                    "api_arguments": [
                        "path"
                    ],
                    "python_environment_requirements": [
                        "pip install speechbrain"
                    ],
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source='speechbrain/sepformer-wham-enhancement', savedir='pretrained_models/sepformer-wham-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\ntorchaudio.save('enhanced_wham.wav', est_sources[:, :, 0].detach().cpu(), 8000)",
                    "performance": {
                        "dataset": "WHAM!",
                        "accuracy": "14.35 dB SI-SNR"
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_803",
                    "name": "sepformer-wham-enhancement"
                },
                "id": "gorilla_huggingface_tool_798",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 2: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.",
            "Tool 3: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 5: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.",
            "Tool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 7: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).",
            "Tool 8: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 9: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.",
            "Tool 10: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 11: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 12: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.",
            "Tool 13: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 14: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.",
            "Tool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 16: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 17: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 18: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 19: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications."
        ],
        "tool_view": {
            "description": "Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that will be processed for source separation by the SepFormer model.",
                    "required": true,
                    "example": "path/to/mixed_audio.wav"
                }
            },
            "response": "The tool returns a separation of an input audio file into its constituent audio sources. The response is typically a multi-dimensional array or tensor representing the estimated audio signals for each separated source.  \n\n**Response structure and components:**\n\n- **Shape and dimensions:** The response is usually a 3-dimensional tensor or array with dimensions corresponding to `[channels, samples, sources]`.  \n  - `channels`: Number of audio channels (e.g., mono or stereo).  \n  - `samples`: Number of audio samples in the audio file.  \n  - `sources`: Number of separated audio sources produced by the model (e.g., two speakers for the WSJ0-2Mix dataset).  \n\n- **Data type:** The elements of the tensor are numerical audio sample values, often in floating-point format, suitable for further processing or saving as audio files.  \n\n- **Semantic meaning:** Each slice along the `sources` dimension corresponds to one separated audio signal. For example, `est_sources[:, :, 0]` represents the first separated source, and `est_sources[:, :, 1]` represents the second.  \n\n- **Intended use:** The output can be directly written to audio files using an audio library (e.g., `torchaudio`) or passed to downstream audio processing tasks.  \n\nIn essence, the response provides all separated audio tracks derived from the input mixture, preserving the temporal structure and channel information.",
            "examples": [
                {
                    "user_intent": "I have a WAV file with two overlapping speakers. I want to separate the voices into two separate audio files.",
                    "tool_call": {
                        "name": "sepformer-wsj02mix",
                        "arguments": {
                            "path": "recordings/conversation_mix.wav"
                        }
                    }
                },
                {
                    "user_intent": "Separate the vocals and background speech from my meeting recording so I can analyze each speaker individually.",
                    "tool_call": {
                        "name": "sepformer-wsj02mix",
                        "arguments": {
                            "path": "meetings/team_meeting_mix.wav"
                        }
                    }
                },
                {
                    "user_intent": "I want to process a batch of short podcast clips that have overlapping voices and extract each speaker separately.",
                    "tool_call": {
                        "name": "sepformer-wsj02mix",
                        "arguments": {
                            "path": "podcasts/episode1_clip_mix.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 1: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 2: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.\nTool 3: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 5: WavLM-Large is a pretrained speech model designed for feature extraction in automatic speech recognition and other speech processing tasks. It leverages a HuBERT-based architecture to capture both spoken content and speaker identity, having been trained on over 90,000 hours of diverse speech data. The model delivers state-of-the-art performance on the SUPERB benchmark, making it suitable for applications like speech recognition and audio classification.\nTool 6: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 7: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).\nTool 8: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 9: This tool uses the Awais/Audio_Source_Separation model from Hugging Face Transformers to perform audio source separation, isolating individual sound sources from mixed audio. Built with the Asteroid framework and trained on the Libri2Mix dataset, it is designed to separate clean speech sources, making it useful for tasks like speech enhancement, remixing, or audio analysis.\nTool 10: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 11: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 12: Hubert-Large is a self-supervised speech representation model that extracts high-quality features from 16kHz audio. It is designed to handle challenges in speech representation learning, including variable-length sound units, multiple sound units per utterance, and the absence of a predefined lexicon during pretraining. By using an offline clustering step to generate target labels for a BERT-style prediction loss, it produces representations that match or surpass state-of-the-art performance on speech benchmarks like Libri-Light, making it useful for tasks requiring robust speech feature extraction.\nTool 13: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 14: A pre-trained RegNet model for image classification, designed to recognize and categorize images based on the ImageNet-1K dataset. It can be used to extract features from images and predict their labels using the Hugging Face Transformers framework, enabling developers to integrate high-performance visual recognition into Python applications.\nTool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 16: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 17: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 18: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 19: This tool performs zero-shot image classification using the CLIP ConvNeXt-Large model, trained on the LAION-2B subset of LAION-5B. It allows for classifying images without needing retraining by providing a list of possible class names. The model, which achieves 75.9-76.9% top-1 accuracy on ImageNet-1k, can be accessed via Hugging Face’s pipeline API, making it easy to integrate into Python applications.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that will be processed for source separation by the SepFormer model.\", \"required\": true, \"example\": \"path/to/mixed_audio.wav\"}}, \"response\": \"The tool returns a separation of an input audio file into its constituent audio sources. The response is typically a multi-dimensional array or tensor representing the estimated audio signals for each separated source.  \\n\\n**Response structure and components:**\\n\\n- **Shape and dimensions:** The response is usually a 3-dimensional tensor or array with dimensions corresponding to `[channels, samples, sources]`.  \\n  - `channels`: Number of audio channels (e.g., mono or stereo).  \\n  - `samples`: Number of audio samples in the audio file.  \\n  - `sources`: Number of separated audio sources produced by the model (e.g., two speakers for the WSJ0-2Mix dataset).  \\n\\n- **Data type:** The elements of the tensor are numerical audio sample values, often in floating-point format, suitable for further processing or saving as audio files.  \\n\\n- **Semantic meaning:** Each slice along the `sources` dimension corresponds to one separated audio signal. For example, `est_sources[:, :, 0]` represents the first separated source, and `est_sources[:, :, 1]` represents the second.  \\n\\n- **Intended use:** The output can be directly written to audio files using an audio library (e.g., `torchaudio`) or passed to downstream audio processing tasks.  \\n\\nIn essence, the response provides all separated audio tracks derived from the input mixture, preserving the temporal structure and channel information.\", \"examples\": [{\"user_intent\": \"I have a WAV file with two overlapping speakers. I want to separate the voices into two separate audio files.\", \"tool_call\": {\"name\": \"sepformer-wsj02mix\", \"arguments\": {\"path\": \"recordings/conversation_mix.wav\"}}}, {\"user_intent\": \"Separate the vocals and background speech from my meeting recording so I can analyze each speaker individually.\", \"tool_call\": {\"name\": \"sepformer-wsj02mix\", \"arguments\": {\"path\": \"meetings/team_meeting_mix.wav\"}}}, {\"user_intent\": \"I want to process a batch of short podcast clips that have overlapping voices and extract each speaker separately.\", \"tool_call\": {\"name\": \"sepformer-wsj02mix\", \"arguments\": {\"path\": \"podcasts/episode1_clip_mix.wav\"}}}]}\n\n# User Query\nWe are developing a virtual assistant and need to integrate a speech enhancement feature in it.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Enhance the clarity of speech in audio recordings for a virtual assistant.",
                    "tool_description": "Process input audio to remove background noise and reverberation using a pretrained speech enhancement model, improving intelligibility and quality for downstream applications like transcription or voice interaction.",
                    "expected_response": "An enhanced audio file or data array where speech signals are clearer and background noise is reduced, suitable for use in virtual assistant pipelines."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The file path or audio data representing the speech to be enhanced, typically in WAV or similar format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_136",
        "query": "Design a promotional poster for a new line of summer clothing featuring people wearing the clothes with a beach background.",
        "instruction": "Given a `promotional design` task, retrieve tools capable of generating high-quality, realistic images by processing text prompts to visualize specific scenes, such as summer clothing with a beach background, ensuring the outcome aligns with the intended thematic and visual appeal of the promotional material.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image",
                    "api_call": "pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)",
                    "api_arguments": {
                        "prompt": "string",
                        "negative_prompt": "string"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\n\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\n\nresult = model(prompt, negative_prompt=negative_prompt)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.",
                    "id": "huggingface_api_37",
                    "name": "Realistic_Vision_V1.4"
                },
                "id": "gorilla_huggingface_tool_37",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 1: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 4: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 5: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 6: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 7: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 8: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 9: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 10: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 11: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 13: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.",
            "Tool 14: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 15: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.",
            "Tool 16: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.",
            "Tool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 18: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.",
            "Tool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks."
        ],
        "tool_view": {
            "description": "A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "parameters": {
                "image": {
                    "type": "object",
                    "description": "The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.",
                    "required": true,
                    "example": "Image.open('path_to_image.jpg')"
                }
            },
            "response": "The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\n\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\n\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\n\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\n\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\n\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.",
            "examples": [
                {
                    "user_intent": "Segment the clothing items in this portrait image to identify shirts, pants, and jackets.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a group photo to separate and highlight different clothing types for fashion research.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e"
                        }
                    }
                },
                {
                    "user_intent": "Process a full-body street style image to extract the clothing regions for a style recommendation app.",
                    "tool_call": {
                        "name": "mattmdjaga/segformer_b2_clothes",
                        "arguments": {
                            "image": "https://images.unsplash.com/photo-1595152772835-219674b2a8a2"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 1: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 2: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 3: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 4: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 5: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 6: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 7: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 8: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 9: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 10: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 11: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 12: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 13: A tabular classification model trained using Hugging Face Transformers and AutoTrain, designed for multi-class prediction on tabular datasets such as CO2 emissions. It leverages an extra trees algorithm to generate predictions from structured data and can be applied to new datasets for automated classification tasks.\nTool 14: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 15: This tool provides a binary classification model for predicting whether a Titanic passenger survived, using features like passenger class, age, sex, and fare. It is designed for tabular data and can be loaded via Scikit-learn and TensorFlow, enabling users to apply the model to new datasets for survival prediction.\nTool 16: CodeT5-large-ntp-py is a pretrained encoder-decoder language model designed for code understanding and generation. It can transform input code into new code or textual representations, making it useful for tasks like code completion, translation, and synthesis. Built on Hugging Face Transformers, it supports tokenization and sequence generation, enabling developers to generate or manipulate code programmatically.\nTool 17: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 18: This tool provides a K-Nearest Neighbors (KNN) model for multi-class classification, trained on the Iris dataset. It uses the Joblib framework to load and apply the model for predictions on new data. The model, which was trained using AutoTrain, achieves an accuracy of 90% on the Iris dataset. It requires the Joblib and Pandas libraries in a Python environment and works by loading a trained model and applying it to a dataset with specific features.\nTool 19: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\", \"parameters\": {\"image\": {\"type\": \"object\", \"description\": \"The input image to be segmented. This can be a PIL Image object, typically loaded from a file or URL.\", \"required\": true, \"example\": \"Image.open('path_to_image.jpg')\"}}, \"response\": \"The tool produces a semantic segmentation output for a given input image. The response can be described as follows:\\n\\n- **logits**: A multi-dimensional tensor representing the raw, unnormalized class scores for each pixel in the input image. Its shape typically corresponds to `(batch_size, num_classes, height, width)` before any resizing or post-processing. Each channel represents the likelihood of a particular class at each pixel location.\\n\\n- **upsampled_logits**: A tensor derived from `logits` that has been resized (interpolated) to match the spatial dimensions of the original input image. This allows for pixel-wise alignment between the model predictions and the original image.\\n\\n- **pred_seg**: A 2D tensor (or array) representing the final predicted segmentation map. Each element corresponds to a pixel in the input image and contains an integer indicating the predicted class label for that pixel. This is obtained by taking the `argmax` over the class dimension of the `upsampled_logits`.\\n\\n- **Visual representation**: While not strictly part of the model output, the predicted segmentation map (`pred_seg`) is often visualized using a plotting library, where different class labels are rendered with distinct colors to indicate segmented regions.\\n\\nIn essence, the response encodes, for each pixel in the input image, which clothing or body-related class the model predicts it belongs to, optionally providing both raw logits and a finalized segmentation map suitable for visualization or further processing.\", \"examples\": [{\"user_intent\": \"Segment the clothing items in this portrait image to identify shirts, pants, and jackets.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d\"}}}, {\"user_intent\": \"Analyze a group photo to separate and highlight different clothing types for fashion research.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\"}}}, {\"user_intent\": \"Process a full-body street style image to extract the clothing regions for a style recommendation app.\", \"tool_call\": {\"name\": \"mattmdjaga/segformer_b2_clothes\", \"arguments\": {\"image\": \"https://images.unsplash.com/photo-1595152772835-219674b2a8a2\"}}}]}\n\n# User Query\nDesign a promotional poster for a new line of summer clothing featuring people wearing the clothes with a beach background.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to generate a promotional poster featuring people wearing summer clothing with a beach background.",
                    "tool_description": "Generate an image based on a text prompt describing people wearing summer clothing with a beach background, leveraging a text-to-image generation model.",
                    "expected_response": "A generated image representing a promotional poster with people wearing summer clothing, set against a beach background."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "A detailed textual description of the scene to be generated, including clothing types, people, and background elements."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_384",
        "query": "We have a large dataset of customer orders in the form of a table. Help us answer questions about this data.",
        "instruction": "Given a `table question answering` task, retrieve tools that analyze tabular data to answer questions based on the dataset by processing table inputs through advanced machine learning models tailored for question answering tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "pipeline('table-question-answering', model='google/tapas-small-finetuned-sqa')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.6155
                    },
                    "description": "TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_441",
                    "name": "google/tapas-small-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_437",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 1: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 3: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 4: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 5: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 6: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 8: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 10: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 11: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 12: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 13: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 14: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 15: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 18: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs."
        ],
        "tool_view": {
            "description": "OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "parameters": {
                "table": {
                    "type": "pd.DataFrame",
                    "description": "A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.",
                    "required": true,
                    "example": "pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})"
                },
                "query": {
                    "type": "string",
                    "description": "A natural language question string to be answered using the provided table data.",
                    "required": true,
                    "example": "In which year did Beijing host the Olympic Games?"
                }
            },
            "response": "The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \n\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \n- **Semantic role:** Each output string conveys the model’s prediction of the answer to the user’s question, derived by reasoning over the table structure, headers, and cell values.  \n\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.",
            "examples": [
                {
                    "user_intent": "I have a table of countries and their populations. I want to know which country has the largest population.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})",
                            "query": "Which country has the largest population?"
                        }
                    }
                },
                {
                    "user_intent": "Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})",
                            "query": "Which quarter had the highest revenue?"
                        }
                    }
                },
                {
                    "user_intent": "I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.",
                    "tool_call": {
                        "name": "neulab/omnitab-large",
                        "arguments": {
                            "table": "pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})",
                            "query": "In which year did London host the Olympic Games?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 1: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 2: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 3: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 4: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 5: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 6: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 7: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 8: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 9: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 10: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 11: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 12: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 13: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 14: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 15: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 18: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 19: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\", \"parameters\": {\"table\": {\"type\": \"pd.DataFrame\", \"description\": \"A pandas DataFrame containing the table data on which the question answering will be performed. Each column should represent a feature and each row a record.\", \"required\": true, \"example\": \"pd.DataFrame({'year': [1896, 1900], 'city': ['athens', 'paris']})\"}, \"query\": {\"type\": \"string\", \"description\": \"A natural language question string to be answered using the provided table data.\", \"required\": true, \"example\": \"In which year did Beijing host the Olympic Games?\"}}, \"response\": \"The tool produces a textual answer to a natural language question about the contents of a table. The response is typically structured as a sequence of text tokens decoded into a human-readable string. Specifically:  \\n\\n- **Primary content:** A single or multi-sentence string that directly answers the input query based on the data in the provided table.  \\n- **Format:** The output is returned as a decoded string (from token IDs generated by the model), with special tokens removed if requested (e.g., `skip_special_tokens=True`).  \\n- **Optional batch structure:** If multiple queries or tables are provided, the output can be a list of strings, each corresponding to one query-table pair.  \\n- **Semantic role:** Each output string conveys the model\\u2019s prediction of the answer to the user\\u2019s question, derived by reasoning over the table structure, headers, and cell values.  \\n\\nThere are no additional metadata fields returned by the model by default; the response is purely the generated answer text.\", \"examples\": [{\"user_intent\": \"I have a table of countries and their populations. I want to know which country has the largest population.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Country': ['USA', 'China', 'India', 'Brazil'], 'Population': [331000000, 1441000000, 1393000000, 212000000]})\", \"query\": \"Which country has the largest population?\"}}}, {\"user_intent\": \"Given a table of sales data per quarter, I want to find out in which quarter the company made the highest revenue.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Quarter': ['Q1', 'Q2', 'Q3', 'Q4'], 'Revenue': [120000, 150000, 130000, 170000]})\", \"query\": \"Which quarter had the highest revenue?\"}}}, {\"user_intent\": \"I have Olympic host cities and years in a table. I want to know when London hosted the Olympics.\", \"tool_call\": {\"name\": \"neulab/omnitab-large\", \"arguments\": {\"table\": \"pd.DataFrame({'Year': [1896, 1900, 1904, 2004, 2008, 2012], 'City': ['Athens', 'Paris', 'St. Louis', 'Athens', 'Beijing', 'London']})\", \"query\": \"In which year did London host the Olympic Games?\"}}}]}\n\n# User Query\nWe have a large dataset of customer orders in the form of a table. Help us answer questions about this data.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to ask questions about our customer orders table and get answers directly from the data.",
                    "tool_description": "Use a table-based question answering model to process a DataFrame of customer orders and a natural language query to generate an accurate answer.",
                    "expected_response": "A textual answer derived from the table data, directly addressing the user's query, formatted as a human-readable string."
                }
            ],
            "extracted_arguments": {
                "table": "A pandas DataFrame containing the customer orders data with rows as orders and columns as order attributes.",
                "query": "A natural language question string related to the customer orders data."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_274",
        "query": "As a business assistant of an international company, find the most relevant sentence among a list of sentences that answers a specific question.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that utilize natural language processing techniques to evaluate and determine the most relevant sentence among a list based on the semantic similarity to the query.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')",
                    "api_arguments": [
                        "query",
                        "docs"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer, util\nquery = How many people live in London?\ndocs = [Around 9 Million people live in London, London is known for its financial district]\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\ndoc_score_pairs = list(zip(docs, scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\nfor doc, score in doc_score_pairs:\n print(score, doc)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "WikiAnswers",
                                "accuracy": 77427422
                            },
                            {
                                "name": "PAQ",
                                "accuracy": 64371441
                            },
                            {
                                "name": "Stack Exchange",
                                "accuracy": 25316456
                            }
                        ]
                    },
                    "description": "This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.",
                    "id": "huggingface_api_693",
                    "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1"
                },
                "id": "gorilla_huggingface_tool_688",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 2: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 3: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 5: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 6: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 7: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 8: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 9: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 10: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 11: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.",
            "Tool 12: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.",
            "Tool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.",
            "Tool 14: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 15: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 16: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.",
            "Tool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.",
            "Tool 19: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages."
        ],
        "tool_view": {
            "description": "This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "parameters": {
                "query": {
                    "type": "string",
                    "description": "A single input sentence or question for which semantic similarity scores are computed against the provided documents.",
                    "required": true,
                    "example": "How many people live in London?"
                },
                "docs": {
                    "type": "array",
                    "description": "A list of sentences or paragraphs representing the documents or texts to compare semantically against the query.",
                    "required": true,
                    "example": [
                        "Around 9 Million people live in London",
                        "London is known for its financial district"
                    ]
                }
            },
            "response": "The tool produces a set of similarity scores between a given query sentence and a collection of candidate documents or sentences. The typical structure of the response includes the following components:\n\n1. **Query embedding**: A high-dimensional vector representing the semantic meaning of the input query. This vector is used as the reference for similarity comparisons.\n\n2. **Document embeddings**: A list of high-dimensional vectors, each representing the semantic meaning of a corresponding document or candidate sentence in the input collection.\n\n3. **Similarity scores**: A numeric value for each document indicating the semantic similarity between the query embedding and that document’s embedding. These scores are typically computed using a dot product or cosine similarity.\n\n4. **Ranked document-score pairs**: An ordered list of tuples or objects where each entry contains:\n   - **Document text**: The original text of the document or candidate sentence.\n   - **Similarity score**: The corresponding numeric similarity value.\n   \n   The list is usually sorted in descending order of similarity, so that the most semantically relevant documents appear first.\n\nOverall, the response is a mapping of each input document to a similarity score with respect to the query, enabling semantic search, ranking, or retrieval tasks. The embeddings themselves may also be optionally returned if further downstream processing is needed.",
            "examples": [
                {
                    "user_intent": "I want to find which product descriptions best match the query 'lightweight running shoes for men'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "lightweight running shoes for men",
                            "docs": [
                                "Men's ultralight running sneakers with breathable mesh",
                                "Comfortable leather hiking boots for long trails",
                                "Lightweight sports shoes for daily jogging and training"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Compare these two customer support answers to see which one better addresses 'How do I reset my password?'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "How do I reset my password?",
                            "docs": [
                                "Click on 'Forgot Password' and follow the instructions to reset your password.",
                                "To change your profile picture, go to settings and upload a new image."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to rank news headlines according to their relevance to the question 'Who won the Nobel Peace Prize in 2023?'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "Who won the Nobel Peace Prize in 2023?",
                            "docs": [
                                "The Nobel Peace Prize 2023 was awarded to Narges Mohammadi for her human rights activism.",
                                "Stock markets surged today as tech companies released quarterly earnings.",
                                "Scientists discovered a new species of frog in the Amazon rainforest."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 1: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 2: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 3: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 4: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 5: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 6: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 7: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 8: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 9: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 10: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 11: A sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and other applications that require measuring sentence similarity.\nTool 12: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\nTool 13: A multilingual sentence-transformers model that converts sentences and paragraphs into 512-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity across multiple languages.\nTool 14: This tool uses a BERT-based sentence-transformers model to convert sentences and paragraphs into 768-dimensional dense vector embeddings, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 15: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 16: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 17: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, enabling tasks such as semantic search, clustering, and measuring sentence similarity. It is built on the Hugging Face Transformers framework and is designed for natural language processing applications that require efficient and meaningful sentence embeddings.\nTool 18: This tool uses the `sentence-transformers/paraphrase-MiniLM-L3-v2` model to convert sentences and paragraphs into 384-dimensional dense vector embeddings, enabling semantic comparison for tasks such as clustering, semantic search, and other sentence similarity applications.\nTool 19: The `facebook/dragon-plus-context-encoder` is a BERT-sized dense retrieval model designed to encode context passages for information retrieval tasks. It uses an asymmetric dual-encoder architecture, with separate query and context encoders, and is trained on augmented MS MARCO data to improve retrieval performance. This model is used to generate contextual embeddings that can be compared with query embeddings to identify the most relevant documents or passages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"A single input sentence or question for which semantic similarity scores are computed against the provided documents.\", \"required\": true, \"example\": \"How many people live in London?\"}, \"docs\": {\"type\": \"array\", \"description\": \"A list of sentences or paragraphs representing the documents or texts to compare semantically against the query.\", \"required\": true, \"example\": [\"Around 9 Million people live in London\", \"London is known for its financial district\"]}}, \"response\": \"The tool produces a set of similarity scores between a given query sentence and a collection of candidate documents or sentences. The typical structure of the response includes the following components:\\n\\n1. **Query embedding**: A high-dimensional vector representing the semantic meaning of the input query. This vector is used as the reference for similarity comparisons.\\n\\n2. **Document embeddings**: A list of high-dimensional vectors, each representing the semantic meaning of a corresponding document or candidate sentence in the input collection.\\n\\n3. **Similarity scores**: A numeric value for each document indicating the semantic similarity between the query embedding and that document\\u2019s embedding. These scores are typically computed using a dot product or cosine similarity.\\n\\n4. **Ranked document-score pairs**: An ordered list of tuples or objects where each entry contains:\\n   - **Document text**: The original text of the document or candidate sentence.\\n   - **Similarity score**: The corresponding numeric similarity value.\\n   \\n   The list is usually sorted in descending order of similarity, so that the most semantically relevant documents appear first.\\n\\nOverall, the response is a mapping of each input document to a similarity score with respect to the query, enabling semantic search, ranking, or retrieval tasks. The embeddings themselves may also be optionally returned if further downstream processing is needed.\", \"examples\": [{\"user_intent\": \"I want to find which product descriptions best match the query 'lightweight running shoes for men'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"lightweight running shoes for men\", \"docs\": [\"Men's ultralight running sneakers with breathable mesh\", \"Comfortable leather hiking boots for long trails\", \"Lightweight sports shoes for daily jogging and training\"]}}}, {\"user_intent\": \"Compare these two customer support answers to see which one better addresses 'How do I reset my password?'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"How do I reset my password?\", \"docs\": [\"Click on 'Forgot Password' and follow the instructions to reset your password.\", \"To change your profile picture, go to settings and upload a new image.\"]}}}, {\"user_intent\": \"I need to rank news headlines according to their relevance to the question 'Who won the Nobel Peace Prize in 2023?'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"Who won the Nobel Peace Prize in 2023?\", \"docs\": [\"The Nobel Peace Prize 2023 was awarded to Narges Mohammadi for her human rights activism.\", \"Stock markets surged today as tech companies released quarterly earnings.\", \"Scientists discovered a new species of frog in the Amazon rainforest.\"]}}}]}\n\n# User Query\nAs a business assistant of an international company, find the most relevant sentence among a list of sentences that answers a specific question.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find the sentence that best answers a specific business-related question from a list of candidate sentences.",
                    "tool_description": "Use a sentence-transformers model to encode the question and a set of candidate sentences into dense vector embeddings, compute semantic similarity scores between the question and each sentence, and rank the sentences based on relevance.",
                    "expected_response": "A ranked list of sentences paired with similarity scores, with the most relevant sentence to the question appearing first."
                }
            ],
            "extracted_arguments": {
                "query": "The specific question or sentence to be answered or matched, as a string.",
                "docs": "A list of candidate sentences or text passages to compare against the query, represented as an array of strings."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_349",
        "query": "Our company is making a chatbot that needs to extract information from a paragraph. Get the named entities in the paragraph.",
        "instruction": "Given a `named entity recognition` task, retrieve tools that process textual inputs to identify and extract named entities, providing insights into categories such as person, organization, location, and other related classifications according to the query requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "SequenceTagger.load('flair/ner-english-ontonotes-fast')",
                    "api_arguments": [
                        "sentence"
                    ],
                    "python_environment_requirements": [
                        "flair"
                    ],
                    "example_code": "from flair.data import Sentence\nfrom flair.models import SequenceTagger\ntagger = SequenceTagger.load(flair/ner-english-ontonotes-fast)\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\ntagger.predict(sentence)\nprint(sentence)\nfor entity in sentence.get_spans('ner'):\n  print(entity)",
                    "performance": {
                        "dataset": "Ontonotes",
                        "accuracy": "F1-Score: 89.3"
                    },
                    "description": "This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF.",
                    "id": "huggingface_api_427",
                    "name": "flair/ner-english-ontonotes-fast"
                },
                "id": "gorilla_huggingface_tool_423",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 1: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 4: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 6: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 7: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 8: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 9: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 11: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 12: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.",
            "Tool 14: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.",
            "Tool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 16: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 17: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 18: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages."
        ],
        "tool_view": {
            "description": "A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "parameters": {
                "inputs": {
                    "type": "string",
                    "description": "The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.",
                    "required": true,
                    "example": "I love AutoTrain"
                }
            },
            "response": "The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\n\n1. **Token-level outputs**:  \n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model’s classification of that token.\n\n2. **Typical fields per token**:  \n   - **`word` or `token`**: The actual text of the token from the input sentence.  \n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\n\n3. **Aggregated entities (optional)**:  \n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \n   - **`entity_group` or `entity`**: The combined label for the entity span.  \n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\n\n4. **Overall semantics**:  \n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \n\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.",
            "examples": [
                {
                    "user_intent": "Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "I recently started working at OpenAI and Microsoft."
                        }
                    }
                },
                {
                    "user_intent": "Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Tesla, SpaceX, and Neuralink are all led by Elon Musk."
                        }
                    }
                },
                {
                    "user_intent": "Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'",
                    "tool_call": {
                        "name": "AutoModelForTokenClassification.from_pretrained",
                        "arguments": {
                            "inputs": "Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 1: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 3: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 4: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 5: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 6: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 7: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 8: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 9: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 10: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 11: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 12: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 13: This tool performs English named entity recognition using the large Ontonotes model in the Flair framework. It identifies 18 types of entities—including dates, monetary amounts, organizations, locations, persons, events, products, and works of art—within text, leveraging document-level XLM-R embeddings for high-accuracy predictions. It is used to automatically detect and classify key information in natural language text for tasks like information extraction, text analysis, and content understanding.\nTool 14: This tool provides a pretrained Hugging Face Transformers model for token-level entity extraction, enabling identification of named entities in text. It uses the DistilBERT architecture, was trained with AutoTrain, and achieves high accuracy on relevant datasets, making it suitable for natural language processing tasks that require precise entity recognition.\nTool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 16: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 17: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 18: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\", \"parameters\": {\"inputs\": {\"type\": \"string\", \"description\": \"The text input that will be processed by the model to perform entity extraction. This is typically the raw text containing entities (like company names) that the model will extract.\", \"required\": true, \"example\": \"I love AutoTrain\"}}, \"response\": \"The tool produces a **token classification response** typical of Hugging Face Transformers models for entity extraction. The response structure can be described as follows:\\n\\n1. **Token-level outputs**:  \\n   The model returns a list of predictions corresponding to each token in the input text. Each element in this list represents a single token and contains information about the model\\u2019s classification of that token.\\n\\n2. **Typical fields per token**:  \\n   - **`word` or `token`**: The actual text of the token from the input sentence.  \\n   - **`entity` or `label`**: The predicted entity label assigned to this token (e.g., company name, organization, or non-entity). Labels usually follow the IOB or BIO tagging scheme.  \\n   - **`score` or `confidence`**: A probability or confidence score indicating how confident the model is in its predicted label.  \\n   - **`start` and `end`** (optional depending on implementation): Character offsets in the original input string that indicate the span of text covered by the token.\\n\\n3. **Aggregated entities (optional)**:  \\n   Some implementations may post-process token-level outputs to group consecutive tokens with the same entity label into a single entity mention, returning:  \\n   - **`entity_group` or `entity`**: The combined label for the entity span.  \\n   - **`text`**: The reconstructed text of the entity from consecutive tokens.  \\n   - **`start` and `end`**: Character offsets for the span of the entity in the original text.  \\n   - **`score`**: Confidence score for the grouped entity, often computed as an aggregate of token-level scores.\\n\\n4. **Overall semantics**:  \\n   The response allows a user to identify **which parts of the input text correspond to company names**, along with model confidence and positional information, enabling downstream tasks such as extraction, highlighting, or database population.  \\n\\nIn summary, the response is a **structured token-level prediction** that may optionally be aggregated into entity spans, providing the predicted label, confidence, and positional information for each recognized entity in the text.\", \"examples\": [{\"user_intent\": \"Extract the company names mentioned in the sentence 'I recently started working at OpenAI and Microsoft.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"I recently started working at OpenAI and Microsoft.\"}}}, {\"user_intent\": \"Identify entities representing companies in the text 'Tesla, SpaceX, and Neuralink are all led by Elon Musk.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Tesla, SpaceX, and Neuralink are all led by Elon Musk.\"}}}, {\"user_intent\": \"Find all company names in the paragraph: 'Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.'\", \"tool_call\": {\"name\": \"AutoModelForTokenClassification.from_pretrained\", \"arguments\": {\"inputs\": \"Amazon and Google are expanding their cloud services, while Facebook focuses on social media innovation.\"}}}]}\n\n# User Query\nOur company is making a chatbot that needs to extract information from a paragraph. Get the named entities in the paragraph.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract all named entities from a given paragraph of text.",
                    "tool_description": "Use a Named Entity Recognition (NER) model to identify and classify entities such as persons, organizations, locations, and other relevant types within unstructured text.",
                    "expected_response": "A structured list of recognized entities, including their type labels, text spans, and optional confidence scores."
                }
            ],
            "extracted_arguments": {
                "input_text": "The paragraph of text from which named entities should be extracted."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_58",
        "query": "We are building an AI-based camera system to ensure safety on a construction site. Detect workers wearing hard hats in a given image.",
        "instruction": "Given an `object detection` task, retrieve tools that analyze images to identify specific objects, such as safety equipment, by processing image inputs and returning detected objects and their attributes, ensuring compliance and safety in specified environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Object Detection",
                    "api_call": "YOLO('keremberke/yolov8m-hard-hat-detection')",
                    "api_arguments": {
                        "image": "URL or local path to the image"
                    },
                    "python_environment_requirements": [
                        "ultralyticsplus==0.0.24",
                        "ultralytics==8.0.23"
                    ],
                    "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
                    "performance": {
                        "dataset": "hard-hat-detection",
                        "accuracy": 0.811
                    },
                    "description": "A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.",
                    "id": "huggingface_api_213",
                    "name": "keremberke/yolov8m-hard-hat-detection"
                },
                "id": "gorilla_huggingface_tool_209",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 1: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.",
            "Tool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 3: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 4: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 5: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 8: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 9: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 10: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 11: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 12: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 13: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.",
            "Tool 15: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 16: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 17: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 18: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 19: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images."
        ],
        "tool_view": {
            "description": "A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The URL or local file path of the image on which to perform hard hat object detection.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns the results of an object detection task based on the input image. Specifically, it detects objects related to \"hard hats\" in images and classifies them into two categories: \"Hardhat\" and \"NO-Hardhat.\" \n\nThe response typically contains the following components:\n\n1. **Detection Results**:\n   - This includes the bounding boxes of the detected objects within the image.\n   - Each detected object is represented by its bounding box coordinates, along with the predicted class label (e.g., \"Hardhat\" or \"NO-Hardhat\").\n   - Additional information such as the confidence score of the detection and the intersection-over-union (IoU) score may also be included, depending on the configuration of the model.\n\n2. **Image Annotations**:\n   - If requested, the response may include an annotated version of the input image where the detected objects are visually highlighted by bounding boxes or other markers, such as labels showing the predicted class (e.g., \"Hardhat\").\n\n3. **Performance Metrics**:\n   - The response may include a summary of the model's performance, such as its accuracy on the specific dataset it was trained on, which is provided as a measure of how well the model performs on typical hard hat detection tasks.\n\n4. **Additional Information**:\n   - This could include metadata related to the model, such as the name and version of the model (e.g., \"keremberke/yolov8m-hard-hat-detection\"), and possibly any configuration or hyperparameter overrides used during inference (such as confidence thresholds and IoU settings).\n\nOverall, the response focuses on providing object detection data, image annotations, and performance summaries for the given task.",
            "examples": [
                {
                    "user_intent": "Check if workers in this construction site photo are wearing hard hats.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-hard-hat-detection')",
                        "arguments": {
                            "image": "https://example.com/construction_site1.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this local image from the factory floor to detect employees without hard hats.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-hard-hat-detection')",
                        "arguments": {
                            "image": "/home/user/images/factory_floor.png"
                        }
                    }
                },
                {
                    "user_intent": "Scan a batch of safety inspection photos online to automatically flag workers not wearing hard hats.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-hard-hat-detection')",
                        "arguments": {
                            "image": "https://company-safety-bucket.s3.amazonaws.com/inspection/photo23.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 1: A YOLOv8-based object detection model designed to identify hard hats and missing hard hats in images, useful for workplace safety monitoring and compliance applications.\nTool 2: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 3: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 4: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 5: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 6: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 8: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 9: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 10: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 11: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 12: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 13: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 14: This tool is a German zero-shot text classification model based on the large German BERT model (deepset.ai), fine-tuned for natural language inference. It allows users to classify text into predefined labels without needing any task-specific training data. The model uses a hypothesis template and performs classification based on the input text and given labels, with an accuracy of 85.6% on the XNLI test set. It can be accessed using the Hugging Face Transformers pipeline for zero-shot classification.\nTool 15: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 16: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 17: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 18: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 19: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model designed to detect hard hats in images, distinguishing between \\\"Hardhat\\\" and \\\"NO-Hardhat\\\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The URL or local file path of the image on which to perform hard hat object detection.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns the results of an object detection task based on the input image. Specifically, it detects objects related to \\\"hard hats\\\" in images and classifies them into two categories: \\\"Hardhat\\\" and \\\"NO-Hardhat.\\\" \\n\\nThe response typically contains the following components:\\n\\n1. **Detection Results**:\\n   - This includes the bounding boxes of the detected objects within the image.\\n   - Each detected object is represented by its bounding box coordinates, along with the predicted class label (e.g., \\\"Hardhat\\\" or \\\"NO-Hardhat\\\").\\n   - Additional information such as the confidence score of the detection and the intersection-over-union (IoU) score may also be included, depending on the configuration of the model.\\n\\n2. **Image Annotations**:\\n   - If requested, the response may include an annotated version of the input image where the detected objects are visually highlighted by bounding boxes or other markers, such as labels showing the predicted class (e.g., \\\"Hardhat\\\").\\n\\n3. **Performance Metrics**:\\n   - The response may include a summary of the model's performance, such as its accuracy on the specific dataset it was trained on, which is provided as a measure of how well the model performs on typical hard hat detection tasks.\\n\\n4. **Additional Information**:\\n   - This could include metadata related to the model, such as the name and version of the model (e.g., \\\"keremberke/yolov8m-hard-hat-detection\\\"), and possibly any configuration or hyperparameter overrides used during inference (such as confidence thresholds and IoU settings).\\n\\nOverall, the response focuses on providing object detection data, image annotations, and performance summaries for the given task.\", \"examples\": [{\"user_intent\": \"Check if workers in this construction site photo are wearing hard hats.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"arguments\": {\"image\": \"https://example.com/construction_site1.jpg\"}}}, {\"user_intent\": \"Analyze this local image from the factory floor to detect employees without hard hats.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"arguments\": {\"image\": \"/home/user/images/factory_floor.png\"}}}, {\"user_intent\": \"Scan a batch of safety inspection photos online to automatically flag workers not wearing hard hats.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"arguments\": {\"image\": \"https://company-safety-bucket.s3.amazonaws.com/inspection/photo23.jpg\"}}}]}\n\n# User Query\nWe are building an AI-based camera system to ensure safety on a construction site. Detect workers wearing hard hats in a given image.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect if workers are wearing hard hats in an image from a construction site.",
                    "tool_description": "Perform hard hat object detection on the input image to classify whether individuals are wearing hard hats or not.",
                    "expected_response": "Detection results showing bounding boxes around detected workers, along with labels of 'Hardhat' or 'NO-Hardhat'."
                }
            ],
            "extracted_arguments": {
                "image": "The URL or local file path of the image to be analyzed for hard hat detection."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_369",
        "query": "We are a company that wants to expand globally. We need to translate our website content from English to Italian.",
        "instruction": "Given a `content translation` task, retrieve tools that facilitate language conversion by processing English text inputs into Italian, ensuring the translated output maintains accuracy and context as per the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it')",
                    "api_arguments": {
                        "source_language": "en",
                        "target_language": "it"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import pipeline; translator = pipeline('translation_en_to_it', model='Helsinki-NLP/opus-mt-en-it'); translator('Hello, world!')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "newssyscomb2009.en.it": {
                                "BLEU": 30.9,
                                "chr-F": 0.606
                            },
                            "newstest2009.en.it": {
                                "BLEU": 31.9,
                                "chr-F": 0.604
                            },
                            "Tatoeba.en.it": {
                                "BLEU": 48.2,
                                "chr-F": 0.695
                            }
                        }
                    },
                    "description": "A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library.",
                    "id": "huggingface_api_540",
                    "name": "Helsinki-NLP/opus-mt-en-it"
                },
                "id": "gorilla_huggingface_tool_536",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 1: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 2: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.",
            "Tool 3: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 7: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 8: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 10: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 13: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 15: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 16: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 17: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 18: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages."
        ],
        "tool_view": {
            "description": "A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "parameters": {},
            "response": "The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\n\n**Response Description:**\n\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\n\nKey components of the response:\n\n- **Input Text**: The original English text passed to the translation model.\n- **Translated Text**: The corresponding German translation generated by the model.\n\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\n\n- **Output Format**: The translation response is typically a string containing the translated German text.\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\n\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.",
            "examples": [
                {
                    "user_intent": "Translate the sentence 'The weather is nice today' from English to German.",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "The weather is nice today"
                        }
                    }
                },
                {
                    "user_intent": "I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Artificial intelligence is transforming the world rapidly."
                        }
                    }
                },
                {
                    "user_intent": "Convert this short English dialogue to German: 'Hello! How are you doing?'",
                    "tool_call": {
                        "name": "sshleifer/tiny-marian-en-de",
                        "arguments": {
                            "text": "Hello! How are you doing?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 1: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 2: This tool provides a Transformer-based model for translating text from English to Italian using the Hugging Face Transformers library. It is built on the OPUS dataset and can be used for various translation tasks, offering good performance on benchmarks like BLEU and chr-F scores. The model is accessed via the `pipeline('translation_en_to_it')` API call and is suitable for users needing accurate English-to-Italian translations in Python environments with the `transformers` library.\nTool 3: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 4: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 5: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 6: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 7: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 8: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 10: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 11: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 13: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 15: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 16: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 17: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 18: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 19: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\", \"parameters\": {}, \"response\": \"The tool is designed for text-to-text generation, specifically English-to-German translation, using a model from the Hugging Face Transformers library.\\n\\n**Response Description:**\\n\\nThe response from this tool would consist of a translated German text corresponding to the input English text. The model accepts an English sentence as input and outputs its translation in German.\\n\\nKey components of the response:\\n\\n- **Input Text**: The original English text passed to the translation model.\\n- **Translated Text**: The corresponding German translation generated by the model.\\n\\nWhile the exact structure of the response is not specified in the raw documentation, it would typically follow this general form:\\n\\n- **Output Format**: The translation response is typically a string containing the translated German text.\\n- **Possible Additional Metadata**: Depending on the specific configuration or implementation, the response may include metadata such as:\\n  - **Model Information**: Information about the model used (e.g., 'sshleifer/tiny-marian-en-de').\\n  - **Translation Quality Metrics**: If applicable, some tools may return quality scores or confidence levels for the translation, though this is not explicitly stated in the documentation.\\n\\nThus, the expected output will primarily consist of the translated German text in response to the provided English input.\", \"examples\": [{\"user_intent\": \"Translate the sentence 'The weather is nice today' from English to German.\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"The weather is nice today\"}}}, {\"user_intent\": \"I want to translate this paragraph into German: 'Artificial intelligence is transforming the world rapidly.'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Artificial intelligence is transforming the world rapidly.\"}}}, {\"user_intent\": \"Convert this short English dialogue to German: 'Hello! How are you doing?'\", \"tool_call\": {\"name\": \"sshleifer/tiny-marian-en-de\", \"arguments\": {\"text\": \"Hello! How are you doing?\"}}}]}\n\n# User Query\nWe are a company that wants to expand globally. We need to translate our website content from English to Italian.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to translate website content from English to Italian.",
                    "tool_description": "Translate text from English to Italian using a transformer-based model trained on the OPUS dataset for high-quality translation.",
                    "expected_response": "A translated version of the English text in Italian."
                }
            ],
            "extracted_arguments": {
                "source_language": "The original language of the content to be translated.",
                "target_language": "The language to which the content should be translated.",
                "text": "The content of the website to be translated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_456",
        "query": "Our multinational company is dealing with a French client. Please help us communicate in French by translating an English sentence into French.",
        "instruction": "Given a `translation` task, retrieve tools that enable translating text from English to French, ensuring the output message effectively facilitates communication with the French client by processing the input sentence and providing an accurate translation.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment ça va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 1: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 3: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 7: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.",
            "Tool 8: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 9: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 10: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 11: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 12: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 13: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 14: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 15: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 16: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.",
            "Tool 17: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 18: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.",
            "Tool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation."
        ],
        "tool_view": {
            "description": "CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.",
                    "required": true,
                    "example": "camembert-base"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.",
                    "required": true,
                    "example": "camembert-base"
                }
            },
            "response": "The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \n\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \n- **score**: A numerical value representing the model’s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \n- **token**: The identifier of the predicted token according to the model’s vocabulary.  \n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \n\nThe response is usually ordered by descending score, so the first entry is the model’s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.",
            "examples": [
                {
                    "user_intent": "Predict the missing word in the French sentence about cheese.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Le camembert est <mask> :)"
                        }
                    }
                },
                {
                    "user_intent": "Complete this French sentence describing the weather.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Aujourd'hui, il fait très <mask> dehors."
                        }
                    }
                },
                {
                    "user_intent": "Fill in the blank for a French sentence about happiness.",
                    "tool_call": {
                        "name": "camembert-base",
                        "arguments": {
                            "model": "camembert-base",
                            "tokenizer": "camembert-base",
                            "text": "Je me sens vraiment <mask> aujourd'hui."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 1: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 2: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 3: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 4: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 6: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 7: A French-language summarization tool that uses the Barthez model fine-tuned on the OrangeSum dataset to generate abstractive summaries from input text.\nTool 8: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 9: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 10: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 11: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 12: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 13: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 14: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 15: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 16: This tool performs sentiment analysis using the multilingual XLM-RoBERTa model, specifically trained on approximately 198 million tweets. It is fine-tuned for sentiment classification in 8 languages (Arabic, English, French, German, Hindi, Italian, Spanish, and Portuguese) but can handle more languages. The model is accessible via the Hugging Face `pipeline` API, making it easy to integrate into NLP workflows.\nTool 17: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 18: Camembert-ner is a French Named Entity Recognition (NER) model based on CamemBERT, fine-tuned on the wikiner-fr dataset. It identifies entities such as persons, organizations, locations, and miscellaneous categories in text, enabling applications like information extraction, text analysis, and entity-based search.\nTool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained CamemBERT model to use for the fill-mask task. This defines the specific model weights and configuration.\", \"required\": true, \"example\": \"camembert-base\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The name or path of the tokenizer corresponding to the CamemBERT model. This ensures proper tokenization of input text for the fill-mask task.\", \"required\": true, \"example\": \"camembert-base\"}}, \"response\": \"The tool produces a response consisting of a ranked list of candidate token predictions that can replace the masked position in the input text. Each entry in the response typically includes the following components:  \\n\\n- **sequence**: The input text with the `<mask>` token replaced by the predicted token, showing the completed sentence.  \\n- **score**: A numerical value representing the model\\u2019s confidence or probability for this prediction, indicating how likely the predicted token fits the masked position.  \\n- **token**: The identifier of the predicted token according to the model\\u2019s vocabulary.  \\n- **token_str**: The human-readable string corresponding to the predicted token, which can be directly inserted into the text.  \\n\\nThe response is usually ordered by descending score, so the first entry is the model\\u2019s most likely prediction. Multiple predictions are returned to allow exploration of alternative plausible completions for the masked token.\", \"examples\": [{\"user_intent\": \"Predict the missing word in the French sentence about cheese.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Le camembert est <mask> :)\"}}}, {\"user_intent\": \"Complete this French sentence describing the weather.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Aujourd'hui, il fait tr\\u00e8s <mask> dehors.\"}}}, {\"user_intent\": \"Fill in the blank for a French sentence about happiness.\", \"tool_call\": {\"name\": \"camembert-base\", \"arguments\": {\"model\": \"camembert-base\", \"tokenizer\": \"camembert-base\", \"text\": \"Je me sens vraiment <mask> aujourd'hui.\"}}}]}\n\n# User Query\nOur multinational company is dealing with a French client. Please help us communicate in French by translating an English sentence into French.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate an English sentence into French for communication with a French client.",
                    "tool_description": "Use a machine translation model to convert input text from English to French, preserving meaning and context.",
                    "expected_response": "The translated French text corresponding to the input English sentence."
                }
            ],
            "extracted_arguments": {
                "source_text": "The English sentence or text that needs to be translated into French."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_5",
        "query": "As a toy company, we are designing a new toy line. We'd like you to create an image of a toy robot using relevant text prompts as control input.",
        "instruction": "Given a `text-to-image generation` task, retrieve tools that utilize text prompts to create visual representations, specifically focusing on generating images of toy robots using models like ControlNet and Stable Diffusion which process text inputs and control conditions for image creation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Image Diffusion Models",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_mlsd')",
                    "api_arguments": [
                        "checkpoint",
                        "torch_dtype"
                    ],
                    "python_environment_requirements": [
                        "diffusers",
                        "transformers",
                        "accelerate",
                        "controlnet_aux"
                    ],
                    "example_code": "import torch\nimport os\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nfrom controlnet_aux import MLSDdetector\nfrom diffusers import (\n ControlNetModel,\n StableDiffusionControlNetPipeline,\n UniPCMultistepScheduler,\n)\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\nimage = load_image(\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\n)\nprompt = royal chamber with fancy bed\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\ncontrol_image = processor(image)\ncontrol_image.save(./images/control.png)\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\nimage.save('images/image_out.png')",
                    "performance": {
                        "dataset": "MLSD",
                        "accuracy": "Not provided"
                    },
                    "description": "Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.",
                    "id": "huggingface_api_278",
                    "name": "lllyasviel/control_v11p_sd15_mlsd"
                },
                "id": "gorilla_huggingface_tool_274",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 2: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 4: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.",
            "Tool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 6: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.",
            "Tool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 8: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 9: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 10: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 11: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.",
            "Tool 14: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 15: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 17: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.",
            "Tool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 19: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs."
        ],
        "tool_view": {
            "description": "Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A detailed textual description of the image you want the model to generate. This is the main input guiding the image creation.",
                    "required": true,
                    "example": "a close-up portrait of a young woman with long hair, pale skin, wearing futuristic clothes, in a city ruins background, high resolution, cinematic lighting"
                },
                "negative_prompt": {
                    "type": "string",
                    "description": "A textual description of elements or features you want the model to avoid including in the generated image, helping to refine image quality.",
                    "required": true,
                    "example": "deformed anatomy, cartoon style, low quality, text, cropped, blurry"
                }
            },
            "response": "The tool is expected to return a generated image based on the provided textual prompts. The response structure typically includes the following components:\n\n1. **image**: This component contains the generated image data, which is usually represented in a format suitable for display or further processing (e.g., a URL to the image, base64-encoded image data, or a binary image file).\n\n2. **prompt**: This field reflects the original textual prompt that was used to generate the image. It serves to confirm the input that led to the creation of the output image.\n\n3. **negative_prompt**: This field includes the negative prompt provided during the generation process. It indicates the aspects or features that were intended to be avoided in the generated image, providing context for the output.\n\n4. **metadata**: This optional component may include additional information about the generation process, such as the model version used, generation time, or any parameters that influenced the output.\n\nOverall, the response encapsulates the generated image along with the prompts used, allowing users to understand the context and constraints under which the image was created.",
            "examples": [
                {
                    "user_intent": "Generate a realistic portrait of a 30-year-old man wearing a leather jacket in a cyberpunk city at night.",
                    "tool_call": {
                        "name": "Realistic_Vision_V1.4",
                        "arguments": {
                            "prompt": "a close-up portrait of a 30-year-old man wearing a leather jacket, neon-lit cyberpunk city at night, highly detailed face, 8k uhd, soft lighting, cinematic, film grain",
                            "negative_prompt": "(cartoon, sketch, 3d render, low quality, blurry, text, extra limbs, mutated face, poorly drawn hands, bad anatomy)"
                        }
                    }
                },
                {
                    "user_intent": "Create a photorealistic image of a serene mountain landscape at sunrise.",
                    "tool_call": {
                        "name": "Realistic_Vision_V1.4",
                        "arguments": {
                            "prompt": "serene mountain landscape at sunrise, fog in the valleys, sunlight hitting the peaks, realistic textures, high detail, 8k uhd, cinematic lighting",
                            "negative_prompt": "(cartoon, painting, low resolution, sketch, out of frame, deformed, blurry, jpeg artifacts, extra limbs, unnatural colors)"
                        }
                    }
                },
                {
                    "user_intent": "Produce a realistic image of a futuristic sports car on a rainy city street.",
                    "tool_call": {
                        "name": "Realistic_Vision_V1.4",
                        "arguments": {
                            "prompt": "futuristic sports car on a rainy city street, reflections on wet asphalt, neon signs, highly detailed vehicle and environment, 8k uhd, cinematic lighting",
                            "negative_prompt": "(sketch, cartoon, poorly drawn car, 3d render, blurry, low quality, extra wheels, deformed vehicle, text, out of frame)"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 2: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 3: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 4: ControlNet v1.1 is a neural network model that enhances diffusion-based image generation by providing additional control through conditioning on NormalBae images. It can be integrated with Stable Diffusion models to guide the generation process based on input images, enabling more precise and structured image-to-image synthesis from text prompts.\nTool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 6: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.\nTool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 8: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 9: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 10: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 11: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 12: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 13: ControlNet v1.1 is a neural network designed to control diffusion models by incorporating additional conditions, such as segmentation images. This specific checkpoint is conditioned on segmentation images and can be used in combination with Stable Diffusion models, like runwayml/stable-diffusion-v1-5, to generate images from text prompts with more control over the output.\nTool 14: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 15: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 17: The \"lllyasviel/control_v11p_sd15_scribble\" tool is a neural network model used to control diffusion models by incorporating additional conditions, specifically using scribble-based input images. It is designed to work with Stable Diffusion, allowing users to guide the generation process with sketches or scribbles, enhancing the creative control over the output. This model is useful in text-to-image generation tasks where precise input constraints, such as rough drawings, are needed to influence the resulting image.\nTool 18: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 19: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A detailed textual description of the image you want the model to generate. This is the main input guiding the image creation.\", \"required\": true, \"example\": \"a close-up portrait of a young woman with long hair, pale skin, wearing futuristic clothes, in a city ruins background, high resolution, cinematic lighting\"}, \"negative_prompt\": {\"type\": \"string\", \"description\": \"A textual description of elements or features you want the model to avoid including in the generated image, helping to refine image quality.\", \"required\": true, \"example\": \"deformed anatomy, cartoon style, low quality, text, cropped, blurry\"}}, \"response\": \"The tool is expected to return a generated image based on the provided textual prompts. The response structure typically includes the following components:\\n\\n1. **image**: This component contains the generated image data, which is usually represented in a format suitable for display or further processing (e.g., a URL to the image, base64-encoded image data, or a binary image file).\\n\\n2. **prompt**: This field reflects the original textual prompt that was used to generate the image. It serves to confirm the input that led to the creation of the output image.\\n\\n3. **negative_prompt**: This field includes the negative prompt provided during the generation process. It indicates the aspects or features that were intended to be avoided in the generated image, providing context for the output.\\n\\n4. **metadata**: This optional component may include additional information about the generation process, such as the model version used, generation time, or any parameters that influenced the output.\\n\\nOverall, the response encapsulates the generated image along with the prompts used, allowing users to understand the context and constraints under which the image was created.\", \"examples\": [{\"user_intent\": \"Generate a realistic portrait of a 30-year-old man wearing a leather jacket in a cyberpunk city at night.\", \"tool_call\": {\"name\": \"Realistic_Vision_V1.4\", \"arguments\": {\"prompt\": \"a close-up portrait of a 30-year-old man wearing a leather jacket, neon-lit cyberpunk city at night, highly detailed face, 8k uhd, soft lighting, cinematic, film grain\", \"negative_prompt\": \"(cartoon, sketch, 3d render, low quality, blurry, text, extra limbs, mutated face, poorly drawn hands, bad anatomy)\"}}}, {\"user_intent\": \"Create a photorealistic image of a serene mountain landscape at sunrise.\", \"tool_call\": {\"name\": \"Realistic_Vision_V1.4\", \"arguments\": {\"prompt\": \"serene mountain landscape at sunrise, fog in the valleys, sunlight hitting the peaks, realistic textures, high detail, 8k uhd, cinematic lighting\", \"negative_prompt\": \"(cartoon, painting, low resolution, sketch, out of frame, deformed, blurry, jpeg artifacts, extra limbs, unnatural colors)\"}}}, {\"user_intent\": \"Produce a realistic image of a futuristic sports car on a rainy city street.\", \"tool_call\": {\"name\": \"Realistic_Vision_V1.4\", \"arguments\": {\"prompt\": \"futuristic sports car on a rainy city street, reflections on wet asphalt, neon signs, highly detailed vehicle and environment, 8k uhd, cinematic lighting\", \"negative_prompt\": \"(sketch, cartoon, poorly drawn car, 3d render, blurry, low quality, extra wheels, deformed vehicle, text, out of frame)\"}}}]}\n\n# User Query\nAs a toy company, we are designing a new toy line. We'd like you to create an image of a toy robot using relevant text prompts as control input.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a realistic image of a toy robot based on descriptive text prompts.",
                    "tool_description": "Create a high-quality, detailed image of a toy robot using a text-to-image model, allowing precise control over visual features through textual prompts.",
                    "expected_response": "A generated image of the toy robot along with the prompts used and optional metadata about the generation process."
                }
            ],
            "extracted_arguments": {
                "prompt": "A detailed textual description of the toy robot's appearance, features, and environment.",
                "negative_prompt": "A textual description of elements to avoid in the toy robot image, such as unwanted styles, distortions, or low-quality renderings."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_462",
        "query": "An interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize computer vision techniques to process photographic images and accurately determine the depth information necessary for remodeling purposes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "DIODE",
                        "accuracy": ""
                    },
                    "description": "A depth estimation model fine-tuned on the DIODE dataset.",
                    "id": "huggingface_api_153",
                    "name": "glpn-nyu-finetuned-diode-221215-093747"
                },
                "id": "gorilla_huggingface_tool_149",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.",
            "Tool 1: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.",
            "Tool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 3: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 4: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.",
            "Tool 5: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 6: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.",
            "Tool 8: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 9: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 11: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 12: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.",
            "Tool 13: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.",
            "Tool 14: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.",
            "Tool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.",
            "Tool 16: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.",
            "Tool 17: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.",
            "Tool 18: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.",
            "Tool 19: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows."
        ],
        "tool_view": {
            "description": "Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text string describing the scene or concept to generate as an image. Include the phrase 'analog style' to produce images in an analog photography style.",
                    "required": true,
                    "example": "analog style landscape with mountains and a river"
                }
            },
            "response": "The tool generates an image based on the provided text prompt, which is processed by the \"wavymulder/Analog-Diffusion\" model. The model is trained to create images with an \"analog style\" similar to analog photographs. \n\nThe response will contain an image generated according to the input text prompt, following these key aspects:\n\n- **Image Data**: The generated image in a specific format (such as a URL to the image or direct binary data), representing the visual output based on the given description in the prompt. \n- **Prompt Interpretation**: The model interprets the provided text prompt, possibly using the term \"analog style\" if specified, to apply the analog photographic style to the image.\n- **Style Compliance**: The response will follow the expected analog aesthetic dictated by the training on the \"analog photographs\" dataset.\n\nThus, the output is primarily an image, with potential metadata related to the style and fidelity of the image generation process.",
            "examples": [
                {
                    "user_intent": "Generate a dreamy analog-style landscape with mountains and a sunset.",
                    "tool_call": {
                        "name": "wavymulder/Analog-Diffusion",
                        "arguments": {
                            "prompt": "analog style landscape with mountains at sunset"
                        }
                    }
                },
                {
                    "user_intent": "Create an analog-style portrait of a vintage 1970s street scene.",
                    "tool_call": {
                        "name": "wavymulder/Analog-Diffusion",
                        "arguments": {
                            "prompt": "analog style 1970s street scene portrait"
                        }
                    }
                },
                {
                    "user_intent": "Produce an analog-style photo of a cozy cabin in a snowy forest.",
                    "tool_call": {
                        "name": "wavymulder/Analog-Diffusion",
                        "arguments": {
                            "prompt": "analog style cozy cabin in snowy forest"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.\nTool 1: The \"lllyasviel/sd-controlnet-depth\" tool is a depth estimation model that integrates with Stable Diffusion to enhance image generation by incorporating depth information. It uses ControlNet, a neural network that conditions diffusion models with additional inputs like depth data, allowing for more accurate and contextually rich outputs. This model is especially useful for applications that require depth-aware image generation or manipulation. The tool is implemented in Python and relies on libraries such as Diffusers, Transformers, and Torch, offering a straightforward pipeline for depth estimation in images.\nTool 2: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 3: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 4: A depth estimation model based on Hugging Face Transformers, fine-tuned on the DIODE dataset, used to predict per-pixel depth from images.\nTool 5: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 6: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 7: A lightweight GLPN model for estimating depth in images, implemented using the Hugging Face Transformers library, suitable for quick testing or prototyping of depth estimation tasks.\nTool 8: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 9: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 11: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 12: A fine-tuned GLPN model for depth estimation, trained on the diode-subset dataset, designed to predict per-pixel depth from images using the Hugging Face Transformers framework.\nTool 13: This model is a fine-tuned version of the GLPN (Global-Local Perception Network) for depth estimation, trained on the DIODE-subset dataset. It uses Hugging Face Transformers and is designed to predict depth from images with notable performance metrics such as a loss of 0.3736 and an RMSE of 0.4321. It is suitable for tasks that require accurate depth estimation from visual data.\nTool 14: This model is a fine-tuned version of vinvino02/glpn-nyu for depth estimation, optimized on the diode-subset dataset. It can be used for estimating depth from images with the Hugging Face Transformers framework.\nTool 15: This model, fine-tuned on the diode-subset dataset, is designed for depth estimation tasks. It is based on the GLPN (Global-Local Planar Network) architecture and can be used for accurately predicting depth information in images.\nTool 16: This tool provides a depth estimation pipeline using a transformer model fine-tuned on the DIODE subset dataset, enabling accurate prediction of per-pixel depth from images for computer vision applications.\nTool 17: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, specifically trained on the diode-subset dataset. It is designed for use with Hugging Face Transformers and is suitable for estimating depth from images.\nTool 18: This tool provides a fine-tuned model for depth estimation, specifically designed for use with the DIoU-Subset dataset. Built on the Hugging Face Transformers framework, it utilizes the \"glpn-nyu\" model pre-trained on depth estimation tasks. The model can be used for tasks that require accurate depth prediction from images, and its performance is benchmarked with metrics such as Loss, MAE, and RMSE.\nTool 19: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text string describing the scene or concept to generate as an image. Include the phrase 'analog style' to produce images in an analog photography style.\", \"required\": true, \"example\": \"analog style landscape with mountains and a river\"}}, \"response\": \"The tool generates an image based on the provided text prompt, which is processed by the \\\"wavymulder/Analog-Diffusion\\\" model. The model is trained to create images with an \\\"analog style\\\" similar to analog photographs. \\n\\nThe response will contain an image generated according to the input text prompt, following these key aspects:\\n\\n- **Image Data**: The generated image in a specific format (such as a URL to the image or direct binary data), representing the visual output based on the given description in the prompt. \\n- **Prompt Interpretation**: The model interprets the provided text prompt, possibly using the term \\\"analog style\\\" if specified, to apply the analog photographic style to the image.\\n- **Style Compliance**: The response will follow the expected analog aesthetic dictated by the training on the \\\"analog photographs\\\" dataset.\\n\\nThus, the output is primarily an image, with potential metadata related to the style and fidelity of the image generation process.\", \"examples\": [{\"user_intent\": \"Generate a dreamy analog-style landscape with mountains and a sunset.\", \"tool_call\": {\"name\": \"wavymulder/Analog-Diffusion\", \"arguments\": {\"prompt\": \"analog style landscape with mountains at sunset\"}}}, {\"user_intent\": \"Create an analog-style portrait of a vintage 1970s street scene.\", \"tool_call\": {\"name\": \"wavymulder/Analog-Diffusion\", \"arguments\": {\"prompt\": \"analog style 1970s street scene portrait\"}}}, {\"user_intent\": \"Produce an analog-style photo of a cozy cabin in a snowy forest.\", \"tool_call\": {\"name\": \"wavymulder/Analog-Diffusion\", \"arguments\": {\"prompt\": \"analog style cozy cabin in snowy forest\"}}}]}\n\n# User Query\nAn interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate the depth of a room from a photograph to understand its spatial layout for remodeling purposes.",
                    "tool_description": "Perform per-pixel depth estimation on room images using a fine-tuned GLPN or DPT-based depth estimation model, enabling accurate 3D understanding of interior spaces.",
                    "expected_response": "A depth map image corresponding to the input photograph, where each pixel encodes the distance from the camera, along with optional accuracy metrics like RMSE or MAE."
                }
            ],
            "extracted_arguments": {
                "input_image": "A photograph of the room whose depth needs to be estimated, in a standard image format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_283",
        "query": "I need an AI feature capable of extracting names of people, organizations, and locations from various news articles in different languages.",
        "instruction": "Given a `named entity extraction` task, retrieve tools that can identify and extract names of people, organizations, and locations from text inputs, specifically from multilingual news articles, by utilizing token classification models designed for Named Entity Recognition in various languages.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Transformers",
                    "functionality": "Named Entity Recognition",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')",
                    "api_arguments": {
                        "model": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
                        "tokenizer": "Davlan/distilbert-base-multilingual-cased-ner-hrl"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\nner_results = nlp(example)\nprint(ner_results)",
                    "performance": {
                        "dataset": [
                            {
                                "name": "ANERcorp",
                                "language": "Arabic"
                            },
                            {
                                "name": "conll 2003",
                                "language": "German"
                            },
                            {
                                "name": "conll 2003",
                                "language": "English"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Spanish"
                            },
                            {
                                "name": "Europeana Newspapers",
                                "language": "French"
                            },
                            {
                                "name": "Italian I-CAB",
                                "language": "Italian"
                            },
                            {
                                "name": "Latvian NER",
                                "language": "Latvian"
                            },
                            {
                                "name": "conll 2002",
                                "language": "Dutch"
                            },
                            {
                                "name": "Paramopama + Second Harem",
                                "language": "Portuguese"
                            },
                            {
                                "name": "MSRA",
                                "language": "Chinese"
                            }
                        ],
                        "accuracy": "Not specified"
                    },
                    "description": "distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).",
                    "id": "huggingface_api_408",
                    "name": "distilbert-base-multilingual-cased-ner-hrl"
                },
                "id": "gorilla_huggingface_tool_404",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 1: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.",
            "Tool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 3: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.",
            "Tool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.",
            "Tool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 6: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 9: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.",
            "Tool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 11: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.",
            "Tool 12: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 13: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.",
            "Tool 14: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 15: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 16: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 17: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 18: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.",
            "Tool 19: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content."
        ],
        "tool_view": {
            "description": "The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "parameters": {
                "sentence": {
                    "type": "string",
                    "description": "The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.",
                    "required": true,
                    "example": "George Washington went to Washington"
                }
            },
            "response": "The tool’s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\n\nEach entity span in the response includes at least the following components:\n\n- The text segment from the sentence identified as an entity.\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\n- The position or span indices within the sentence marking where the entity occurs.\n- A confidence score or probability reflecting the model’s certainty about the prediction (typically included in Flair’s entity spans).\n\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.",
            "examples": [
                {
                    "user_intent": "Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Barack Obama visited Paris last summer."
                        }
                    }
                },
                {
                    "user_intent": "I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Apple announced a new product in New York today."
                        }
                    }
                },
                {
                    "user_intent": "Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'",
                    "tool_call": {
                        "name": "flair/ner-english",
                        "arguments": {
                            "sentence": "Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 1: The \"flair/ner-english-ontonotes-fast\" model is a fast Named Entity Recognition (NER) tool for English, capable of identifying 18 different entity types such as person names, dates, locations, organizations, money, and more. Built using Flair embeddings and an LSTM-CRF architecture, this model achieves an F1-score of 89.3 on the Ontonotes dataset. It is designed to extract structured information from text by recognizing entities in a given sentence, making it useful for various natural language processing applications like information extraction and text annotation.\nTool 2: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 3: A fine-tuned T5 model designed for summarizing news articles, converting full-length articles into concise summaries. It is optimized for text-to-text generation tasks using the Hugging Face Transformers framework and is intended to simplify and condense news content efficiently.\nTool 4: A high-accuracy English named entity recognition (NER) model from Flair that identifies person names, locations, organizations, and miscellaneous entities in text. It leverages document-level XLM-R embeddings and FLERT for robust predictions and can be used to automatically extract structured entity information from sentences.\nTool 5: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 6: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 8: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 9: A natural-language summarization tool based on the IT5 Base model, fine-tuned on Italian news datasets to generate concise summaries of news articles and other text, enabling efficient understanding and processing of Italian-language content.\nTool 10: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 11: A text summarization tool based on the PEGASUS model, fine-tuned to condense long passages into concise summaries while preserving key information, commonly used to quickly extract the main points from news articles, reports, or other lengthy documents.\nTool 12: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 13: A natural language processing model based on PEGASUS, fine-tuned specifically for summarizing financial news. It generates concise abstractive summaries of financial texts, such as articles on stocks, markets, currencies, and corporate transactions, helping users quickly extract key information from lengthy reports.\nTool 14: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 15: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 16: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 17: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 18: A French-language T5 Transformer model fine-tuned for abstractive text summarization, capable of generating concise summaries of articles with strong performance on the CNN/DailyMail dataset.\nTool 19: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"flair/ner-english\\\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\", \"parameters\": {\"sentence\": {\"type\": \"string\", \"description\": \"The input text sentence on which the Named Entity Recognition (NER) model will perform entity tagging.\", \"required\": true, \"example\": \"George Washington went to Washington\"}}, \"response\": \"The tool\\u2019s response consists of an annotated sentence object that includes recognized named entities identified within the input text. Specifically, after the model processes the input sentence, the response encapsulates the sentence along with labeled entity spans corresponding to one of four predefined categories: PER (person names), LOC (location names), ORG (organization names), and MISC (miscellaneous names).\\n\\nEach entity span in the response includes at least the following components:\\n\\n- The text segment from the sentence identified as an entity.\\n- The entity label indicating the type of named entity (PER, LOC, ORG, or MISC).\\n- The position or span indices within the sentence marking where the entity occurs.\\n- A confidence score or probability reflecting the model\\u2019s certainty about the prediction (typically included in Flair\\u2019s entity spans).\\n\\nOverall, the response structure is a richly annotated sentence object that integrates the original text with its token-level classification into named entity categories, enabling downstream use such as extraction, display, or further analysis of recognized entities.\", \"examples\": [{\"user_intent\": \"Identify all the named entities in the sentence 'Barack Obama visited Paris last summer.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Barack Obama visited Paris last summer.\"}}}, {\"user_intent\": \"I want to extract organizations and locations mentioned in 'Apple announced a new product in New York today.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Apple announced a new product in New York today.\"}}}, {\"user_intent\": \"Find all the people, places, and miscellaneous entities in 'Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.'\", \"tool_call\": {\"name\": \"flair/ner-english\", \"arguments\": {\"sentence\": \"Elon Musk and Jeff Bezos attended a conference in Berlin hosted by SpaceX.\"}}}]}\n\n# User Query\nI need an AI feature capable of extracting names of people, organizations, and locations from various news articles in different languages.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Extract names of people, organizations, and locations from news articles in multiple languages.",
                    "tool_description": "Perform Named Entity Recognition (NER) on multilingual text to identify and classify entities such as persons, organizations, and locations using a pre-trained multilingual model.",
                    "expected_response": "A structured list of entities for each input text, where each entity includes its text, type (person, organization, location), position in the text, and a confidence score."
                }
            ],
            "extracted_arguments": {
                "text": "The input news article or text content from which named entities should be extracted, provided as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_128",
        "query": "We need to estimate the depth of a scene in an image using a pretrained model. Can you please suggest a way?",
        "instruction": "Given a `depth estimation` task, retrieve tools that utilize pretrained models to analyze image inputs and provide depth predictions. Ensure the tools are compatible with image processing and equipped with the necessary model architectures for accurate depth estimation.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": null,
                    "performance": {
                        "dataset": "DIODE",
                        "accuracy": null
                    },
                    "description": "A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.",
                    "id": "huggingface_api_155",
                    "name": "glpn-nyu-finetuned-diode-221215-095508"
                },
                "id": "gorilla_huggingface_tool_151",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "Tool 1: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.",
            "Tool 2: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.",
            "Tool 3: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.",
            "Tool 4: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.",
            "Tool 5: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.",
            "Tool 6: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 7: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.",
            "Tool 8: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.",
            "Tool 9: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.",
            "Tool 11: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 12: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.",
            "Tool 13: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.",
            "Tool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 15: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.",
            "Tool 16: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 17: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.",
            "Tool 18: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.",
            "Tool 19: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms."
        ],
        "tool_view": {
            "description": "A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.",
            "parameters": {},
            "response": "The tool loads a depth estimation model based on the GLPN architecture that has been fine-tuned on the DIODE dataset. The model, once instantiated via the `AutoModel.from_pretrained` call, returns an object representing the pre-trained transformer model, which can be used for performing depth estimation on input images.  \n\nThe response (i.e., the model object) typically includes the following components:\n\n- **Model architecture and configuration**: Metadata about the GLPN model, including layer definitions, input/output shapes, and hyperparameters. This defines how input images are processed to generate depth predictions.  \n- **Pre-trained weights**: The learned parameters obtained from fine-tuning on the DIODE dataset, enabling the model to predict depth maps accurately.  \n- **Forward method**: A callable interface for passing image tensors through the model to compute outputs. For depth estimation, this produces depth maps corresponding to the input images.  \n- **Utility methods**: Functions for model inspection, saving/loading, and possibly device management (CPU/GPU).  \n- **Configuration object (`config`)**: Contains settings related to the pre-trained model, such as normalization parameters, expected input resolution, and any model-specific options.  \n\nThe model object itself does not return depth predictions until an input image tensor is passed through it; therefore, the immediate response is a configured, ready-to-use model instance rather than actual depth maps.",
            "examples": [
                {
                    "user_intent": "I want to estimate the depth of objects in a single image using a pre-trained model.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_path": "living_room.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Generate a depth map for an outdoor scene photo I took yesterday.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_path": "park_scene.png"
                        }
                    }
                },
                {
                    "user_intent": "I want to create a depth visualization for multiple images in my dataset to analyze scene geometry.",
                    "tool_call": {
                        "name": "glpn-nyu-finetuned-diode-221215-095508",
                        "arguments": {
                            "image_paths": [
                                "kitchen.png",
                                "office.jpg",
                                "street_view.jpeg"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\nTool 1: A pre-trained Global-Local Path Networks (GLPN) model for estimating depth from a single image, designed for monocular depth prediction using the NYUv2 dataset. It processes images to generate per-pixel depth maps, enabling applications in computer vision tasks that require understanding scene geometry and spatial relationships.\nTool 2: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE-subset dataset, enabling accurate prediction of scene depth from images. It is designed for computer vision tasks that require understanding the geometric structure of a scene.\nTool 3: This tool provides a pre-trained GLPN model fine-tuned on the diode-subset dataset for depth estimation in computer vision, enabling accurate prediction of scene depth from images using the Hugging Face Transformers framework.\nTool 4: This tool provides a fine-tuned GLPN model for depth estimation in computer vision, designed to predict per-pixel depth from images using the diode-subset dataset, enabling tasks that require understanding scene geometry and spatial structure.\nTool 5: A depth estimation model based on the DPT architecture that predicts scene depth from images, useful for applications in 3D reconstruction, robotics, and computer vision tasks requiring spatial understanding.\nTool 6: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 7: This tool provides a fine-tuned depth estimation model for computer vision, built on Hugging Face Transformers. It is designed to predict per-pixel depth from images using the diode-subset dataset, enabling applications that require understanding scene geometry and spatial structure.\nTool 8: This tool provides a Hugging Face Transformers model fine-tuned for depth estimation on the DIODE dataset, allowing users to predict scene depth from images with improved accuracy over the base GLPN-NYU model.\nTool 9: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 10: This tool provides a fine-tuned depth estimation model based on the GLPN architecture, trained on the diode-subset dataset. It predicts per-pixel depth from images with strong accuracy, offering metrics such as RMSE, MAE, and relative error, making it useful for computer vision tasks that require reliable scene depth understanding.\nTool 11: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 12: A SegFormer model fine-tuned for semantic image segmentation on the CityScapes dataset at 1024×1024 resolution, designed to efficiently label each pixel in urban scene images using transformer-based features. It can be used to perform accurate scene parsing and extract structured information from street-level imagery.\nTool 13: This tool uses a Hugging Face Transformers pipeline to perform depth estimation from images, leveraging a model fine-tuned on the DIODE dataset to predict per-pixel depth with high accuracy, making it useful for computer vision tasks that require understanding scene geometry.\nTool 14: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 15: Performs semantic image segmentation using the UperNet framework with a ConvNeXt backbone, enabling detailed scene understanding in computer vision tasks.\nTool 16: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 17: This tool performs depth estimation in computer vision applications using a fine-tuned model based on the GLPN-KITTI architecture, trained on the Diode-Subset dataset. It leverages the Hugging Face Transformers framework and can be accessed through the 'depth-estimation' pipeline for accurate depth predictions.\nTool 18: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.\nTool 19: A MobileNet-v3 image classification model pretrained on ImageNet-1k, designed to recognize and categorize images using the Hugging Face Transformers framework. It can be used to quickly perform accurate image classification tasks with minimal setup by leveraging a pretrained model and standardized preprocessing transforms.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A depth estimation model based on the GLPN architecture, fine-tuned on the DIODE dataset to predict scene depth from images using Hugging Face Transformers.\", \"parameters\": {}, \"response\": \"The tool loads a depth estimation model based on the GLPN architecture that has been fine-tuned on the DIODE dataset. The model, once instantiated via the `AutoModel.from_pretrained` call, returns an object representing the pre-trained transformer model, which can be used for performing depth estimation on input images.  \\n\\nThe response (i.e., the model object) typically includes the following components:\\n\\n- **Model architecture and configuration**: Metadata about the GLPN model, including layer definitions, input/output shapes, and hyperparameters. This defines how input images are processed to generate depth predictions.  \\n- **Pre-trained weights**: The learned parameters obtained from fine-tuning on the DIODE dataset, enabling the model to predict depth maps accurately.  \\n- **Forward method**: A callable interface for passing image tensors through the model to compute outputs. For depth estimation, this produces depth maps corresponding to the input images.  \\n- **Utility methods**: Functions for model inspection, saving/loading, and possibly device management (CPU/GPU).  \\n- **Configuration object (`config`)**: Contains settings related to the pre-trained model, such as normalization parameters, expected input resolution, and any model-specific options.  \\n\\nThe model object itself does not return depth predictions until an input image tensor is passed through it; therefore, the immediate response is a configured, ready-to-use model instance rather than actual depth maps.\", \"examples\": [{\"user_intent\": \"I want to estimate the depth of objects in a single image using a pre-trained model.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_path\": \"living_room.jpg\"}}}, {\"user_intent\": \"Generate a depth map for an outdoor scene photo I took yesterday.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_path\": \"park_scene.png\"}}}, {\"user_intent\": \"I want to create a depth visualization for multiple images in my dataset to analyze scene geometry.\", \"tool_call\": {\"name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"arguments\": {\"image_paths\": [\"kitchen.png\", \"office.jpg\", \"street_view.jpeg\"]}}}]}\n\n# User Query\nWe need to estimate the depth of a scene in an image using a pretrained model. Can you please suggest a way?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth of objects in a single image using a pre-trained model.",
                    "tool_description": "Use a GLPN-based depth estimation model, fine-tuned on the DIODE dataset, to predict per-pixel depth from input images using Hugging Face Transformers.",
                    "expected_response": "A per-pixel depth map representing the distances of scene elements from the camera, corresponding to the input image."
                }
            ],
            "extracted_arguments": {
                "image_path": "The file path or identifier of the input image for depth estimation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_270",
        "query": "A game studio is now creating a story for their new action game, they need a hint for creating the setting of the game.",
        "instruction": "Given a `creative writing assistance` task, retrieve tools that support the development of narrative elements by generating text based on creative prompts or contextual hints to help writers build engaging stories.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text Generation",
                    "api_call": "pipeline('text-generation', model='bigscience/bloom-7b1')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-generation', model='bigscience/bloom-7b1')\nresult = model('Once upon a time')\nprint(result)",
                    "performance": {
                        "dataset": "Training Data",
                        "accuracy": {
                            "Training Loss": 2.3,
                            "Validation Loss": 2.9,
                            "Perplexity": 16
                        }
                    },
                    "description": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based language model designed for text generation and as a pretrained base model for fine-tuning on specific tasks. It supports 48 languages and has 7,069,016,064 parameters. The model is trained on a diverse corpus containing 45 natural languages, 12 programming languages, and 1.5TB of pre-processed text.",
                    "id": "huggingface_api_606",
                    "name": "bigscience/bloom-7b1"
                },
                "id": "gorilla_huggingface_tool_601",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 4: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 5: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 6: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.",
            "Tool 7: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 9: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.",
            "Tool 10: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 11: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.",
            "Tool 12: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 13: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 14: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 15: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.",
            "Tool 16: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.",
            "Tool 17: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 18: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.",
            "Tool 19: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions."
        ],
        "tool_view": {
            "description": "A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                }
            },
            "response": "The tool’s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\n\n1. **Boxes / Bounding Boxes**  \n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \n   - These coordinates define the rectangular region enclosing each detected object.\n\n2. **Class / Label**  \n   - Each detected object is assigned a class label indicating its category, such as \"dropped spike,\" \"enemy,\" \"planted spike,\" or \"teammate.\"  \n   - The label identifies the type of object detected by the model.\n\n3. **Confidence Score**  \n   - A numerical value representing the model’s confidence that the detected object belongs to the predicted class.  \n   - This value is constrained by the `conf` threshold specified in the API arguments.\n\n4. **Optional Additional Metadata**  \n   - Depending on the framework, the response may include additional information such as:\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\n     - **Tracking ID** if sequential detection or object tracking is enabled.\n     - **Mask or segmentation data** if the model supports instance segmentation.\n\n5. **Rendering / Visualization (Optional)**  \n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\n\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.",
            "examples": [
                {
                    "user_intent": "I want to detect all enemies and teammates in a Valorant gameplay screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000
                        }
                    }
                },
                {
                    "user_intent": "Scan a live stream frame from Valorant to check if the spike has been dropped or planted.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500
                        }
                    }
                },
                {
                    "user_intent": "Analyze a sequence of Valorant game images to locate all objects with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8m-valorant-detection')",
                        "arguments": {
                            "conf": 0.5,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 1000
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 1: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 2: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 3: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 4: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 5: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 6: Generate human-like text using the Facebook OPT-1.3B model, a pre-trained transformer designed for natural language generation tasks, suitable for creating coherent continuations of input text or evaluating downstream NLP tasks.\nTool 7: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 8: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 9: This tool uses the T5-base model fine-tuned for end-to-end question generation, allowing users to input a text passage and automatically generate multiple relevant questions. It is useful for creating quizzes, study materials, or datasets for question-answering tasks.\nTool 10: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 11: Pygmalion 1.3B is a conversational text-generation model designed to produce character-driven dialogue. It generates responses based on a structured input that includes a character’s persona, dialogue history, and the user’s message, making it useful for creating interactive, persona-based conversations or role-playing scenarios.\nTool 12: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 13: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 14: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 15: Vintedois-Diffusion v0-1 is a text-to-image model that generates high-quality images, including detailed and realistic faces, from textual prompts with minimal prompt engineering. It is designed for creating visually appealing images efficiently and can be used through the Hugging Face pipeline with configurable parameters like guidance scale, scheduler, steps, and seed.\nTool 16: This tool leverages ControlNet conditioned on Canny edges to guide image-to-image generation using Stable Diffusion. It allows users to transform an input image while providing additional control over the output through edge-based conditions, making it useful for creating highly detailed or stylistically constrained images from sketches or structured outlines.\nTool 17: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 18: Protogen v2.2 is a text-to-image model that generates high-quality images based on detailed text prompts. Fine-tuned on large, trending datasets, it leverages the Stable Diffusion Pipeline and supports trigger words to control specific visual styles, such as \"modelshoot style\" for camera-like shots. This model is used for creating photorealistic artwork from descriptive text inputs.\nTool 19: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The minimum confidence threshold for detected objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold for non-maximum suppression. Determines how much overlap is allowed between detected bounding boxes before they are merged.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic non-maximum suppression. If true, overlapping boxes are suppressed regardless of their class.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image. Limits the number of bounding boxes returned by the model.\", \"required\": true, \"default\": 1000, \"example\": 500}}, \"response\": \"The tool\\u2019s response is a structured result representing the objects detected in an input image. Each response typically contains the following components:\\n\\n1. **Boxes / Bounding Boxes**  \\n   - A list or array of detected objects, where each object includes the spatial location of the object in the image, usually expressed as coordinates (e.g., `[x_min, y_min, x_max, y_max]`) or as a center point with width and height.  \\n   - These coordinates define the rectangular region enclosing each detected object.\\n\\n2. **Class / Label**  \\n   - Each detected object is assigned a class label indicating its category, such as \\\"dropped spike,\\\" \\\"enemy,\\\" \\\"planted spike,\\\" or \\\"teammate.\\\"  \\n   - The label identifies the type of object detected by the model.\\n\\n3. **Confidence Score**  \\n   - A numerical value representing the model\\u2019s confidence that the detected object belongs to the predicted class.  \\n   - This value is constrained by the `conf` threshold specified in the API arguments.\\n\\n4. **Optional Additional Metadata**  \\n   - Depending on the framework, the response may include additional information such as:\\n     - **Scores per class** if the model predicts probabilities for multiple classes per detection.\\n     - **Tracking ID** if sequential detection or object tracking is enabled.\\n     - **Mask or segmentation data** if the model supports instance segmentation.\\n\\n5. **Rendering / Visualization (Optional)**  \\n   - While not strictly part of the raw detection response, the `render_result` function produces a visual representation of the image overlaid with bounding boxes and labels.  \\n   - This output is typically an image object or displayable rendering that visually indicates the detected objects.\\n\\nIn summary, the response is primarily a collection of detected objects, each with spatial coordinates, class label, and confidence score, suitable for further processing, filtering, or visualization.\", \"examples\": [{\"user_intent\": \"I want to detect all enemies and teammates in a Valorant gameplay screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}}}, {\"user_intent\": \"Scan a live stream frame from Valorant to check if the spike has been dropped or planted.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500}}}, {\"user_intent\": \"Analyze a sequence of Valorant game images to locate all objects with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"arguments\": {\"conf\": 0.5, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 1000}}}]}\n\n# User Query\nA game studio is now creating a story for their new action game, they need a hint for creating the setting of the game.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Provide a creative hint or idea for the setting of a new action game.",
                    "tool_description": "Generate a contextually relevant suggestion for a video game setting based on action/adventure themes, suitable for inspiring story or environment design.",
                    "expected_response": "A short descriptive text presenting a potential game setting, including elements like location, atmosphere, and narrative hooks."
                }
            ],
            "extracted_arguments": {
                "game_genre": "The genre or type of the game for which the setting is being designed, e.g., action, adventure.",
                "story_theme": "The narrative tone or theme to guide the setting, such as suspenseful, futuristic, or fantastical."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_239",
        "query": "Our team has built a Japanese language learning app for students. We want to make sure that the user-submitted image is an anime art created by humans and not AI-generated.",
        "instruction": "Given an `anime art classification` task, retrieve tools capable of classifying images by evaluating whether an anime artwork is human-created or AI-generated, utilizing image inputs to verify authenticity.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('image-classification', model='saltacc/anime-ai-detect')",
                    "api_arguments": [
                        "image"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "aibooru and imageboard sites",
                        "accuracy": "96%"
                    },
                    "description": "A BEiT classifier to see if anime art was made by an AI or a human.",
                    "id": "huggingface_api_196",
                    "name": "saltacc/anime-ai-detect"
                },
                "id": "gorilla_huggingface_tool_192",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 1: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 2: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.",
            "Tool 3: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.",
            "Tool 4: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 5: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.",
            "Tool 6: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 8: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 9: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 10: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 11: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 12: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.",
            "Tool 13: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 14: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.",
            "Tool 15: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 16: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.",
            "Tool 17: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.",
            "Tool 18: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 19: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses."
        ],
        "tool_view": {
            "description": "A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path or URL of the image to be classified by the anime AI detector.",
                    "required": true,
                    "example": "path/to/anime_image.jpg"
                }
            },
            "response": "The tool returns a classification result for the input image, indicating whether the anime art was created by an AI or a human. The response would likely include the following components:\n\n1. **Class Label**: A textual or categorical label identifying the classification result, which could be \"AI\" or \"Human\" based on the model's determination.\n   \n2. **Confidence Score**: A numeric value representing the model's confidence in its classification, typically a probability or likelihood score, indicating how certain it is about the prediction.\n\nThe exact format and structure of the response are not explicitly defined, but based on the functionality and common usage of image classification models, the response will typically contain the class label and its associated confidence score.",
            "examples": [
                {
                    "user_intent": "I want to check if this anime illustration was created by AI or a human.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "/home/user/images/anime_art1.png"
                        }
                    }
                },
                {
                    "user_intent": "Classify a batch of anime character images to see which ones are AI-generated.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "/home/user/images/ai_batch/character_05.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I found this fan art online and I want to know if it’s human-made or AI-generated.",
                    "tool_call": {
                        "name": "saltacc/anime-ai-detect",
                        "arguments": {
                            "image": "https://example.com/fanart_image.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 1: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 2: Dreamlike Anime 1.0 is a high-quality anime-style text-to-image model that generates detailed 768x768px images from user-provided prompts. Designed for use with the Hugging Face Stable Diffusion Pipeline, it excels at creating visually rich anime illustrations when prompts emphasize elements like “masterpiece” and “high quality,” allowing users to produce polished, creative anime artwork efficiently.\nTool 3: A fine-tuned VAE decoder for the Stable Diffusion Pipeline that generates images from text prompts. It integrates with the diffusers library and can be used by specifying the VAE when creating a StableDiffusionPipeline. The model has been trained on LAION-Aesthetics and LAION-Humans datasets and has demonstrated strong image reconstruction quality on benchmarks such as COCO 2017 and LAION-Aesthetics 5+.\nTool 4: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 5: This tool provides a fine-tuned variational autoencoder (VAE) for use with the Stable Diffusion text-to-image pipeline, enabling high-quality image generation from text prompts. It has been trained on a balanced mix of LAION-Aesthetics and LAION-Humans datasets and can be used as a drop-in replacement for the default autoencoder to improve image fidelity and aesthetic quality.\nTool 6: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 7: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 8: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 9: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 10: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 11: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 12: Perform zero-shot image classification on biomedical images using a vision-language model pretrained on 15 million figure-caption pairs from PubMed Central. This tool leverages BiomedCLIP, which combines a Vision Transformer image encoder with a PubMedBERT text encoder, to assign images to user-provided class labels without requiring additional training, achieving state-of-the-art performance on biomedical datasets.\nTool 13: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 14: Pix2Struct is a multimodal model that converts images into text, enabling tasks such as image captioning and visual question answering. It is pretrained on large-scale image-text pairs, learning to encode visual content and generate accurate textual descriptions, and achieves state-of-the-art performance on datasets like TextCaps. This tool is useful for applications that require understanding and summarizing visual information in natural language.\nTool 15: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 16: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.\nTool 17: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.\nTool 18: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 19: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path or URL of the image to be classified by the anime AI detector.\", \"required\": true, \"example\": \"path/to/anime_image.jpg\"}}, \"response\": \"The tool returns a classification result for the input image, indicating whether the anime art was created by an AI or a human. The response would likely include the following components:\\n\\n1. **Class Label**: A textual or categorical label identifying the classification result, which could be \\\"AI\\\" or \\\"Human\\\" based on the model's determination.\\n   \\n2. **Confidence Score**: A numeric value representing the model's confidence in its classification, typically a probability or likelihood score, indicating how certain it is about the prediction.\\n\\nThe exact format and structure of the response are not explicitly defined, but based on the functionality and common usage of image classification models, the response will typically contain the class label and its associated confidence score.\", \"examples\": [{\"user_intent\": \"I want to check if this anime illustration was created by AI or a human.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"/home/user/images/anime_art1.png\"}}}, {\"user_intent\": \"Classify a batch of anime character images to see which ones are AI-generated.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"/home/user/images/ai_batch/character_05.jpg\"}}}, {\"user_intent\": \"I found this fan art online and I want to know if it\\u2019s human-made or AI-generated.\", \"tool_call\": {\"name\": \"saltacc/anime-ai-detect\", \"arguments\": {\"image\": \"https://example.com/fanart_image.png\"}}}]}\n\n# User Query\nOur team has built a Japanese language learning app for students. We want to make sure that the user-submitted image is an anime art created by humans and not AI-generated.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to check if a user-submitted anime image was created by a human or AI.",
                    "tool_description": "Classify the submitted anime artwork to determine whether it was created by an AI or a human using a BEiT-based Hugging Face Transformers model.",
                    "expected_response": "A classification result indicating whether the anime art was created by AI or human, along with a confidence score."
                }
            ],
            "extracted_arguments": {
                "image": "The file path or URL of the image to be classified by the anime AI detector."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_454",
        "query": "Our PR team requires a machine learning approach to creative sentence generation for marketing content creation.",
        "instruction": "Given a `creative sentence generation` task, retrieve tools that utilize machine learning approaches to produce imaginative and contextually relevant sentences for marketing content, focusing on natural language processing techniques and accommodating domain-specific requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Generative Commonsense Reasoning",
                    "api_call": "AutoModelWithLMHead.from_pretrained('mrm8488/t5-base-finetuned-common_gen')",
                    "api_arguments": [
                        "words",
                        "max_length"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\ndef gen_sentence(words, max_length=32):\n input_text = words\n features = tokenizer([input_text], return_tensors='pt')\noutput = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'], max_length=max_length)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nwords = tree plant ground hole dig\ngen_sentence(words)",
                    "performance": {
                        "dataset": "common_gen",
                        "accuracy": {
                            "ROUGE-2": 17.1,
                            "ROUGE-L": 39.47
                        }
                    },
                    "description": "Google's T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.",
                    "id": "huggingface_api_631",
                    "name": "mrm8488/t5-base-finetuned-common_gen"
                },
                "id": "gorilla_huggingface_tool_626",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 1: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 2: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 4: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 5: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 6: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 8: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.",
            "Tool 9: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.",
            "Tool 10: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.",
            "Tool 11: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.",
            "Tool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 13: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.",
            "Tool 14: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"",
            "Tool 15: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.",
            "Tool 16: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.",
            "Tool 17: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.",
            "Tool 18: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.",
            "Tool 19: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing."
        ],
        "tool_view": {
            "description": "GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text that serves as the prompt for the text generation model.",
                    "required": true,
                    "example": "Hello, I'm a language model"
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum length of the generated text, including the input text.",
                    "required": true,
                    "example": 30
                },
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of distinct text generation sequences to return.",
                    "required": true,
                    "example": 5
                }
            },
            "response": "The tool returns a collection of generated text sequences based on a given input prompt. Each response typically includes the following components:  \n\n1. **Generated Text** – The primary content of each sequence, which is the text continuation or completion produced by the model from the provided prompt. This text reflects the model’s probabilistic prediction of likely next tokens.  \n\n2. **Metadata (optional, depending on implementation)** – Some versions of the pipeline may include additional fields such as:\n   - **`score` or `logprob`**: A numerical value representing the model’s confidence or likelihood estimate for the generated sequence.\n   - **Sequence index**: When multiple sequences are returned (`num_return_sequences > 1`), each generated text is associated with its position in the output array.  \n\nThe number of returned sequences is controlled by the `num_return_sequences` parameter, and each sequence is limited in length by `max_length`. The tool does not produce labels, structured data, or annotations beyond the generated text itself and optional confidence metadata.  \n\nOverall, the response is an array of text objects, each representing a self-contained continuation of the input prompt.",
            "examples": [
                {
                    "user_intent": "Generate a short creative story starting with 'Once upon a time in a magical forest'.",
                    "tool_call": {
                        "name": "gpt2",
                        "arguments": {
                            "text": "Once upon a time in a magical forest",
                            "max_length": 100,
                            "num_return_sequences": 3
                        }
                    }
                },
                {
                    "user_intent": "Create multiple alternative endings for the sentence 'The scientist opened the mysterious box and'.",
                    "tool_call": {
                        "name": "gpt2",
                        "arguments": {
                            "text": "The scientist opened the mysterious box and",
                            "max_length": 50,
                            "num_return_sequences": 5
                        }
                    }
                },
                {
                    "user_intent": "Write a concise motivational quote starting with 'Believe in yourself and'.",
                    "tool_call": {
                        "name": "gpt2",
                        "arguments": {
                            "text": "Believe in yourself and",
                            "max_length": 30,
                            "num_return_sequences": 2
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 1: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 2: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 3: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 4: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 5: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 6: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 7: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 8: This tool is a GAN-based model designed for unconditional image generation of butterflies, trained on a small dataset of 1,000 images to produce high-quality, realistic butterfly images. It is intended for experimentation, learning, and creative applications, allowing users to generate novel butterfly visuals from random latent vectors using a lightweight, transformer-based GAN framework.\nTool 9: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.\nTool 10: Generate coherent and contextually relevant text using the Facebook OPT-350M model, a pre-trained decoder-only transformer from Meta AI. This tool is designed for natural language generation tasks, allowing users to produce multiple text continuations from a given prompt, useful for research, content creation, and evaluating language model behavior.\nTool 11: csebuetnlp/mT5_multilingual_XLSum is a multilingual abstractive summarization model fine-tuned on the XL-Sum dataset, capable of generating concise summaries from input text in 43 languages. It uses a text-to-text generation approach to condense long documents into shorter, coherent summaries, making it useful for quickly extracting key information from multilingual content.\nTool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 13: A neural machine translation tool that converts English text into Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model trained on the OPUS dataset and is designed for accurate sentence-level translation.\nTool 14: The \"albert-base-v2\" tool is a masked language model based on the ALBERT architecture, pre-trained on a large English corpus using a self-supervised learning approach. It can predict masked words in a given sentence, making it useful for tasks like filling in missing words in text. This tool is implemented through the Hugging Face Transformers pipeline and provides high performance on a range of natural language processing benchmarks, with an overall accuracy of 82.3%. It is used by calling the pipeline with a masked text input, such as \"Hello I'm a [MASK] model.\"\nTool 15: This tool uses the \"sentence-transformers/distilbert-base-nli-mean-tokens\" model to convert sentences and paragraphs into 768-dimensional dense vector representations, enabling tasks such as clustering and semantic search. It leverages the Hugging Face Transformers framework for feature extraction and requires the `sentence-transformers` library.\nTool 16: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.\nTool 17: A pretrained RoBERTa-large model designed for contrastive learning of sentence embeddings, used for extracting high-quality features from text. It is effective for semantic textual similarity (STS) tasks and can support various downstream NLP applications.\nTool 18: UniXcoder is a pre-trained model that generates rich code representations by leveraging multimodal inputs such as code comments and abstract syntax trees (ASTs). Based on RoBERTa and trained on English code data, it is designed for feature engineering tasks, enabling developers and researchers to extract meaningful features from code for analysis, retrieval, or downstream machine learning applications.\nTool 19: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text that serves as the prompt for the text generation model.\", \"required\": true, \"example\": \"Hello, I'm a language model\"}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum length of the generated text, including the input text.\", \"required\": true, \"example\": 30}, \"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of distinct text generation sequences to return.\", \"required\": true, \"example\": 5}}, \"response\": \"The tool returns a collection of generated text sequences based on a given input prompt. Each response typically includes the following components:  \\n\\n1. **Generated Text** \\u2013 The primary content of each sequence, which is the text continuation or completion produced by the model from the provided prompt. This text reflects the model\\u2019s probabilistic prediction of likely next tokens.  \\n\\n2. **Metadata (optional, depending on implementation)** \\u2013 Some versions of the pipeline may include additional fields such as:\\n   - **`score` or `logprob`**: A numerical value representing the model\\u2019s confidence or likelihood estimate for the generated sequence.\\n   - **Sequence index**: When multiple sequences are returned (`num_return_sequences > 1`), each generated text is associated with its position in the output array.  \\n\\nThe number of returned sequences is controlled by the `num_return_sequences` parameter, and each sequence is limited in length by `max_length`. The tool does not produce labels, structured data, or annotations beyond the generated text itself and optional confidence metadata.  \\n\\nOverall, the response is an array of text objects, each representing a self-contained continuation of the input prompt.\", \"examples\": [{\"user_intent\": \"Generate a short creative story starting with 'Once upon a time in a magical forest'.\", \"tool_call\": {\"name\": \"gpt2\", \"arguments\": {\"text\": \"Once upon a time in a magical forest\", \"max_length\": 100, \"num_return_sequences\": 3}}}, {\"user_intent\": \"Create multiple alternative endings for the sentence 'The scientist opened the mysterious box and'.\", \"tool_call\": {\"name\": \"gpt2\", \"arguments\": {\"text\": \"The scientist opened the mysterious box and\", \"max_length\": 50, \"num_return_sequences\": 5}}}, {\"user_intent\": \"Write a concise motivational quote starting with 'Believe in yourself and'.\", \"tool_call\": {\"name\": \"gpt2\", \"arguments\": {\"text\": \"Believe in yourself and\", \"max_length\": 30, \"num_return_sequences\": 2}}}]}\n\n# User Query\nOur PR team requires a machine learning approach to creative sentence generation for marketing content creation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate creative marketing sentences for PR content.",
                    "tool_description": "Use a pretrained language model to produce coherent and contextually relevant English text continuations from input prompts, enabling creative content generation for marketing purposes.",
                    "expected_response": "A collection of generated text sequences, each providing a unique, contextually appropriate marketing sentence or phrase."
                }
            ],
            "extracted_arguments": {
                "text_prompt": "The initial text or concept to inspire the generated marketing content.",
                "max_length": "The maximum allowed length of the generated sentence or paragraph.",
                "num_return_sequences": "The number of alternative creative sentences to generate."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_195",
        "query": "We need to classify actions of athletes in sports videos. Can you help us to analyze and classify these videos?",
        "instruction": "Given a `video classification` task, retrieve tools that are capable of analyzing video content to classify actions within sports videos, leveraging models that process video inputs as sequences of frames or patches to extract features for classification.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Video Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')",
                    "api_arguments": "video",
                    "python_environment_requirements": "transformers",
                    "example_code": "from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss",
                    "performance": {
                        "dataset": "Something-Something-v2",
                        "accuracy": ""
                    },
                    "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.",
                    "id": "huggingface_api_337",
                    "name": "videomae-base-ssv2"
                },
                "id": "gorilla_huggingface_tool_333",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 1: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 3: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 4: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 5: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.",
            "Tool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 7: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 8: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.",
            "Tool 9: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 11: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 12: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.",
            "Tool 13: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 14: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 18: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 19: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization."
        ],
        "tool_view": {
            "description": "A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list of video frames represented as arrays, where each frame is typically a 3D array corresponding to (channels, height, width). This input represents the video to be classified.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return, such as 'pt' for PyTorch or 'tf' for TensorFlow, which determines the format of the model inputs.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool produces a structured output representing the classification results of a video input. The response typically contains the following components:  \n\n1. **logits**:  \n   - A tensor of raw, unnormalized scores produced by the model for each possible video class.  \n   - Its dimensionality corresponds to the number of classes the model can predict.  \n   - Higher values indicate greater confidence that the input video belongs to the corresponding class, but they are not probabilities.  \n\n2. **predicted_class_idx** (optional, often derived from logits):  \n   - An integer representing the index of the class with the highest logit score.  \n   - This index can be mapped to a human-readable label using the model’s `id2label` mapping.  \n\n3. **id2label mapping** (from `model.config`):  \n   - A dictionary that maps class indices to descriptive class names.  \n   - Allows conversion of `predicted_class_idx` into a human-readable label.  \n\n4. **Additional model outputs** (optional, depending on the configuration of `TimesformerForVideoClassification`):  \n   - May include intermediate hidden states, attention weights, or other auxiliary outputs if the model is configured to return them.  \n\nIn practice, the typical workflow is: the video is preprocessed into a tensor, fed into the model, the model outputs the `logits`, and the highest-scoring class index is converted to a label using `id2label`. This provides the predicted action or category for the input video.",
            "examples": [
                {
                    "user_intent": "I want to classify a short video clip of someone stirring a cup of coffee to see what action it contains.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 representing the coffee stirring video",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this 10-second sports video to automatically identify the type of sport being played.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 extracted evenly across the sports video",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Classify an office meeting video to detect the primary action, like typing, presenting, or talking.",
                    "tool_call": {
                        "name": "TimesformerForVideoClassification",
                        "arguments": {
                            "images": "np.array of 16 frames of size 448x448x3 sampled from the meeting video",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 1: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 2: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 3: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 4: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 5: A TimeSformer model pre-trained on the Kinetics-600 dataset for video classification, capable of categorizing videos into one of 600 action classes. It uses space-time attention to analyze video frames efficiently and can be applied via Hugging Face Transformers for tasks that require automated understanding of video content.\nTool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 7: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 8: The \"MCG-NJU/videomae-base-finetuned-ssv2\" is a pre-trained video classification model based on the VideoMAE architecture, fine-tuned on the Something-Something-v2 dataset. It uses a self-supervised pre-training approach followed by supervised fine-tuning. This model can classify videos and is implemented using the Hugging Face Transformers framework. It achieves high performance with 70.6% top-1 accuracy and 92.6% top-5 accuracy.\nTool 9: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 10: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 11: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 12: This tool provides a pre-trained TimeSformer model for video classification, capable of categorizing videos into one of 400 Kinetics-400 action classes. It leverages space-time attention to analyze video frames and is designed for use in computer vision tasks that require understanding temporal and spatial patterns in video data.\nTool 13: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 14: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 15: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 17: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 18: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 19: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list of video frames represented as arrays, where each frame is typically a 3D array corresponding to (channels, height, width). This input represents the video to be classified.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return, such as 'pt' for PyTorch or 'tf' for TensorFlow, which determines the format of the model inputs.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool produces a structured output representing the classification results of a video input. The response typically contains the following components:  \\n\\n1. **logits**:  \\n   - A tensor of raw, unnormalized scores produced by the model for each possible video class.  \\n   - Its dimensionality corresponds to the number of classes the model can predict.  \\n   - Higher values indicate greater confidence that the input video belongs to the corresponding class, but they are not probabilities.  \\n\\n2. **predicted_class_idx** (optional, often derived from logits):  \\n   - An integer representing the index of the class with the highest logit score.  \\n   - This index can be mapped to a human-readable label using the model\\u2019s `id2label` mapping.  \\n\\n3. **id2label mapping** (from `model.config`):  \\n   - A dictionary that maps class indices to descriptive class names.  \\n   - Allows conversion of `predicted_class_idx` into a human-readable label.  \\n\\n4. **Additional model outputs** (optional, depending on the configuration of `TimesformerForVideoClassification`):  \\n   - May include intermediate hidden states, attention weights, or other auxiliary outputs if the model is configured to return them.  \\n\\nIn practice, the typical workflow is: the video is preprocessed into a tensor, fed into the model, the model outputs the `logits`, and the highest-scoring class index is converted to a label using `id2label`. This provides the predicted action or category for the input video.\", \"examples\": [{\"user_intent\": \"I want to classify a short video clip of someone stirring a cup of coffee to see what action it contains.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 representing the coffee stirring video\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Analyze this 10-second sports video to automatically identify the type of sport being played.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 extracted evenly across the sports video\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Classify an office meeting video to detect the primary action, like typing, presenting, or talking.\", \"tool_call\": {\"name\": \"TimesformerForVideoClassification\", \"arguments\": {\"images\": \"np.array of 16 frames of size 448x448x3 sampled from the meeting video\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nWe need to classify actions of athletes in sports videos. Can you help us to analyze and classify these videos?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify sports videos to identify the actions athletes are performing.",
                    "tool_description": "Use a pre-trained video classification model (e.g., TimeSformer or VideoMAE) to analyze video frames and predict action classes in sports videos.",
                    "expected_response": "A structured output containing logits for each action class, the predicted class index, and a mapping from class indices to human-readable action labels."
                }
            ],
            "extracted_arguments": {
                "video_frames": "An array of video frames representing the sports video to be classified, where each frame is a 3D array of pixel values (channels, height, width).",
                "return_tensors": "The type of tensors to return for model input, e.g., 'pt' for PyTorch or 'tf' for TensorFlow."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_197",
        "query": "The security department needs assistance to detect suspicious objects and people using a zero-shot text-conditioned object detection system.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize zero-shot text-conditioned models to detect and identify suspicious objects and individuals by processing text and image inputs in varied environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "zero-shot-object-detection",
                    "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')",
                    "api_arguments": [
                        "texts",
                        "images"
                    ],
                    "python_environment_requirements": [
                        "requests",
                        "PIL",
                        "torch",
                        "transformers"
                    ],
                    "example_code": "processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)",
                    "performance": {
                        "dataset": "COCO",
                        "accuracy": "Not provided"
                    },
                    "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.",
                    "id": "huggingface_api_221",
                    "name": "google/owlvit-base-patch16"
                },
                "id": "gorilla_huggingface_tool_217",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 2: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 4: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 5: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 6: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 7: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.",
            "Tool 8: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 9: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 10: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 11: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.",
            "Tool 12: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.",
            "Tool 13: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.",
            "Tool 14: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.",
            "Tool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 16: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.",
            "Tool 17: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 18: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian."
        ],
        "tool_view": {
            "description": "OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "parameters": {
                "texts": {
                    "type": "array",
                    "description": "A list of text queries or labels used to detect corresponding objects in the image. Each element can be a descriptive string of the object to find.",
                    "required": true,
                    "example": [
                        "a photo of a cat",
                        "a photo of a dog"
                    ]
                },
                "images": {
                    "type": "image or PIL.Image",
                    "description": "The input image in which objects will be detected. This can be a PIL Image object or a compatible image format.",
                    "required": true,
                    "example": "Image.open('example.jpg')"
                }
            },
            "response": "The tool’s response is a structured output representing the detected objects in the provided image conditioned on the given text queries. Each component of the response typically includes the following elements:\n\n1. **Detected Objects List**: A collection of entries, where each entry corresponds to one detected object in the image that matches one of the text queries.  \n\n2. **Per-Object Fields**: Each detected object usually contains:  \n   - **Label/Query Match**: The text query or class label that the detected object corresponds to.  \n   - **Bounding Box**: Coordinates specifying the location of the detected object within the image. This is typically a tuple or list of four values representing `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.  \n   - **Confidence Score**: A numeric value representing the model’s confidence that the object corresponds to the given text query, often between 0 and 1.  \n\n3. **Post-Processed Alignment**: The bounding boxes are adjusted to match the original image dimensions using the `target_sizes` parameter, ensuring that coordinates are meaningful relative to the input image.\n\n4. **Optional Metadata**: Depending on the processing, the response may include additional information such as the number of detected objects, processing time, or internal model outputs like raw logits before post-processing.\n\nOverall, the response semantically maps each input text query to all regions in the image where the model predicts the object is present, providing both spatial localization and confidence for each match.",
            "examples": [
                {
                    "user_intent": "Detect objects in an image of a cat and a dog using a text description.",
                    "tool_call": {
                        "name": "google/owlvit-base-patch16",
                        "arguments": {
                            "texts": [
                                [
                                    "a photo of a cat",
                                    "a photo of a dog"
                                ]
                            ],
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Classify objects in an image based on a custom text query: 'a bird and a tree'.",
                    "tool_call": {
                        "name": "google/owlvit-base-patch16",
                        "arguments": {
                            "texts": [
                                [
                                    "a bird",
                                    "a tree"
                                ]
                            ],
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Use OWL-ViT to detect multiple objects in a street scene, with text queries for 'car', 'building', and 'tree'.",
                    "tool_call": {
                        "name": "google/owlvit-base-patch16",
                        "arguments": {
                            "texts": [
                                [
                                    "car",
                                    "building",
                                    "tree"
                                ]
                            ],
                            "images": "http://images.cocodataset.org/val2017/000000039769.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 2: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 3: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 4: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 5: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 6: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 7: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.\nTool 8: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 9: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 10: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 11: A pretrained DETR (End-to-End Object Detection) model with a ResNet-101 backbone designed for detecting objects in images. It is trained on the COCO 2017 dataset and achieves high accuracy, making it suitable for applications that require automated identification and localization of multiple object categories within images.\nTool 12: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\nTool 13: The `kakaobrain/align-base` model is a dual-encoder system that aligns images and text using contrastive learning, enabling zero-shot image classification and multi-modal embedding retrieval. It uses EfficientNet for visual encoding and BERT for text encoding, is trained on the open-source COYO dataset, and achieves performance on par with or exceeding comparable ALIGN models. This tool allows users to classify images without task-specific training by providing candidate text labels and corresponding images.\nTool 14: This tool uses a YOLOv8-based model to detect blood cells—including platelets, red blood cells, and white blood cells—in images. It is designed for precise object detection in medical imaging, offering configurable confidence and non-maximum suppression settings, and is trained on a dedicated blood cell detection dataset to achieve high accuracy.\nTool 15: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 16: This tool performs zero-shot classification using a Cross-Encoder model trained on the SNLI and MultiNLI datasets. It can classify text into predefined categories without requiring task-specific training, making it useful for categorizing text based on its content. The model can be accessed through the `zero-shot-classification` pipeline in the Transformers framework.\nTool 17: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 18: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 19: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\", \"parameters\": {\"texts\": {\"type\": \"array\", \"description\": \"A list of text queries or labels used to detect corresponding objects in the image. Each element can be a descriptive string of the object to find.\", \"required\": true, \"example\": [\"a photo of a cat\", \"a photo of a dog\"]}, \"images\": {\"type\": \"image or PIL.Image\", \"description\": \"The input image in which objects will be detected. This can be a PIL Image object or a compatible image format.\", \"required\": true, \"example\": \"Image.open('example.jpg')\"}}, \"response\": \"The tool\\u2019s response is a structured output representing the detected objects in the provided image conditioned on the given text queries. Each component of the response typically includes the following elements:\\n\\n1. **Detected Objects List**: A collection of entries, where each entry corresponds to one detected object in the image that matches one of the text queries.  \\n\\n2. **Per-Object Fields**: Each detected object usually contains:  \\n   - **Label/Query Match**: The text query or class label that the detected object corresponds to.  \\n   - **Bounding Box**: Coordinates specifying the location of the detected object within the image. This is typically a tuple or list of four values representing `(x_min, y_min, x_max, y_max)` or `(x, y, width, height)`.  \\n   - **Confidence Score**: A numeric value representing the model\\u2019s confidence that the object corresponds to the given text query, often between 0 and 1.  \\n\\n3. **Post-Processed Alignment**: The bounding boxes are adjusted to match the original image dimensions using the `target_sizes` parameter, ensuring that coordinates are meaningful relative to the input image.\\n\\n4. **Optional Metadata**: Depending on the processing, the response may include additional information such as the number of detected objects, processing time, or internal model outputs like raw logits before post-processing.\\n\\nOverall, the response semantically maps each input text query to all regions in the image where the model predicts the object is present, providing both spatial localization and confidence for each match.\", \"examples\": [{\"user_intent\": \"Detect objects in an image of a cat and a dog using a text description.\", \"tool_call\": {\"name\": \"google/owlvit-base-patch16\", \"arguments\": {\"texts\": [[\"a photo of a cat\", \"a photo of a dog\"]], \"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}}}, {\"user_intent\": \"Classify objects in an image based on a custom text query: 'a bird and a tree'.\", \"tool_call\": {\"name\": \"google/owlvit-base-patch16\", \"arguments\": {\"texts\": [[\"a bird\", \"a tree\"]], \"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}}}, {\"user_intent\": \"Use OWL-ViT to detect multiple objects in a street scene, with text queries for 'car', 'building', and 'tree'.\", \"tool_call\": {\"name\": \"google/owlvit-base-patch16\", \"arguments\": {\"texts\": [[\"car\", \"building\", \"tree\"]], \"images\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}}}]}\n\n# User Query\nThe security department needs assistance to detect suspicious objects and people using a zero-shot text-conditioned object detection system.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect suspicious objects and people in an image using a zero-shot text-conditioned object detection model.",
                    "tool_description": "Use a zero-shot, text-conditioned object detection model to identify and localize objects and people in an image based on provided descriptive text queries, without requiring task-specific training.",
                    "expected_response": "A structured list of detected objects, each including the matched text query, bounding box coordinates, and confidence score, representing the locations of suspicious objects and people in the image."
                }
            ],
            "extracted_arguments": {
                "texts": "An array of descriptive text queries specifying objects or people to detect (e.g., 'suspicious object', 'person').",
                "images": "The input image or images in which to perform object detection, provided in a compatible image format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_32",
        "query": "We are building a platform to compare and contrast user input sentences with existing sentences in our database. It should return similar results.",
        "instruction": "Given a `sentence similarity` task, retrieve tools that utilize natural language processing techniques to compare user input sentences with a database, returning results based on semantic similarity.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentence Transformers",
                    "api_call": "SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Automated evaluation"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_705",
                    "name": "sentence-transformers/paraphrase-distilroberta-base-v2"
                },
                "id": "gorilla_huggingface_tool_700",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 1: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.",
            "Tool 2: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 3: The \"sentence-transformers/paraphrase-albert-small-v2\" model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering or semantic search. It utilizes the Hugging Face Transformers framework and is based on the SentenceTransformer API, which encodes input sentences into embeddings for comparison and analysis.",
            "Tool 4: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 5: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.",
            "Tool 6: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 7: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 8: The \"sentence-transformers/all-MiniLM-L12-v1\" model maps sentences and paragraphs to a 384-dimensional dense vector space, enabling tasks like clustering and semantic search. It uses the SentenceTransformer framework from Hugging Face Transformers to encode sentences into embeddings for efficient similarity comparison.",
            "Tool 9: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.",
            "Tool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 11: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.",
            "Tool 12: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 13: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.",
            "Tool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 15: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 16: The `sentence-transformers/distilbert-base-nli-stsb-mean-tokens` model maps sentences and paragraphs to a 768-dimensional dense vector space, enabling tasks such as clustering and semantic search. It uses Hugging Face's Transformers framework and can be accessed via the SentenceTransformer API, which generates sentence embeddings from input text.",
            "Tool 17: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 18: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.",
            "Tool 19: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity."
        ],
        "tool_view": {
            "description": "This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "parameters": {
                "query": {
                    "type": "string",
                    "description": "The input sentence or question for which you want to find semantically similar documents.",
                    "required": true,
                    "example": "How many people live in London?"
                },
                "docs": {
                    "type": "array",
                    "description": "A list of sentences or paragraphs to compare against the query for semantic similarity.",
                    "required": true,
                    "example": [
                        "Around 9 million people live in London",
                        "London is known for its financial district"
                    ]
                }
            },
            "response": "The tool produces a response that consists of similarity scores between a query sentence and a list of candidate sentences or documents. The structure and meaning of each component are as follows:\n\n1. **Scores List**:  \n   - The primary output is a list of numerical values representing the semantic similarity between the query and each document in the input list.  \n   - Each score is typically a floating-point number indicating the closeness in meaning, often computed as a cosine similarity between the embeddings of the query and the document.  \n   - Higher scores indicate greater semantic similarity.\n\n2. **Document-Score Pairing (Optional/Typical)**:  \n   - For practical usage, the scores are often paired with the corresponding documents, forming tuples or objects like `(document, score)`.  \n   - This allows the user to easily identify which documents are most relevant to the query.\n\n3. **Sorted Results (Optional/Typical)**:  \n   - The document-score pairs are frequently sorted in descending order of similarity, so the most semantically relevant documents appear first.\n\n4. **Embedding Vectors (Internal/Optional)**:  \n   - The model internally produces dense 768-dimensional vector embeddings for both the query and the documents.  \n   - While these embeddings may be returned if explicitly requested, the typical end-user response focuses on the similarity scores rather than the raw vectors.\n\nIn summary, the response provides a quantitative measure of semantic similarity between the query and each document, often structured as a sorted list of document-score pairs for easy interpretation and downstream usage.",
            "examples": [
                {
                    "user_intent": "I want to find which news headlines are most relevant to the question 'What are the latest advancements in AI research?'",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "What are the latest advancements in AI research?",
                            "docs": [
                                "Google announces new AI language model that surpasses previous benchmarks",
                                "Stock markets are volatile today due to economic uncertainty",
                                "Breakthrough in quantum computing could accelerate AI simulations"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to match customer questions to the most relevant FAQ entries on our website.",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "How can I reset my account password?",
                            "docs": [
                                "To reset your password, go to account settings and click 'Reset Password'",
                                "Shipping options vary depending on your location",
                                "You can track your order using the tracking number sent to your email"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Compare user reviews to see which product description they most closely relate to.",
                    "tool_call": {
                        "name": "SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')",
                        "arguments": {
                            "query": "This smartphone has an excellent camera and battery life.",
                            "docs": [
                                "The phone's camera takes stunning low-light photos and battery lasts all day",
                                "This tablet is perfect for reading and light browsing",
                                "Our headphones provide immersive sound but limited battery life"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 1: A Chinese sentence similarity model that uses a transformer-based architecture to compare a source sentence with multiple target sentences, enabling semantic similarity analysis. It is built as a derivative of the text2vec-base-chinese model, replacing MacBERT with LERT while maintaining the original training setup, and is designed for use with Hugging Face Transformers.\nTool 2: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 3: The \"sentence-transformers/paraphrase-albert-small-v2\" model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering or semantic search. It utilizes the Hugging Face Transformers framework and is based on the SentenceTransformer API, which encodes input sentences into embeddings for comparison and analysis.\nTool 4: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 5: A T5-based model that generates paraphrased versions of input sentences, useful for rewording text while preserving its original meaning.\nTool 6: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 7: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 8: The \"sentence-transformers/all-MiniLM-L12-v1\" model maps sentences and paragraphs to a 384-dimensional dense vector space, enabling tasks like clustering and semantic search. It uses the SentenceTransformer framework from Hugging Face Transformers to encode sentences into embeddings for efficient similarity comparison.\nTool 9: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\nTool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 11: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\nTool 12: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 13: An unsupervised sentence embedding model based on RoBERTa, trained with the SimCSE approach to generate meaningful vector representations of sentences for measuring semantic similarity.\nTool 14: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 15: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 16: The `sentence-transformers/distilbert-base-nli-stsb-mean-tokens` model maps sentences and paragraphs to a 768-dimensional dense vector space, enabling tasks such as clustering and semantic search. It uses Hugging Face's Transformers framework and can be accessed via the SentenceTransformer API, which generates sentence embeddings from input text.\nTool 17: Generate dense vector embeddings for sentences and paragraphs using the `paraphrase-mpnet-base-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 18: A pre-trained sentence-transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\nTool 19: Generate dense vector representations of sentences and paragraphs using the `paraphrase-MiniLM-L6-v2` model, enabling tasks such as semantic search, clustering, and measuring sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question\\u2013answer pairs to capture rich semantic relationships between sentences.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"The input sentence or question for which you want to find semantically similar documents.\", \"required\": true, \"example\": \"How many people live in London?\"}, \"docs\": {\"type\": \"array\", \"description\": \"A list of sentences or paragraphs to compare against the query for semantic similarity.\", \"required\": true, \"example\": [\"Around 9 million people live in London\", \"London is known for its financial district\"]}}, \"response\": \"The tool produces a response that consists of similarity scores between a query sentence and a list of candidate sentences or documents. The structure and meaning of each component are as follows:\\n\\n1. **Scores List**:  \\n   - The primary output is a list of numerical values representing the semantic similarity between the query and each document in the input list.  \\n   - Each score is typically a floating-point number indicating the closeness in meaning, often computed as a cosine similarity between the embeddings of the query and the document.  \\n   - Higher scores indicate greater semantic similarity.\\n\\n2. **Document-Score Pairing (Optional/Typical)**:  \\n   - For practical usage, the scores are often paired with the corresponding documents, forming tuples or objects like `(document, score)`.  \\n   - This allows the user to easily identify which documents are most relevant to the query.\\n\\n3. **Sorted Results (Optional/Typical)**:  \\n   - The document-score pairs are frequently sorted in descending order of similarity, so the most semantically relevant documents appear first.\\n\\n4. **Embedding Vectors (Internal/Optional)**:  \\n   - The model internally produces dense 768-dimensional vector embeddings for both the query and the documents.  \\n   - While these embeddings may be returned if explicitly requested, the typical end-user response focuses on the similarity scores rather than the raw vectors.\\n\\nIn summary, the response provides a quantitative measure of semantic similarity between the query and each document, often structured as a sorted list of document-score pairs for easy interpretation and downstream usage.\", \"examples\": [{\"user_intent\": \"I want to find which news headlines are most relevant to the question 'What are the latest advancements in AI research?'\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"What are the latest advancements in AI research?\", \"docs\": [\"Google announces new AI language model that surpasses previous benchmarks\", \"Stock markets are volatile today due to economic uncertainty\", \"Breakthrough in quantum computing could accelerate AI simulations\"]}}}, {\"user_intent\": \"I need to match customer questions to the most relevant FAQ entries on our website.\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"How can I reset my account password?\", \"docs\": [\"To reset your password, go to account settings and click 'Reset Password'\", \"Shipping options vary depending on your location\", \"You can track your order using the tracking number sent to your email\"]}}}, {\"user_intent\": \"Compare user reviews to see which product description they most closely relate to.\", \"tool_call\": {\"name\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"arguments\": {\"query\": \"This smartphone has an excellent camera and battery life.\", \"docs\": [\"The phone's camera takes stunning low-light photos and battery lasts all day\", \"This tablet is perfect for reading and light browsing\", \"Our headphones provide immersive sound but limited battery life\"]}}}]}\n\n# User Query\nWe are building a platform to compare and contrast user input sentences with existing sentences in our database. It should return similar results.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find which existing sentences in our database are most similar to a user's input sentence.",
                    "tool_description": "Encode the input sentence and a set of database sentences into dense vector embeddings and compute similarity scores between them to identify semantically close matches.",
                    "expected_response": "A list of similarity scores or paired results showing which database sentences are most semantically similar to the input sentence."
                }
            ],
            "extracted_arguments": {
                "user_input_sentence": "The sentence or paragraph provided by the user for semantic comparison.",
                "database_sentences": "A list of existing sentences or paragraphs stored in the platform against which the user input will be compared."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_305",
        "query": "In our new app, we are building a feature that recommends books in different languages. To do this, first, we need to translate the book title and details from English to French. Help us to decide the best translation model to use here.",
        "instruction": "Given a `translation` task, retrieve tools that specialize in converting text from English to French by using models optimized for language translation and capable of handling diverse linguistic features to maintain accuracy and context in the output.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Translation",
                    "api_call": "translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')",
                    "api_arguments": [
                        "input_text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.en.fr": 33.8,
                                "newsdiscusstest2015-enfr.en.fr": 40.0,
                                "newssyscomb2009.en.fr": 29.8,
                                "news-test2008.en.fr": 27.5,
                                "newstest2009.en.fr": 29.4,
                                "newstest2010.en.fr": 32.7,
                                "newstest2011.en.fr": 34.3,
                                "newstest2012.en.fr": 31.8,
                                "newstest2013.en.fr": 33.2,
                                "Tatoeba.en.fr": 50.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.",
                    "id": "huggingface_api_522",
                    "name": "Helsinki-NLP/opus-mt-en-fr"
                },
                "id": "gorilla_huggingface_tool_518",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 1: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 2: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.",
            "Tool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 4: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 5: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 6: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 7: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 8: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 9: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 11: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 12: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.",
            "Tool 13: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.",
            "Tool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 15: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.",
            "Tool 16: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.",
            "Tool 17: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 18: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 19: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages."
        ],
        "tool_view": {
            "description": "The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in French that needs to be translated into English.",
                    "required": true,
                    "example": "Bonjour, comment ça va?"
                }
            },
            "response": "The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\n\nHere’s a description of the expected response:\n\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\n  \nIn principle, the response will consist of the following:\n\n1. **Translated Text**: A string containing the English translation of the input French text.\n\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\n\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool’s design or configuration.",
            "examples": [
                {
                    "user_intent": "Translate this French sentence to English: 'Je voudrais réserver une table pour deux ce soir.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Je voudrais réserver une table pour deux ce soir."
                        }
                    }
                },
                {
                    "user_intent": "I need the English version of this French paragraph about climate change: 'Le réchauffement climatique est une menace sérieuse pour notre planète.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Le réchauffement climatique est une menace sérieuse pour notre planète."
                        }
                    }
                },
                {
                    "user_intent": "Convert this casual French message into English: 'Salut! On se voit demain au café?'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Salut! On se voit demain au café?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 1: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 2: This tool provides a multilingual machine translation model, mBART-50, that can translate text directly between any of 50 languages. It leverages Hugging Face’s Text2Text generation framework and is designed for use in natural language processing tasks requiring accurate and flexible translation across multiple languages.\nTool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 4: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 5: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 6: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 7: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 8: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 9: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 10: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 11: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 12: mT5 is a multilingual text-to-text model based on the T5 architecture, pre-trained on a large Common Crawl dataset covering 101 languages. It uses a unified text-to-text framework to perform a wide range of multilingual NLP tasks, such as translation, summarization, and text generation, enabling consistent input-to-output processing across different languages.\nTool 13: A transformer-based translation tool trained on the OPUS dataset that converts English text into various Romance languages, providing accurate and fluent translations for multilingual applications.\nTool 14: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 15: KoBART-base-v2 is a Korean encoder-decoder language model based on the BART architecture, trained on over 40GB of diverse Korean text, including Wikipedia, news, and books. It can be used for feature extraction in natural language processing tasks, enabling applications such as text representation and analysis.\nTool 16: This tool is a model for single-column regression, trained to predict US housing prices using the jwan2021/autotrain-data-us-housing-prices dataset. It is implemented with the Joblib framework and can be used to make predictions based on input data after loading the model with `joblib.load('model.joblib')`. The model performs with an R2 of 0.884 and an MAE of 95,586.887, indicating a good predictive accuracy. It requires the joblib, pandas, and json libraries for use, and it processes data by selecting relevant features from a CSV file before making predictions.\nTool 17: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 18: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 19: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in French that needs to be translated into English.\", \"required\": true, \"example\": \"Bonjour, comment \\u00e7a va?\"}}, \"response\": \"The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\\n\\nHere\\u2019s a description of the expected response:\\n\\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\\n  \\nIn principle, the response will consist of the following:\\n\\n1. **Translated Text**: A string containing the English translation of the input French text.\\n\\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\\n\\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool\\u2019s design or configuration.\", \"examples\": [{\"user_intent\": \"Translate this French sentence to English: 'Je voudrais r\\u00e9server une table pour deux ce soir.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Je voudrais r\\u00e9server une table pour deux ce soir.\"}}}, {\"user_intent\": \"I need the English version of this French paragraph about climate change: 'Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.\"}}}, {\"user_intent\": \"Convert this casual French message into English: 'Salut! On se voit demain au caf\\u00e9?'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Salut! On se voit demain au caf\\u00e9?\"}}}]}\n\n# User Query\nIn our new app, we are building a feature that recommends books in different languages. To do this, first, we need to translate the book title and details from English to French. Help us to decide the best translation model to use here.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate the book title and details from English to French for our multilingual recommendation feature.",
                    "tool_description": "Use a machine translation model to convert English text into French, ensuring accurate and fluent translation suitable for displaying book information in French.",
                    "expected_response": "A string containing the French translation of the input English book title and details."
                },
                {
                    "user_intent": "Determine the most suitable translation model for translating English book information to French.",
                    "tool_description": "Compare available English-to-French translation models based on accuracy, reliability, and suitability for book content translation, and select the optimal model for the app.",
                    "expected_response": "A recommendation of the best translation model to use, including model name and reasoning for selection."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the original text that needs to be translated (English).",
                "target_language": "The language into which the text should be translated (French).",
                "text_content": "The book title and details text to be translated, represented as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_216",
        "query": "Our company's voice assistant needs to be able to detect voice activity in a conversation.",
        "instruction": "Given a `voice activity detection` task, retrieve tools that can identify active speech segments in audio inputs, allowing the system to distinguish between speech and non-speech elements in conversational audio data.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Voice Activity Detection",
                    "api_call": "Inference('julien-c/voice-activity-detection')",
                    "api_arguments": {
                        "audio": "TheBigBangTheory.wav"
                    },
                    "python_environment_requirements": "pyannote.audio",
                    "example_code": "from pyannote.audio.core.inference import Inference\nmodel = Inference('julien-c/voice-activity-detection', device='cuda')\nmodel({\n audio: TheBigBangTheory.wav\n})",
                    "performance": {
                        "dataset": "dihard",
                        "accuracy": "Not provided"
                    },
                    "description": "Example pyannote-audio Voice Activity Detection model using PyanNet. Imported from https://github.com/pyannote/pyannote-audio-hub and trained by @hbredin.",
                    "id": "huggingface_api_835",
                    "name": "julien-c/voice-activity-detection"
                },
                "id": "gorilla_huggingface_tool_830",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 1: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 2: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 3: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.",
            "Tool 5: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 6: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 9: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 10: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 11: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 12: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.",
            "Tool 13: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.",
            "Tool 15: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 16: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.",
            "Tool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 18: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 19: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio."
        ],
        "tool_view": {
            "description": "FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "parameters": {},
            "response": "The tool, FSMN-VAD, is designed for Voice Activity Detection (VAD). The response from this tool will likely contain information about the presence or absence of speech (voice activity) within a given audio input. \n\nIn general, the response from the tool is expected to include:\n\n1. **Speech Segments**: The main output will be a list or a set of labeled time intervals that indicate where speech occurs in the audio input. Each segment corresponds to a portion of the audio where voice activity was detected. This might include:\n   - **Start Time**: The beginning of the segment where voice activity is detected.\n   - **End Time**: The end of the segment where voice activity is detected.\n   \n2. **Confidence Scores** (optional): Depending on the implementation, the response could include a confidence score or probability associated with each detected segment, reflecting the model's certainty that the segment is speech-related.\n\n3. **Non-Speech Periods** (optional): The response might also highlight periods where no speech activity is detected, though this is typically inferred from the absence of speech segments.\n\nOverall, the structure is expected to focus on marking the regions of the audio that contain speech, potentially including start and end times for these regions, and possibly including confidence scores or other metadata related to the detected speech activity.",
            "examples": [
                {
                    "user_intent": "I want to detect when someone is speaking in this meeting recording.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "team_meeting_2025_12_30.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this podcast audio and identify all segments with voice activity.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "tech_podcast_episode12.mp3"
                        }
                    }
                },
                {
                    "user_intent": "I have surveillance audio from last night and I want to find all periods of speech.",
                    "tool_call": {
                        "name": "FSMN-VAD",
                        "arguments": {
                            "audio_file": "security_cam_2025_12_29.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 1: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 2: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 3: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 4: A Hugging Face Voice Activity Detection model by Eklavya designed to identify segments of speech within audio, useful for applications like speech processing, transcription, or audio analysis.\nTool 5: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 6: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 9: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 10: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 11: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 12: FastSpeech 2 is a text-to-speech model from Fairseq that converts English text into high-quality speech using a single female voice trained on the LJSpeech dataset, enabling natural and expressive audio synthesis for applications like voice assistants, narration, and accessibility tools.\nTool 13: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 14: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\nTool 15: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 16: Convert written Telugu text into natural-sounding speech using a male voice, leveraging the ESPnet framework for high-quality text-to-speech synthesis.\nTool 17: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 18: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 19: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\", \"parameters\": {}, \"response\": \"The tool, FSMN-VAD, is designed for Voice Activity Detection (VAD). The response from this tool will likely contain information about the presence or absence of speech (voice activity) within a given audio input. \\n\\nIn general, the response from the tool is expected to include:\\n\\n1. **Speech Segments**: The main output will be a list or a set of labeled time intervals that indicate where speech occurs in the audio input. Each segment corresponds to a portion of the audio where voice activity was detected. This might include:\\n   - **Start Time**: The beginning of the segment where voice activity is detected.\\n   - **End Time**: The end of the segment where voice activity is detected.\\n   \\n2. **Confidence Scores** (optional): Depending on the implementation, the response could include a confidence score or probability associated with each detected segment, reflecting the model's certainty that the segment is speech-related.\\n\\n3. **Non-Speech Periods** (optional): The response might also highlight periods where no speech activity is detected, though this is typically inferred from the absence of speech segments.\\n\\nOverall, the structure is expected to focus on marking the regions of the audio that contain speech, potentially including start and end times for these regions, and possibly including confidence scores or other metadata related to the detected speech activity.\", \"examples\": [{\"user_intent\": \"I want to detect when someone is speaking in this meeting recording.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"team_meeting_2025_12_30.wav\"}}}, {\"user_intent\": \"Analyze this podcast audio and identify all segments with voice activity.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"tech_podcast_episode12.mp3\"}}}, {\"user_intent\": \"I have surveillance audio from last night and I want to find all periods of speech.\", \"tool_call\": {\"name\": \"FSMN-VAD\", \"arguments\": {\"audio_file\": \"security_cam_2025_12_29.wav\"}}}]}\n\n# User Query\nOur company's voice assistant needs to be able to detect voice activity in a conversation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect when someone is speaking during a conversation audio recording.",
                    "tool_description": "Use a Voice Activity Detection (VAD) model to analyze audio input and identify segments where speech is present.",
                    "expected_response": "A list of time intervals indicating speech segments in the audio, optionally including confidence scores for each detected segment."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The audio data containing the conversation, provided as a file or stream for analysis."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_348",
        "query": "A company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.",
        "instruction": "Given a `survey analysis` task, retrieve tools that can count specific responses for survey questions by processing tabular data inputs to provide insights about the response distribution for each question.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')",
                    "api_arguments": {
                        "table": "table_data",
                        "query": "query"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline"
                    },
                    "example_code": "from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model & tokenizer\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\n\n# Get predictions\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\nprint(result)",
                    "performance": {
                        "dataset": "wikisql",
                        "accuracy": "Not provided"
                    },
                    "description": "TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.",
                    "id": "huggingface_api_462",
                    "name": "navteca/tapas-large-finetuned-wtq"
                },
                "id": "gorilla_huggingface_tool_458",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "Tool 1: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.",
            "Tool 2: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 3: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.",
            "Tool 4: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.",
            "Tool 5: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.",
            "Tool 6: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 7: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.",
            "Tool 8: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.",
            "Tool 9: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 10: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 11: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.",
            "Tool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.",
            "Tool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 14: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 18: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 19: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context."
        ],
        "tool_view": {
            "description": "A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.",
            "parameters": {
                "model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pre-trained model to load for question answering. This can be a model identifier from the Hugging Face Hub or a local path.",
                    "required": true,
                    "example": "deepset/tinyroberta-squad2"
                },
                "question": {
                    "type": "string",
                    "description": "The question to be answered using the specified model and context.",
                    "required": true,
                    "example": "Why is model conversion important?"
                },
                "context": {
                    "type": "string",
                    "description": "The context or passage in which the model should search for the answer to the question.",
                    "required": true,
                    "example": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                }
            },
            "response": "The tool produces a structured response containing the answer to a natural language question based on a given context. The typical response structure includes the following components:\n\n1. **`score`**: A numerical value representing the model's confidence in the predicted answer. Higher values indicate greater confidence that the answer is correct and relevant to the question.\n\n2. **`start`**: An integer representing the starting character index of the predicted answer within the provided context. This allows locating the answer precisely in the context text.\n\n3. **`end`**: An integer representing the ending character index of the predicted answer within the context. This complements the `start` index to define the span of the answer in the text.\n\n4. **`answer`**: A string containing the text span extracted from the context that the model predicts as the answer to the input question. This is the primary output users typically consume.\n\nIn essence, the response identifies a text span from the context that answers the question, provides the span’s position, and includes a confidence score reflecting the model’s certainty.",
            "examples": [
                {
                    "user_intent": "I want to know why converting models between frameworks can be useful.",
                    "tool_call": {
                        "name": "deepset/tinyroberta-squad2",
                        "arguments": {
                            "model_name_or_path": "deepset/tinyroberta-squad2",
                            "question": "Why is model conversion important?",
                            "context": "The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks."
                        }
                    }
                },
                {
                    "user_intent": "Explain the benefit of using a smaller, faster QA model.",
                    "tool_call": {
                        "name": "deepset/tinyroberta-squad2",
                        "arguments": {
                            "model_name_or_path": "deepset/tinyroberta-squad2",
                            "question": "What are the advantages of the tinyroberta-squad2 model?",
                            "context": "This is the distilled version of the deepset/roberta-base-squad2 model. It has a comparable prediction quality and runs at twice the speed of the base model."
                        }
                    }
                },
                {
                    "user_intent": "Find out how this QA model performs on SQuAD v2 dataset.",
                    "tool_call": {
                        "name": "deepset/tinyroberta-squad2",
                        "arguments": {
                            "model_name_or_path": "deepset/tinyroberta-squad2",
                            "question": "How accurate is this model on SQuAD v2?",
                            "context": "The model achieves an exact match score of 78.69% and an F1 score of 81.92% on the SQuAD v2 dataset."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\nTool 1: This tool uses the BERT-base uncased model fine-tuned on the SQuAD v1 dataset for question answering. It processes a given context and question to provide an answer, making it suitable for natural language processing tasks where extracting specific information from text is required. The model is case-insensitive, meaning it treats lowercase and uppercase text equivalently. It can be easily integrated into Python 3.7.5 environments using the Hugging Face Transformers framework.\nTool 2: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 3: M2M100 is a multilingual sequence-to-sequence model that enables direct translation between 100 languages across 9,900 language pairs. It allows text to be translated from any source language to a specified target language by setting the target language ID, making it useful for applications requiring high-coverage, many-to-many machine translation.\nTool 4: A multilingual question-answering tool based on a BERT model fine-tuned on the XQuAD dataset, capable of extracting answers from text in 11 languages, useful for retrieving precise responses to questions given a specific context.\nTool 5: M2M100 is a multilingual encoder-decoder model designed for many-to-many translation across 100 languages. It can translate text between any of these languages by generating the target language directly, using a forced language token. This tool is built using Hugging Face Transformers and is useful for applications requiring flexible and efficient multilingual text translation.\nTool 6: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 7: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.\nTool 8: This tool uses the BLIP vision-language model to perform visual question answering, allowing users to input an image and a natural-language question to receive a relevant answer. It is designed for tasks where understanding the content of an image in context with a specific query is needed, offering improved accuracy on VQA benchmarks by leveraging pretrained vision-language representations.\nTool 9: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 10: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 11: The deepset/bert-medium-squad2-distilled model is a distilled version of BERT trained on the SQuAD 2.0 dataset for question answering tasks. Built on the BERT-medium architecture, it leverages the Hugging Face Transformers library and can be used to answer questions based on a given context. It provides a high level of accuracy, achieving an exact match score of 68.64% and an F1 score of 72.76% on the SQuAD 2.0 dataset.\nTool 12: A natural language processing tool that uses the TAPAS medium model fine-tuned on Sequential Question Answering to answer questions about tabular data. It leverages relative position embeddings and is pretrained on large-scale English Wikipedia data, enabling conversational-style querying of tables for insights such as totals, counts, or other specific values.\nTool 13: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 14: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 16: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 17: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 18: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 19: A Korean-language question answering model based on Electra, trained on the KorQuAD dataset, designed to extract precise answers from a given text context.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A compact, high-speed question-answering model based on RoBERTa, designed to extract answers from a given context. It delivers performance comparable to larger models while running faster, making it suitable for efficiently answering questions on texts such as SQuAD v2.\", \"parameters\": {\"model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pre-trained model to load for question answering. This can be a model identifier from the Hugging Face Hub or a local path.\", \"required\": true, \"example\": \"deepset/tinyroberta-squad2\"}, \"question\": {\"type\": \"string\", \"description\": \"The question to be answered using the specified model and context.\", \"required\": true, \"example\": \"Why is model conversion important?\"}, \"context\": {\"type\": \"string\", \"description\": \"The context or passage in which the model should search for the answer to the question.\", \"required\": true, \"example\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}, \"response\": \"The tool produces a structured response containing the answer to a natural language question based on a given context. The typical response structure includes the following components:\\n\\n1. **`score`**: A numerical value representing the model's confidence in the predicted answer. Higher values indicate greater confidence that the answer is correct and relevant to the question.\\n\\n2. **`start`**: An integer representing the starting character index of the predicted answer within the provided context. This allows locating the answer precisely in the context text.\\n\\n3. **`end`**: An integer representing the ending character index of the predicted answer within the context. This complements the `start` index to define the span of the answer in the text.\\n\\n4. **`answer`**: A string containing the text span extracted from the context that the model predicts as the answer to the input question. This is the primary output users typically consume.\\n\\nIn essence, the response identifies a text span from the context that answers the question, provides the span\\u2019s position, and includes a confidence score reflecting the model\\u2019s certainty.\", \"examples\": [{\"user_intent\": \"I want to know why converting models between frameworks can be useful.\", \"tool_call\": {\"name\": \"deepset/tinyroberta-squad2\", \"arguments\": {\"model_name_or_path\": \"deepset/tinyroberta-squad2\", \"question\": \"Why is model conversion important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and lets people easily switch between frameworks.\"}}}, {\"user_intent\": \"Explain the benefit of using a smaller, faster QA model.\", \"tool_call\": {\"name\": \"deepset/tinyroberta-squad2\", \"arguments\": {\"model_name_or_path\": \"deepset/tinyroberta-squad2\", \"question\": \"What are the advantages of the tinyroberta-squad2 model?\", \"context\": \"This is the distilled version of the deepset/roberta-base-squad2 model. It has a comparable prediction quality and runs at twice the speed of the base model.\"}}}, {\"user_intent\": \"Find out how this QA model performs on SQuAD v2 dataset.\", \"tool_call\": {\"name\": \"deepset/tinyroberta-squad2\", \"arguments\": {\"model_name_or_path\": \"deepset/tinyroberta-squad2\", \"question\": \"How accurate is this model on SQuAD v2?\", \"context\": \"The model achieves an exact match score of 78.69% and an F1 score of 81.92% on the SQuAD v2 dataset.\"}}}]}\n\n# User Query\nA company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to count the number of respondents who gave a specific answer for a survey question.",
                    "tool_description": "Extract and compute the number of respondents providing a specific answer to a given survey question, based on the dataset containing responses.",
                    "expected_response": "A count of the number of responses matching the specific answer for the given survey question."
                }
            ],
            "extracted_arguments": {
                "survey_question": "The specific survey question for which responses need to be analyzed.",
                "answer": "The specific answer from respondents to the question being analyzed."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_405",
        "query": "The company's legal team is working on a case. They need a highly accurate tool to extract answers from a large set of legal documents. Develop a tool for this purpose.",
        "instruction": "Given a `legal document analysis` task, retrieve tools that specialize in natural language processing, specifically focusing on extracting precise answers from text by processing large sets of legal documents to meet the query's accuracy requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')",
                    "api_arguments": {
                        "model_name_or_path": "deepset/deberta-v3-large-squad2",
                        "tokenizer": "deepset/deberta-v3-large-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": {
                        "a": {
                            "code": "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)"
                        },
                        "b": {
                            "code": "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
                        }
                    },
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "exact": 87.6105449338836,
                            "f1": 90.75307008866517
                        }
                    },
                    "description": "This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.",
                    "id": "huggingface_api_488",
                    "name": "deepset/deberta-v3-large-squad2"
                },
                "id": "gorilla_huggingface_tool_484",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "Tool 1: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 4: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.",
            "Tool 5: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.",
            "Tool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 8: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.",
            "Tool 9: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 10: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.",
            "Tool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 14: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 16: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.",
            "Tool 17: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.",
            "Tool 18: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model."
        ],
        "tool_view": {
            "description": "Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.",
            "parameters": {
                "tokenizer": {
                    "type": "object",
                    "description": "A tokenizer object used to preprocess input text for the RoBERTa-based Question Answering model. This tokenizer converts text into token IDs that the model can understand.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Rakib/roberta-base-on-cuad')"
                }
            },
            "response": "The tool is designed for **Question Answering** using a model fine-tuned specifically on the CUAD dataset, which focuses on legal documents. The model is based on the **RoBERTa** architecture, and it can extract answers from legal contracts and other legal documents.\n\n### Expected Response Structure and Meaning:\n\n1. **Predicted Answer**: \n   - The primary output will contain the **answer** to the question posed. This is typically a text string that represents the model's best guess for the correct response based on the input question and context (i.e., the legal document).\n   \n2. **Start Position**:\n   - This component indicates the **starting position** in the context (the legal document) where the answer begins. It is typically represented as an index within the text.\n\n3. **End Position**:\n   - The **end position** represents the index of where the answer finishes in the context text.\n\n4. **Confidence Score** (Optional but common in NLP models):\n   - The model may also include a **confidence score** or probability indicating how certain it is about the predicted answer. This score would typically range from 0 to 1, with higher values indicating greater confidence in the prediction.\n\n### General Behavior:\n- Given a **question** and a **context** (i.e., a segment of a legal document), the tool's output will typically consist of the most likely answer found within that context, along with the positions where the answer starts and ends in the provided context. If the model cannot find a clear answer, it might either output an empty string or indicate an uncertain result (possibly with a low confidence score).",
            "examples": [
                {
                    "user_intent": "I have a legal contract and want to know the termination clause details.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "This Agreement may be terminated by either party with thirty (30) days written notice...",
                            "question": "What are the termination terms in this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Extract the liability limitations from this service agreement.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "The liability of the Company shall be limited to direct damages not exceeding $10,000...",
                            "question": "What limitations of liability does the agreement specify?"
                        }
                    }
                },
                {
                    "user_intent": "Check the governing law stated in this contract document.",
                    "tool_call": {
                        "name": "Rakib/roberta-base-on-cuad",
                        "arguments": {
                            "context": "This Agreement shall be governed by and construed in accordance with the laws of the State of New York...",
                            "question": "Which law governs this contract?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\nTool 1: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 2: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 3: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 4: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\nTool 5: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 6: A multimodal document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers from images of documents. Given an image and a question, it encodes the visual content and generates text responses, enabling automated understanding and querying of document contents.\nTool 7: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 8: This tool provides a DeBERTa-v3-large model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling accurate responses to questions based on a given text context, including handling unanswerable questions. It is used to extract precise answers from passages and can be easily integrated via Hugging Face Transformers for NLP applications requiring reliable comprehension and reasoning over text.\nTool 9: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 10: A question-answering tool based on a fine-tuned SapBERT model from PubMedBERT, designed to extract precise answers from a given text context using the SQuAD v2 dataset. It is intended for natural language processing applications that require accurate retrieval of information from biomedical or general text passages.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: This tool utilizes a fine-tuned version of the multi-modal LayoutLM model to perform question answering on documents, allowing users to extract specific information from images of documents by providing a question and the image URL.\nTool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 14: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 15: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 16: A question-answering tool based on a DistilBERT model fine-tuned on SQuAD v1.1, designed to extract precise answers from a given context using natural language queries.\nTool 17: A Hugging Face Transformers model based on Longformer, fine-tuned on the SQuAD v1 dataset for question answering. It is designed to handle long text inputs—up to 4096 tokens—allowing users to extract precise answers from lengthy documents with high accuracy.\nTool 18: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 19: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Rakib/roberta-base-on-cuad is a RoBERTa-based model designed for question answering on legal documents. Trained on the CUAD dataset, it can extract answers from contracts and other legal texts, making it useful for automating information retrieval and analysis in legal contexts.\", \"parameters\": {\"tokenizer\": {\"type\": \"object\", \"description\": \"A tokenizer object used to preprocess input text for the RoBERTa-based Question Answering model. This tokenizer converts text into token IDs that the model can understand.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Rakib/roberta-base-on-cuad')\"}}, \"response\": \"The tool is designed for **Question Answering** using a model fine-tuned specifically on the CUAD dataset, which focuses on legal documents. The model is based on the **RoBERTa** architecture, and it can extract answers from legal contracts and other legal documents.\\n\\n### Expected Response Structure and Meaning:\\n\\n1. **Predicted Answer**: \\n   - The primary output will contain the **answer** to the question posed. This is typically a text string that represents the model's best guess for the correct response based on the input question and context (i.e., the legal document).\\n   \\n2. **Start Position**:\\n   - This component indicates the **starting position** in the context (the legal document) where the answer begins. It is typically represented as an index within the text.\\n\\n3. **End Position**:\\n   - The **end position** represents the index of where the answer finishes in the context text.\\n\\n4. **Confidence Score** (Optional but common in NLP models):\\n   - The model may also include a **confidence score** or probability indicating how certain it is about the predicted answer. This score would typically range from 0 to 1, with higher values indicating greater confidence in the prediction.\\n\\n### General Behavior:\\n- Given a **question** and a **context** (i.e., a segment of a legal document), the tool's output will typically consist of the most likely answer found within that context, along with the positions where the answer starts and ends in the provided context. If the model cannot find a clear answer, it might either output an empty string or indicate an uncertain result (possibly with a low confidence score).\", \"examples\": [{\"user_intent\": \"I have a legal contract and want to know the termination clause details.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"This Agreement may be terminated by either party with thirty (30) days written notice...\", \"question\": \"What are the termination terms in this contract?\"}}}, {\"user_intent\": \"Extract the liability limitations from this service agreement.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"The liability of the Company shall be limited to direct damages not exceeding $10,000...\", \"question\": \"What limitations of liability does the agreement specify?\"}}}, {\"user_intent\": \"Check the governing law stated in this contract document.\", \"tool_call\": {\"name\": \"Rakib/roberta-base-on-cuad\", \"arguments\": {\"context\": \"This Agreement shall be governed by and construed in accordance with the laws of the State of New York...\", \"question\": \"Which law governs this contract?\"}}}]}\n\n# User Query\nThe company's legal team is working on a case. They need a highly accurate tool to extract answers from a large set of legal documents. Develop a tool for this purpose.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I need to extract answers from a large set of legal documents for a case.",
                    "tool_description": "Utilize a question-answering tool trained on legal documents to extract specific answers from a collection of legal texts, such as contracts and case files, based on posed questions.",
                    "expected_response": "A list of extracted answers, each associated with the question posed, including relevant start and end positions in the document, as well as confidence scores."
                }
            ],
            "extracted_arguments": {
                "document_set": "A collection of legal documents to be processed.",
                "question": "The question related to the legal case that needs an answer from the documents."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_81",
        "query": "A business collaboration project requires staff to translate Russian documents into English.",
        "instruction": "Given a `document translation` task, retrieve tools that facilitate the process of translating text from one language to another, specifically from Russian to English, by employing advanced natural language processing models to deliver accurate translations.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Translation",
                    "api_call": "AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-ru-en')",
                    "api_arguments": {
                        "from_pretrained": "Helsinki-NLP/opus-mt-ru-en"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoTokenizer, AutoModelForSeq2SeqLM"
                    },
                    "example_code": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Helsinki-NLP/opus-mt-ru-en)",
                    "performance": {
                        "dataset": "newstest2019-ruen.ru.en",
                        "accuracy": 31.4
                    },
                    "description": "A Russian to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Transformer-align architecture and trained on the OPUS dataset. The model can be used for translation and text-to-text generation tasks.",
                    "id": "huggingface_api_527",
                    "name": "Helsinki-NLP/opus-mt-ru-en"
                },
                "id": "gorilla_huggingface_tool_523",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 1: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.",
            "Tool 2: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 3: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.",
            "Tool 4: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.",
            "Tool 5: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 6: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.",
            "Tool 7: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 8: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 9: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 10: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 11: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.",
            "Tool 12: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.",
            "Tool 13: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 14: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 16: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.",
            "Tool 17: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 18: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 19: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages."
        ],
        "tool_view": {
            "description": "A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "parameters": {
                "src_text": {
                    "type": "array",
                    "description": "A list of text strings that need to be translated. Each string should include the appropriate language token (e.g., '>>por<<' for Portuguese) to indicate the source language.",
                    "required": true,
                    "example": [
                        ">>por<< Tom tried to stab me.",
                        ">>por<< He has been to Hawaii several times."
                    ]
                }
            },
            "response": "The tool returns the result of neural machine translation (NMT) from English (en) to Portuguese (pt) for a given list of text strings. \n\nThe structure of the response can be expected to contain:\n\n1. **Translated Texts**: \n   - A collection of translated strings, where each input string from the `src_text` is mapped to a translated string in Portuguese (pt). The number of translated strings will correspond to the number of input text strings provided.\n\n2. **Model Details**:\n   - Information about the specific model used for the translation, in this case, the `opus-mt-tc-big-en-pt` model, which is part of the OPUS-MT project. This can include details like the model's training data or target languages (though these might not be explicitly included in every response).\n\n3. **Translation Accuracy**:\n   - While the accuracy is typically part of the performance metrics for the model, this may not be part of each individual response. However, the model has shown an accuracy of 50.4% on the `flores101-devtest` dataset and 49.6% on the `tatoeba-test-v2021-08-07` dataset, as shown in the tool’s documentation. These figures represent the model's performance on standard benchmarks.\n\nThe exact format of the translated output strings (such as whether they are returned as a simple list or along with metadata) will depend on the implementation of the specific translation API. However, the response would generally be a collection of the translated texts that corresponds in order to the input text strings.",
            "examples": [
                {
                    "user_intent": "Translate a list of English sentences into Portuguese.",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< Tom tried to stab me.",
                                ">>por<< He has been to Hawaii several times."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to translate some English text into Portuguese using a neural machine translation model.",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< I love learning new languages.",
                                ">>por<< The weather is great today."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Can you help me translate this sentence into Portuguese?",
                    "tool_call": {
                        "name": "opus-mt-tc-big-en-pt",
                        "arguments": {
                            "src_text": [
                                ">>por<< This is an amazing translation model!"
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 1: A neural machine translation model that converts English text to Russian, based on the Marian NMT framework and trained on the OPUS dataset. It can be used via Hugging Face Transformers for accurate English-to-Russian translation tasks.\nTool 2: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 3: This tool provides speech recognition for the Russian language using a fine-tuned version of the XLSR-53 model (wav2vec2-large-xlsr-53-russian). It transcribes audio files into text by leveraging the Hugging Face Transformers framework and requires libraries like huggingsound, torch, and librosa. The model is specifically trained on the Common Voice 6.1 dataset, offering an accuracy of 13.3% word error rate (WER) and 2.88% character error rate (CER). It is suitable for applications where accurate Russian speech-to-text transcription is needed.\nTool 4: This tool uses the wav2vec2 model to classify emotions in Russian speech, including anger, disgust, enthusiasm, fear, happiness, neutrality, and sadness. It is based on the Hugging Face Transformers framework and can process audio files with a sampling rate of 16 kHz. The model is pre-trained on the Russian Emotional Speech Dialogs dataset and achieves an accuracy of 72%. It requires Python libraries such as torch, torchaudio, transformers, librosa, and numpy for operation.\nTool 5: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 6: A Russian-to-English translation model developed by the University of Helsinki, based on the Transformer-align architecture and trained on the OPUS dataset, designed for translating text and performing text-to-text generation tasks.\nTool 7: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 8: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 9: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 10: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 11: DeepPavlov/rubert-base-cased is a pretrained Russian-language BERT model that extracts contextual text features for natural language processing tasks, built on the Hugging Face Transformers framework and trained on Russian Wikipedia and news data.\nTool 12: A Fairseq-based transformer text-to-speech model that converts Russian text into speech using a single male voice. It is pre-trained on Common Voice v7 and fine-tuned on the CSS10 dataset, producing high-quality audio with the HiFi-GAN vocoder, suitable for applications that require natural-sounding Russian TTS.\nTool 13: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 14: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 15: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 16: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.\nTool 17: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 18: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 19: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\", \"parameters\": {\"src_text\": {\"type\": \"array\", \"description\": \"A list of text strings that need to be translated. Each string should include the appropriate language token (e.g., '>>por<<' for Portuguese) to indicate the source language.\", \"required\": true, \"example\": [\">>por<< Tom tried to stab me.\", \">>por<< He has been to Hawaii several times.\"]}}, \"response\": \"The tool returns the result of neural machine translation (NMT) from English (en) to Portuguese (pt) for a given list of text strings. \\n\\nThe structure of the response can be expected to contain:\\n\\n1. **Translated Texts**: \\n   - A collection of translated strings, where each input string from the `src_text` is mapped to a translated string in Portuguese (pt). The number of translated strings will correspond to the number of input text strings provided.\\n\\n2. **Model Details**:\\n   - Information about the specific model used for the translation, in this case, the `opus-mt-tc-big-en-pt` model, which is part of the OPUS-MT project. This can include details like the model's training data or target languages (though these might not be explicitly included in every response).\\n\\n3. **Translation Accuracy**:\\n   - While the accuracy is typically part of the performance metrics for the model, this may not be part of each individual response. However, the model has shown an accuracy of 50.4% on the `flores101-devtest` dataset and 49.6% on the `tatoeba-test-v2021-08-07` dataset, as shown in the tool\\u2019s documentation. These figures represent the model's performance on standard benchmarks.\\n\\nThe exact format of the translated output strings (such as whether they are returned as a simple list or along with metadata) will depend on the implementation of the specific translation API. However, the response would generally be a collection of the translated texts that corresponds in order to the input text strings.\", \"examples\": [{\"user_intent\": \"Translate a list of English sentences into Portuguese.\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< Tom tried to stab me.\", \">>por<< He has been to Hawaii several times.\"]}}}, {\"user_intent\": \"I need to translate some English text into Portuguese using a neural machine translation model.\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< I love learning new languages.\", \">>por<< The weather is great today.\"]}}}, {\"user_intent\": \"Can you help me translate this sentence into Portuguese?\", \"tool_call\": {\"name\": \"opus-mt-tc-big-en-pt\", \"arguments\": {\"src_text\": [\">>por<< This is an amazing translation model!\"]}}}]}\n\n# User Query\nA business collaboration project requires staff to translate Russian documents into English.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate a list of Russian documents into English for a business collaboration project.",
                    "tool_description": "Use a neural machine translation model to convert text from Russian to English, ensuring accurate semantic and syntactic translation suitable for business documentation.",
                    "expected_response": "A collection of translated English text strings corresponding to the input Russian documents."
                }
            ],
            "extracted_arguments": {
                "src_text": "A list of Russian text strings or documents that need to be translated into English."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_321",
        "query": "A marketing firm has asked us to build an application to classify social media images into various categories.",
        "instruction": "Given an `image classification` task, retrieve tools that process social media images to categorize them into various categories by utilizing computer vision models optimized for classifying images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')",
                    "api_arguments": [
                        "learning_rate",
                        "train_batch_size",
                        "eval_batch_size",
                        "seed",
                        "gradient_accumulation_steps",
                        "total_train_batch_size",
                        "optimizer",
                        "lr_scheduler_type",
                        "lr_scheduler_warmup_ratio",
                        "num_epochs"
                    ],
                    "python_environment_requirements": [
                        "Transformers 4.28.1",
                        "Pytorch 2.0.0+cu118",
                        "Datasets 2.11.0",
                        "Tokenizers 0.13.3"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "imagefolder",
                        "accuracy": 0.9726
                    },
                    "description": "This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.",
                    "id": "huggingface_api_197",
                    "name": "swin-tiny-patch4-window7-224-bottom_cleaned_data"
                },
                "id": "gorilla_huggingface_tool_193",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 2: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.",
            "Tool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 5: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 6: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 7: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.",
            "Tool 8: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.",
            "Tool 9: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.",
            "Tool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 11: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.",
            "Tool 13: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.",
            "Tool 14: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.",
            "Tool 15: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.",
            "Tool 16: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.",
            "Tool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 18: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 19: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application."
        ],
        "tool_view": {
            "description": "A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The path to the pre-trained model or model identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.",
                    "required": true,
                    "example": "cardiffnlp/twitter-roberta-base-sentiment-latest"
                }
            },
            "response": "The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\n\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\n\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\n\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\n\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "I love the new features in the latest smartphone update!"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": "Markets crashed today after unexpected news."
                        }
                    }
                },
                {
                    "user_intent": "Determine the sentiment of multiple tweets regarding a recent movie release.",
                    "tool_call": {
                        "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        "arguments": {
                            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "tokenizer": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                            "text": [
                                "Absolutely loved the new movie! Best I've seen this year.",
                                "The movie was okay, not really memorable.",
                                "Terrible film, wasted two hours of my life."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 1: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 2: This tool performs sentiment analysis using the \"cardiffnlp/twitter-roberta-base-sentiment\" model, a RoBERTa-based model fine-tuned on approximately 58 million tweets. It is designed to classify the sentiment of text, particularly for English-language tweets, and is based on the TweetEval benchmark. This model is ideal for analyzing social media content and determining the sentiment expressed in short text sequences.\nTool 3: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 4: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 5: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 6: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 7: A pre-trained TimeSformer model for video classification that assigns input videos to one of 400 Kinetics-400 action categories, using space-time attention to capture both spatial and temporal information, suitable for analyzing video content in research or application settings.\nTool 8: The \"michellejieli/NSFW_text_classifier\" is a sentiment analysis tool based on a fine-tuned DistilBERT model. It classifies text as either \"NSFW\" (not safe for work) or \"SFW\" (safe for work), specifically designed to detect inappropriate or unprofessional content. The model was trained on 14,317 Reddit posts and can be used to filter NSFW content from text in applications like social media or forums.\nTool 9: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\nTool 10: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 11: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 12: A zero-shot image classification tool based on OpenAI’s CLIP model that allows users to classify images into arbitrary categories without task-specific training, enabling robust generalization across a wide range of computer vision datasets.\nTool 13: A ViT-based image classification model that leverages Hugging Face Transformers and PyTorch to classify images, originally trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI, suitable for accurately recognizing a wide range of visual categories.\nTool 14: A Hugging Face Transformers model for image classification, BEiT (microsoft/beit-base-patch16-224) is pre-trained on ImageNet-21k and fine-tuned on ImageNet 2012, allowing it to classify images into 1,000 categories with high accuracy. It is used to process images and predict their content by mapping input images to class labels.\nTool 15: ByT5 is a tokenizer-free version of Google's T5 designed for text-to-text generation, capable of handling noisy or unstructured text effectively. The `google/byt5-small` model follows the MT5 architecture and was pre-trained on the mC4 dataset without supervised fine-tuning, requiring task-specific fine-tuning for downstream applications. It is particularly useful for applications where text may contain irregularities, such as social media content, and can be integrated using the Hugging Face Transformers library.\nTool 16: A Hugging Face pipeline for zero-shot image classification using the CLIP ConvNeXt-Large model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without additional training, achieving 75.9% top-1 accuracy on ImageNet-1k.\nTool 17: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 18: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 19: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The path to the pre-trained model or model identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The path to the pre-trained tokenizer or tokenizer identifier from the Hugging Face model hub.\", \"required\": true, \"example\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}}, \"response\": \"The tool is designed to perform sentiment analysis on text inputs, specifically leveraging a RoBERTa-based model trained on a substantial dataset of tweets. The expected response from the tool would typically include the following components:\\n\\n1. **Label**: This component indicates the sentiment classification assigned to the input text. It may represent categories such as positive, negative, or neutral, reflecting the emotional tone of the analyzed text.\\n\\n2. **Score**: This component provides a numerical value representing the confidence level of the sentiment classification. It quantifies the model's certainty regarding the assigned label, often expressed as a probability or percentage.\\n\\n3. **Input Text**: This component may echo the original text that was analyzed, providing context for the sentiment classification and allowing users to verify the input against the output.\\n\\nOverall, the response encapsulates the sentiment analysis results, offering insights into the emotional content of the provided text along with the model's confidence in its assessment.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this tweet: 'I love the new features in the latest smartphone update!'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"I love the new features in the latest smartphone update!\"}}}, {\"user_intent\": \"Check whether this tweet about the stock market is positive, negative, or neutral: 'Markets crashed today after unexpected news.'\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": \"Markets crashed today after unexpected news.\"}}}, {\"user_intent\": \"Determine the sentiment of multiple tweets regarding a recent movie release.\", \"tool_call\": {\"name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"arguments\": {\"model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"tokenizer\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text\": [\"Absolutely loved the new movie! Best I've seen this year.\", \"The movie was okay, not really memorable.\", \"Terrible film, wasted two hours of my life.\"]}}}]}\n\n# User Query\nA marketing firm has asked us to build an application to classify social media images into various categories.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify images into predefined categories based on content.",
                    "tool_description": "Perform zero-shot image classification to categorize input images into custom, predefined categories without task-specific training.",
                    "expected_response": "A list of predicted categories for each input image."
                }
            ],
            "extracted_arguments": {
                "image_data": "The image or images to be classified, provided in a format that the model can process.",
                "categories": "The predefined categories into which the images will be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_78",
        "query": "Make a summary video for our last team meeting. The audio from the video must identify who is speaking and when.",
        "instruction": "Given a `video summary creation` task, retrieve tools with audio analysis capabilities to identify speakers and their speaking times during a meeting, then generate a concise video summary reflecting the discussions accurately and efficiently.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Voice Activity Detection",
                    "framework": "pyannote.audio",
                    "functionality": "Speaker diarization",
                    "api_call": "Pipeline.from_pretrained('pyannote/speaker-diarization@2.1')",
                    "api_arguments": [
                        "num_speakers",
                        "min_speakers",
                        "max_speakers",
                        "segmentation_onset"
                    ],
                    "python_environment_requirements": "pyannote.audio 2.0",
                    "example_code": {
                        "load_pipeline": "from pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)",
                        "apply_pipeline": "diarization = pipeline(audio.wav)",
                        "save_output": "with open(audio.rttm, w) as rttm:\n  diarization.write_rttm(rttm)"
                    },
                    "performance": {
                        "dataset": [
                            {
                                "name": "AISHELL-4",
                                "accuracy": {
                                    "DER%": 14.61,
                                    "FA%": 3.31,
                                    "Miss%": 4.35,
                                    "Conf%": 6.95
                                }
                            },
                            {
                                "name": "AMI Mix-Headset only_words",
                                "accuracy": {
                                    "DER%": 18.21,
                                    "FA%": 3.28,
                                    "Miss%": 11.07,
                                    "Conf%": 3.87
                                }
                            },
                            {
                                "name": "AMI Array1-01 only_words",
                                "accuracy": {
                                    "DER%": 29.0,
                                    "FA%": 2.71,
                                    "Miss%": 21.61,
                                    "Conf%": 4.68
                                }
                            },
                            {
                                "name": "CALLHOME Part2",
                                "accuracy": {
                                    "DER%": 30.24,
                                    "FA%": 3.71,
                                    "Miss%": 16.86,
                                    "Conf%": 9.66
                                }
                            },
                            {
                                "name": "DIHARD 3 Full",
                                "accuracy": {
                                    "DER%": 20.99,
                                    "FA%": 4.25,
                                    "Miss%": 10.74,
                                    "Conf%": 6.0
                                }
                            },
                            {
                                "name": "REPERE Phase 2",
                                "accuracy": {
                                    "DER%": 12.62,
                                    "FA%": 1.55,
                                    "Miss%": 3.3,
                                    "Conf%": 7.76
                                }
                            },
                            {
                                "name": "VoxConverse v0.0.2",
                                "accuracy": {
                                    "DER%": 12.76,
                                    "FA%": 3.45,
                                    "Miss%": 3.85,
                                    "Conf%": 5.46
                                }
                            }
                        ]
                    },
                    "description": "This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.",
                    "id": "huggingface_api_846",
                    "name": "johnislarry/cloned-pyannote-speaker-diarization-endpoint"
                },
                "id": "gorilla_huggingface_tool_840",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 3: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 4: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 5: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 6: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 7: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.",
            "Tool 8: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 10: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 12: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 14: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.",
            "Tool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.",
            "Tool 17: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 18: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.",
            "Tool 19: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis."
        ],
        "tool_view": {
            "description": "The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "parameters": {
                "pretrained_model_name_or_path": {
                    "type": "string",
                    "description": "The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.",
                    "required": true,
                    "example": "nateraw/videomae-base-finetuned-ucf101"
                }
            },
            "response": "The tool’s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\n\n1. **logits**:  \n   - A tensor containing raw, unnormalized scores for each possible action class in the model’s label set.  \n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \n   - These scores represent the model’s confidence for each action before applying any normalization such as softmax.\n\n2. **predicted label (optional convenience mapping)**:  \n   - A single integer or string representing the action class with the highest score.  \n   - If an integer, it indexes into the model’s internal `id2label` mapping.  \n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\"Archery\"`).  \n\n3. **id2label mapping (metadata)**:  \n   - A dictionary mapping integer class indices to human-readable labels.  \n   - Used to interpret the predicted label in a readable format.  \n\nIn practice, the typical response workflow is:\n\n- The model returns the `logits` tensor.  \n- The user selects the class with the maximum score to obtain the `predicted_label`.  \n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \n\nNo additional fields are defined by the tool; the response focuses on the model’s prediction scores and derived class label for the input video clip.",
            "examples": [
                {
                    "user_intent": "I want to classify a video of a person performing an action using a pre-trained model.",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                },
                {
                    "user_intent": "Can I recognize actions in my video using a Hugging Face model for action classification?",
                    "tool_call": {
                        "name": "videomae-base-finetuned-ucf101",
                        "arguments": {
                            "pretrained_model_name_or_path": "nateraw/videomae-base-finetuned-ucf101"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 1: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 2: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 3: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 4: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 5: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 6: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 7: A fine-tuned video classification model based on VideoMAE, designed to categorize video content by leveraging pretrained features from the Kinetics dataset. It can be used to automatically label videos for tasks like action recognition or video content analysis.\nTool 8: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 9: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 10: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 11: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 12: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 13: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 14: A pre-trained VideoMAE model designed for video classification, fine-tuned on the Kinetics-400 dataset. It processes video inputs to predict action classes with high accuracy and is suitable for applications requiring efficient and accurate understanding of video content.\nTool 15: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 16: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture. It processes videos as sequences of fixed-size patches, using a Transformer encoder with positional embeddings to learn rich video representations. By pre-training on large video datasets, the model captures features that can be used for downstream tasks, such as adding a linear layer on the [CLS] token to classify entire videos. This tool is used to extract meaningful video features and perform video classification efficiently.\nTool 17: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 18: This tool provides a pretrained VideoMAE model for classifying videos, fine-tuned on the Kinetics-400 dataset. It processes input video frames and predicts action categories with high accuracy, making it suitable for applications in automated video understanding, activity recognition, and video content analysis.\nTool 19: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The \\\"videomae-base-finetuned-ucf101\\\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\", \"parameters\": {\"pretrained_model_name_or_path\": {\"type\": \"string\", \"description\": \"The name or path of the pretrained VideoMAE model to load for video action recognition. This can be a model ID from the Hugging Face Hub or a local path.\", \"required\": true, \"example\": \"nateraw/videomae-base-finetuned-ucf101\"}}, \"response\": \"The tool\\u2019s response is the output of the video action recognition model when given a video input. Its structure can be described as follows:\\n\\n1. **logits**:  \\n   - A tensor containing raw, unnormalized scores for each possible action class in the model\\u2019s label set.  \\n   - The dimension of this tensor corresponds to the number of classes the model was trained on (e.g., 101 for UCF101).  \\n   - These scores represent the model\\u2019s confidence for each action before applying any normalization such as softmax.\\n\\n2. **predicted label (optional convenience mapping)**:  \\n   - A single integer or string representing the action class with the highest score.  \\n   - If an integer, it indexes into the model\\u2019s internal `id2label` mapping.  \\n   - If a string, it is the human-readable class name corresponding to the highest-scoring class (e.g., `\\\"Archery\\\"`).  \\n\\n3. **id2label mapping (metadata)**:  \\n   - A dictionary mapping integer class indices to human-readable labels.  \\n   - Used to interpret the predicted label in a readable format.  \\n\\nIn practice, the typical response workflow is:\\n\\n- The model returns the `logits` tensor.  \\n- The user selects the class with the maximum score to obtain the `predicted_label`.  \\n- The `id2label` mapping allows converting the numeric prediction into a human-readable action category.  \\n\\nNo additional fields are defined by the tool; the response focuses on the model\\u2019s prediction scores and derived class label for the input video clip.\", \"examples\": [{\"user_intent\": \"I want to classify a video of a person performing an action using a pre-trained model.\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"How can I use the VideoMAE model to recognize actions in a video clip from UCF101 dataset?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}, {\"user_intent\": \"Can I recognize actions in my video using a Hugging Face model for action classification?\", \"tool_call\": {\"name\": \"videomae-base-finetuned-ucf101\", \"arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}}}]}\n\n# User Query\nMake a summary video for our last team meeting. The audio from the video must identify who is speaking and when.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a summary video from a textual description or input video content.",
                    "tool_description": "Create a video that condenses the key points of a meeting or event, optionally using text or existing video as input, using a text-to-video generation model.",
                    "expected_response": "A video file representing the summarized content of the meeting."
                },
                {
                    "user_intent": "Analyze the audio from the video to identify who is speaking and when.",
                    "tool_description": "Perform speaker diarization on the meeting audio to segment speech by different speakers, labeling each segment with the corresponding speaker identity and time intervals.",
                    "expected_response": "A structured output listing time-stamped segments for each detected speaker, typically in RTTM format or equivalent."
                }
            ],
            "extracted_arguments": {
                "input_content": "The source content for the summary video, which can be a previous meeting video or textual notes.",
                "speaker_audio": "Audio track from the video used for speaker identification and segmentation.",
                "output_video": "The resulting video file containing the summarized content with annotated speaker information."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_56",
        "query": "###Instruction:",
        "instruction": "Given a `audio separation` task, retrieve tools that process audio inputs to isolate and separate different audio sources within a single audio file, enhancing clarity and individual sound component identification.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Asteroid",
                    "api_call": "pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')",
                    "api_arguments": "audio",
                    "python_environment_requirements": "Asteroid",
                    "example_code": "",
                    "performance": {
                        "dataset": "Libri2Mix",
                        "accuracy": {
                            "si_sdr": 14.764543634468069,
                            "si_sdr_imp": 14.764029375607246,
                            "sdr": 15.29337970745095,
                            "sdr_imp": 15.114146605113111,
                            "sir": 24.092904661115366,
                            "sir_imp": 23.913669683141528,
                            "sar": 16.06055906916849,
                            "sar_imp": -51.980784441287454,
                            "stoi": 0.9311142440593033,
                            "stoi_imp": 0.21817376142710482
                        }
                    },
                    "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.",
                    "id": "huggingface_api_786",
                    "name": "Awais/Audio_Source_Separation"
                },
                "id": "gorilla_huggingface_tool_781",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.",
            "Tool 1: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.",
            "Tool 2: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 3: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.",
            "Tool 4: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.",
            "Tool 5: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 6: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 7: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 8: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis.",
            "Tool 9: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.",
            "Tool 10: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 11: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.",
            "Tool 13: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 14: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 16: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.",
            "Tool 17: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 18: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 19: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models."
        ],
        "tool_view": {
            "description": "Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.",
            "parameters": {
                "instruction": {
                    "type": "string",
                    "description": "A text string representing the task or instruction to guide the model in generating embeddings for a given sentence.",
                    "required": true,
                    "example": "Represent the Science title:"
                },
                "sentence": {
                    "type": "string",
                    "description": "The sentence or text to be processed and embedded by the model based on the given instruction.",
                    "required": true,
                    "example": "3D ActionSLAM: wearable person tracking in multi-floor environments"
                }
            },
            "response": "The tool is designed to generate text embeddings for sentence similarity tasks using the `hkunlp/instructor-base` model from Hugging Face's Transformers framework. The tool takes an instruction and a sentence as input and returns the generated embeddings, which represent the semantic meaning of the input sentence in a high-dimensional vector space.\n\n### Expected Response Structure:\n\n- **embeddings**: \n  - **type**: Array or Matrix (depending on the implementation)\n  - **description**: This is the primary output of the model, representing the sentence in a numerical vector format. The embeddings are generated based on the provided instruction and sentence, capturing the semantic content of the sentence with respect to the task specified by the instruction. The dimensions and format of the embeddings depend on the model's architecture (usually a fixed-size vector).\n\n### Notes on Response:\n- The response will not include raw textual content but will consist of a numerical array or matrix, which the user can use for tasks such as classification, clustering, retrieval, or other NLP-related tasks that require semantic representation of text.\n  \n- The precise shape and dimensionality of the embeddings may vary depending on the specific model and implementation details, but they generally reflect the deep semantic features extracted by the model.\n\n- There are no other explicitly defined fields in the response, as the focus is on the embeddings output.\n\n",
            "examples": [
                {
                    "user_intent": "Generate an embedding for a scientific paper title so I can compare it with other research articles.",
                    "tool_call": {
                        "name": "hkunlp/instructor-base",
                        "arguments": {
                            "instruction": "Represent the Science title",
                            "sentence": "3D ActionSLAM: wearable person tracking in multi-floor environments"
                        }
                    }
                },
                {
                    "user_intent": "Create an embedding for a customer review to later classify sentiment.",
                    "tool_call": {
                        "name": "hkunlp/instructor-base",
                        "arguments": {
                            "instruction": "Represent the customer review for sentiment analysis",
                            "sentence": "The product arrived on time but the packaging was damaged and the item was scratched."
                        }
                    }
                },
                {
                    "user_intent": "Encode a financial news headline to find similar articles for a recommendation system.",
                    "tool_call": {
                        "name": "hkunlp/instructor-base",
                        "arguments": {
                            "instruction": "Represent the news headline for semantic similarity",
                            "sentence": "Stock markets surge as tech companies report record quarterly profits"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.\nTool 1: GODEL-v1.1 is a large-scale, pre-trained Transformer-based model designed for goal-directed, conversational response generation. It produces contextually and empathically appropriate replies by grounding responses in both the ongoing dialog and optional external knowledge, making it suitable for tasks that require understanding multi-turn conversations or incorporating additional information. Trained on millions of Reddit threads and instruction- or knowledge-grounded dialogs, it can be fine-tuned efficiently for new dialog tasks with relatively few examples.\nTool 2: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 3: This tool provides a pre-trained PPO reinforcement learning agent for the BreakoutNoFrameskip-v4 environment using the Stable-Baselines3 library and the RL Zoo framework. It enables users to quickly load and deploy a high-performing agent without training from scratch, supporting evaluation, further training, or integration into custom experiments with optimized hyperparameters.\nTool 4: X-CLIP is an extension of CLIP designed for video-language understanding. Trained on video-text pairs using contrastive learning, it supports tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. This model, available via Hugging Face Transformers, is pre-trained on the Kinetics 400 dataset and achieves 80.4% top-1 accuracy and 95.0% top-5 accuracy.\nTool 5: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 6: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 7: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 8: Generate high-quality, unconditional images using a score-based generative model with stochastic differential equations (SDE). This model, accessible via Hugging Face’s DiffusionPipeline, produces high-fidelity 1024×1024 images and achieves state-of-the-art performance on benchmarks like CIFAR-10, making it suitable for tasks requiring realistic image synthesis.\nTool 9: This is a diffusion model designed for unconditional image generation, specifically for creating images of cute content. It utilizes the Hugging Face Transformers framework and can be accessed through the `DDPMPipeline` with the pre-trained model `myunus1/diffmodels_galaxies_scratchbook`.\nTool 10: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 11: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 12: This tool is a diffusion model for generating images of cute butterflies without any specific conditioning, using the Hugging Face Transformers framework. It allows for the creation of butterfly images through an unconditional image generation pipeline.\nTool 13: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 14: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 15: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 16: A diffusion-based model for generating images of cute butterflies without any input conditions, enabling unconditional image creation for artistic or research purposes.\nTool 17: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 18: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 19: Generate high-quality, unconditional images using a Denoising Diffusion Probabilistic Model (DDPM) trained on 256×256 LSUN Church and CIFAR-10 datasets. This tool allows image synthesis without conditioning inputs and supports different noise scheduling methods to control the generation process, making it useful for research, creative applications, or benchmarking generative models.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Instructor (`hkunlp/instructor-base`) is an instruction-tuned text embedding model that generates task-specific embeddings for a wide range of applications, such as classification, retrieval, clustering, and text evaluation, across diverse domains like science and finance. By providing a natural-language instruction alongside the input text, it produces embeddings tailored to the task without requiring additional fine-tuning, achieving strong performance on numerous benchmark tasks.\", \"parameters\": {\"instruction\": {\"type\": \"string\", \"description\": \"A text string representing the task or instruction to guide the model in generating embeddings for a given sentence.\", \"required\": true, \"example\": \"Represent the Science title:\"}, \"sentence\": {\"type\": \"string\", \"description\": \"The sentence or text to be processed and embedded by the model based on the given instruction.\", \"required\": true, \"example\": \"3D ActionSLAM: wearable person tracking in multi-floor environments\"}}, \"response\": \"The tool is designed to generate text embeddings for sentence similarity tasks using the `hkunlp/instructor-base` model from Hugging Face's Transformers framework. The tool takes an instruction and a sentence as input and returns the generated embeddings, which represent the semantic meaning of the input sentence in a high-dimensional vector space.\\n\\n### Expected Response Structure:\\n\\n- **embeddings**: \\n  - **type**: Array or Matrix (depending on the implementation)\\n  - **description**: This is the primary output of the model, representing the sentence in a numerical vector format. The embeddings are generated based on the provided instruction and sentence, capturing the semantic content of the sentence with respect to the task specified by the instruction. The dimensions and format of the embeddings depend on the model's architecture (usually a fixed-size vector).\\n\\n### Notes on Response:\\n- The response will not include raw textual content but will consist of a numerical array or matrix, which the user can use for tasks such as classification, clustering, retrieval, or other NLP-related tasks that require semantic representation of text.\\n  \\n- The precise shape and dimensionality of the embeddings may vary depending on the specific model and implementation details, but they generally reflect the deep semantic features extracted by the model.\\n\\n- There are no other explicitly defined fields in the response, as the focus is on the embeddings output.\\n\\n\", \"examples\": [{\"user_intent\": \"Generate an embedding for a scientific paper title so I can compare it with other research articles.\", \"tool_call\": {\"name\": \"hkunlp/instructor-base\", \"arguments\": {\"instruction\": \"Represent the Science title\", \"sentence\": \"3D ActionSLAM: wearable person tracking in multi-floor environments\"}}}, {\"user_intent\": \"Create an embedding for a customer review to later classify sentiment.\", \"tool_call\": {\"name\": \"hkunlp/instructor-base\", \"arguments\": {\"instruction\": \"Represent the customer review for sentiment analysis\", \"sentence\": \"The product arrived on time but the packaging was damaged and the item was scratched.\"}}}, {\"user_intent\": \"Encode a financial news headline to find similar articles for a recommendation system.\", \"tool_call\": {\"name\": \"hkunlp/instructor-base\", \"arguments\": {\"instruction\": \"Represent the news headline for semantic similarity\", \"sentence\": \"Stock markets surge as tech companies report record quarterly profits\"}}}]}\n\n# User Query\n###Instruction:\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an embedding for a specific text based on a task instruction.",
                    "tool_description": "Produce a task-specific numerical embedding for a given sentence using a natural-language instruction to guide the semantic representation.",
                    "expected_response": "A numerical vector or matrix representing the semantic content of the input sentence according to the specified instruction."
                }
            ],
            "extracted_arguments": {
                "instruction": "A text string describing the task or purpose for generating the embedding.",
                "sentence": "The text or sentence to be embedded based on the provided instruction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_440",
        "query": "We need to analyze some pictures from nature and classify them to protect some species of animals.",
        "instruction": "Given an `image classification` task, retrieve tools that analyze and classify images by processing visual data inputs to identify different species, aiding in wildlife protection efforts.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Image Classification",
                    "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "google/mobilenet_v1_0.75_192"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])",
                    "performance": {
                        "dataset": "imagenet-1k",
                        "accuracy": "Not provided"
                    },
                    "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.",
                    "id": "huggingface_api_186",
                    "name": "google/mobilenet_v1_0.75_192"
                },
                "id": "gorilla_huggingface_tool_182",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "Tool 1: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 2: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 3: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 4: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 5: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.",
            "Tool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.",
            "Tool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.",
            "Tool 10: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.",
            "Tool 12: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 13: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.",
            "Tool 14: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 15: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 16: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 18: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.",
            "Tool 19: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.",
            "parameters": {
                "input_ids": {
                    "type": "tensor",
                    "description": "A tensor containing tokenized input text, which is passed into the model for text generation. It must be tokenized using a compatible tokenizer such as the one from the 'AutoTokenizer' class.",
                    "required": true,
                    "example": "tensor([[101, 2023, 2003, 1037, 2189, 1012]])"
                }
            },
            "response": "The tool produces generated text based on a given input prompt using a causal language model. The response is typically structured as a sequence of text strings representing the model’s continuation of the input. Specifically, the response has the following components and characteristics:\n\n1. **Generated Text Sequence**:  \n   - This is the main content of the response. It consists of one or more strings of natural language text that extend or complete the user-provided prompt.  \n   - The text is produced token by token using the model’s learned probabilities, following a causal (left-to-right) language modeling approach.\n\n2. **Token-Level Representation (Optional in Code)**:  \n   - Internally, the model may return `generated_ids`, a tensor or list of integer token IDs corresponding to the generated tokens.  \n   - These token IDs are typically decoded into human-readable text using the tokenizer, producing the final string(s).\n\n3. **Batch or Single Output**:  \n   - Depending on the input and generation call, the response can be a single generated text string or a list of strings if multiple prompts or generation sequences are provided.\n\n4. **Text Postprocessing**:  \n   - Special tokens (e.g., end-of-sequence markers) can be optionally removed (`skip_special_tokens=True`) to produce a clean natural language output.\n\n5. **Characteristics of Generated Text**:  \n   - The output reflects the language patterns learned during pretraining, predominantly in English but may include some non-English content.  \n   - The model aims to produce coherent and contextually relevant continuations, roughly matching the performance of models in the GPT-3 class.  \n\nIn summary, the tool’s response is a natural language continuation of the input prompt, optionally returned as decoded text strings derived from token IDs generated by the model. The structure is generally a single string or a list of strings corresponding to generated sequences.",
            "examples": [
                {
                    "user_intent": "Generate a creative story opening about a space adventure with alien civilizations.",
                    "tool_call": {
                        "name": "facebook/opt-13b",
                        "arguments": {
                            "input_ids": "tokenizer('In a distant galaxy, Captain Mira discovered an ancient alien civilization that...', return_tensors='pt').input_ids.cuda()"
                        }
                    }
                },
                {
                    "user_intent": "Write a professional email response declining a job offer politely.",
                    "tool_call": {
                        "name": "facebook/opt-13b",
                        "arguments": {
                            "input_ids": "tokenizer('Dear Mr. Johnson, thank you very much for offering me the position at...', return_tensors='pt').input_ids.cuda()"
                        }
                    }
                },
                {
                    "user_intent": "Continue this philosophical paragraph about the nature of consciousness.",
                    "tool_call": {
                        "name": "facebook/opt-13b",
                        "arguments": {
                            "input_ids": "tokenizer('Consciousness is not merely a byproduct of neuronal activity; it represents a deeper...', return_tensors='pt').input_ids.cuda()"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3–class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\nTool 1: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 2: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 3: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 4: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 5: This tool uses a fine-tuned version of the \"videomae-large\" model for video classification, leveraging the Hugging Face Transformers framework. It is designed to classify videos by processing them from a provided file path, and it requires specific Python environment dependencies such as `transformers`, `torch`, `datasets`, and `tokenizers`. The model has been fine-tuned on an unknown dataset, achieving an accuracy of 72.12%.\nTool 6: TimeSformer is a video classification model pre-trained on Kinetics-400 that uses space-time attention to analyze video content. It allows users to classify videos into one of 400 action categories, providing a straightforward way to generate predicted labels from video inputs using the Hugging Face Transformers framework.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: This tool uses the multilingual mDeBERTa-v3 model for zero-shot classification, enabling natural language inference (NLI) across 100 languages. It can classify text into predefined categories without needing training data specific to the task. The model, pre-trained by Microsoft on a large multilingual dataset and fine-tuned on XNLI and other NLI datasets, is capable of accurately processing various languages and domains. It is particularly useful for scenarios where you need to classify text into multiple categories without task-specific training.\nTool 9: A pre-trained TimeSformer model for video classification that leverages space-time attention to analyze video frames and predict action classes, originally trained on the Something Something v2 dataset. It can be used to automatically classify actions in videos using the Hugging Face Transformers framework.\nTool 10: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 11: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and Vision Transformer (ViT) architecture, designed to predict pixel values for masked video patches. It is used to analyze and classify video content by learning spatiotemporal representations, making it suitable for tasks like action recognition and other video understanding applications.\nTool 12: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 13: This tool provides a zero-shot image classification model based on CLIP ConvNeXt-Base, trained on subsets of the LAION-5B dataset. It allows users to classify images without additional training by comparing them to a list of candidate class labels, achieving around 71% top-1 accuracy on ImageNet-1k. It is useful for tasks such as image classification, image-to-text matching, and text-to-image retrieval in scenarios where labeled data is limited or unavailable.\nTool 14: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 15: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 16: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 18: A Table Transformer (DETR) model designed for detecting structural elements in tables, such as rows and columns, using the Hugging Face Transformers framework. It is trained on the PubTables1M dataset and can be used via the object-detection pipeline to analyze and extract table layouts from images.\nTool 19: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"facebook/opt-13b is a 13-billion-parameter decoder-only transformer designed for natural language text generation. It uses causal language modeling to produce coherent and contextually relevant text based on input prompts, achieving performance comparable to GPT-3\\u2013class models. The model is trained primarily on English text with some multilingual data, making it suitable for research and applications that require large-scale, high-quality language generation.\", \"parameters\": {\"input_ids\": {\"type\": \"tensor\", \"description\": \"A tensor containing tokenized input text, which is passed into the model for text generation. It must be tokenized using a compatible tokenizer such as the one from the 'AutoTokenizer' class.\", \"required\": true, \"example\": \"tensor([[101, 2023, 2003, 1037, 2189, 1012]])\"}}, \"response\": \"The tool produces generated text based on a given input prompt using a causal language model. The response is typically structured as a sequence of text strings representing the model\\u2019s continuation of the input. Specifically, the response has the following components and characteristics:\\n\\n1. **Generated Text Sequence**:  \\n   - This is the main content of the response. It consists of one or more strings of natural language text that extend or complete the user-provided prompt.  \\n   - The text is produced token by token using the model\\u2019s learned probabilities, following a causal (left-to-right) language modeling approach.\\n\\n2. **Token-Level Representation (Optional in Code)**:  \\n   - Internally, the model may return `generated_ids`, a tensor or list of integer token IDs corresponding to the generated tokens.  \\n   - These token IDs are typically decoded into human-readable text using the tokenizer, producing the final string(s).\\n\\n3. **Batch or Single Output**:  \\n   - Depending on the input and generation call, the response can be a single generated text string or a list of strings if multiple prompts or generation sequences are provided.\\n\\n4. **Text Postprocessing**:  \\n   - Special tokens (e.g., end-of-sequence markers) can be optionally removed (`skip_special_tokens=True`) to produce a clean natural language output.\\n\\n5. **Characteristics of Generated Text**:  \\n   - The output reflects the language patterns learned during pretraining, predominantly in English but may include some non-English content.  \\n   - The model aims to produce coherent and contextually relevant continuations, roughly matching the performance of models in the GPT-3 class.  \\n\\nIn summary, the tool\\u2019s response is a natural language continuation of the input prompt, optionally returned as decoded text strings derived from token IDs generated by the model. The structure is generally a single string or a list of strings corresponding to generated sequences.\", \"examples\": [{\"user_intent\": \"Generate a creative story opening about a space adventure with alien civilizations.\", \"tool_call\": {\"name\": \"facebook/opt-13b\", \"arguments\": {\"input_ids\": \"tokenizer('In a distant galaxy, Captain Mira discovered an ancient alien civilization that...', return_tensors='pt').input_ids.cuda()\"}}}, {\"user_intent\": \"Write a professional email response declining a job offer politely.\", \"tool_call\": {\"name\": \"facebook/opt-13b\", \"arguments\": {\"input_ids\": \"tokenizer('Dear Mr. Johnson, thank you very much for offering me the position at...', return_tensors='pt').input_ids.cuda()\"}}}, {\"user_intent\": \"Continue this philosophical paragraph about the nature of consciousness.\", \"tool_call\": {\"name\": \"facebook/opt-13b\", \"arguments\": {\"input_ids\": \"tokenizer('Consciousness is not merely a byproduct of neuronal activity; it represents a deeper...', return_tensors='pt').input_ids.cuda()\"}}}]}\n\n# User Query\nWe need to analyze some pictures from nature and classify them to protect some species of animals.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Analyze images from nature to extract meaningful visual features.",
                    "tool_description": "Use a pretrained Vision Transformer (ViT) model to process images and extract rich feature representations suitable for downstream classification tasks.",
                    "expected_response": "A set of feature vectors representing the visual content of each input image."
                },
                {
                    "user_intent": "Classify the extracted image features into categories to identify animal species.",
                    "tool_description": "Use a zero-shot or pretrained image classification model to assign each image to a specific animal species based on visual features.",
                    "expected_response": "A list of predicted labels or categories corresponding to the animal species present in each image."
                }
            ],
            "extracted_arguments": {
                "input_images": "A collection of digital images depicting natural scenes or wildlife, used as input for feature extraction and classification.",
                "classification_labels": "The set of possible animal species or categories into which each image should be classified."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_24",
        "query": "A product manager needs an explanation related to model conversion. They are confused why it is important. Can you please help them by providing an answer?",
        "instruction": "Given a `question answering` task, retrieve tools that utilize natural language processing to interpret and provide answers to specific questions based on provided context or information.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))",
                    "api_arguments": {
                        "model_name": "deepset/bert-large-uncased-whole-word-masking-squad2",
                        "tokenizer": "deepset/bert-large-uncased-whole-word-masking-squad2"
                    },
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "QA_input = {\n 'question': 'Why is model conversion important?',\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)",
                    "performance": {
                        "dataset": "squad_v2",
                        "accuracy": {
                            "Exact Match": 80.885,
                            "F1": 83.876
                        }
                    },
                    "description": "This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.",
                    "id": "huggingface_api_467",
                    "name": "bert-large-uncased-whole-word-masking-squad2"
                },
                "id": "gorilla_huggingface_tool_463",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 3: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 5: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 6: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 7: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.",
            "Tool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 10: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 11: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 12: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 13: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.",
            "Tool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.",
            "Tool 15: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 16: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 17: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 18: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.",
            "Tool 19: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time."
        ],
        "tool_view": {
            "description": "FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "parameters": {
                "image": {
                    "type": "file",
                    "description": "The image file to be classified using the zero-shot fashion model.",
                    "required": true,
                    "example": "path/to/shoe.jpg"
                },
                "class_names": {
                    "type": "string",
                    "description": "A comma-separated string of class names or labels that the model will use for zero-shot classification.",
                    "required": true,
                    "example": "sneakers, boots, sandals, heels"
                }
            },
            "response": "The tool returns a structured prediction representing the likelihood that the input image belongs to each of the specified classes. The response is typically a list or array of numerical values, where each value corresponds to the probability or confidence score for a single class in the `class_names` list. These probabilities are normalized, usually summing to 1 across all classes, reflecting the model’s assessment of how well the image matches each textual class description.  \n\nKey components of the response:\n\n- **Class probabilities**: A numeric vector or list, with one entry per class provided in `class_names`. Each entry indicates the model’s predicted likelihood that the image belongs to that class.  \n- **Alignment with `class_names`**: The order of probabilities directly corresponds to the order of the input class names, so the first probability aligns with the first class name, the second with the second class name, and so on.  \n- **Optional logits**: In some implementations, the raw unnormalized scores (logits) may also be returned before applying the softmax transformation. These represent the relative affinity between the image and each class prior to normalization.  \n\nOverall, the response conveys a zero-shot classification result, allowing downstream code to identify the most probable class for the image or rank classes by likelihood.",
            "examples": [
                {
                    "user_intent": "I want to classify an image of a red dress and see if it matches categories like 'dress', 'shirt', or 'jacket'.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "red_dress.jpg",
                            "class_names": "dress,shirt,jacket"
                        }
                    }
                },
                {
                    "user_intent": "Check whether this image of sneakers can be labeled as 'sneakers', 'boots', or 'sandals'.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "white_sneakers.png",
                            "class_names": "sneakers,boots,sandals"
                        }
                    }
                },
                {
                    "user_intent": "Determine if this photo belongs to 'handbag', 'backpack', or 'tote bag' category for my e-commerce inventory.",
                    "tool_call": {
                        "name": "patrickjohncyh/fashion-clip",
                        "arguments": {
                            "image": "leather_bag.jpeg",
                            "class_names": "handbag,backpack,tote bag"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 1: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 2: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 3: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 4: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 5: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 6: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 7: This tool provides a Text-to-Speech (TTS) model built with the ESPnet framework, specifically trained using the \"amadeus\" recipe by mio. It allows for the conversion of input text into speech and can be used in an inference API for on-demand usage.\nTool 8: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 9: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 10: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 11: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 12: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 13: A text-to-text tool that automatically corrects grammatical errors in input text, producing a revised version with improved grammar. It uses the Hugging Face Transformers framework via Happy Transformer and is designed to help refine written language for clarity and correctness.\nTool 14: This tool is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector embeddings, enabling applications such as semantic search, clustering, and measuring sentence similarity. It is designed for natural language processing tasks where understanding the semantic meaning of text is important.\nTool 15: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 16: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 17: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 18: A Spanish single-speaker male text-to-speech model from Fairseq that uses a transformer architecture and HiFi-GAN vocoder, trained on the CSS10 dataset, allowing conversion of written Spanish text into natural-sounding speech.\nTool 19: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\", \"parameters\": {\"image\": {\"type\": \"file\", \"description\": \"The image file to be classified using the zero-shot fashion model.\", \"required\": true, \"example\": \"path/to/shoe.jpg\"}, \"class_names\": {\"type\": \"string\", \"description\": \"A comma-separated string of class names or labels that the model will use for zero-shot classification.\", \"required\": true, \"example\": \"sneakers, boots, sandals, heels\"}}, \"response\": \"The tool returns a structured prediction representing the likelihood that the input image belongs to each of the specified classes. The response is typically a list or array of numerical values, where each value corresponds to the probability or confidence score for a single class in the `class_names` list. These probabilities are normalized, usually summing to 1 across all classes, reflecting the model\\u2019s assessment of how well the image matches each textual class description.  \\n\\nKey components of the response:\\n\\n- **Class probabilities**: A numeric vector or list, with one entry per class provided in `class_names`. Each entry indicates the model\\u2019s predicted likelihood that the image belongs to that class.  \\n- **Alignment with `class_names`**: The order of probabilities directly corresponds to the order of the input class names, so the first probability aligns with the first class name, the second with the second class name, and so on.  \\n- **Optional logits**: In some implementations, the raw unnormalized scores (logits) may also be returned before applying the softmax transformation. These represent the relative affinity between the image and each class prior to normalization.  \\n\\nOverall, the response conveys a zero-shot classification result, allowing downstream code to identify the most probable class for the image or rank classes by likelihood.\", \"examples\": [{\"user_intent\": \"I want to classify an image of a red dress and see if it matches categories like 'dress', 'shirt', or 'jacket'.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"red_dress.jpg\", \"class_names\": \"dress,shirt,jacket\"}}}, {\"user_intent\": \"Check whether this image of sneakers can be labeled as 'sneakers', 'boots', or 'sandals'.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"white_sneakers.png\", \"class_names\": \"sneakers,boots,sandals\"}}}, {\"user_intent\": \"Determine if this photo belongs to 'handbag', 'backpack', or 'tote bag' category for my e-commerce inventory.\", \"tool_call\": {\"name\": \"patrickjohncyh/fashion-clip\", \"arguments\": {\"image\": \"leather_bag.jpeg\", \"class_names\": \"handbag,backpack,tote bag\"}}}]}\n\n# User Query\nA product manager needs an explanation related to model conversion. They are confused why it is important. Can you please help them by providing an answer?\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Explain why model conversion is important for product managers.",
                    "tool_description": "Provide a detailed, conceptual explanation about the significance of converting machine learning models between formats or frameworks, highlighting benefits such as deployment flexibility, performance optimization, and compatibility across platforms.",
                    "expected_response": "A structured text explanation outlining key reasons for model conversion, including practical advantages and potential impacts on product workflows."
                }
            ],
            "extracted_arguments": {
                "audience": "The target recipient of the explanation, describing their role and technical background.",
                "topic": "The specific subject to explain, in this case model conversion in machine learning."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_386",
        "query": "We need an AI-generated insect image for our biology article about African habitats.",
        "instruction": "Given an `image generation` task, retrieve tools that are capable of generating AI images based on specified themes, such as creating visual representations of insects suited for a biology article, by leveraging pre-trained models and image synthesis techniques.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')",
                    "api_arguments": {
                        "pretrained_model": "schdoel/sd-class-AFHQ-32"
                    },
                    "python_environment_requirements": {
                        "package": "diffusers",
                        "import": "from diffusers import DDPMPipeline"
                    },
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "AFHQ",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model for unconditional image generation of cute 🦋.",
                    "id": "huggingface_api_315",
                    "name": "sd-class-pandas-32"
                },
                "id": "gorilla_huggingface_tool_311",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 1: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 2: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.",
            "Tool 3: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 4: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.",
            "Tool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 6: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 7: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.",
            "Tool 8: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 9: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 11: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 12: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.",
            "Tool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 14: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.",
            "Tool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 16: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.",
            "Tool 17: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.",
            "Tool 18: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 19: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation."
        ],
        "tool_view": {
            "description": "This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The text prompt describing the scene or subject that guides the image generation process.",
                    "required": true,
                    "example": "chef in the kitchen"
                },
                "image": {
                    "type": "string",
                    "description": "The input image used as a reference or condition for the pose-guided image generation.",
                    "required": true,
                    "example": "image.png"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.",
                    "required": true,
                    "example": 20
                }
            },
            "response": "The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\n\n1. **Generated Image**: \n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \"chef in the kitchen\") and the pose derived from the provided input image.\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\n\n2. **Inference Details**: \n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\n\n3. **Performance Information (Optional)**: \n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \"200k pose-image, caption pairs\") or other performance-related details if available or relevant to the particular response.\n\n4. **Image Format and Saving**:\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\n\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.",
            "examples": [
                {
                    "user_intent": "Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "yoga instructor in a studio",
                            "image": "yoga_pose_reference.png",
                            "num_inference_steps": 25
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "basketball player jumping in a stadium",
                            "image": "player_jump.png",
                            "num_inference_steps": 30
                        }
                    }
                },
                {
                    "user_intent": "Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "fashion model walking on a runway",
                            "image": "model_pose_reference.jpg",
                            "num_inference_steps": 20
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 1: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 2: Parrot is a text paraphrasing tool designed to generate alternative phrasings of input sentences, helping to augment and diversify training data for natural language understanding models. It allows users to control the adequacy, fluency, and diversity of the generated paraphrases, making it particularly useful for improving the robustness of conversational AI and other NLP applications that rely on varied textual inputs.\nTool 3: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 4: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.\nTool 5: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 6: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 7: PromptCap is a multimodal image-to-text model that generates captions or answers based on natural language instructions, including questions about the content of an image. It is designed for controlled image captioning and knowledge-based visual question answering, achieving state-of-the-art performance on COCO captioning and VQA benchmarks such as OK-VQA and A-OKVQA.\nTool 8: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 9: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 10: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 11: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 12: BLIP-2 with Flan T5-xxl is a multimodal model that combines a frozen image encoder with a large language model to perform tasks such as image captioning, visual question answering, and conversational interactions about images. It processes an image along with a text prompt or question and generates detailed, context-aware textual responses, making it useful for applications that require understanding and describing visual content.\nTool 13: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 14: BLIP-2 OPT-2.7b is a multimodal model that combines a large language model with frozen image encoders to generate text based on visual inputs. It can perform tasks such as image captioning, visual question answering, and conversational reasoning about images by taking an image and a text prompt as input and predicting the next text token. This makes it useful for applications that require understanding and describing visual content in natural language.\nTool 15: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 16: The \"microsoft/git-base-textvqa\" model is a Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically on the TextVQA dataset. It generates text-based answers to questions about images or videos by conditioning on both image and text tokens. The model is designed to predict the next text token based on the given image and preceding text, and is useful for applications like image captioning, VQA, and image classification. It can be used via the Hugging Face Transformers framework.\nTool 17: The \"microsoft/git-large-vqav2\" is a large Transformer-based model fine-tuned for visual question answering (VQA) tasks, specifically designed to answer questions about images or videos. It leverages a generative image-to-text approach, conditioned on both CLIP image tokens and text tokens, and is capable of tasks such as image captioning and classification. This model is available through the Hugging Face Transformers library and can be easily integrated into Python applications using the provided API and example code.\nTool 18: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 19: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The text prompt describing the scene or subject that guides the image generation process.\", \"required\": true, \"example\": \"chef in the kitchen\"}, \"image\": {\"type\": \"string\", \"description\": \"The input image used as a reference or condition for the pose-guided image generation.\", \"required\": true, \"example\": \"image.png\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.\", \"required\": true, \"example\": 20}}, \"response\": \"The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\\n\\n1. **Generated Image**: \\n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \\\"chef in the kitchen\\\") and the pose derived from the provided input image.\\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\\n\\n2. **Inference Details**: \\n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\\n\\n3. **Performance Information (Optional)**: \\n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \\\"200k pose-image, caption pairs\\\") or other performance-related details if available or relevant to the particular response.\\n\\n4. **Image Format and Saving**:\\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\\n\\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.\", \"examples\": [{\"user_intent\": \"Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"yoga instructor in a studio\", \"image\": \"yoga_pose_reference.png\", \"num_inference_steps\": 25}}}, {\"user_intent\": \"Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"basketball player jumping in a stadium\", \"image\": \"player_jump.png\", \"num_inference_steps\": 30}}}, {\"user_intent\": \"Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"fashion model walking on a runway\", \"image\": \"model_pose_reference.jpg\", \"num_inference_steps\": 20}}}]}\n\n# User Query\nWe need an AI-generated insect image for our biology article about African habitats.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate an AI image of an insect suitable for a biology article about African habitats.",
                    "tool_description": "Use a Stable Diffusion-based image generation model to create an image of an insect, guided by a text prompt describing its environment and appearance.",
                    "expected_response": "A high-quality image of an insect in an African habitat that can be used in an article, optionally including metadata about the generation process."
                }
            ],
            "extracted_arguments": {
                "text": "A descriptive text prompt specifying the insect species and its African habitat context.",
                "num_inference_steps": "The number of steps the model should perform to refine the image quality."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_46",
        "query": "A travel agency needs a system to predict whether a client's vacation will be successful based on their chosen destination, accommodation, and travel style. We want to provide suggestions for clients who may need additional support.",
        "instruction": "Given a `vacation success prediction` task, retrieve tools that perform binary classification using tabular data related to vacation attributes such as destination, accommodation, and travel style to determine the likelihood of a successful trip and provide suggestions for additional support.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Scikit-learn",
                    "functionality": "Binary Classification",
                    "api_call": "load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))",
                    "api_arguments": [
                        "new_data"
                    ],
                    "python_environment_requirements": [
                        "huggingface_hub",
                        "joblib",
                        "pandas",
                        "numpy",
                        "tensorflow"
                    ],
                    "example_code": "from huggingface_hub import hf_hub_url, cached_download\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nREPO_ID = 'danupurnomo/dummy-titanic'\nPIPELINE_FILENAME = 'final_pipeline.pkl'\nTF_FILENAME = 'titanic_model.h5'\nmodel_pipeline = joblib.load(cached_download(\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\n))\nmodel_seq = load_model(cached_download(\n hf_hub_url(REPO_ID, TF_FILENAME)\n))",
                    "performance": {
                        "dataset": "Titanic",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.",
                    "id": "huggingface_api_851",
                    "name": "danupurnomo/dummy-titanic"
                },
                "id": "gorilla_huggingface_tool_845",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 1: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.",
            "Tool 2: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.",
            "Tool 3: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 5: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.",
            "Tool 6: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 9: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.",
            "Tool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.",
            "Tool 11: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 12: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.",
            "Tool 13: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.",
            "Tool 14: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.",
            "Tool 15: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 16: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 17: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 19: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories."
        ],
        "tool_view": {
            "description": "EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that specifies the content and style of the image to generate. This is the main input guiding the model's image generation.",
                    "required": true,
                    "example": "A serene anime landscape with cherry blossoms and a flowing river"
                }
            },
            "response": "The tool generates an image based on a textual prompt provided by the user. The response is a single image object, typically represented in memory as an image array or image file format (such as PNG or JPEG). Semantically, the response contains:\n\n- **Generated Image**: The primary component of the response, representing the visual content synthesized from the input text prompt. This image reflects the style and characteristics of the model’s training data, specifically high-quality, detailed anime and landscape imagery.  \n- **Optional Metadata** (in some frameworks): May include information about the generation process such as image dimensions, seed values used for the diffusion process, or processing parameters.  \n\nThe response does not inherently include multiple images unless the tool is explicitly configured to generate batches. The main purpose of the response is to provide a visual depiction that corresponds closely to the user’s input prompt in a coherent anime or landscape style.",
            "examples": [
                {
                    "user_intent": "Generate an anime-style illustration of a futuristic city at sunset.",
                    "tool_call": {
                        "name": "EimisAnimeDiffusion_1.0v",
                        "arguments": {
                            "prompt": "futuristic city skyline at sunset, anime style, highly detailed, vibrant colors"
                        }
                    }
                },
                {
                    "user_intent": "Create a cute anime cat character sitting on a cloud.",
                    "tool_call": {
                        "name": "EimisAnimeDiffusion_1.0v",
                        "arguments": {
                            "prompt": "adorable anime cat character sitting on a fluffy cloud, soft lighting, whimsical style"
                        }
                    }
                },
                {
                    "user_intent": "Produce a dramatic landscape of a snowy mountain with cherry blossoms in anime style.",
                    "tool_call": {
                        "name": "EimisAnimeDiffusion_1.0v",
                        "arguments": {
                            "prompt": "snowy mountain landscape with blooming cherry blossoms, anime style, cinematic lighting, highly detailed"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 1: A binary tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain, designed to predict whether a passenger survived based on features like age, gender, and passenger class.\nTool 2: A CLIP ConvNeXt-XXLarge model trained on the LAION-2B dataset that enables zero-shot image classification, allowing users to classify images into arbitrary categories without additional training. It achieves 79.1–79.4% top-1 accuracy on ImageNet-1k and can also support tasks like image-text retrieval and related multimodal applications.\nTool 3: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 4: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 5: A Hugging Face Transformers model for monocular depth estimation that uses a Vision Transformer (ViT) backbone with additional layers to predict dense depth maps from single images, enabling applications that require understanding scene geometry from a single camera input.\nTool 6: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 9: This tool is a HiFIGAN vocoder pre-trained on a custom German dataset, designed to convert spectrograms into waveforms. It is typically used in combination with a Text-to-Speech (TTS) model, where the TTS system generates a spectrogram from input text, and the HiFIGAN vocoder transforms it into natural-sounding audio.\nTool 10: A BERT-large model fine-tuned on the SQuAD 2.0 dataset for extractive question answering in English, designed to provide precise answers to questions based on a given context.\nTool 11: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 12: A distilled RoBERTa-based model fine-tuned on the SQuAD 2.0 dataset for question answering, enabling users to provide a context and a question to obtain precise answers with high accuracy and F1 performance.\nTool 13: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.\nTool 14: Pygmalion-350M is a conversational AI model fine-tuned from Facebook's OPT-350M for dialogue generation, designed as a proof-of-concept to explore interactive text responses and serve as a foundation for larger models. It is intended for adult users and may produce unsafe content due to the nature of its training data.\nTool 15: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 16: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 17: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 18: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 19: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that specifies the content and style of the image to generate. This is the main input guiding the model's image generation.\", \"required\": true, \"example\": \"A serene anime landscape with cherry blossoms and a flowing river\"}}, \"response\": \"The tool generates an image based on a textual prompt provided by the user. The response is a single image object, typically represented in memory as an image array or image file format (such as PNG or JPEG). Semantically, the response contains:\\n\\n- **Generated Image**: The primary component of the response, representing the visual content synthesized from the input text prompt. This image reflects the style and characteristics of the model\\u2019s training data, specifically high-quality, detailed anime and landscape imagery.  \\n- **Optional Metadata** (in some frameworks): May include information about the generation process such as image dimensions, seed values used for the diffusion process, or processing parameters.  \\n\\nThe response does not inherently include multiple images unless the tool is explicitly configured to generate batches. The main purpose of the response is to provide a visual depiction that corresponds closely to the user\\u2019s input prompt in a coherent anime or landscape style.\", \"examples\": [{\"user_intent\": \"Generate an anime-style illustration of a futuristic city at sunset.\", \"tool_call\": {\"name\": \"EimisAnimeDiffusion_1.0v\", \"arguments\": {\"prompt\": \"futuristic city skyline at sunset, anime style, highly detailed, vibrant colors\"}}}, {\"user_intent\": \"Create a cute anime cat character sitting on a cloud.\", \"tool_call\": {\"name\": \"EimisAnimeDiffusion_1.0v\", \"arguments\": {\"prompt\": \"adorable anime cat character sitting on a fluffy cloud, soft lighting, whimsical style\"}}}, {\"user_intent\": \"Produce a dramatic landscape of a snowy mountain with cherry blossoms in anime style.\", \"tool_call\": {\"name\": \"EimisAnimeDiffusion_1.0v\", \"arguments\": {\"prompt\": \"snowy mountain landscape with blooming cherry blossoms, anime style, cinematic lighting, highly detailed\"}}}]}\n\n# User Query\nA travel agency needs a system to predict whether a client's vacation will be successful based on their chosen destination, accommodation, and travel style. We want to provide suggestions for clients who may need additional support.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to predict the success of a vacation based on destination, accommodation, and travel style.",
                    "tool_description": "Use a predictive model to estimate the likelihood of a vacation's success based on input features such as destination, accommodation, and travel style.",
                    "expected_response": "A prediction object indicating the likelihood of success, potentially including a confidence score or classification (e.g., 'successful' or 'needs improvement')."
                },
                {
                    "user_intent": "I want to suggest additional support for clients who may face issues during their vacation.",
                    "tool_description": "Provide suggestions or recommendations for additional support based on the predicted outcome, targeting clients identified as needing extra assistance.",
                    "expected_response": "A set of suggestions or resources for improving the vacation experience, such as customized travel support or assistance services."
                }
            ],
            "extracted_arguments": {
                "destination": "The chosen vacation destination, typically represented by the name of a city, region, or country.",
                "accommodation": "The type and characteristics of the accommodation, such as hotel, resort, or rental property.",
                "travel_style": "The client's preferred travel style, which could include adventure, relaxation, cultural, or luxury preferences."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_252",
        "query": "We are building an IoT device to monitor apartment corridors for security. Please detect objects in the image and notify if anything abnormal is detected.",
        "instruction": "Given an `object detection` task, retrieve tools that utilize computer vision to analyze images for object identification and alert upon detecting anomalies by processing image inputs to pinpoint objects and their attributes.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Object Detection",
                    "framework": "Transformers",
                    "functionality": "Object Detection",
                    "api_call": "yolov5.load('fcakyon/yolov5s-v7.0')",
                    "api_arguments": {
                        "conf": 0.25,
                        "iou": 0.45,
                        "agnostic": false,
                        "multi_label": false,
                        "max_det": 1000,
                        "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                        "size": 640,
                        "augment": true
                    },
                    "python_environment_requirements": "pip install -U yolov5",
                    "example_code": "import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')",
                    "performance": {
                        "dataset": "detection-datasets/coco",
                        "accuracy": null
                    },
                    "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.",
                    "id": "huggingface_api_226",
                    "name": "fcakyon/yolov5s-v7.0"
                },
                "id": "gorilla_huggingface_tool_222",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "Tool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 2: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.",
            "Tool 3: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 6: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.",
            "Tool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 8: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 10: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 11: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 12: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 13: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 14: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 15: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.",
            "Tool 16: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.",
            "Tool 18: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 19: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging."
        ],
        "tool_view": {
            "description": "This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\n\n1. **Boxes**  \n   - A collection of bounding boxes corresponding to detected potholes in the image.  \n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - May include additional metadata such as confidence scores for each detection.\n\n2. **Masks**  \n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\n\n3. **Class Labels (optional)**  \n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\n\n4. **Confidence Scores (optional)**  \n   - A numeric confidence value for each detection, reflecting the model’s certainty that the detected object is indeed a pothole.\n\n5. **Rendered Output (optional)**  \n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\n\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.",
            "examples": [
                {
                    "user_intent": "I want to detect potholes in an image using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/pothole_image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Can I use a local image to detect potholes with the YOLOv8 model?",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "/path/to/local/image.jpg"
                        }
                    }
                },
                {
                    "user_intent": "I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.",
                    "tool_call": {
                        "name": "keremberke/yolov8s-pothole-segmentation",
                        "arguments": {
                            "image": "https://example.com/street_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\nTool 1: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 2: A YOLOv8 model designed to detect hard hats in images, distinguishing between \"Hardhat\" and \"NO-Hardhat\" classes. It is used to monitor and enforce safety compliance in construction sites or industrial environments by automatically identifying whether individuals are wearing the required protective equipment.\nTool 3: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 5: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 6: This tool provides a ControlNet model pretrained on HED boundaries, enabling users to guide Stable Diffusion image generation using edge-based conditions. It allows transforming an input image into a new image while adhering to the structure defined by detected edges, making it useful for tasks like style transfer, image editing, or creating detailed renditions based on a reference outline.\nTool 7: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 8: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 10: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 11: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 12: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 13: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 14: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 15: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\nTool 16: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 17: GTA5_PROCESS_LEARNING_AI is an AI model built with Hugging Face Transformers that trains a neural network on the MNIST dataset with a configurable data cap, using GPU acceleration if available. It is designed to monitor and interact with the GTA5 game process, ensuring the target application is running, and allows saving the trained model in the ONNX format for integration or further use.\nTool 18: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 19: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a YOLOv8-based image segmentation model designed to detect potholes in images. It identifies potholes and outputs both bounding boxes and segmentation masks, enabling precise localization and shape analysis of detected potholes. It is useful for automated road inspection and infrastructure monitoring tasks where accurate pothole detection is needed.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. This can be a URL pointing to an image or a local file path on your system.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns a structured result representing the output of the YOLOv8 pothole segmentation model for a given input image. The response typically contains the following components:\\n\\n1. **Boxes**  \\n   - A collection of bounding boxes corresponding to detected potholes in the image.  \\n   - Each bounding box defines the spatial location of a detected pothole, usually via coordinates (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - May include additional metadata such as confidence scores for each detection.\\n\\n2. **Masks**  \\n   - A set of binary or probabilistic masks, each corresponding to a detected pothole.  \\n   - Each mask identifies the exact pixels in the image that belong to the pothole, enabling precise segmentation beyond the bounding box.\\n\\n3. **Class Labels (optional)**  \\n   - Labels or identifiers indicating the object type. In this model, all detections are potholes, so the class label is likely consistent across detections.\\n\\n4. **Confidence Scores (optional)**  \\n   - A numeric confidence value for each detection, reflecting the model\\u2019s certainty that the detected object is indeed a pothole.\\n\\n5. **Rendered Output (optional)**  \\n   - If post-processing is performed (e.g., via `render_result`), the response can include a visual rendering of the detections overlayed on the input image, combining bounding boxes and masks for visualization.\\n\\nOverall, the response encapsulates both the **geometric localization** (boxes) and **pixel-level segmentation** (masks) of potholes in the input image, potentially accompanied by confidence scores and visualization data.\", \"examples\": [{\"user_intent\": \"I want to detect potholes in an image using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/pothole_image.jpg\"}}}, {\"user_intent\": \"Can I use a local image to detect potholes with the YOLOv8 model?\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"/path/to/local/image.jpg\"}}}, {\"user_intent\": \"I need to analyze a street image and find any potholes using the YOLOv8 pothole segmentation model.\", \"tool_call\": {\"name\": \"keremberke/yolov8s-pothole-segmentation\", \"arguments\": {\"image\": \"https://example.com/street_image.jpg\"}}}]}\n\n# User Query\nWe are building an IoT device to monitor apartment corridors for security. Please detect objects in the image and notify if anything abnormal is detected.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect objects in images from the apartment corridor for security monitoring.",
                    "tool_description": "Use a YOLOv8 or zero-shot object detection model to identify and localize multiple objects in input images, enabling automated monitoring of visual scenes.",
                    "expected_response": "A structured result containing detected objects with bounding boxes, class labels, and confidence scores."
                },
                {
                    "user_intent": "Alert or notify if any unusual or abnormal object is detected in the corridor.",
                    "tool_description": "Analyze detected objects and compare them against normal expected objects to identify anomalies, triggering notifications when irregular items or activities are present.",
                    "expected_response": "A notification or alert indicating the presence of abnormal or unexpected objects in the scene."
                }
            ],
            "extracted_arguments": {
                "image": "The input image from the IoT corridor camera, which can be a URL or local file path.",
                "abnormal_object_criteria": "A definition or set of rules specifying what constitutes an abnormal object or event for security purposes."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_260",
        "query": "I am interested in building a wall of generated pictures for my gallery. My specifications include a size of 256x256 pixels.",
        "instruction": "Given an `image generation` task, retrieve tools that perform unconditional image synthesis, processing parameters such as image size to produce high-quality generated pictures according to the query's specifications.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('google/ddpm-church-256')",
                    "api_arguments": [
                        "model_id"
                    ],
                    "python_environment_requirements": [
                        "diffusers"
                    ],
                    "example_code": "!pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = google/ddpm-church-256\nddpm = DDPMPipeline.from_pretrained(model_id)\nimage = ddpm().images[0]\nimage.save(ddpm_generated_image.png)",
                    "performance": {
                        "dataset": "CIFAR10",
                        "accuracy": {
                            "Inception_score": 9.46,
                            "FID_score": 3.17
                        }
                    },
                    "description": "Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.",
                    "id": "huggingface_api_292",
                    "name": "google/ddpm-church-256"
                },
                "id": "gorilla_huggingface_tool_288",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 1: This tool provides a fine-tuned video classification model, `sayakpaul/videomae-base-finetuned-ucf101-subset`, based on the Hugging Face Transformers framework. It allows users to perform video classification using pre-trained models, with customizable training parameters such as learning rate, batch size, optimizer, and training steps. The model is optimized for UCF101 dataset subsets, achieving an accuracy of 86.45% on an unknown evaluation set. It is compatible with Python environments that include libraries like Transformers, PyTorch, and Datasets.",
            "Tool 2: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 5: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 6: This tool uses Denoising Diffusion Probabilistic Models (DDPM) for high-quality, unconditional image synthesis. Trained on datasets like CIFAR10 and 256x256 LSUN, it achieves state-of-the-art image generation performance with an Inception score of 9.46 and an FID score of 3.17. It is implemented using Hugging Face Transformers' Diffusers library and can be accessed through the `DDPMPipeline.from_pretrained` method.",
            "Tool 7: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.",
            "Tool 8: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 9: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.",
            "Tool 10: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 11: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.",
            "Tool 12: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 13: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.",
            "Tool 14: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.",
            "Tool 15: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.",
            "Tool 16: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.",
            "Tool 17: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.",
            "Tool 18: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.",
            "Tool 19: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis."
        ],
        "tool_view": {
            "description": "BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "parameters": {
                "model_name": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained BlenderBot model to load from the Hugging Face Transformers library.",
                    "required": true,
                    "example": "facebook/blenderbot-3B"
                }
            },
            "response": "The tool facilitates conversational interactions using the **BlenderBot-3B** model, which is designed for open-domain chatbot applications. The response produced by this tool is expected to be a text-based output that represents the model's reply to an input message. The structure of the response would likely include the following components:\n\n1. **Response Text**: This will be a generated message that serves as the chatbot's reply. It can include information, questions, statements, or empathetic responses depending on the context of the conversation.\n   \n2. **Context Awareness**: The response is generated with an understanding of previous conversation history, allowing the model to maintain context across multiple turns of dialogue.\n\n3. **Personality and Empathy**: The response will reflect the model's trained ability to exhibit personality traits, empathy, and engage with the user in a way that feels natural and human-like.\n\n4. **Knowledge Integration**: The model may draw on factual knowledge to answer questions, offer suggestions, or discuss various topics, depending on the nature of the conversation.\n\nIn summary, the response from this tool would be a text message generated by the BlenderBot-3B model, informed by the conversation history, designed to be engaging, informative, and empathetic.",
            "examples": [
                {
                    "user_intent": "I want to start a conversation with BlenderBot-3B.",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                },
                {
                    "user_intent": "Can I use BlenderBot-3B for chatting about general topics?",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                },
                {
                    "user_intent": "Test BlenderBot-3B for a casual chat about movies.",
                    "tool_call": {
                        "name": "facebook/blenderbot-3B",
                        "arguments": {
                            "model_name": "facebook/blenderbot-3B"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 1: This tool provides a fine-tuned video classification model, `sayakpaul/videomae-base-finetuned-ucf101-subset`, based on the Hugging Face Transformers framework. It allows users to perform video classification using pre-trained models, with customizable training parameters such as learning rate, batch size, optimizer, and training steps. The model is optimized for UCF101 dataset subsets, achieving an accuracy of 86.45% on an unknown evaluation set. It is compatible with Python environments that include libraries like Transformers, PyTorch, and Datasets.\nTool 2: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 3: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 4: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 5: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 6: This tool uses Denoising Diffusion Probabilistic Models (DDPM) for high-quality, unconditional image synthesis. Trained on datasets like CIFAR10 and 256x256 LSUN, it achieves state-of-the-art image generation performance with an Inception score of 9.46 and an FID score of 3.17. It is implemented using Hugging Face Transformers' Diffusers library and can be accessed through the `DDPMPipeline.from_pretrained` method.\nTool 7: Generate human-like conversational responses using a GPT-4–based model, designed for dialogue interactions and suitable for building chatbots or other interactive conversational applications.\nTool 8: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 9: `deepset/minilm-uncased-squad2` is a lightweight language model fine-tuned for extractive question answering using the SQuAD 2.0 dataset. It can identify precise answers to questions given a text context and is compatible with the Hugging Face Transformers library, making it suitable for building efficient QA systems in Python.\nTool 10: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 11: This tool provides a Keras-based convolutional autoencoder model for detecting anomalies in time series data. It is designed to identify unusual patterns or deviations in ordered, timestamped metrics, using datasets like the Numenta Anomaly Benchmark (NAB) that include labeled anomalous periods. The model is trained with the Adam optimizer and supports float32 precision, making it suitable for monitoring and analyzing time series signals for abnormal behavior.\nTool 12: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 13: DialoGPT-medium is a large-scale pretrained conversational AI model designed for generating human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can produce coherent and contextually relevant replies, making it suitable for building chatbots or other dialogue systems that require natural and engaging interactions.\nTool 14: This tool utilizes the ControlNet model conditioned on M-LSD straight line detection to control diffusion models, particularly in combination with Stable Diffusion. It enables precise image generation by incorporating additional conditions based on straight line detection, making it useful for tasks that require geometric consistency in generated images.\nTool 15: This tool provides a ControlNet model conditioned on Canny edges, allowing users to guide image generation in Stable Diffusion by providing edge-based input. It is used to produce more controlled and structure-aware outputs in image-to-image tasks, enabling precise manipulation of generated images based on edge information.\nTool 16: ControlNet v1.1 is a neural network model that enhances Stable Diffusion by guiding image generation with additional conditions derived from MLSD (line segment detection) images. It allows precise control over the structure and layout of generated images, making it useful for creating images that adhere to specific geometric or structural constraints.\nTool 17: Pygmalion 2.7B is a conversational text-generation model fine-tuned on 56MB of dialogue data from real and machine-generated sources. It is designed to generate character-driven dialogue by taking a character’s persona, dialogue history, and user input as input, making it useful for creating interactive, role-based conversational responses.\nTool 18: This tool provides a logistic regression model trained for multi-class classification on the Iris dataset. It can be used for predicting outcomes based on input data and is optimized for deployment with Joblib. The model, which has an accuracy of 0.9, can be loaded with Joblib and applied to tabular data, making it suitable for tasks involving classification in structured datasets. It is compatible with Python environments that include Joblib and Pandas, and can be used with the Hugging Face Inference API for streamlined integration.\nTool 19: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\", \"parameters\": {\"model_name\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained BlenderBot model to load from the Hugging Face Transformers library.\", \"required\": true, \"example\": \"facebook/blenderbot-3B\"}}, \"response\": \"The tool facilitates conversational interactions using the **BlenderBot-3B** model, which is designed for open-domain chatbot applications. The response produced by this tool is expected to be a text-based output that represents the model's reply to an input message. The structure of the response would likely include the following components:\\n\\n1. **Response Text**: This will be a generated message that serves as the chatbot's reply. It can include information, questions, statements, or empathetic responses depending on the context of the conversation.\\n   \\n2. **Context Awareness**: The response is generated with an understanding of previous conversation history, allowing the model to maintain context across multiple turns of dialogue.\\n\\n3. **Personality and Empathy**: The response will reflect the model's trained ability to exhibit personality traits, empathy, and engage with the user in a way that feels natural and human-like.\\n\\n4. **Knowledge Integration**: The model may draw on factual knowledge to answer questions, offer suggestions, or discuss various topics, depending on the nature of the conversation.\\n\\nIn summary, the response from this tool would be a text message generated by the BlenderBot-3B model, informed by the conversation history, designed to be engaging, informative, and empathetic.\", \"examples\": [{\"user_intent\": \"I want to start a conversation with BlenderBot-3B.\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}, {\"user_intent\": \"Can I use BlenderBot-3B for chatting about general topics?\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}, {\"user_intent\": \"Test BlenderBot-3B for a casual chat about movies.\", \"tool_call\": {\"name\": \"facebook/blenderbot-3B\", \"arguments\": {\"model_name\": \"facebook/blenderbot-3B\"}}}]}\n\n# User Query\nI am interested in building a wall of generated pictures for my gallery. My specifications include a size of 256x256 pixels.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate high-quality images with specific dimensions for a gallery wall.",
                    "tool_description": "Use a Denoising Diffusion Probabilistic Model (DDPM) to create unconditional images at a specified resolution, allowing for artistic image generation based on learned patterns from training datasets.",
                    "expected_response": "A collection of generated images matching the requested dimensions, each represented as image data suitable for display or further processing."
                }
            ],
            "extracted_arguments": {
                "image_size": "The desired resolution of the generated images, specified as width and height in pixels."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_70",
        "query": "I am developing a home security software which can detect intruders entering the house. In case any door or object is tampered, the application will ask, \"Who entered the room?\", to assist quick analysis of the CCTV recordings.",
        "instruction": "Given a `home security visual query` task, retrieve tools that enable visual question answering by processing visual inputs and questions to provide relevant answers or insights.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Visual Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Visual Question Answering",
                    "api_call": "BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')",
                    "api_arguments": {
                        "raw_image": "RGB image",
                        "question": "string"
                    },
                    "python_environment_requirements": {
                        "transformers": "BlipProcessor, BlipForQuestionAnswering"
                    },
                    "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = how many dogs are in the picture?\ninputs = processor(raw_image, question, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))",
                    "performance": {
                        "dataset": "VQA",
                        "accuracy": "+1.6% in VQA score"
                    },
                    "description": "BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.",
                    "id": "huggingface_api_105",
                    "name": "Salesforce/blip-vqa-capfilt-large"
                },
                "id": "gorilla_huggingface_tool_104",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.",
            "Tool 3: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.",
            "Tool 4: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.",
            "Tool 5: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.",
            "Tool 6: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.",
            "Tool 7: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 8: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.",
            "Tool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.",
            "Tool 10: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.",
            "Tool 11: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 12: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.",
            "Tool 14: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 15: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 16: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 17: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 18: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.",
            "Tool 19: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis."
        ],
        "tool_view": {
            "description": "This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "parameters": {
                "audio_file": {
                    "type": "string",
                    "description": "The path to the audio file to be analyzed for voice activity detection.",
                    "required": true,
                    "example": "audio.wav"
                }
            },
            "response": "The tool produces a response representing the detected voice activity segments within an input audio file. The response is structured as an object that provides a timeline of speech activity, typically containing the following components:\n\n1. **Timeline object**: This is the main structure in the response that encodes the temporal regions of the audio where speech is detected. It organizes detected speech segments in chronological order.  \n\n2. **Speech segments**: Each element in the timeline represents a contiguous period of active speech. Each segment includes:\n   - **start**: The timestamp marking the beginning of the speech segment.\n   - **end**: The timestamp marking the end of the speech segment.\n   - Optionally, **confidence or score**: Some implementations may include a confidence value indicating the likelihood that the segment contains speech.\n\n3. **Methods for accessing segments**: The response typically provides methods to query or iterate over the timeline, such as `get_timeline().support()`, which returns the collection of speech segments.\n\nIn essence, the response captures **when speech occurs in the audio** and allows programmatic access to these intervals for further processing or analysis.",
            "examples": [
                {
                    "user_intent": "I want to detect all segments where someone is speaking in a meeting recording.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "meeting_recording_2025-12-30.wav"
                        }
                    }
                },
                {
                    "user_intent": "Analyze this podcast episode and mark all periods of active speech.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "podcast_episode_42.mp3"
                        }
                    }
                },
                {
                    "user_intent": "Process an interview audio file to find where the speaker is talking versus silence.",
                    "tool_call": {
                        "name": "pyannote/voice-activity-detection",
                        "arguments": {
                            "audio.wav": "interview_session_15.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 1: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 2: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\nTool 3: A Named Entity Recognition model designed to identify 107 biomedical entities in English text, such as case reports. Built on distilBERT and trained on the Maccrobat dataset, it can extract structured biomedical information from unstructured text for tasks like medical text analysis or information retrieval.\nTool 4: This tool provides a German-language Text-to-Speech (TTS) model using Tacotron2, trained on a custom German dataset with 12 days of voice data. It leverages the Speechbrain framework and includes the HIFIGAN vocoder, which can be used independently of language. While the model has room for improvement due to its training duration of 39 epochs, it can convert text input into spoken audio, and is suited for generating German speech.\nTool 5: EleutherAI/gpt-neo-2.7B is a transformer-based language model designed to generate coherent text from a given prompt. Trained on the large-scale Pile dataset, it replicates the GPT-3 architecture and can be used via a Hugging Face Transformers pipeline to produce creative or contextually relevant text outputs. This model is suitable for tasks that require text continuation, storytelling, or any application needing fluent natural language generation.\nTool 6: Detect and locate tables in document images using a Transformer-based object detection model trained on the PubTables1M dataset, enabling automated table extraction from unstructured documents.\nTool 7: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 8: This tool is a speech emotion recognition model built with the Wav2Vec2 architecture using Hugging Face Transformers. It analyzes audio recordings to identify emotions such as anger, disgust, fear, happiness, and sadness, and is trained on the JTES v1.1 dataset, achieving an overall accuracy of approximately 81%. It is used to automatically detect and classify emotions in speech for applications like sentiment analysis, human-computer interaction, and audio-based emotion monitoring.\nTool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone, combining a Vision Transformer for visual feature extraction and a language model for text feature encoding, enabling flexible object detection without task-specific training.\nTool 10: This tool provides an end-to-end object detection model, DETR (DEtection TRansformer) with a ResNet-50 backbone, trained on the COCO 2017 dataset. It allows users to detect and localize objects in images using the Hugging Face Transformers framework, offering a simple API for loading the pretrained model and processing images for inference. It is useful for applications requiring automated object recognition and localization in diverse visual data.\nTool 11: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 12: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 13: A spoken language identification model based on the ECAPA-TDNN architecture that can classify speech utterances into 107 different languages. Trained on the VoxLingua107 dataset using SpeechBrain, it is designed to accurately detect the language being spoken in an audio signal, making it useful for multilingual audio processing and analysis.\nTool 14: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 15: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 16: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 17: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 18: A lightweight VideoMAE model for classifying videos, implemented using Hugging Face Transformers, suitable for quick experiments or testing video classification pipelines.\nTool 19: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\", \"parameters\": {\"audio_file\": {\"type\": \"string\", \"description\": \"The path to the audio file to be analyzed for voice activity detection.\", \"required\": true, \"example\": \"audio.wav\"}}, \"response\": \"The tool produces a response representing the detected voice activity segments within an input audio file. The response is structured as an object that provides a timeline of speech activity, typically containing the following components:\\n\\n1. **Timeline object**: This is the main structure in the response that encodes the temporal regions of the audio where speech is detected. It organizes detected speech segments in chronological order.  \\n\\n2. **Speech segments**: Each element in the timeline represents a contiguous period of active speech. Each segment includes:\\n   - **start**: The timestamp marking the beginning of the speech segment.\\n   - **end**: The timestamp marking the end of the speech segment.\\n   - Optionally, **confidence or score**: Some implementations may include a confidence value indicating the likelihood that the segment contains speech.\\n\\n3. **Methods for accessing segments**: The response typically provides methods to query or iterate over the timeline, such as `get_timeline().support()`, which returns the collection of speech segments.\\n\\nIn essence, the response captures **when speech occurs in the audio** and allows programmatic access to these intervals for further processing or analysis.\", \"examples\": [{\"user_intent\": \"I want to detect all segments where someone is speaking in a meeting recording.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"meeting_recording_2025-12-30.wav\"}}}, {\"user_intent\": \"Analyze this podcast episode and mark all periods of active speech.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"podcast_episode_42.mp3\"}}}, {\"user_intent\": \"Process an interview audio file to find where the speaker is talking versus silence.\", \"tool_call\": {\"name\": \"pyannote/voice-activity-detection\", \"arguments\": {\"audio.wav\": \"interview_session_15.wav\"}}}]}\n\n# User Query\nI am developing a home security software which can detect intruders entering the house. In case any door or object is tampered, the application will ask, \"Who entered the room?\", to assist quick analysis of the CCTV recordings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Detect when someone enters a room or tampers with objects in the house using CCTV footage.",
                    "tool_description": "Perform object detection and activity recognition on video streams to identify intrusions, unauthorized access, or interaction with doors and other objects.",
                    "expected_response": "A list of detected events including timestamps, object locations, and types of intrusion or interaction."
                },
                {
                    "user_intent": "Ask who entered the room by analyzing audio from CCTV recordings.",
                    "tool_description": "Use voice activity detection and speaker recognition to identify and segment speech within recorded audio, assisting in determining the speaker's identity.",
                    "expected_response": "Detected speech segments with timestamps and optionally recognized speaker labels."
                }
            ],
            "extracted_arguments": {
                "cctv_video": "The path or URL to the video feed from home security cameras.",
                "audio_recording": "The audio stream or file corresponding to the video feed for analyzing speech.",
                "intrusion_objects": "A list of target objects (e.g., doors, windows) to monitor for tampering or unauthorized access."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_112",
        "query": "As a book store owner, I want to classify customer reviews into positive and negative sentiments.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that classify text data into categories such as positive and negative sentiments by processing input text and utilizing pre-trained language models to evaluate and assign sentiment labels.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Transformers",
                    "functionality": "Text Classification",
                    "api_call": "DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')",
                    "api_arguments": [
                        "inputs"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]",
                    "performance": {
                        "dataset": "glue",
                        "accuracy": 0.911
                    },
                    "description": "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.",
                    "id": "huggingface_api_370",
                    "name": "distilbert-base-uncased-finetuned-sst-2-english"
                },
                "id": "gorilla_huggingface_tool_366",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 2: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 3: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 4: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 5: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 6: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 7: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 8: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 11: This tool performs zero-shot text classification using the `valhalla/distilbart-mnli-12-6` model from Hugging Face, allowing users to assign input text to one or more candidate labels without any task-specific training. It is useful for quickly categorizing text across a wide range of topics or sentiments, leveraging a distilled version of BART trained on the MNLI dataset for high accuracy in natural language inference.",
            "Tool 12: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 14: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.",
            "Tool 15: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.",
            "Tool 16: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.",
            "Tool 17: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 18: A fine-tuned audio classification model based on Facebook’s data2vec-audio-base-960h, designed to accurately classify audio inputs using the Hugging Face Transformers pipeline.",
            "Tool 19: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework."
        ],
        "tool_view": {
            "description": "A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "parameters": {
                "tokenizer": {
                    "type": "string",
                    "description": "The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('bert-base-uncased')"
                },
                "config": {
                    "type": "string",
                    "description": "The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.",
                    "required": true,
                    "example": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                }
            },
            "response": "The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\n\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\n\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \"positive\" or \"negative\"), which is the predicted sentiment of the input text.\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model’s certainty about its classification.\n  \n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\n\nThus, the expected response would primarily contain:\n- **Tokenized representations** of the input text.\n- **Predicted sentiment labels** along with any associated confidence or probability values.\n- Potential **error or diagnostic messages** if the process fails.",
            "examples": [
                {
                    "user_intent": "Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this review: 'Service was slow but the dessert was delightful.'",
                    "tool_call": {
                        "name": "results-yelp",
                        "arguments": {
                            "tokenizer": "AutoTokenizer.from_pretrained('bert-base-uncased')",
                            "config": "AutoConfig.from_pretrained('potatobunny/results-yelp')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 1: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 2: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 3: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 4: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 5: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 6: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 7: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 8: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 9: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 10: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 11: This tool performs zero-shot text classification using the `valhalla/distilbart-mnli-12-6` model from Hugging Face, allowing users to assign input text to one or more candidate labels without any task-specific training. It is useful for quickly categorizing text across a wide range of topics or sentiments, leveraging a distilled version of BART trained on the MNLI dataset for high accuracy in natural language inference.\nTool 12: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 13: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 14: Perform zero-shot image classification using a lightweight CLIPSeg model, allowing users to classify images into custom categories without prior training.\nTool 15: A computer vision tool that uses a Hugging Face Transformers model to classify images as either “hotdog” or “not hotdog,” useful for automated image recognition tasks with moderate accuracy.\nTool 16: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\nTool 17: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 18: A fine-tuned audio classification model based on Facebook’s data2vec-audio-base-960h, designed to accurately classify audio inputs using the Hugging Face Transformers pipeline.\nTool 19: A Vision Transformer (ViT) model for image classification that is pretrained on ImageNet-21k and fine-tuned on ImageNet-1k, designed to classify images into predefined categories using the Hugging Face Transformers framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\", \"parameters\": {\"tokenizer\": {\"type\": \"string\", \"description\": \"The model tokenizer to be used for tokenizing the input text. This tokenizer is responsible for converting raw text into tokenized representations that the model can understand.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\"}, \"config\": {\"type\": \"string\", \"description\": \"The configuration for the model, which includes various hyperparameters and settings. This helps configure the model behavior during training or inference.\", \"required\": true, \"example\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}, \"response\": \"The tool response is not explicitly defined in the provided documentation. However, based on the described functionality and typical behavior of the Hugging Face Transformers library, a plausible and typical response structure would include:\\n\\n1. **Tokenized Text**: The response would likely contain a representation of the input text that has been tokenized by the model's tokenizer. This would include:\\n   - A list or array of token IDs corresponding to the text's tokenized form, ready for further processing or model inference.\\n   - Additional information about the tokens, such as their types or attention masks, depending on the tokenizer's configuration.\\n\\n2. **Model Output**: If the tool is performing inference (such as sentiment analysis):\\n   - **Predicted Label(s)**: The model would return a sentiment classification label (e.g., \\\"positive\\\" or \\\"negative\\\"), which is the predicted sentiment of the input text.\\n   - **Confidence/Probability Scores**: The tool might include the probability or confidence associated with the predicted label, indicating the model\\u2019s certainty about its classification.\\n  \\n3. **Error or Warning Messages (if any)**: If there is an issue during tokenization or inference (such as invalid input, unsupported token types, etc.), the response might include error messages or warnings.\\n\\nThus, the expected response would primarily contain:\\n- **Tokenized representations** of the input text.\\n- **Predicted sentiment labels** along with any associated confidence or probability values.\\n- Potential **error or diagnostic messages** if the process fails.\", \"examples\": [{\"user_intent\": \"Analyze this restaurant review and tell me if it's positive or negative: 'The pasta was amazing and the staff were incredibly friendly.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"I want to classify a batch of Yelp restaurant reviews to see which ones are negative for further analysis.\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}, {\"user_intent\": \"Check the sentiment of this review: 'Service was slow but the dessert was delightful.'\", \"tool_call\": {\"name\": \"results-yelp\", \"arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}}}]}\n\n# User Query\nAs a book store owner, I want to classify customer reviews into positive and negative sentiments.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Classify customer reviews for my bookstore as positive or negative.",
                    "tool_description": "Perform sentiment analysis on textual customer reviews using a fine-tuned BERT-based model to classify each review as either positive or negative.",
                    "expected_response": "A list of review objects with predicted sentiment labels (positive or negative) and associated confidence scores."
                }
            ],
            "extracted_arguments": {
                "reviews_text": "A collection of customer review texts in English to be analyzed for sentiment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_185",
        "query": "I need to create a new piece of art to add to my digital gallery that resembles WikiArt images.",
        "instruction": "Given an `art creation` task, retrieve tools that facilitate digital art generation by leveraging image generation capabilities to produce artwork resembling specific art styles or collections, such as WikiArt.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "https://huggingface.co/datasets/huggan/wikiart",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.",
                    "id": "huggingface_api_297",
                    "name": "johnowhitaker/sd-class-wikiart-from-bedrooms"
                },
                "id": "gorilla_huggingface_tool_293",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 1: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 2: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 3: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 4: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 5: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 6: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 7: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 10: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.",
            "Tool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 12: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 13: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 15: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.",
            "Tool 16: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 17: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 18: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts."
        ],
        "tool_view": {
            "description": "Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text description that specifies the content, style, and details of the image to be generated by the model.",
                    "required": true,
                    "example": "A futuristic cityscape at sunset with flying cars, in a cyberpunk art style"
                }
            },
            "response": "The tool is designed to generate images from text prompts using the \"StableDiffusionPipeline\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \n\nThe expected response format includes the following components:\n\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\n\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\n\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\n\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.",
            "examples": [
                {
                    "user_intent": "Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "futuristic city skyline at sunset, retro 80s art style"
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a medieval knight in full armor, riding a horse.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "medieval knight in full armor, riding a horse"
                        }
                    }
                },
                {
                    "user_intent": "I want a retro-style image of different cars with various colors and shapes.",
                    "tool_call": {
                        "name": "prompthero/openjourney",
                        "arguments": {
                            "prompt": "retro series of different cars with different colors and shapes, mdjrny-v4 style"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 1: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 2: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 3: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 4: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 5: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 6: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 7: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 8: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 10: This tool provides a pre-trained ControlNet model that guides a Stable Diffusion image generation pipeline using lineart anime images as conditional input, allowing users to create detailed anime-style images that closely follow the structure of a reference lineart.\nTool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 12: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 13: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 14: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 15: A tool for generating high-fidelity, unconditional images using score-based generative modeling with stochastic differential equations. It leverages the Hugging Face DiffusionPipeline with the `google/ncsnpp-ffhq-256` model to produce realistic images and has demonstrated state-of-the-art performance on datasets like CIFAR-10.\nTool 16: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 17: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 18: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 19: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text description that specifies the content, style, and details of the image to be generated by the model.\", \"required\": true, \"example\": \"A futuristic cityscape at sunset with flying cars, in a cyberpunk art style\"}}, \"response\": \"The tool is designed to generate images from text prompts using the \\\"StableDiffusionPipeline\\\" from the Hugging Face library. Given a text prompt, it returns an image that corresponds to the described content in the prompt. \\n\\nThe expected response format includes the following components:\\n\\n1. **Images**: The primary content of the response is one or more generated images that correspond to the given text prompt. These images are typically returned in an image format (e.g., PNG or JPEG). The response will likely contain a list or array of images, but the example code suggests that only the first image is retrieved for further use.\\n\\n2. **Image Output**: The response contains the actual generated image(s), which may be saved or displayed depending on the context of the request. The images will be in a format suitable for saving or further processing (e.g., `image.save()` in the example code).\\n\\n3. **Optional Metadata**: Although not explicitly mentioned in the documentation, additional metadata about the image generation process, such as model version, runtime information, or processing parameters, could be included. However, this is not required based on the provided description.\\n\\nIn summary, the response consists mainly of the image(s) generated from the input text prompt, with optional additional information about the generation process that may be included in specific use cases.\", \"examples\": [{\"user_intent\": \"Generate an image of a futuristic city skyline at sunset in the style of retro 80s art.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"futuristic city skyline at sunset, retro 80s art style\"}}}, {\"user_intent\": \"Create an illustration of a medieval knight in full armor, riding a horse.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"medieval knight in full armor, riding a horse\"}}}, {\"user_intent\": \"I want a retro-style image of different cars with various colors and shapes.\", \"tool_call\": {\"name\": \"prompthero/openjourney\", \"arguments\": {\"prompt\": \"retro series of different cars with different colors and shapes, mdjrny-v4 style\"}}}]}\n\n# User Query\nI need to create a new piece of art to add to my digital gallery that resembles WikiArt images.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a new artwork that resembles the style of WikiArt for my digital gallery.",
                    "tool_description": "Use a diffusion-based image generation model trained on the WikiArt dataset to create an artwork-style image from a random latent input, producing a high-quality artistic image without requiring a specific text prompt.",
                    "expected_response": "A generated image in the style of WikiArt suitable for display in a digital gallery."
                }
            ],
            "extracted_arguments": {
                "style_dataset": "The reference dataset or artistic style to emulate in the generated image, in this case, WikiArt."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_204",
        "query": "We recently received low resolution images of newly released products and need to upscale them for better quality.",
        "instruction": "Given an `image enhancement` task, retrieve tools that focus on image processing operations, particularly upscaling images to improve resolution and quality by applying state-of-the-art transformer models designed for image super-resolution.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').",
                    "api_arguments": "feature_extractor, model",
                    "python_environment_requirements": "transformers, torch",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.",
                    "id": "huggingface_api_277",
                    "name": "swin2SR-lightweight-x2-64"
                },
                "id": "gorilla_huggingface_tool_273",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 1: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.",
            "Tool 2: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.",
            "Tool 3: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 4: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.",
            "Tool 5: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.",
            "Tool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 7: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.",
            "Tool 8: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.",
            "Tool 9: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 10: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.",
            "Tool 11: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.",
            "Tool 13: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.",
            "Tool 14: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.",
            "Tool 15: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.",
            "Tool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 17: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 18: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 19: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity."
        ],
        "tool_view": {
            "description": "The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A text string describing the desired content or modifications for the image. This serves as the main input guiding the image generation or upscaling process.",
                    "required": true,
                    "example": "a white cat sitting on a windowsill"
                },
                "image": {
                    "type": "image",
                    "description": "The low-resolution input image to be upscaled or modified. This should be provided as a PIL Image object.",
                    "required": true,
                    "example": "PIL.Image.open('low_res_cat.png')"
                },
                "noise_level": {
                    "type": "number",
                    "description": "An optional parameter that controls the amount of noise added to the input image according to the diffusion schedule. Higher values introduce more noise.",
                    "required": false,
                    "example": 0.2
                }
            },
            "response": "The tool produces a response consisting of an upscaled or modified image generated from a combination of a text prompt and an input image. The response structure can be described as follows:\n\n- **images**: A list or collection containing one or more images resulting from the pipeline. Each item in this list represents a generated or upscaled image.  \n  - Each image is typically represented as a PIL `Image` object (or another standard image format in memory) that can be saved to disk or further processed.  \n\n- **pipeline metadata (implicit)**: While not explicitly returned, the pipeline internally uses parameters such as:\n  - `prompt`: The text prompt guiding the generation or modification.\n  - `image`: The low-resolution input image to be upscaled or modified.\n  - `noise_level` (optional): The amount of noise applied according to the diffusion schedule.\n  - Device information (e.g., whether the model ran on CUDA or CPU).\n\nSemantically, the response contains the visual output of the diffusion-based upscaler, reflecting the content described in the prompt while preserving or enhancing details from the input image. The primary component of interest is the upscaled image itself, accessible via `response.images[0]` in typical usage.",
            "examples": [
                {
                    "user_intent": "I have a small photo of my dog and I want to upscale it to high resolution while keeping the details sharp.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a realistic portrait of a golden retriever",
                            "image_path": "low_res_dog.png",
                            "output_path": "upscaled_dog.png"
                        }
                    }
                },
                {
                    "user_intent": "Enhance a low-resolution concept art image of a futuristic city to make it look more detailed and polished.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a highly detailed futuristic cityscape at sunset",
                            "image_path": "low_res_city.png",
                            "output_path": "upscaled_city.png"
                        }
                    }
                },
                {
                    "user_intent": "Upscale a small illustration of a fantasy dragon while keeping its colors vibrant and textures sharp.",
                    "tool_call": {
                        "name": "StableDiffusionUpscalePipeline.from_pretrained",
                        "arguments": {
                            "model_id": "stabilityai/stable-diffusion-x4-upscaler",
                            "torch_dtype": "torch.float16",
                            "prompt": "a majestic fantasy dragon in vibrant colors",
                            "image_path": "low_res_dragon.png",
                            "output_path": "upscaled_dragon.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 1: Enhance the resolution of images by upscaling them four times using the Swin2SR Transformer model, enabling high-quality image super-resolution and restoration.\nTool 2: The Stable Diffusion x2 latent upscaler is a diffusion-based model that enhances the resolution of images generated by Stable Diffusion by upscaling their latent embeddings. It enables fast and high-quality text-to-image generation and image upscaling, making it useful for producing detailed, high-resolution outputs from lower-resolution latent images. The model is compatible with all Stable Diffusion checkpoints and was trained on a high-resolution subset of the LAION-2B dataset.\nTool 3: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 4: A self-supervised Vision Transformer (ViT) model pretrained on ImageNet-1k using the DINO method, designed to extract feature representations from images. It processes images by splitting them into fixed-size patches, embedding them with positional information, and passing them through a Transformer encoder, producing rich visual features useful for downstream tasks such as classification or similarity analysis.\nTool 5: The tool provides a pre-trained SegFormer model for semantic image segmentation, fine-tuned on the ADE20k dataset at a resolution of 512x512. It allows for processing images and segmenting them into meaningful regions using the Segformer architecture, which is based on transformers. This model is accessed via Hugging Face Transformers and can be used in Python with required libraries like Transformers, PIL, and Requests. It is designed for tasks like semantic segmentation, where images are classified into different segments based on their content.\nTool 6: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 7: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.\nTool 8: This tool provides a pretrained Denoising Diffusion Probabilistic Model (DDPM) for generating high-quality, unconditional images, exemplified by a model trained on cat images. It leverages diffusion-based generative modeling to iteratively denoise latent variables into realistic images and supports different noise scheduling strategies. It is commonly used for creating diverse, photorealistic images and has demonstrated strong performance on benchmarks like CIFAR-10, achieving a high Inception score and a low FID score.\nTool 9: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 10: A tool that uses the Swin2SR model to perform 2× image super-resolution, enhancing the resolution of input images. It leverages the SwinV2 Transformer architecture and is implemented via the Hugging Face Transformers framework, suitable for applications that require upscaling or restoring image details.\nTool 11: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 12: This tool performs zero-shot image classification using the `laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup` model, which is based on the CLIP ConvNeXt-Large architecture. It allows for classifying images into specified categories without the need for fine-tuning on specific datasets. The model is trained on the LAION-2B subset and utilizes both image and text towers to enable high-quality cross-modal classification. It can be used by passing an image path and a list of class names, with an accuracy of 76.9% on the ImageNet-1k dataset.\nTool 13: Swin2SR-lightweight-x2-64 is a lightweight image super-resolution model that upscales images by a factor of two using a SwinV2 transformer architecture, designed for efficient image restoration and enhancement.\nTool 14: A compact GPT-2 model from Hugging Face designed for text generation, offering fast inference and suitability for low-resource environments, which generates text based on a given prompt.\nTool 15: This tool is a score-based generative model that uses stochastic differential equations (SDE) to perform unconditional image generation. It enables the creation of high-fidelity images, achieving state-of-the-art performance on datasets like CIFAR-10, and can generate high-resolution images from learned distributions without any conditional input.\nTool 16: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 17: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 18: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 19: The \"sentence-transformers/all-roberta-large-v1\" model maps sentences and paragraphs to a 1024-dimensional vector space, making it suitable for tasks like semantic search and clustering. It uses the SentenceTransformer framework from Hugging Face and can be accessed via the `SentenceTransformer` API. This model helps to encode sentences into embeddings, enabling better comparison and analysis of sentence similarity.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A text string describing the desired content or modifications for the image. This serves as the main input guiding the image generation or upscaling process.\", \"required\": true, \"example\": \"a white cat sitting on a windowsill\"}, \"image\": {\"type\": \"image\", \"description\": \"The low-resolution input image to be upscaled or modified. This should be provided as a PIL Image object.\", \"required\": true, \"example\": \"PIL.Image.open('low_res_cat.png')\"}, \"noise_level\": {\"type\": \"number\", \"description\": \"An optional parameter that controls the amount of noise added to the input image according to the diffusion schedule. Higher values introduce more noise.\", \"required\": false, \"example\": 0.2}}, \"response\": \"The tool produces a response consisting of an upscaled or modified image generated from a combination of a text prompt and an input image. The response structure can be described as follows:\\n\\n- **images**: A list or collection containing one or more images resulting from the pipeline. Each item in this list represents a generated or upscaled image.  \\n  - Each image is typically represented as a PIL `Image` object (or another standard image format in memory) that can be saved to disk or further processed.  \\n\\n- **pipeline metadata (implicit)**: While not explicitly returned, the pipeline internally uses parameters such as:\\n  - `prompt`: The text prompt guiding the generation or modification.\\n  - `image`: The low-resolution input image to be upscaled or modified.\\n  - `noise_level` (optional): The amount of noise applied according to the diffusion schedule.\\n  - Device information (e.g., whether the model ran on CUDA or CPU).\\n\\nSemantically, the response contains the visual output of the diffusion-based upscaler, reflecting the content described in the prompt while preserving or enhancing details from the input image. The primary component of interest is the upscaled image itself, accessible via `response.images[0]` in typical usage.\", \"examples\": [{\"user_intent\": \"I have a small photo of my dog and I want to upscale it to high resolution while keeping the details sharp.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a realistic portrait of a golden retriever\", \"image_path\": \"low_res_dog.png\", \"output_path\": \"upscaled_dog.png\"}}}, {\"user_intent\": \"Enhance a low-resolution concept art image of a futuristic city to make it look more detailed and polished.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a highly detailed futuristic cityscape at sunset\", \"image_path\": \"low_res_city.png\", \"output_path\": \"upscaled_city.png\"}}}, {\"user_intent\": \"Upscale a small illustration of a fantasy dragon while keeping its colors vibrant and textures sharp.\", \"tool_call\": {\"name\": \"StableDiffusionUpscalePipeline.from_pretrained\", \"arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\", \"prompt\": \"a majestic fantasy dragon in vibrant colors\", \"image_path\": \"low_res_dragon.png\", \"output_path\": \"upscaled_dragon.png\"}}}]}\n\n# User Query\nWe recently received low resolution images of newly released products and need to upscale them for better quality.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to upscale low-resolution product images to a higher quality.",
                    "tool_description": "Upscale the resolution of low-resolution product images using a diffusion-based model to generate high-quality outputs.",
                    "expected_response": "A list of upscaled product images, each with enhanced resolution and visual quality."
                }
            ],
            "extracted_arguments": {
                "image": "The low-resolution input product image to be upscaled.",
                "prompt": "The description or context of the product image to guide the upscale process."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_298",
        "query": "We need to analyze satellite images to categorize the types of land use. For this purpose, I need to segment the images and identify different objects.",
        "instruction": "Given an `image segmentation` task, retrieve tools that analyze satellite images for land use categorization by processing visual inputs to identify and segment different objects in the images.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image Segmentation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "OneFormerForUniversalSegmentation.from_pretrained('shi-labs/oneformer_coco_swin_large')",
                    "api_arguments": {
                        "images": "image",
                        "task_inputs": [
                            "semantic",
                            "instance",
                            "panoptic"
                        ],
                        "return_tensors": "pt"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\nfrom PIL import Image\nimport requests\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\n\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\nsemantic_outputs = model(**semantic_inputs)\n\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]",
                    "performance": {
                        "dataset": "ydshieh/coco_dataset_script",
                        "accuracy": "Not provided"
                    },
                    "description": "OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.",
                    "id": "huggingface_api_242",
                    "name": "shi-labs/oneformer_coco_swin_large"
                },
                "id": "gorilla_huggingface_tool_238",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "Tool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.",
            "Tool 3: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.",
            "Tool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 5: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.",
            "Tool 8: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.",
            "Tool 9: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 10: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 12: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 13: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.",
            "Tool 14: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.",
            "Tool 15: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 16: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 17: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 19: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework."
        ],
        "tool_view": {
            "description": "This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.",
                    "required": true,
                    "example": 0.25
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.",
                    "required": true,
                    "example": 0.45
                },
                "agnostic_nms": {
                    "type": "boolean",
                    "description": "Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.",
                    "required": true,
                    "example": false
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections to return per image.",
                    "required": true,
                    "example": 1000
                },
                "image": {
                    "type": "string",
                    "description": "The input image for segmentation. Can be a local file path or a URL pointing to the image.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool’s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \n\n1. **`boxes`** – A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \n   - A confidence score indicating the model’s certainty that the object belongs to the target class.  \n   - Optional class label or ID identifying the object category (here, all are buildings).  \n\n2. **`masks`** – Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \n\n3. **`scores`** – Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \n\n4. **`class_ids` or `labels`** – Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \n\n5. **`metadata`** (optional) – May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \n\n6. **Visualization helpers** – Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \n\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.",
            "examples": [
                {
                    "user_intent": "I want to segment all buildings in a satellite image of a city block.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.3,
                            "iou": 0.5,
                            "agnostic_nms": false,
                            "max_det": 500,
                            "image": "https://example.com/satellite_city_block.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Detect and segment buildings in a rural area satellite image with high confidence.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.6,
                            "iou": 0.4,
                            "agnostic_nms": true,
                            "max_det": 200,
                            "image": "https://example.com/rural_area_satellite.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Process a satellite image to extract building masks for GIS analysis.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8s-building-segmentation')",
                        "arguments": {
                            "conf": 0.25,
                            "iou": 0.45,
                            "agnostic_nms": false,
                            "max_det": 1000,
                            "image": "https://example.com/gis_project_image.jpg"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\nTool 1: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 2: A YOLOv8 model for detecting and segmenting buildings in satellite images, enabling precise identification of building locations and shapes from input images.\nTool 3: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.\nTool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 5: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: This tool provides speaker diarization using the pyannote.audio framework, allowing users to analyze audio files and identify when different speakers are talking. It supports specifying the number of speakers or a range of speakers and adjusting segmentation parameters, and outputs results in the standard RTTM format for downstream processing or analysis.\nTool 8: A pretrained DETR (DEtection TRansformer) model using a ResNet-101 backbone for end-to-end object detection on images, capable of identifying and localizing objects as trained on the COCO 2017 dataset, and designed for use with the Hugging Face Transformers framework.\nTool 9: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 10: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 11: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 12: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 13: This tool provides a Named Entity Recognition (NER) model for English, capable of identifying 18 different entity types, such as persons, organizations, dates, locations, and more. It uses Flair embeddings and an LSTM-CRF architecture for accurate predictions. The model is pre-trained on the Ontonotes dataset and offers an accuracy of 89.27%. It can be easily integrated with the Hugging Face Transformers framework and requires the Flair library for use.\nTool 14: A fine-tuned CLIP model optimized for zero-shot image classification on remote sensing images, enabling users to match images with descriptive text labels without additional training and facilitating text-to-image and image-to-image retrieval in aerial and satellite imagery contexts.\nTool 15: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 16: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 17: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 19: YOLOS-small is a Vision Transformer model fine-tuned for object detection on the COCO 2017 dataset. It uses a transformer-based architecture with DETR-style training to identify and localize objects in images, achieving 36.1 AP on COCO validation. The model can be loaded via Hugging Face Transformers and is suitable for detecting multiple object categories in images using a simple, end-to-end framework.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can accurately detect and segment buildings. The model is accessed through the Hugging Face Transformers framework and offers configurable parameters for detection thresholds, non-maxima suppression, and maximum detection limits. It is designed to work with the ultralyticsplus Python package for seamless integration and use.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be discarded.\", \"required\": true, \"example\": 0.25}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for non-maximum suppression to filter overlapping detections.\", \"required\": true, \"example\": 0.45}, \"agnostic_nms\": {\"type\": \"boolean\", \"description\": \"Determines whether non-maximum suppression is class-agnostic. If true, overlapping boxes are filtered regardless of class.\", \"required\": true, \"example\": false}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections to return per image.\", \"required\": true, \"example\": 1000}, \"image\": {\"type\": \"string\", \"description\": \"The input image for segmentation. Can be a local file path or a URL pointing to the image.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool\\u2019s response is a structured object representing the results of performing image segmentation on the provided input image. The response typically contains the following components:  \\n\\n1. **`boxes`** \\u2013 A collection of bounding boxes for each detected object (in this case, buildings). Each bounding box usually includes:  \\n   - Coordinates of the box (e.g., top-left and bottom-right corners or center coordinates with width and height).  \\n   - A confidence score indicating the model\\u2019s certainty that the object belongs to the target class.  \\n   - Optional class label or ID identifying the object category (here, all are buildings).  \\n\\n2. **`masks`** \\u2013 Segmentation masks for each detected object. Each mask is generally a pixel-wise representation (binary or probabilistic) showing which pixels belong to the object. Masks can be returned as arrays or encoded formats suitable for rendering.  \\n\\n3. **`scores`** \\u2013 Confidence scores for each detected object, often aligned with the corresponding bounding boxes and masks.  \\n\\n4. **`class_ids` or `labels`** \\u2013 Optional identifiers for the detected object class; in this model, all detections correspond to buildings.  \\n\\n5. **`metadata`** (optional) \\u2013 May include additional information such as the number of detections, inference time, or model settings used for the prediction (e.g., `conf`, `iou`, `agnostic_nms`, `max_det`).  \\n\\n6. **Visualization helpers** \\u2013 Although not part of the raw output, functions like `render_result` use the response to produce a visual overlay of bounding boxes and masks on the input image.  \\n\\nOverall, the response provides both spatial (boxes, masks) and confidence information for each detected building, structured in a way that allows programmatic analysis or visual rendering.\", \"examples\": [{\"user_intent\": \"I want to segment all buildings in a satellite image of a city block.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.3, \"iou\": 0.5, \"agnostic_nms\": false, \"max_det\": 500, \"image\": \"https://example.com/satellite_city_block.jpg\"}}}, {\"user_intent\": \"Detect and segment buildings in a rural area satellite image with high confidence.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.6, \"iou\": 0.4, \"agnostic_nms\": true, \"max_det\": 200, \"image\": \"https://example.com/rural_area_satellite.jpg\"}}}, {\"user_intent\": \"Process a satellite image to extract building masks for GIS analysis.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://example.com/gis_project_image.jpg\"}}}]}\n\n# User Query\nWe need to analyze satellite images to categorize the types of land use. For this purpose, I need to segment the images and identify different objects.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Segment all detectable objects in a satellite image for land use analysis.",
                    "tool_description": "Use a YOLOv8-based segmentation model to detect and segment all relevant objects in a satellite image, providing pixel-level masks and bounding boxes for each object.",
                    "expected_response": "A structured object containing bounding boxes, segmentation masks, confidence scores, and class labels for all detected objects in the satellite image."
                },
                {
                    "user_intent": "Classify the segmented objects into different land use categories.",
                    "tool_description": "Analyze the segmented objects to categorize each one according to its land use type, based on visual features and object context.",
                    "expected_response": "A mapping of each detected object to a specific land use category, potentially including confidence scores for each classification."
                }
            ],
            "extracted_arguments": {
                "image": "The input satellite image for segmentation and analysis, provided as a local file path or URL.",
                "conf": "Confidence threshold for object detection, determining the minimum score for including a detection.",
                "iou": "Intersection over Union threshold for non-maximum suppression to filter overlapping detections.",
                "agnostic_nms": "Boolean indicating whether non-maximum suppression should be class-agnostic.",
                "max_det": "Maximum number of objects to detect and return per image."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_459",
        "query": "Help us improve the listener experience from our customers by enhancing the audio of noisy recordings.",
        "instruction": "Given an `audio enhancement` task, retrieve tools that improve the audio quality of noisy recordings by processing audio inputs to perform denoising and dereverberation, resulting in clearer listener experiences.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio-to-Audio",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Speech Enhancement",
                    "api_call": "separator.from_hparams(source='speechbrain/sepformer-whamr-enhancement', savedir='pretrained_models/sepformer-whamr-enhancement')",
                    "api_arguments": {
                        "path": "Path to the input audio file."
                    },
                    "python_environment_requirements": "pip install speechbrain",
                    "example_code": "from speechbrain.pretrained import SepformerSeparation as separator\nimport torchaudio\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir='pretrained_models/sepformer-whamr-enhancement')\nest_sources = model.separate_file(path='speechbrain/sepformer-whamr-enhancement/example_whamr.wav')\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)",
                    "performance": {
                        "dataset": "WHAMR!",
                        "accuracy": "10.59 dB SI-SNR"
                    },
                    "description": "This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.",
                    "id": "huggingface_api_782",
                    "name": "speechbrain/sepformer-whamr-enhancement"
                },
                "id": "gorilla_huggingface_tool_777",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "Tool 1: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.",
            "Tool 2: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.",
            "Tool 3: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.",
            "Tool 4: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.",
            "Tool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 6: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.",
            "Tool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.",
            "Tool 8: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.",
            "Tool 9: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 10: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 11: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.",
            "Tool 12: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.",
            "Tool 13: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.",
            "Tool 14: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.",
            "Tool 15: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).",
            "Tool 16: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.",
            "Tool 17: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.",
            "Tool 18: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.",
            "Tool 19: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment."
        ],
        "tool_view": {
            "description": "This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.",
            "parameters": {
                "path": {
                    "type": "string",
                    "description": "The file path to the input audio file that will be processed for speech enhancement using the SepFormer model.",
                    "required": true,
                    "example": "example_wham.wav"
                }
            },
            "response": "The tool’s response is an enhanced (denoised) version of the input audio, produced by the SepFormer speech enhancement model. Structurally, the response is typically an audio tensor or array representing one or more separated/enhanced audio sources. The components of the response can be described as follows:  \n\n- **Audio Tensor/Array**: A multi-dimensional numeric array representing the waveform(s) of the enhanced audio.  \n  - **Dimensions**: Usually structured as `[channels, samples]` or `[sources, channels, samples]` depending on whether multiple sources are returned.  \n  - **Values**: Floating-point numbers corresponding to the amplitude of the audio signal at each sample point.  \n\n- **Source Separation Component (if multiple outputs)**: When the model separates overlapping speech, the response includes multiple tensors, each corresponding to an estimated source.  \n\n- **Sampling Rate Context**: While not embedded in the tensor itself, the model assumes a fixed sampling rate (8 kHz for this pretrained model), which is necessary when saving or playing back the audio.  \n\nIn summary, the response provides the denoised or enhanced waveform data, optionally separated into multiple sources, ready to be saved to an audio file or further processed.",
            "examples": [
                {
                    "user_intent": "I have a noisy recording of a meeting and want to enhance the speech clarity.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "recordings/meeting_noisy.wav"
                        }
                    }
                },
                {
                    "user_intent": "Enhance the audio quality of a podcast episode that has background noise.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "podcasts/episode3_raw.wav"
                        }
                    }
                },
                {
                    "user_intent": "Clean up an old voicemail recording for transcription.",
                    "tool_call": {
                        "name": "sepformer-wham-enhancement",
                        "arguments": {
                            "path": "voicemails/old_message.wav"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8 kHz sampling frequency.\nTool 1: This tool uses a pretrained ConvTasNet model from the Asteroid framework to perform audio source separation, specifically separating noisy mixed speech recordings from the Libri2Mix dataset, enabling improved clarity and isolation of individual speakers in noisy environments.\nTool 2: This tool performs audio source separation using the SepFormer model, implemented with SpeechBrain and pretrained on the WHAM! dataset, which consists of noisy speech mixtures. It allows for the extraction of individual audio sources from mixed recordings, making it useful for tasks like separating overlapping speech or isolating sound sources in noisy environments.\nTool 3: This tool uses a pretrained SepFormer model from SpeechBrain to perform audio source separation, isolating individual sound sources from mixed audio recordings. It is designed for handling noisy and reverberant environments, as it is trained on the WHAMR! dataset, and can be applied to separate speech signals in applications like speech enhancement, transcription, or audio analysis.\nTool 4: DCUNet_Libri1Mix_enhsingle_16k is an audio-to-audio model trained with the Asteroid framework to enhance single-speaker audio from mixed recordings. It processes input audio to improve clarity and separation, using the Libri1Mix dataset for training, and achieves strong performance on metrics like SI-SDR, SDR, SAR, and STOI.\nTool 5: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 6: This tool provides speech enhancement (denoising and dereverberation) using a SepFormer model, implemented with SpeechBrain and pretrained on the WHAMR! dataset. It can process audio files to improve clarity by reducing environmental noise and reverberation, particularly for recordings with 8kHz sampling frequency.\nTool 7: FSMN-VAD is a Voice Activity Detection tool that uses the FSMN-VAD model with the Hugging Face Transformers library to detect voice activity in audio data. It is designed to help identify segments of speech within an audio stream.\nTool 8: Perform audio source separation using a pretrained SepFormer model from SpeechBrain, enabling the extraction of individual audio sources from mixed recordings for applications like speech enhancement or analysis.\nTool 9: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 10: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 11: DCCRNet_Libri1Mix_enhsingle_16k is an audio-to-audio model built with Hugging Face Transformers and Asteroid, designed for speech enhancement. Trained on the Libri1Mix dataset using the enh_single task, it improves speech quality by separating and enhancing single-speaker audio, achieving high performance on standard metrics such as SI-SDR and STOI.\nTool 12: This tool loads a pre-trained single-column regression model, specifically designed for predicting US housing prices. The model, trained on the jwan2021/autotrain-data-us-housing-prices dataset, is compatible with the Joblib framework and requires the latest versions of Joblib and Pandas. It predicts housing prices based on input data prepared from a CSV file, using specified feature columns.\nTool 13: ConvTasNet_Libri2Mix_sepclean_16k is an audio-to-audio model designed for source separation, trained on the Libri2Mix dataset using the Asteroid framework. It isolates individual speakers from mixed audio recordings, achieving high separation performance as measured by metrics like SI-SDR, SDR, SIR, and STOI, making it suitable for tasks such as speech enhancement, transcription, or audio analysis.\nTool 14: This tool performs audio-to-audio tasks using the \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\" model, trained on the Libri1Mix dataset to enhance single-channel audio. It is built on the Hugging Face Transformers and Asteroid frameworks and is suitable for tasks such as audio source separation or enhancement. The model delivers high performance metrics such as a SDR of 15.40 and an STOI of 0.93, making it effective for improving audio quality in noisy environments.\nTool 15: This tool is a MetricGAN-trained speech enhancement model that improves the quality and intelligibility of noisy audio. It is designed to process audio inputs and produce clearer, enhanced speech, making it useful for applications such as voice communication, transcription, and audio restoration. The model is trained on the Voicebank dataset and provides measurable improvements in perceptual quality (PESQ) and intelligibility (STOI).\nTool 16: Detects overlapping speech in audio recordings by identifying time segments where two or more speakers are speaking simultaneously, using the pyannote.audio framework for automatic speech analysis.\nTool 17: This tool uses the DPRNNTasNet model from Asteroid to perform audio source separation, isolating individual sound sources from a mixed audio file. It is trained on the WHAM! dataset for clean speech separation and is accessed via a Hugging Face Transformers pipeline, making it suitable for tasks like separating voices or instruments in recordings.\nTool 18: This tool provides a pretrained voice activity detection pipeline that identifies active speech in audio files using the pyannote.audio framework. It can be used to automatically detect speech segments within audio recordings, making it useful for tasks such as speech transcription or audio analysis.\nTool 19: Brouhaha is a PyTorch-based model that performs joint voice activity detection, estimates speech-to-noise ratio, and evaluates C50 room acoustics from audio signals. It is used to analyze and characterize audio recordings, providing insights into speech presence, signal quality, and acoustic properties of the recording environment.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses a pretrained SepFormer model from SpeechBrain to perform speech enhancement, removing noise and reverberation from audio recordings. It is designed to improve speech clarity and intelligibility in noisy environments, and is trained on the WHAM! dataset, making it suitable for denoising real-world audio with 8\\u202fkHz sampling frequency.\", \"parameters\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to the input audio file that will be processed for speech enhancement using the SepFormer model.\", \"required\": true, \"example\": \"example_wham.wav\"}}, \"response\": \"The tool\\u2019s response is an enhanced (denoised) version of the input audio, produced by the SepFormer speech enhancement model. Structurally, the response is typically an audio tensor or array representing one or more separated/enhanced audio sources. The components of the response can be described as follows:  \\n\\n- **Audio Tensor/Array**: A multi-dimensional numeric array representing the waveform(s) of the enhanced audio.  \\n  - **Dimensions**: Usually structured as `[channels, samples]` or `[sources, channels, samples]` depending on whether multiple sources are returned.  \\n  - **Values**: Floating-point numbers corresponding to the amplitude of the audio signal at each sample point.  \\n\\n- **Source Separation Component (if multiple outputs)**: When the model separates overlapping speech, the response includes multiple tensors, each corresponding to an estimated source.  \\n\\n- **Sampling Rate Context**: While not embedded in the tensor itself, the model assumes a fixed sampling rate (8 kHz for this pretrained model), which is necessary when saving or playing back the audio.  \\n\\nIn summary, the response provides the denoised or enhanced waveform data, optionally separated into multiple sources, ready to be saved to an audio file or further processed.\", \"examples\": [{\"user_intent\": \"I have a noisy recording of a meeting and want to enhance the speech clarity.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"recordings/meeting_noisy.wav\"}}}, {\"user_intent\": \"Enhance the audio quality of a podcast episode that has background noise.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"podcasts/episode3_raw.wav\"}}}, {\"user_intent\": \"Clean up an old voicemail recording for transcription.\", \"tool_call\": {\"name\": \"sepformer-wham-enhancement\", \"arguments\": {\"path\": \"voicemails/old_message.wav\"}}}]}\n\n# User Query\nHelp us improve the listener experience from our customers by enhancing the audio of noisy recordings.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to enhance the audio of noisy recordings to improve the listener experience.",
                    "tool_description": "Perform speech enhancement using a pretrained SepFormer model from SpeechBrain to remove noise and reverberation from audio recordings, improving clarity and intelligibility in noisy environments.",
                    "expected_response": "An enhanced version of the input audio, with noise and reverberation removed, making it clearer and more intelligible."
                }
            ],
            "extracted_arguments": {
                "path": "The file path to the input audio file that will be processed for speech enhancement."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_219",
        "query": "We are making a mobile app related to fitness. We need to estimate the human pose from an image of a user performing an exercise.",
        "instruction": "Given a `human pose estimation` task, retrieve tools that process images to estimate and output the human pose, leveraging neural network models to handle visual input specific to fitness-related tasks.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Image-to-Image",
                    "framework": "Hugging Face",
                    "functionality": "Human Pose Estimation",
                    "api_call": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                    "api_arguments": {
                        "text": "chef in the kitchen",
                        "image": "image",
                        "num_inference_steps": 20
                    },
                    "python_environment_requirements": {
                        "diffusers": "pip install diffusers",
                        "transformers": "pip install transformers",
                        "accelerate": "pip install accelerate",
                        "controlnet_aux": "pip install controlnet_aux"
                    },
                    "example_code": "from PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers.utils import load_image\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\nimage = openpose(image)\ncontrolnet = ControlNetModel.from_pretrained(\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\nimage.save('images/chef_pose_out.png')",
                    "performance": {
                        "dataset": "200k pose-image, caption pairs",
                        "accuracy": "Not specified"
                    },
                    "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.",
                    "id": "huggingface_api_262",
                    "name": "lllyasviel/sd-controlnet-openpose"
                },
                "id": "gorilla_huggingface_tool_258",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "Tool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.",
            "Tool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.",
            "Tool 3: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 4: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 5: A zero-shot image classification tool using OpenAI's CLIP model, allowing images to be classified based on arbitrary text descriptions without task-specific training. It leverages the CLIP transformer architecture to generalize across a wide range of computer vision datasets, making it useful for evaluating model robustness and performing flexible image recognition directly from text prompts.",
            "Tool 6: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.",
            "Tool 7: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.",
            "Tool 8: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.",
            "Tool 9: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.",
            "Tool 10: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.",
            "Tool 11: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.",
            "Tool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.",
            "Tool 13: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.",
            "Tool 15: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.",
            "Tool 16: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 18: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.",
            "Tool 19: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis."
        ],
        "tool_view": {
            "description": "This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The text prompt describing the scene or subject that guides the image generation process.",
                    "required": true,
                    "example": "chef in the kitchen"
                },
                "image": {
                    "type": "string",
                    "description": "The input image used as a reference or condition for the pose-guided image generation.",
                    "required": true,
                    "example": "image.png"
                },
                "num_inference_steps": {
                    "type": "number",
                    "description": "The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.",
                    "required": true,
                    "example": 20
                }
            },
            "response": "The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\n\n1. **Generated Image**: \n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \"chef in the kitchen\") and the pose derived from the provided input image.\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\n\n2. **Inference Details**: \n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\n\n3. **Performance Information (Optional)**: \n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \"200k pose-image, caption pairs\") or other performance-related details if available or relevant to the particular response.\n\n4. **Image Format and Saving**:\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\n\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.",
            "examples": [
                {
                    "user_intent": "Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "yoga instructor in a studio",
                            "image": "yoga_pose_reference.png",
                            "num_inference_steps": 25
                        }
                    }
                },
                {
                    "user_intent": "Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "basketball player jumping in a stadium",
                            "image": "player_jump.png",
                            "num_inference_steps": 30
                        }
                    }
                },
                {
                    "user_intent": "Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.",
                    "tool_call": {
                        "name": "ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')",
                        "arguments": {
                            "text": "fashion model walking on a runway",
                            "image": "model_pose_reference.jpg",
                            "num_inference_steps": 20
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\nTool 1: A vision transformer model fine-tuned to estimate a person's age from a facial image, enabling automated age classification in computer vision applications.\nTool 2: A tabular regression model trained with AutoTrain to predict carbon emissions from input features, enabling users to estimate emissions based on structured data for analysis or decision-making.\nTool 3: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 4: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 5: A zero-shot image classification tool using OpenAI's CLIP model, allowing images to be classified based on arbitrary text descriptions without task-specific training. It leverages the CLIP transformer architecture to generalize across a wide range of computer vision datasets, making it useful for evaluating model robustness and performing flexible image recognition directly from text prompts.\nTool 6: A lightweight MobileNet V1 model pre-trained on ImageNet-1k for efficient image classification, designed to run with low latency and minimal resource usage. It can accurately classify images while being suitable for deployment on mobile or resource-constrained devices.\nTool 7: GIT-large-TextVQA is a generative image-to-text transformer model designed for multimodal visual question answering. It takes an image and a related question as input and generates textual answers, leveraging training on large image–text datasets. The model can also be applied to tasks such as image and video captioning or text-based image classification, making it useful for interpreting visual content in natural language.\nTool 8: Pix2Struct is a model designed for multimodal tasks like image captioning and visual question answering. It uses an image encoder and text decoder, trained on image-text pairs, to generate text from images. The model, pre-trained on parsing masked screenshots into simplified HTML, excels in a variety of domains, including documents, illustrations, user interfaces, and natural images. It achieves state-of-the-art results in several tasks, making it useful for applications that require understanding and generating textual descriptions from visual content.\nTool 9: A tabular regression tool that uses a pre-trained model to estimate carbon emissions from input features, enabling users to make predictions from structured data with Python.\nTool 10: A multi-class classification model that predicts carbon emissions from tabular data, enabling users to estimate emission levels based on selected input features.\nTool 11: This tool provides a fine-tuned version of a diffusion model, specifically designed for generating vintage-style images. It utilizes the DDPMPipeline from the Hugging Face Diffusers library, making it suitable for tasks related to computer vision and image generation. The model is pre-trained and fine-tuned for specific applications, offering an easy-to-use interface for generating images through a straightforward API call.\nTool 12: A visual question answering tool that analyzes an image in conjunction with a user-provided question and generates answers based on the visual content and contextual information in the question.\nTool 13: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 14: A multi-class classification tool that predicts CO2 emissions from tabular data using a pre-trained model, enabling users to analyze and estimate carbon output based on input features.\nTool 15: A multi-class classification model built with Hugging Face AutoTrain that predicts carbon emissions from tabular input data, designed for analyzing features to estimate emission levels with high accuracy.\nTool 16: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 18: Counterfeit-V2.5 is a text-to-image model that generates high-quality anime-style images from text prompts. It is designed for ease of use, supports negative prompts to refine outputs, and is ideal for creating detailed and visually appealing illustrations based on user-provided descriptions.\nTool 19: StreetCLIP is a foundation model for zero-shot image geolocalization that identifies the geographic location of street-level images. Trained on over a million geo-tagged urban and rural images, it enables accurate location predictions without additional task-specific training and performs strongly on open-domain geolocalization benchmarks, making it useful for geographic and climate-related analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool provides a ControlNet model specialized for human pose estimation, enabling users to guide image generation in Stable Diffusion based on specific body poses. It allows conditioning generated images on an input image and pose structure, making it useful for applications that require precise control over human figures in AI-generated visuals.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The text prompt describing the scene or subject that guides the image generation process.\", \"required\": true, \"example\": \"chef in the kitchen\"}, \"image\": {\"type\": \"string\", \"description\": \"The input image used as a reference or condition for the pose-guided image generation.\", \"required\": true, \"example\": \"image.png\"}, \"num_inference_steps\": {\"type\": \"number\", \"description\": \"The number of inference steps the model should perform during image generation. Higher numbers generally improve quality but take longer.\", \"required\": true, \"example\": 20}}, \"response\": \"The tool described in the raw JSON is designed for human pose estimation using a ControlNet model in combination with Stable Diffusion. The expected response from the tool would likely consist of the following components:\\n\\n1. **Generated Image**: \\n   - The main output would be a newly generated image that incorporates the human pose specified in the input. This image is generated based on the given text prompt (e.g., \\\"chef in the kitchen\\\") and the pose derived from the provided input image.\\n   - The image would typically have the human subject in the pose captured by the pose estimation model, modified or integrated into the scene described by the text prompt.\\n\\n2. **Inference Details**: \\n   - The response might include metadata or information about the inference process. This could include the number of inference steps performed (in this case, specified as 20) and any relevant configurations or parameters used during generation (e.g., model details, device used for computation).\\n\\n3. **Performance Information (Optional)**: \\n   - Though accuracy isn't explicitly provided, the response could potentially include the dataset used for training (e.g., \\\"200k pose-image, caption pairs\\\") or other performance-related details if available or relevant to the particular response.\\n\\n4. **Image Format and Saving**:\\n   - The generated image would be provided in a format suitable for saving or further use (e.g., a file path or image object), as indicated by the example code where the output is saved as 'images/chef_pose_out.png'.\\n\\nOverall, the response will primarily focus on delivering the resulting image that reflects the input pose and text prompt, along with any relevant metadata or parameters used during the generation process.\", \"examples\": [{\"user_intent\": \"Generate an image of a yoga instructor in a studio while keeping the exact human pose from a reference image.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"yoga instructor in a studio\", \"image\": \"yoga_pose_reference.png\", \"num_inference_steps\": 25}}}, {\"user_intent\": \"Create an illustration of a basketball player in mid-air jump, preserving the pose from my uploaded photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"basketball player jumping in a stadium\", \"image\": \"player_jump.png\", \"num_inference_steps\": 30}}}, {\"user_intent\": \"Produce an image of a fashion model on a runway while keeping the pose from an existing fashion shoot photo.\", \"tool_call\": {\"name\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-openpose')\", \"arguments\": {\"text\": \"fashion model walking on a runway\", \"image\": \"model_pose_reference.jpg\", \"num_inference_steps\": 20}}}]}\n\n# User Query\nWe are making a mobile app related to fitness. We need to estimate the human pose from an image of a user performing an exercise.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Estimate the human pose from an image of a person performing an exercise.",
                    "tool_description": "Use a ControlNet model specialized for human pose estimation to detect and extract body keypoints from a given image, enabling pose-guided analysis or further processing.",
                    "expected_response": "A structured representation of detected human pose keypoints, including coordinates for major body joints."
                }
            ],
            "extracted_arguments": {
                "image": "The input image containing the person performing the exercise."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_299",
        "query": "A food delivery app wants to help users understand ingredients in the food item by analyzing the images. We need a solution to process the food images and give textual information about the items.",
        "instruction": "Given an `image-to-text analysis` task, retrieve tools that process image inputs to provide detailed textual descriptions or information about the food items depicted, aligning with the query's requirements for ingredient identification.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Image-to-Text",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                    "api_arguments": {
                        "img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg",
                        "question": "how many dogs are in the picture?"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "PIL",
                        "requests"
                    ],
                    "example_code": {
                        "import_requests": "import requests",
                        "import_PIL": "from PIL import Image",
                        "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration",
                        "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')",
                        "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')",
                        "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')",
                        "generate_output": "out = model.generate(**inputs)",
                        "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"
                    },
                    "performance": {
                        "dataset": "LAION",
                        "accuracy": "Not specified"
                    },
                    "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
                    "id": "huggingface_api_64",
                    "name": "blip2-opt-2.7b"
                },
                "id": "gorilla_huggingface_tool_64",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 1: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 4: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 5: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.",
            "Tool 6: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.",
            "Tool 7: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 8: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.",
            "Tool 9: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.",
            "Tool 10: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.",
            "Tool 11: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.",
            "Tool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.",
            "Tool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 14: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 15: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.",
            "Tool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 17: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 18: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.",
            "Tool 19: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements."
        ],
        "tool_view": {
            "description": "Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.",
                    "required": true,
                    "example": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                },
                "tokenizer": {
                    "type": "string",
                    "description": "The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.",
                    "required": true,
                    "example": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                }
            },
            "response": "The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\n\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\n\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\n\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\n   - **F1 Score**: A balance of precision and recall.\n   - **Precision**: The accuracy of positive predictions.\n   - **Recall**: The model’s ability to detect relevant entities.\n\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.",
            "examples": [
                {
                    "user_intent": "I need to identify food-related entities in a sentence about a meal.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Detect food items mentioned in a casual social media post about food.",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                },
                {
                    "user_intent": "Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'",
                    "tool_call": {
                        "name": "Dizex/InstaFoodRoBERTa-NER",
                        "arguments": {
                            "model": "AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')",
                            "tokenizer": "AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 1: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 2: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 3: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 4: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 5: The \"microsoft/git-base-vqav2\" is a model fine-tuned for visual question answering (VQA), designed to generate textual answers based on images and related questions. It utilizes the GenerativeImage2Text (GIT) model, a transformer-based architecture, and is trained on the VQAv2 dataset. This tool is used to extract information from images by answering questions about their content, leveraging the Hugging Face Transformers framework.\nTool 6: This tool provides a visual question answering (VQA) model that answers questions about images using the Hugging Face Transformers library. It utilizes the `temp_vilt_vqa` model and tokenizer to process and interpret visual content in response to text-based queries.\nTool 7: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 8: A Hugging Face Transformers image-to-text model that generates captions for images, providing a proof-of-concept solution for automated image description using the COCO dataset.\nTool 9: Perform zero-shot image classification using a CLIP ViT-B/32 model trained on the LAION-2B dataset, allowing users to classify images into arbitrary categories without task-specific training. This tool is useful for exploring and analyzing images in a flexible, label-independent way.\nTool 10: A LayoutLMv2-based model fine-tuned on the DocVQA dataset that enables question answering directly from documents, combining text and layout information to understand and extract answers from structured document content.\nTool 11: The `microsoft/trocr-base-printed` tool is a Transformer-based OCR model that converts images of printed text into machine-readable text. It uses an image encoder and text decoder architecture to accurately recognize and transcribe text from images, making it useful for digitizing printed documents and extracting textual information from visual data.\nTool 12: A Visual Question Answering tool that uses the JosephusCheung/GuanacoVQAOnConsumerHardware model to answer questions about images, optimized to run efficiently on consumer hardware such as Colab’s free GPU. It enables users to provide an image and a question to receive relevant visual insights.\nTool 13: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 14: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 15: This tool provides a Transformer-based optical character recognition (OCR) model that converts images of printed text into machine-readable text. It uses an image encoder to process the visual content and a text decoder to generate the corresponding textual output, making it suitable for extracting text from scanned documents, receipts, or other printed materials.\nTool 16: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 17: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 18: Donut-base-finetuned-cord-v2 is a multimodal image-to-text model that converts images of documents into structured text. It uses a Swin Transformer as a vision encoder to extract image embeddings and a BART-based decoder to generate text autoregressively. Fine-tuned on the CORD document parsing dataset, it is designed for extracting textual information from images of forms, receipts, or other documents.\nTool 19: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The pre-trained token classification model to load for Named Entity Recognition tasks. This model has been fine-tuned for food entity recognition in informal text.\", \"required\": true, \"example\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}, \"tokenizer\": {\"type\": \"string\", \"description\": \"The tokenizer associated with the pre-trained model, used to convert text into token IDs suitable for model input.\", \"required\": true, \"example\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}, \"response\": \"The tool is designed for Named Entity Recognition (NER) in the domain of food-related entities, particularly focusing on informal text such as social media posts. The response format from this tool would typically contain the following components:\\n\\n1. **Named Entities**: A list of identified entities in the input text, specifically food-related entities. Each entity would have:\\n   - **Entity Type**: The type or category of the entity, which, in this case, would be 'FOOD' for food-related terms.\\n   - **Start and End Position**: The positions in the input text where the entity starts and ends.\\n   - **Entity Label**: A label that categorizes the entity, which would typically be the word 'FOOD' in this case.\\n   - **Confidence/Score**: The model's confidence score for the classification of the entity.\\n\\n2. **Aggregation Strategy**: The results may be aggregated based on the specified strategy (e.g., 'simple'), which could determine whether the tool returns entities in a raw form or in a more processed, simplified format.\\n\\n3. **Performance Metrics**: Although the response to a specific input may not include these directly, performance statistics such as accuracy, precision, recall, and F1 score are part of the tool's performance metrics and could be referenced to give a sense of the model's quality and reliability. These include:\\n   - **F1 Score**: A balance of precision and recall.\\n   - **Precision**: The accuracy of positive predictions.\\n   - **Recall**: The model\\u2019s ability to detect relevant entities.\\n\\nIn summary, the tool's response would typically consist of a list of recognized food entities from the input text along with relevant information such as their positions, labels, and confidence scores. The response would reflect the model's ability to identify food-related terms in informal language such as Instagram posts.\", \"examples\": [{\"user_intent\": \"I need to identify food-related entities in a sentence about a meal.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Detect food items mentioned in a casual social media post about food.\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}, {\"user_intent\": \"Classify the food entities in this Instagram-style text: 'Today's meal: Fresh olive pok bowl topped with chia seeds.'\", \"tool_call\": {\"name\": \"Dizex/InstaFoodRoBERTa-NER\", \"arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}}}]}\n\n# User Query\nA food delivery app wants to help users understand ingredients in the food item by analyzing the images. We need a solution to process the food images and give textual information about the items.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to identify the food items present in an image.",
                    "tool_description": "Analyze the image of a food item using a model trained for food-related entity recognition to extract and classify ingredients or food components.",
                    "expected_response": "A list of identified food entities within the image, each with a label, confidence score, and optionally positional information if relevant."
                },
                {
                    "user_intent": "Provide textual descriptions or ingredient information for the recognized food items.",
                    "tool_description": "Convert the visual representation of the food items into descriptive text, highlighting the ingredients and components detected in the image.",
                    "expected_response": "Textual information detailing the food entities detected in the image, suitable for display to app users."
                }
            ],
            "extracted_arguments": {
                "food_image": "An image file containing the food item to analyze.",
                "model": "The pre-trained food entity recognition model capable of processing images or associated text.",
                "tokenizer": "The tokenizer associated with the model, used to prepare text inputs if required."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_382",
        "query": "Create Japanese audio from the following text: \"こんにちは、私たちはあなたの助けが必要です。\"",
        "instruction": "Given a `text-to-speech` task, retrieve tools that convert text inputs into audio outputs in Japanese by leveraging models capable of generating natural-sounding speech aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')",
                    "api_arguments": "text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.",
                    "id": "huggingface_api_741",
                    "name": "kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804"
                },
                "id": "gorilla_huggingface_tool_736",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "Tool 1: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.",
            "Tool 2: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 4: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.",
            "Tool 5: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.",
            "Tool 6: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.",
            "Tool 7: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.",
            "Tool 8: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.",
            "Tool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.",
            "Tool 10: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.",
            "Tool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.",
            "Tool 12: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 13: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 14: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.",
            "Tool 17: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 19: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning."
        ],
        "tool_view": {
            "description": "A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.",
            "parameters": {},
            "response": "The tool produces a list of predicted token substitutions for a masked position in a Japanese text input. Each item in the response typically includes the following components:\n\n1. **`sequence`** – The full text resulting from replacing the masked token with the predicted token. This shows the completed sentence as interpreted by the model.\n\n2. **`score`** – A numerical value representing the model’s confidence or probability assigned to the predicted token at the masked position. Higher scores indicate higher likelihood according to the model.\n\n3. **`token`** – The integer ID corresponding to the predicted token in the model’s vocabulary.\n\n4. **`token_str`** – The textual representation of the predicted token, showing what actual character(s) would fill the masked position.\n\nThe response is ordered by confidence, with the highest-scoring predictions appearing first. It allows downstream applications to select one or multiple likely replacements for the masked token in Japanese text.",
            "examples": [
                {
                    "user_intent": "I want to complete a Japanese sentence by filling in the missing word.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-char",
                        "arguments": {
                            "text": "[MASK]を食べる"
                        }
                    }
                },
                {
                    "user_intent": "Can you help me predict the masked word in this Japanese sentence?",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-char",
                        "arguments": {
                            "text": "私は[MASK]に行く予定です。"
                        }
                    }
                },
                {
                    "user_intent": "Fill the missing part of a Japanese sentence for me using the BERT model.",
                    "tool_call": {
                        "name": "cl-tohoku/bert-base-japanese-char",
                        "arguments": {
                            "text": "彼は[MASK]を読んでいます。"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\nTool 1: A BERT-based Japanese language model designed for masked language modeling, using word-level tokenization with the IPA dictionary and WordPiece subword tokenization, trained with whole word masking to predict missing words in Japanese text.\nTool 2: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 3: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 4: A specialized OCR tool designed for recognizing Japanese text in manga, capable of accurately reading vertical and horizontal text, text with furigana, text overlaid on images, and a wide variety of fonts and styles, even in low-quality images. It uses a Vision Encoder-Decoder framework and can also serve as a general-purpose printed Japanese OCR, providing robust text extraction in diverse manga-specific scenarios.\nTool 5: This tool uses the pre-trained BERT model \"cl-tohoku/bert-base-japanese\" to perform fill-mask tasks on Japanese text. It processes input using word-level tokenization based on the IPA dictionary, followed by WordPiece subword tokenization. This is useful for tasks like predicting missing words in a sentence when given a masked token.\nTool 6: This tool provides speech recognition for Japanese using a fine-tuned version of the XLSR-53 large model, based on Hugging Face Transformers. It transcribes audio files (e.g., MP3, WAV) into text and is designed to work with audio sampled at 16kHz. The model, trained on datasets like Common Voice, CSS10, and JSUT, achieves an accuracy of 81.8% word error rate (WER) and 20.16% character error rate (CER). It requires libraries such as `huggingsound`, `torch`, `librosa`, `datasets`, and `transformers` in a Python environment.\nTool 7: This tool is a Japanese text-to-speech model, built using the ESPnet framework, that converts text input into natural-sounding speech. It utilizes the \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\" model for efficient speech synthesis.\nTool 8: Openjourney is an open-source Stable Diffusion model fine-tuned on Midjourney images that generates AI art from text prompts, allowing users to create high-quality, stylized images by providing descriptive text inputs.\nTool 9: Openjourney v4 is a text-to-image model that generates images based on textual descriptions, trained on over 124,000 Midjourney v4 images. It is used to create visual content from natural language prompts.\nTool 10: Dreamlike Diffusion 1.0 is a text-to-image model fine-tuned on high-quality art that generates detailed and stylistic images from textual prompts, enabling users to create visually rich and imaginative artwork programmatically.\nTool 11: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality, detailed images from textual prompts, allowing users to create realistic portraits, landscapes, and other visual content with precise control over visual elements.\nTool 12: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 13: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 14: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 15: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 16: Protogen x5.8 is a text-to-image model that generates high-quality images from descriptive text prompts. Built on Stable Diffusion and enhanced with dreamlikePhotoRealV2.ckpt, it uses adaptive learning techniques for fine-grained image details, enabling photorealistic and highly detailed visual outputs. This model can be used like any other Stable Diffusion pipeline to create artwork, concept designs, or illustrations directly from textual descriptions.\nTool 17: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 18: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 19: A fine-tuned Dutch speech recognition model based on XLSR-53 that transcribes audio into text. It is optimized for Dutch using Common Voice and CSS10 datasets and can be used to convert spoken Dutch from audio files into accurate text, supporting applications like transcription, voice interfaces, and automated captioning.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A BERT-based model pretrained on Japanese text that predicts missing words in a sentence. It uses word-level tokenization with the IPA dictionary followed by character-level tokenization, making it suitable for tasks like masked word prediction or text completion in Japanese.\", \"parameters\": {}, \"response\": \"The tool produces a list of predicted token substitutions for a masked position in a Japanese text input. Each item in the response typically includes the following components:\\n\\n1. **`sequence`** \\u2013 The full text resulting from replacing the masked token with the predicted token. This shows the completed sentence as interpreted by the model.\\n\\n2. **`score`** \\u2013 A numerical value representing the model\\u2019s confidence or probability assigned to the predicted token at the masked position. Higher scores indicate higher likelihood according to the model.\\n\\n3. **`token`** \\u2013 The integer ID corresponding to the predicted token in the model\\u2019s vocabulary.\\n\\n4. **`token_str`** \\u2013 The textual representation of the predicted token, showing what actual character(s) would fill the masked position.\\n\\nThe response is ordered by confidence, with the highest-scoring predictions appearing first. It allows downstream applications to select one or multiple likely replacements for the masked token in Japanese text.\", \"examples\": [{\"user_intent\": \"I want to complete a Japanese sentence by filling in the missing word.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-char\", \"arguments\": {\"text\": \"[MASK]\\u3092\\u98df\\u3079\\u308b\"}}}, {\"user_intent\": \"Can you help me predict the masked word in this Japanese sentence?\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-char\", \"arguments\": {\"text\": \"\\u79c1\\u306f[MASK]\\u306b\\u884c\\u304f\\u4e88\\u5b9a\\u3067\\u3059\\u3002\"}}}, {\"user_intent\": \"Fill the missing part of a Japanese sentence for me using the BERT model.\", \"tool_call\": {\"name\": \"cl-tohoku/bert-base-japanese-char\", \"arguments\": {\"text\": \"\\u5f7c\\u306f[MASK]\\u3092\\u8aad\\u3093\\u3067\\u3044\\u307e\\u3059\\u3002\"}}}]}\n\n# User Query\nCreate Japanese audio from the following text: \"こんにちは、私たちはあなたの助けが必要です。\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Convert Japanese text into natural-sounding speech audio.",
                    "tool_description": "Generate speech from Japanese text using a text-to-speech model, producing audio output that reflects natural pronunciation and intonation.",
                    "expected_response": "An audio file or stream containing the synthesized Japanese speech corresponding to the input text."
                }
            ],
            "extracted_arguments": {
                "text_input": "The Japanese text string that needs to be converted into speech."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_134",
        "query": "Let's build a Chatbot that can solve daily life problems of a research Assistant.",
        "instruction": "Given a `conversational AI building` task, retrieve tools that can develop a chatbot capable of understanding and solving daily life problems by utilizing pre-trained natural language processing models optimized for multi-turn dialogue generation. Ensure the tools support conversation modeling and response generation aligned with the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Generation",
                    "framework": "Transformers",
                    "functionality": "Conversational",
                    "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')",
                    "api_arguments": [
                        "bot_input_ids",
                        "max_length",
                        "pad_token_id"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)",
                    "performance": {
                        "dataset": "Reddit discussion thread",
                        "accuracy": "Comparable to human response quality under a single-turn conversation Turing test"
                    },
                    "description": "DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multi-turn conversations. The model is trained on 147M multi-turn dialogues from Reddit discussion threads.",
                    "id": "huggingface_api_577",
                    "name": "microsoft/DialoGPT-large"
                },
                "id": "gorilla_huggingface_tool_573",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 2: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.",
            "Tool 3: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 5: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.",
            "Tool 6: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 7: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 8: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.",
            "Tool 9: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.",
            "Tool 10: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 11: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.",
            "Tool 12: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.",
            "Tool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 14: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.",
            "Tool 15: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.",
            "Tool 16: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.",
            "Tool 17: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.",
            "Tool 18: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 19: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research."
        ],
        "tool_view": {
            "description": "BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "parameters": {
                "message": {
                    "type": "string",
                    "description": "The input text message from the user that the chatbot will respond to.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces a text-based conversational response generated by the BlenderBot-400M-distill model. The response structure is a single text string representing the model’s reply to the user-provided input message. Semantically, this response is intended to continue the dialogue in an open-domain conversational manner, reflecting understanding of the input message and generating coherent, contextually relevant text.  \n\nKey components and their meaning:  \n\n- **message input (api_arguments: 'message')**: The user-provided text that serves as the prompt or query for the chatbot.  \n- **response text**: A string generated by the model that constitutes the chatbot’s reply. This is the primary output and represents the model’s interpretation and continuation of the conversation.  \n- **conversational context (implicit, optional)**: If the tool maintains multi-turn context, the response may implicitly incorporate prior dialogue history to produce coherent and context-aware replies.  \n\nThe tool does not explicitly return structured metadata, confidence scores, or other auxiliary fields in its default usage; the response is solely the generated text output suitable for conversational interaction.",
            "examples": [
                {
                    "user_intent": "I want to have a casual chat and get some friendly advice about productivity.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "Hi! Do you have any tips for staying productive while working from home?"
                        }
                    }
                },
                {
                    "user_intent": "Ask a fun, general knowledge question to see how the chatbot responds.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "Can you tell me a fun fact about space?"
                        }
                    }
                },
                {
                    "user_intent": "Engage the chatbot in a light-hearted conversation about movies.",
                    "tool_call": {
                        "name": "facebook/blenderbot-400M-distill",
                        "arguments": {
                            "message": "What's your favorite movie genre, and why?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 1: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 2: This tool is a fine-tuned video classification model based on the VideoMAE architecture, designed to identify real-life violent situations in videos. It uses the Hugging Face Transformers framework and provides a pretrained model for accurate and efficient detection of violent content in video data.\nTool 3: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 5: A compact and efficient DistilBART model for summarizing English text, trained on the CNN/Daily Mail and XSum datasets. It provides fast, high-quality text summarization using a smaller version of BART, making it suitable for generating concise summaries of longer documents.\nTool 6: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 7: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 8: BlenderBot-90M is a conversational AI model by Facebook AI designed to generate engaging, human-like responses in multi-turn dialogues. It is trained on the Blended Skill Talk dataset and can be used to build chatbots or interactive dialogue systems, taking a text input and producing context-aware conversational replies. Although this specific version is deprecated, it demonstrates how transformer-based models can facilitate natural language conversations.\nTool 9: A tool for generating concise summaries of long texts using the BART large model fine-tuned on the CNN Daily Mail dataset. It leverages a transformer-based encoder-decoder architecture to produce coherent, human-like summaries, making it useful for quickly extracting key information from articles, reports, or other lengthy documents.\nTool 10: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 11: This tool provides a pre-trained DQN reinforcement learning agent for the Acrobot-v1 environment using the Stable-Baselines3 library and the RL Zoo framework, allowing users to quickly evaluate or build upon a tested agent without training from scratch.\nTool 12: facebook/opt-125m is a pre-trained transformer model designed for generating coherent and contextually relevant text. It can produce text continuations or responses based on a given prompt, making it useful for natural language generation tasks, research, and evaluation of downstream applications.\nTool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 14: Cerebras-GPT-111M is a 111-million-parameter transformer-based language model designed for text generation and natural language processing research. Trained on the Pile dataset using a GPT-3 style architecture, it can serve as a foundation model for NLP applications, fine-tuning, and studies in ethics and alignment.\nTool 15: The `bigscience/bloom-560m` tool is a transformer-based multilingual language model designed for text generation. It can generate coherent and contextually relevant text in 45 natural languages and 12 programming languages, making it useful for applications like content creation, code generation, and language modeling research.\nTool 16: TF_Decision_Trees uses TensorFlow to build gradient-boosted decision forest models for binary classification of structured tabular data. It encodes categorical features relative to their target value co-occurrences and trains a model to predict binary outcomes, such as determining whether an individual earns over $50k annually. This tool is designed for high-accuracy classification tasks on datasets with mixed numerical and categorical features.\nTool 17: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that creates images from text prompts. It uses a pretrained text encoder (OpenCLIP-ViT/H) and is designed for research purposes, enabling exploration of model deployment, generative model limitations, biases, and artwork generation. This model is particularly useful for tasks in multimodal research and creative applications.\nTool 18: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 19: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\", \"parameters\": {\"message\": {\"type\": \"string\", \"description\": \"The input text message from the user that the chatbot will respond to.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces a text-based conversational response generated by the BlenderBot-400M-distill model. The response structure is a single text string representing the model\\u2019s reply to the user-provided input message. Semantically, this response is intended to continue the dialogue in an open-domain conversational manner, reflecting understanding of the input message and generating coherent, contextually relevant text.  \\n\\nKey components and their meaning:  \\n\\n- **message input (api_arguments: 'message')**: The user-provided text that serves as the prompt or query for the chatbot.  \\n- **response text**: A string generated by the model that constitutes the chatbot\\u2019s reply. This is the primary output and represents the model\\u2019s interpretation and continuation of the conversation.  \\n- **conversational context (implicit, optional)**: If the tool maintains multi-turn context, the response may implicitly incorporate prior dialogue history to produce coherent and context-aware replies.  \\n\\nThe tool does not explicitly return structured metadata, confidence scores, or other auxiliary fields in its default usage; the response is solely the generated text output suitable for conversational interaction.\", \"examples\": [{\"user_intent\": \"I want to have a casual chat and get some friendly advice about productivity.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"Hi! Do you have any tips for staying productive while working from home?\"}}}, {\"user_intent\": \"Ask a fun, general knowledge question to see how the chatbot responds.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"Can you tell me a fun fact about space?\"}}}, {\"user_intent\": \"Engage the chatbot in a light-hearted conversation about movies.\", \"tool_call\": {\"name\": \"facebook/blenderbot-400M-distill\", \"arguments\": {\"message\": \"What's your favorite movie genre, and why?\"}}}]}\n\n# User Query\nLet's build a Chatbot that can solve daily life problems of a research Assistant.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a chatbot that can provide advice and solutions for daily life problems of a research assistant.",
                    "tool_description": "Deploy a conversational AI model to generate context-aware text responses suitable for addressing practical questions and issues faced by a research assistant.",
                    "expected_response": "A text-based conversational response that offers guidance, suggestions, or solutions related to the tasks and challenges of a research assistant."
                }
            ],
            "extracted_arguments": {
                "user_message": "The input text from the research assistant describing their problem or question, as a string."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_275",
        "query": "I am the director of a movie theater chain, and we are thinking of implementing a chat product to get movie goers to classify movie reviews as positive or negative through AI.",
        "instruction": "Given a `sentiment analysis implementation` task, retrieve tools that facilitate AI-driven categorization of movie reviews as positive or negative by leveraging machine learning models capable of binary classification on text data.",
        "labels": [
            {
                "doc": {
                    "domain": "Tabular Tabular Classification",
                    "framework": "Hugging Face",
                    "functionality": "Binary Classification",
                    "api_call": "joblib.load('model.joblib')",
                    "api_arguments": [
                        "data"
                    ],
                    "python_environment_requirements": [
                        "joblib",
                        "pandas"
                    ],
                    "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)",
                    "performance": {
                        "dataset": "desertdev/autotrain-data-imdb-sentiment-analysis",
                        "accuracy": 0.565
                    },
                    "description": "A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.",
                    "id": "huggingface_api_858",
                    "name": "desertdev/autotrain-imdb-sentiment-analysis-44994113085"
                },
                "id": "gorilla_huggingface_tool_851",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 2: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 3: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 4: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.",
            "Tool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 6: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 7: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.",
            "Tool 8: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.",
            "Tool 9: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.",
            "Tool 10: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.",
            "Tool 11: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 12: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.",
            "Tool 14: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.",
            "Tool 15: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 16: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.",
            "Tool 17: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 18: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.",
            "Tool 19: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations."
        ],
        "tool_view": {
            "description": "A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "parameters": {},
            "response": "The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \n\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \n- **score**: A numeric value representing the model’s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \n\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers’ sentiment-analysis pipeline.",
            "examples": [
                {
                    "user_intent": "Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "The plot was captivating and the acting was superb."
                        }
                    }
                },
                {
                    "user_intent": "I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "I couldn't stand the boring dialogue and slow pacing."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of a short review: 'It was an average movie, nothing special.'",
                    "tool_call": {
                        "name": "lvwerra/distilbert-imdb",
                        "arguments": {
                            "text": "It was an average movie, nothing special."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 1: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 2: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 3: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 4: A RoBERTa-based model fine-tuned for sentiment analysis on English tweets, designed to classify text as positive, negative, or neutral, making it useful for analyzing public opinion or social media content.\nTool 5: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 6: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 7: JohnBot is a conversational AI model based on the GPT-2 architecture, designed to generate text responses in a chat interface using the Hugging Face Transformers library.\nTool 8: A conversational AI model that mimics Elon Musk’s style, trained on his Twitter posts, allowing users to chat interactively with responses that reflect his typical tone and phrasing.\nTool 9: A Spanish-language sentiment analysis tool based on the BETO BERT model, trained on the TASS 2020 corpus of approximately 5,000 tweets across various Spanish dialects, which classifies text into positive, negative, or neutral sentiment.\nTool 10: A text classification tool that performs sentiment analysis on English tweets using the BERTweet model, categorizing text as positive, negative, or neutral. It is trained on the SemEval 2017 dataset and is designed to quickly assess sentiment in social media content.\nTool 11: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 12: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 13: A PyTorch-based conversational AI tool that uses a DialoGPT model fine-tuned on the dialogue of the game character Joshua from *The World Ends With You* to generate context-aware, human-like text responses in interactive chat applications.\nTool 14: Stable Diffusion v2 Depth is a text-to-image model that generates and modifies images based on English text prompts, optionally using an initial image as a starting point. It allows users to guide the output with positive and negative prompts and adjust the influence of the text on the final image, making it suitable for creative image synthesis and research applications.\nTool 15: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 16: A computer vision tool that uses a BEiT-based Hugging Face Transformers model to classify anime artwork and determine whether it was created by an AI or a human, achieving high accuracy on datasets from aibooru and imageboard sites.\nTool 17: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 18: The \"Jean-Baptiste/roberta-large-ner-english\" is a named entity recognition (NER) model based on the RoBERTa-large architecture, fine-tuned on the CONLL2003 dataset. It is optimized for recognizing entities in English text, with a specific advantage in handling entities that don't start with uppercase letters. This model performs exceptionally well on emails and chat data, outperforming other models in this context. It is available for use through the Hugging Face Transformers library, requiring Python’s `transformers` package version 4.0.0 or higher.\nTool 19: BaekBot is a GPT-2–based conversational model designed to generate human-like responses in chat environments, enabling natural and interactive text-based conversations.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\", \"parameters\": {}, \"response\": \"The tool returns a structured list of classification results for the input text. Each element in the list represents a single prediction and typically contains the following components:  \\n\\n- **label**: A string indicating the predicted class for the input text. In this case, the labels correspond to sentiment categories (e.g., positive or negative sentiment for movie reviews).  \\n- **score**: A numeric value representing the model\\u2019s confidence in the prediction. This is usually a floating-point number between 0 and 1, where higher values indicate greater confidence.  \\n\\nThe response can contain multiple predictions if batch inputs are provided, with each input text producing its own corresponding result object. The structure is consistent with the output of Hugging Face Transformers\\u2019 sentiment-analysis pipeline.\", \"examples\": [{\"user_intent\": \"Analyze the sentiment of this movie review: 'The plot was captivating and the acting was superb.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"The plot was captivating and the acting was superb.\"}}}, {\"user_intent\": \"I want to see if this film review is positive or negative: 'I couldn't stand the boring dialogue and slow pacing.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"I couldn't stand the boring dialogue and slow pacing.\"}}}, {\"user_intent\": \"Check the sentiment of a short review: 'It was an average movie, nothing special.'\", \"tool_call\": {\"name\": \"lvwerra/distilbert-imdb\", \"arguments\": {\"text\": \"It was an average movie, nothing special.\"}}}]}\n\n# User Query\nI am the director of a movie theater chain, and we are thinking of implementing a chat product to get movie goers to classify movie reviews as positive or negative through AI.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to classify movie reviews as positive or negative using AI.",
                    "tool_description": "Use a fine-tuned DistilBERT-based sentiment analysis model to classify movie review text into positive or negative categories.",
                    "expected_response": "A structured list of classification results, each containing a predicted sentiment label and a confidence score."
                },
                {
                    "user_intent": "Integrate this sentiment analysis capability into a chat interface for movie goers.",
                    "tool_description": "Deploy the sentiment analysis model within a conversational AI system to allow users to input reviews and receive automatic sentiment classifications in real-time chat.",
                    "expected_response": "Don't need response."
                }
            ],
            "extracted_arguments": {
                "review_text": "The text of a movie review provided by a user, in string format.",
                "user_input": "The message or query from the chat participant, in string format."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_287",
        "query": "A teacher wants to create a quiz for her students. We are now working on the questions and answers for the quiz that be arranged in a table format.",
        "instruction": "Given a `quiz creation` task, retrieve tools that facilitate the development of quiz content by supporting the compilation of questions and answers into a structured table format, tailored to meet educational requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "PyTorch Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "pipeline('table-question-answering', model='Meena/table-question-answering-tapas')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "This model can be loaded on the Inference API on-demand.",
                    "performance": {
                        "dataset": [
                            {
                                "name": "SQA (Sequential Question Answering by Microsoft)",
                                "accuracy": null
                            },
                            {
                                "name": "WTQ (Wiki Table Questions by Stanford University)",
                                "accuracy": null
                            },
                            {
                                "name": "WikiSQL (by Salesforce)",
                                "accuracy": null
                            }
                        ]
                    },
                    "description": "TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.",
                    "id": "huggingface_api_452",
                    "name": "table-question-answering-tapas"
                },
                "id": "gorilla_huggingface_tool_448",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "Tool 1: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 2: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.",
            "Tool 3: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.",
            "Tool 4: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.",
            "Tool 5: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 7: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.",
            "Tool 8: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 10: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 11: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.",
            "Tool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 13: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.",
            "Tool 14: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.",
            "Tool 15: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 16: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 18: TAPEX is a transformer-based model designed for answering natural language questions over structured tables by generating and executing SQL queries. Built on the BART architecture, it enables table reasoning by learning from a synthetic corpus of executable SQL queries, allowing users to input a table and a query in natural language and receive accurate, structured responses.",
            "Tool 19: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval."
        ],
        "tool_view": {
            "description": "A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name of the pre-trained model to use for the table question answering task. This model will be responsible for processing the input table and query.",
                    "required": true,
                    "example": "google/tapas-large-finetuned-wtq"
                },
                "task": {
                    "type": "string",
                    "description": "The specific task to perform. In this case, it is set to 'table-question-answering', which signals the model to answer questions based on table data.",
                    "required": true,
                    "example": "table-question-answering"
                }
            },
            "response": "The tool returns a structured response representing the answer to a natural language question based on the content of a table. The typical response has the following components:\n\n1. **`answer`** – A string containing the predicted answer to the input query. This is usually extracted or inferred from the table data and may be a single value, a phrase, or a list depending on the table structure and question type.  \n\n2. **`coordinates`** *(optional, depending on implementation)* – Information indicating the location(s) of the cell(s) in the table that were used to generate the answer. This can include row and column indices or other positional markers within the table.  \n\n3. **`cells`** *(optional)* – A list of the actual table cell values that contributed to the answer, allowing verification or further processing.  \n\n4. **`aggregation`** *(optional)* – If the answer involves aggregation (e.g., sum, average, count), this field may indicate the type of aggregation applied to the selected cells.  \n\nIn principle, the response provides both the final textual answer and metadata about how the model derived it from the table, supporting interpretability and downstream use.",
            "examples": [
                {
                    "user_intent": "What is the population of Germany according to this table?",
                    "tool_call": {
                        "name": "google/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Country",
                                    "Capital",
                                    "Population"
                                ],
                                "rows": [
                                    [
                                        "Germany",
                                        "Berlin",
                                        "83 million"
                                    ],
                                    [
                                        "France",
                                        "Paris",
                                        "67 million"
                                    ],
                                    [
                                        "Italy",
                                        "Rome",
                                        "60 million"
                                    ]
                                ]
                            },
                            "query": "What is the population of Germany?"
                        }
                    }
                },
                {
                    "user_intent": "Which company has the highest revenue in this quarterly report?",
                    "tool_call": {
                        "name": "google/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Company",
                                    "Q1 Revenue (in millions)",
                                    "Q2 Revenue (in millions)"
                                ],
                                "rows": [
                                    [
                                        "Alpha Corp",
                                        "120",
                                        "130"
                                    ],
                                    [
                                        "Beta Inc",
                                        "150",
                                        "140"
                                    ],
                                    [
                                        "Gamma LLC",
                                        "110",
                                        "160"
                                    ]
                                ]
                            },
                            "query": "Which company has the highest revenue?"
                        }
                    }
                },
                {
                    "user_intent": "List all products that cost more than $50 from this price list.",
                    "tool_call": {
                        "name": "google/tapas-large-finetuned-wtq",
                        "arguments": {
                            "table": {
                                "columns": [
                                    "Product",
                                    "Category",
                                    "Price"
                                ],
                                "rows": [
                                    [
                                        "Wireless Mouse",
                                        "Electronics",
                                        "45"
                                    ],
                                    [
                                        "Mechanical Keyboard",
                                        "Electronics",
                                        "75"
                                    ],
                                    [
                                        "USB Cable",
                                        "Accessories",
                                        "10"
                                    ],
                                    [
                                        "Monitor Stand",
                                        "Furniture",
                                        "55"
                                    ]
                                ]
                            },
                            "query": "Which products cost more than 50 dollars?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\nTool 1: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 2: TAPAS is a transformer-based model designed for answering questions over tabular data. It is pretrained on English Wikipedia text and fine-tuned to predict cell selections and aggregations in tables, enabling users to provide a table and a natural-language question and receive accurate answers from the table’s content.\nTool 3: TAPEX is a transformer-based model fine-tuned for answering natural language questions over structured tables. It uses a BART architecture to encode a table and a query, generating precise answers by leveraging learned table reasoning skills, making it useful for tasks that require extracting information or performing computations directly from tabular data.\nTool 4: A TAPAS small model fine-tuned for table question answering on the WikiTable Questions (WTQ) dataset, designed to extract answers from structured tables based on natural language queries. It leverages relative position embeddings to handle table structure and can predict both cell locations and aggregation operations, enabling automated reasoning over tabular data.\nTool 5: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 6: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 7: A compact TAPAS model designed for answering questions based on tabular data, suitable for lightweight table question-answering tasks.\nTool 8: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 10: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 11: This tool uses the TAPAS model, a BERT-based transformer designed for answering questions about tabular data. It processes tables and associated text to extract features that enable question answering, entailment detection, and reasoning over table contents. TAPAS encodes table structure with specialized token types and relative position embeddings and is pre-trained on millions of tables and texts, making it suitable for natural language table question-answering tasks.\nTool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 13: A large TAPAS model fine-tuned for answering questions over tables, capable of interpreting table structure and cell relationships to provide accurate responses to sequential question-answering tasks.\nTool 14: TAPEX is a transformer-based model fine-tuned for table question answering, enabling users to query structured tables in natural language and receive precise answers. It leverages pre-trained table reasoning skills to interpret and generate responses from tabular data, making it useful for tasks that require extracting or computing information from tables.\nTool 15: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 16: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 17: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 18: TAPEX is a transformer-based model designed for answering natural language questions over structured tables by generating and executing SQL queries. Built on the BART architecture, it enables table reasoning by learning from a synthetic corpus of executable SQL queries, allowing users to input a table and a query in natural language and receive accurate, structured responses.\nTool 19: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A transformer-based model designed for table question answering, fine-tuned on WikiTable Questions (WTQ), that interprets queries over tabular data to provide precise answers by leveraging relative position embeddings and structured table understanding.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name of the pre-trained model to use for the table question answering task. This model will be responsible for processing the input table and query.\", \"required\": true, \"example\": \"google/tapas-large-finetuned-wtq\"}, \"task\": {\"type\": \"string\", \"description\": \"The specific task to perform. In this case, it is set to 'table-question-answering', which signals the model to answer questions based on table data.\", \"required\": true, \"example\": \"table-question-answering\"}}, \"response\": \"The tool returns a structured response representing the answer to a natural language question based on the content of a table. The typical response has the following components:\\n\\n1. **`answer`** \\u2013 A string containing the predicted answer to the input query. This is usually extracted or inferred from the table data and may be a single value, a phrase, or a list depending on the table structure and question type.  \\n\\n2. **`coordinates`** *(optional, depending on implementation)* \\u2013 Information indicating the location(s) of the cell(s) in the table that were used to generate the answer. This can include row and column indices or other positional markers within the table.  \\n\\n3. **`cells`** *(optional)* \\u2013 A list of the actual table cell values that contributed to the answer, allowing verification or further processing.  \\n\\n4. **`aggregation`** *(optional)* \\u2013 If the answer involves aggregation (e.g., sum, average, count), this field may indicate the type of aggregation applied to the selected cells.  \\n\\nIn principle, the response provides both the final textual answer and metadata about how the model derived it from the table, supporting interpretability and downstream use.\", \"examples\": [{\"user_intent\": \"What is the population of Germany according to this table?\", \"tool_call\": {\"name\": \"google/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"columns\": [\"Country\", \"Capital\", \"Population\"], \"rows\": [[\"Germany\", \"Berlin\", \"83 million\"], [\"France\", \"Paris\", \"67 million\"], [\"Italy\", \"Rome\", \"60 million\"]]}, \"query\": \"What is the population of Germany?\"}}}, {\"user_intent\": \"Which company has the highest revenue in this quarterly report?\", \"tool_call\": {\"name\": \"google/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"columns\": [\"Company\", \"Q1 Revenue (in millions)\", \"Q2 Revenue (in millions)\"], \"rows\": [[\"Alpha Corp\", \"120\", \"130\"], [\"Beta Inc\", \"150\", \"140\"], [\"Gamma LLC\", \"110\", \"160\"]]}, \"query\": \"Which company has the highest revenue?\"}}}, {\"user_intent\": \"List all products that cost more than $50 from this price list.\", \"tool_call\": {\"name\": \"google/tapas-large-finetuned-wtq\", \"arguments\": {\"table\": {\"columns\": [\"Product\", \"Category\", \"Price\"], \"rows\": [[\"Wireless Mouse\", \"Electronics\", \"45\"], [\"Mechanical Keyboard\", \"Electronics\", \"75\"], [\"USB Cable\", \"Accessories\", \"10\"], [\"Monitor Stand\", \"Furniture\", \"55\"]]}, \"query\": \"Which products cost more than 50 dollars?\"}}}]}\n\n# User Query\nA teacher wants to create a quiz for her students. We are now working on the questions and answers for the quiz that be arranged in a table format.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Create a table to organize quiz questions and answers.",
                    "tool_description": "Generate a structured table format that can store quiz questions along with their corresponding answers, suitable for processing or display.",
                    "expected_response": "A table object with rows representing each question-answer pair and columns for question text, answer, and optionally metadata such as difficulty or category."
                },
                {
                    "user_intent": "Populate the table with quiz questions and answers.",
                    "tool_description": "Fill the structured table with natural language quiz questions and their respective answers, ensuring proper formatting and alignment within the table.",
                    "expected_response": "A completed table containing all provided quiz questions and answers, ready for further use or export."
                }
            ],
            "extracted_arguments": {
                "questions": "A list of natural language quiz questions to include in the table.",
                "answers": "A list of corresponding answers for each quiz question, matched by index.",
                "table_structure": "The schema of the table including columns for question text, answer, and any additional metadata such as category or difficulty level."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_381",
        "query": "We are working on a language learning app for Chinese. We need to give audio examples for each lesson.",
        "instruction": "Given an `audio example generation` task for a language learning app, retrieve tools that utilize Text-to-Speech capabilities to convert text lessons into audio examples by processing text inputs and producing accurate and clear speech outputs in the specified language.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Text-to-Speech",
                    "framework": "ESPnet",
                    "functionality": "Text-to-Speech",
                    "api_call": "Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "torch",
                        "espnet_model_zoo"
                    ],
                    "example_code": "import soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\ntext = 春江潮水连海平，海上明月共潮生\nspeech = text2speech(text)[wav]\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)",
                    "performance": {
                        "dataset": "csmsc",
                        "accuracy": "Not specified"
                    },
                    "description": "A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.",
                    "id": "huggingface_api_737",
                    "name": "kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best"
                },
                "id": "gorilla_huggingface_tool_732",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 1: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 2: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.",
            "Tool 3: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 4: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.",
            "Tool 5: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.",
            "Tool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.",
            "Tool 9: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.",
            "Tool 10: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.",
            "Tool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.",
            "Tool 12: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 13: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.",
            "Tool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 16: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.",
            "Tool 17: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.",
            "Tool 18: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.",
            "Tool 19: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role."
        ],
        "tool_view": {
            "description": "Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "parameters": {
                "images": {
                    "type": "image",
                    "description": "The input image or list of images to be processed by the model for feature extraction and classification.",
                    "required": true,
                    "example": "Image.open('path/to/image.jpg')"
                },
                "text": {
                    "type": "array",
                    "description": "A list of text strings that serve as candidate labels or prompts for zero-shot classification.",
                    "required": true,
                    "example": [
                        "cat",
                        "dog",
                        "car"
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensor format to return from the processor. Commonly 'pt' for PyTorch tensors.",
                    "required": true,
                    "default": "pt",
                    "example": "pt"
                },
                "padding": {
                    "type": "boolean",
                    "description": "Indicates whether to pad the text or image inputs to the same length for batching purposes.",
                    "required": true,
                    "default": true,
                    "example": true
                }
            },
            "response": "The tool produces a structured output containing the results of zero-shot image classification for a given image against a set of candidate text labels. The response structure typically includes the following components:\n\n1. **`logits_per_image`**  \n   - A numeric tensor representing the raw similarity scores between the input image and each candidate text label.  \n   - Each element corresponds to one text label and indicates the degree of alignment between the image and that label in the model’s learned embedding space. Higher values indicate stronger correspondence.  \n\n2. **`probs`**  \n   - A numeric tensor containing the normalized probabilities obtained by applying a softmax function over `logits_per_image`.  \n   - Each probability corresponds to one candidate text label and represents the model’s confidence that the image matches that label.  \n   - The sum of all probabilities equals 1.  \n\n3. **Optional metadata (depending on implementation)**  \n   - Information about the input images and text labels used in the classification, including shapes or identifiers.  \n   - In some cases, the model may also return embeddings for the image (`image_features`) and text (`text_features`), which are normalized vectors in the shared embedding space. These can be used for further similarity calculations or downstream tasks.\n\nIn essence, the response provides both the raw similarity scores and the probabilistic interpretation of which text label best matches the input image in a zero-shot classification setting.",
            "examples": [
                {
                    "user_intent": "I have a photo of a cat and want to find out which Chinese labels best describe it.",
                    "tool_call": {
                        "name": "OFA-Sys/chinese-clip-vit-large-patch14-336px",
                        "arguments": {
                            "images": "cat_photo.jpg",
                            "text": [
                                "猫",
                                "狗",
                                "鸟",
                                "汽车"
                            ],
                            "return_tensors": "pt",
                            "padding": "True"
                        }
                    }
                },
                {
                    "user_intent": "Classify a satellite image to see which land types it contains using Chinese labels.",
                    "tool_call": {
                        "name": "OFA-Sys/chinese-clip-vit-large-patch14-336px",
                        "arguments": {
                            "images": "satellite_image.png",
                            "text": [
                                "森林",
                                "城市",
                                "水域",
                                "沙漠"
                            ],
                            "return_tensors": "pt",
                            "padding": "True"
                        }
                    }
                },
                {
                    "user_intent": "I want to label a set of product photos with the most relevant Chinese text descriptions.",
                    "tool_call": {
                        "name": "OFA-Sys/chinese-clip-vit-large-patch14-336px",
                        "arguments": {
                            "images": [
                                "shirt.jpg",
                                "shoes.jpg",
                                "bag.jpg"
                            ],
                            "text": [
                                "衣服",
                                "鞋子",
                                "包",
                                "饰品"
                            ],
                            "return_tensors": "pt",
                            "padding": "True"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 1: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 2: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\nTool 3: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 4: A Chinese-language question answering model based on RoBERTa with whole word masking, designed for machine reading comprehension tasks. It takes a context and a question as input and predicts the answer, delivering strong performance on Chinese reading comprehension benchmarks such as Dureader-2021.\nTool 5: This tool uses a fine-tuned wav2vec2 model within the SpeechBrain framework to perform emotion recognition from audio files. Trained on the IEMOCAP dataset, it analyzes speech to classify emotions such as anger, happiness, or sadness, providing probabilities and labels for each input. It is designed for applications that require automated detection of emotional states in spoken audio.\nTool 6: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: This tool uses the `facebook/bart-large-mnli` model to perform zero-shot text classification by leveraging natural language inference (NLI). It allows you to classify a text into arbitrary candidate labels without task-specific training by treating the input text as a premise and each label as a hypothesis, then computing the probability that the premise entails each hypothesis. It is useful for quickly categorizing text into custom categories using the Hugging Face Transformers framework.\nTool 9: This tool is a fine-tuned speech recognition model based on the Wav2Vec2 architecture, specifically trained for Chinese (Mandarin) using datasets like Common Voice, CSS10, and ST-CMDS. It converts audio files into transcriptions, making it useful for applications requiring automatic speech-to-text conversion in Chinese. The model can be accessed through the Hugging Face Transformers framework and requires a Python environment with libraries like `huggingsound`, `torch`, and `librosa`.\nTool 10: The `shibing624/text2vec-base-chinese` model is a sentence embedding tool that maps Chinese sentences into a 768-dimensional dense vector space, enabling tasks such as semantic similarity comparison, text matching, and semantic search. It leverages the Hugging Face Transformers framework and is designed to produce vector representations of sentences for natural language processing applications.\nTool 11: This tool provides a Chinese ALBERT model pre-trained on the CLUECorpusSmall dataset, designed for masked language modeling tasks. It can predict missing words in text, enabling applications such as text generation, language understanding, and feature extraction in natural language processing.\nTool 12: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 13: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 14: This tool is a Hokkien-language text-to-speech model based on the Unit HiFiGAN vocoder from Fairseq. It converts written text into natural-sounding speech in the Taiwanese Hokkien accent and was trained on the TAT-TTS dataset featuring four speakers. It is used to generate high-quality audio from text input, making it suitable for applications like speech synthesis, voice assistants, and language learning tools.\nTool 15: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 16: This tool provides a pre-trained BERT-based model for traditional Chinese word segmentation, enabling token-level analysis in natural language processing tasks such as word segmentation, part-of-speech tagging, and named entity recognition.\nTool 17: This tool uses OpenAI’s CLIP model to perform zero-shot image classification, allowing users to categorize images into arbitrary classes without task-specific training. It processes images alongside textual labels and computes the likelihood that each image corresponds to each label, making it useful for general-purpose image recognition and evaluating model robustness across a wide range of visual datasets.\nTool 18: Chinese-CLIP-ViT-Large-Patch14 is a large-scale Chinese CLIP model that combines a ViT-L/14 image encoder with a RoBERTa-based text encoder to perform zero-shot image classification. Trained on approximately 200 million Chinese image-text pairs, it enables matching images with textual descriptions without task-specific fine-tuning, making it useful for tasks such as image-text retrieval and labeling across diverse datasets.\nTool 19: A pre-trained BERT model fine-tuned for English Part-of-Speech (POS) tagging, used to classify each word in text according to its grammatical role.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\", \"parameters\": {\"images\": {\"type\": \"image\", \"description\": \"The input image or list of images to be processed by the model for feature extraction and classification.\", \"required\": true, \"example\": \"Image.open('path/to/image.jpg')\"}, \"text\": {\"type\": \"array\", \"description\": \"A list of text strings that serve as candidate labels or prompts for zero-shot classification.\", \"required\": true, \"example\": [\"cat\", \"dog\", \"car\"]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensor format to return from the processor. Commonly 'pt' for PyTorch tensors.\", \"required\": true, \"default\": \"pt\", \"example\": \"pt\"}, \"padding\": {\"type\": \"boolean\", \"description\": \"Indicates whether to pad the text or image inputs to the same length for batching purposes.\", \"required\": true, \"default\": true, \"example\": true}}, \"response\": \"The tool produces a structured output containing the results of zero-shot image classification for a given image against a set of candidate text labels. The response structure typically includes the following components:\\n\\n1. **`logits_per_image`**  \\n   - A numeric tensor representing the raw similarity scores between the input image and each candidate text label.  \\n   - Each element corresponds to one text label and indicates the degree of alignment between the image and that label in the model\\u2019s learned embedding space. Higher values indicate stronger correspondence.  \\n\\n2. **`probs`**  \\n   - A numeric tensor containing the normalized probabilities obtained by applying a softmax function over `logits_per_image`.  \\n   - Each probability corresponds to one candidate text label and represents the model\\u2019s confidence that the image matches that label.  \\n   - The sum of all probabilities equals 1.  \\n\\n3. **Optional metadata (depending on implementation)**  \\n   - Information about the input images and text labels used in the classification, including shapes or identifiers.  \\n   - In some cases, the model may also return embeddings for the image (`image_features`) and text (`text_features`), which are normalized vectors in the shared embedding space. These can be used for further similarity calculations or downstream tasks.\\n\\nIn essence, the response provides both the raw similarity scores and the probabilistic interpretation of which text label best matches the input image in a zero-shot classification setting.\", \"examples\": [{\"user_intent\": \"I have a photo of a cat and want to find out which Chinese labels best describe it.\", \"tool_call\": {\"name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"arguments\": {\"images\": \"cat_photo.jpg\", \"text\": [\"\\u732b\", \"\\u72d7\", \"\\u9e1f\", \"\\u6c7d\\u8f66\"], \"return_tensors\": \"pt\", \"padding\": \"True\"}}}, {\"user_intent\": \"Classify a satellite image to see which land types it contains using Chinese labels.\", \"tool_call\": {\"name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"arguments\": {\"images\": \"satellite_image.png\", \"text\": [\"\\u68ee\\u6797\", \"\\u57ce\\u5e02\", \"\\u6c34\\u57df\", \"\\u6c99\\u6f20\"], \"return_tensors\": \"pt\", \"padding\": \"True\"}}}, {\"user_intent\": \"I want to label a set of product photos with the most relevant Chinese text descriptions.\", \"tool_call\": {\"name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"arguments\": {\"images\": [\"shirt.jpg\", \"shoes.jpg\", \"bag.jpg\"], \"text\": [\"\\u8863\\u670d\", \"\\u978b\\u5b50\", \"\\u5305\", \"\\u9970\\u54c1\"], \"return_tensors\": \"pt\", \"padding\": \"True\"}}}]}\n\n# User Query\nWe are working on a language learning app for Chinese. We need to give audio examples for each lesson.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate natural-sounding Chinese audio from text for lesson examples.",
                    "tool_description": "Convert written Chinese text into speech using a pre-trained Chinese Text-to-Speech model, producing natural-sounding audio suitable for language learning applications.",
                    "expected_response": "Audio files or streams corresponding to the input Chinese text, ready for playback in the app."
                }
            ],
            "extracted_arguments": {
                "lesson_text": "The Chinese text content for which audio examples need to be generated, provided as a string or list of strings."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_126",
        "query": "We need to generate a short video showing Spider-Man water skiing in redshift style based on a textual prompt.",
        "instruction": "Given a `text-to-video` generation task, retrieve tools that process textual prompts to create short videos by leveraging models designed for multimodal output, specifically in styles such as redshift.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Text-to-Video",
                    "framework": "Hugging Face",
                    "functionality": "Text-to-Video Generation",
                    "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                    "api_arguments": {
                        "prompt": "string",
                        "video_length": "int",
                        "height": "int",
                        "width": "int",
                        "num_inference_steps": "int",
                        "guidance_scale": "float"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "tuneavideo"
                    ],
                    "example_code": "from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\nfrom tuneavideo.models.unet import UNet3DConditionModel\nfrom tuneavideo.util import save_videos_grid\nimport torch\npretrained_model_path = nitrosocke/redshift-diffusion\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\npipe.enable_xformers_memory_efficient_attention()\nprompt = (redshift style) spider man is skiing\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\nsave_videos_grid(video, f./{prompt}.gif)",
                    "performance": {
                        "dataset": "N/A",
                        "accuracy": "N/A"
                    },
                    "description": "Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.",
                    "id": "huggingface_api_98",
                    "name": "redshift-man-skiing"
                },
                "id": "gorilla_huggingface_tool_97",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "Tool 1: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.",
            "Tool 2: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.",
            "Tool 3: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.",
            "Tool 4: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.",
            "Tool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 6: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 7: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.",
            "Tool 8: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 9: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.",
            "Tool 10: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.",
            "Tool 11: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.",
            "Tool 12: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.",
            "Tool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.",
            "Tool 14: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.",
            "Tool 15: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 16: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 17: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.",
            "Tool 18: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables."
        ],
        "tool_view": {
            "description": "Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.",
            "parameters": {
                "prompt": {
                    "type": "string",
                    "description": "A textual description or prompt that guides the video generation process. This should describe the content or scene to be generated in the video.",
                    "required": true,
                    "example": "(redshift style) spider man is skiing"
                },
                "video_length": {
                    "type": "int",
                    "description": "The length of the generated video, measured in seconds.",
                    "required": true,
                    "example": 8
                },
                "height": {
                    "type": "int",
                    "description": "The height (in pixels) of the generated video frames.",
                    "required": true,
                    "example": 512
                },
                "width": {
                    "type": "int",
                    "description": "The width (in pixels) of the generated video frames.",
                    "required": true,
                    "example": 512
                },
                "num_inference_steps": {
                    "type": "int",
                    "description": "The number of inference steps to be used during the video generation process. A higher value generally leads to more detailed results, but requires more computation time.",
                    "required": true,
                    "example": 50
                },
                "guidance_scale": {
                    "type": "float",
                    "description": "A float value that controls how strongly the model should follow the prompt. Higher values encourage the model to stick closer to the prompt, while lower values allow more creativity in the generated video.",
                    "required": true,
                    "example": 7.5
                }
            },
            "response": "The tool produces a structured response containing the video(s) generated from the provided textual prompt. The response structure can be described as follows:\n\n- **videos**: This is the primary component of the response. It contains one or more video outputs generated by the model according to the input parameters. Each video is represented as a multi-dimensional array or a video object suitable for further processing, visualization, or saving to disk. The video length, resolution, and quality are determined by the corresponding input parameters: `video_length`, `height`, `width`, `num_inference_steps`, and `guidance_scale`.\n\nIn principle, the `videos` field encapsulates all generated video frames in temporal order, allowing downstream code to render, save, or manipulate the video. There are no additional metadata fields explicitly defined in the documentation, but the video content is aligned with the semantic content described in the `prompt`.  \n\nOverall, the response is essentially a container for the generated video data, structured to be directly compatible with video handling utilities like `save_videos_grid`.",
            "examples": [
                {
                    "user_intent": "I want to generate a short video of a person skiing in the redshift style.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "redshift style man skiing",
                            "video_length": 6,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 50,
                            "guidance_scale": 7.5
                        }
                    }
                },
                {
                    "user_intent": "Create a video where Spider-Man is skiing in the redshift style for 10 seconds.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "(redshift style) spider man is skiing",
                            "video_length": 10,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 75,
                            "guidance_scale": 8.0
                        }
                    }
                },
                {
                    "user_intent": "Generate a video of a man skiing in the redshift style with a higher guidance scale.",
                    "tool_call": {
                        "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))",
                        "api_arguments": {
                            "prompt": "redshift style man skiing",
                            "video_length": 8,
                            "height": 512,
                            "width": 512,
                            "num_inference_steps": 60,
                            "guidance_scale": 9.0
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\nTool 1: Tune-A-Video (mo-di-bear-guitar) is a Hugging Face–based text-to-video generation tool that creates videos from user-provided textual prompts, allowing control over video length, resolution, and style. It is designed to produce visually coherent, stylized videos, such as modern Disney–inspired animations, from descriptive text inputs.\nTool 2: A Hugging Face model that converts Persian and English text prompts into videos, enabling users to generate video content directly from textual descriptions.\nTool 3: Generate video content from textual descriptions using the Hugging Face Transformers library, enabling the creation of videos directly from input text.\nTool 4: A Hugging Face-based tool that generates videos from short English text descriptions using a multi-stage text-to-video diffusion model, enabling users to synthesize videos that visually match the input text.\nTool 5: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 6: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 7: A multi-stage text-to-video generation model that converts an English text description into a corresponding video by progressively mapping text features to video frames. It is used to create videos from textual prompts, leveraging diffusion-based synthesis for applications in content creation, storytelling, and multimedia generation.\nTool 8: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 9: The \"andite/anything-v4.0\" is a latent diffusion model designed to generate high-quality, detailed anime-style images from text prompts. It supports Danbooru tags and functions similarly to other Stable Diffusion models, allowing users to create images by providing simple textual descriptions.\nTool 10: Generate videos from text prompts using a Hugging Face model, enabling tasks such as video creation and text-based video summarization.\nTool 11: The GIT-large-r-textcaps model is a generative image-to-text transformer fine-tuned on the TextCaps dataset, designed to generate textual descriptions from images. It predicts text tokens based on image content, enabling applications such as image and video captioning, visual question answering, and image classification by producing descriptive or class-based text output.\nTool 12: EimisAnimeDiffusion_1.0v is a text-to-image model designed for generating high-quality anime images based on textual prompts. It is particularly effective for anime and landscape-style generations and includes support for a Gradio Web UI, making it easy to interact with.\nTool 13: A Hugging Face Transformers text generation tool using the bigscience/test-bloomd-6b3 model, designed to generate coherent text continuations based on a given input prompt.\nTool 14: This tool provides a pre-trained ControlNet model that guides stable diffusion text-to-image generation using openpose images as control conditions, allowing users to produce images that follow specific human poses while maintaining the style and content dictated by a text prompt.\nTool 15: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 16: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 17: Nitro Diffusion is a fine-tuned Stable Diffusion model that generates images from text prompts, supporting three distinct art styles while keeping them separate. It enables precise control over style selection, mixing, and weighting, making it ideal for creating customized, high-quality artwork based on textual descriptions.\nTool 18: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Redshift-Man-Skiing is a text-to-video generation tool that creates videos from textual prompts using the Tune-A-Video Redshift model. It allows users to specify video length, resolution, and generation parameters to produce visually coherent videos from descriptions, enabling creative and customized video content directly from text.\", \"parameters\": {\"prompt\": {\"type\": \"string\", \"description\": \"A textual description or prompt that guides the video generation process. This should describe the content or scene to be generated in the video.\", \"required\": true, \"example\": \"(redshift style) spider man is skiing\"}, \"video_length\": {\"type\": \"int\", \"description\": \"The length of the generated video, measured in seconds.\", \"required\": true, \"example\": 8}, \"height\": {\"type\": \"int\", \"description\": \"The height (in pixels) of the generated video frames.\", \"required\": true, \"example\": 512}, \"width\": {\"type\": \"int\", \"description\": \"The width (in pixels) of the generated video frames.\", \"required\": true, \"example\": 512}, \"num_inference_steps\": {\"type\": \"int\", \"description\": \"The number of inference steps to be used during the video generation process. A higher value generally leads to more detailed results, but requires more computation time.\", \"required\": true, \"example\": 50}, \"guidance_scale\": {\"type\": \"float\", \"description\": \"A float value that controls how strongly the model should follow the prompt. Higher values encourage the model to stick closer to the prompt, while lower values allow more creativity in the generated video.\", \"required\": true, \"example\": 7.5}}, \"response\": \"The tool produces a structured response containing the video(s) generated from the provided textual prompt. The response structure can be described as follows:\\n\\n- **videos**: This is the primary component of the response. It contains one or more video outputs generated by the model according to the input parameters. Each video is represented as a multi-dimensional array or a video object suitable for further processing, visualization, or saving to disk. The video length, resolution, and quality are determined by the corresponding input parameters: `video_length`, `height`, `width`, `num_inference_steps`, and `guidance_scale`.\\n\\nIn principle, the `videos` field encapsulates all generated video frames in temporal order, allowing downstream code to render, save, or manipulate the video. There are no additional metadata fields explicitly defined in the documentation, but the video content is aligned with the semantic content described in the `prompt`.  \\n\\nOverall, the response is essentially a container for the generated video data, structured to be directly compatible with video handling utilities like `save_videos_grid`.\", \"examples\": [{\"user_intent\": \"I want to generate a short video of a person skiing in the redshift style.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"redshift style man skiing\", \"video_length\": 6, \"height\": 512, \"width\": 512, \"num_inference_steps\": 50, \"guidance_scale\": 7.5}}}, {\"user_intent\": \"Create a video where Spider-Man is skiing in the redshift style for 10 seconds.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"(redshift style) spider man is skiing\", \"video_length\": 10, \"height\": 512, \"width\": 512, \"num_inference_steps\": 75, \"guidance_scale\": 8.0}}}, {\"user_intent\": \"Generate a video of a man skiing in the redshift style with a higher guidance scale.\", \"tool_call\": {\"api_call\": \"TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))\", \"api_arguments\": {\"prompt\": \"redshift style man skiing\", \"video_length\": 8, \"height\": 512, \"width\": 512, \"num_inference_steps\": 60, \"guidance_scale\": 9.0}}}]}\n\n# User Query\nWe need to generate a short video showing Spider-Man water skiing in redshift style based on a textual prompt.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a short video of Spider-Man water skiing in redshift style.",
                    "tool_description": "Create a video from a textual prompt using the Tune-A-Video Redshift model, allowing specification of video length, resolution, inference steps, and guidance scale to ensure visual coherence and adherence to the prompt.",
                    "expected_response": "A structured response containing the generated video of Spider-Man water skiing, including all frames in temporal order, formatted as a video object or multi-dimensional array suitable for playback or saving."
                }
            ],
            "extracted_arguments": {
                "prompt": "A textual description of the scene to be depicted in the video, including characters, actions, and style.",
                "video_length": "The duration of the video in seconds, as an integer.",
                "height": "The height of the video frames in pixels.",
                "width": "The width of the video frames in pixels.",
                "num_inference_steps": "The number of model inference steps to generate the video, controlling detail level.",
                "guidance_scale": "A float controlling how closely the video should follow the textual prompt."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_71",
        "query": "Calculate the distance between the objects in an image for an autonomous vehicle driving in a parking lot.",
        "instruction": "Given a `depth estimation for autonomous vehicles` task, retrieve tools that perform computer vision analysis by processing image inputs to calculate object distances, leveraging models designed for depth estimation tasks in automotive environments.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Depth Estimation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')",
                    "api_arguments": [],
                    "python_environment_requirements": [
                        "transformers==4.24.0",
                        "torch==1.12.1+cu116",
                        "tokenizers==0.13.2"
                    ],
                    "example_code": "",
                    "performance": {
                        "dataset": "diode-subset",
                        "accuracy": {
                            "Loss": 0.3497,
                            "Mae": 0.2847,
                            "Rmse": 0.3977,
                            "Abs Rel": 0.3477,
                            "Log Mae": 0.1203,
                            "Log Rmse": 0.1726,
                            "Delta1": 0.5217,
                            "Delta2": 0.8246,
                            "Delta3": 0.9436
                        }
                    },
                    "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.",
                    "id": "huggingface_api_169",
                    "name": "glpn-kitti-finetuned-diode-221214-123047"
                },
                "id": "gorilla_huggingface_tool_165",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "Tool 1: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.",
            "Tool 2: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.",
            "Tool 3: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.",
            "Tool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.",
            "Tool 5: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.",
            "Tool 6: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.",
            "Tool 7: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.",
            "Tool 8: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.",
            "Tool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.",
            "Tool 10: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.",
            "Tool 11: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.",
            "Tool 12: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.",
            "Tool 13: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.",
            "Tool 14: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.",
            "Tool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.",
            "Tool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.",
            "Tool 17: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.",
            "Tool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.",
            "Tool 19: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages."
        ],
        "tool_view": {
            "description": "A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.",
            "parameters": {
                "conf": {
                    "type": "number",
                    "description": "The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.",
                    "required": true,
                    "default": 0.25,
                    "example": 0.3
                },
                "iou": {
                    "type": "number",
                    "description": "The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.",
                    "required": true,
                    "default": 0.45,
                    "example": 0.5
                },
                "agnostic": {
                    "type": "boolean",
                    "description": "Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "multi_label": {
                    "type": "boolean",
                    "description": "If True, allows each detected box to have multiple class labels.",
                    "required": true,
                    "default": false,
                    "example": true
                },
                "max_det": {
                    "type": "number",
                    "description": "The maximum number of detections allowed per image.",
                    "required": true,
                    "default": 1000,
                    "example": 500
                },
                "img": {
                    "type": "string",
                    "description": "The input image for detection. Can be a URL or local file path.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                },
                "size": {
                    "type": "number",
                    "description": "The size to which the input image will be resized before detection.",
                    "required": true,
                    "default": 640,
                    "example": 512
                },
                "augment": {
                    "type": "boolean",
                    "description": "If True, applies data augmentation during inference for potentially improved results.",
                    "required": true,
                    "default": true,
                    "example": false
                }
            },
            "response": "The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\n\n1. **pred**  \n   - A list (or tensor) containing the detection predictions for each input image.  \n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \n   - Each row contains:  \n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \n     - **Confidence score** (usually the fifth element) indicating the model’s certainty for that detection.  \n     - **Class index** (usually the sixth element) representing the detected object’s category; here it corresponds to the license plate class.\n\n2. **boxes**  \n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\n\n3. **scores**  \n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\n\n4. **categories**  \n   - Extracted from `pred`, this array contains the class indices for each detection.\n\n5. **Visualization methods**  \n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\n\n6. **Additional metadata** (implicit)  \n   - Number of detections returned is limited by `max_det`.  \n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \n   - The response may include augmented results if `augment=True` is used during inference.\n\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.",
            "examples": [
                {
                    "user_intent": "Detect license plates in a car image I took on the highway.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/highway_car.jpg",
                            "conf": 0.3,
                            "iou": 0.45,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 500,
                            "size": 640,
                            "augment": true
                        }
                    }
                },
                {
                    "user_intent": "Scan multiple parking lot images for license plates and save the results.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/parking_lot_1.jpg",
                            "conf": 0.25,
                            "iou": 0.5,
                            "agnostic": false,
                            "multi_label": false,
                            "max_det": 1000,
                            "size": 640,
                            "augment": false
                        }
                    }
                },
                {
                    "user_intent": "Quickly check license plates in a low-resolution surveillance image.",
                    "tool_call": {
                        "name": "yolov5.load('keremberke/yolov5m-license-plate')",
                        "arguments": {
                            "img": "https://example.com/surveillance_frame.jpg",
                            "conf": 0.2,
                            "iou": 0.4,
                            "agnostic": true,
                            "multi_label": false,
                            "max_det": 200,
                            "size": 320,
                            "augment": true
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\nTool 1: YOLOS-Tiny is a lightweight Vision Transformer model designed for object detection, capable of identifying and localizing objects in images using a DETR-based approach. It processes images to predict object classes and bounding boxes with a one-to-one query matching strategy, making it suitable for applications that require automated detection of multiple objects within an image, such as image analysis, visual search, or automated annotation.\nTool 2: This tool uses a fine-tuned GLPN transformer model to perform depth estimation on images, providing per-pixel distance predictions, and is designed for applications in computer vision that require understanding scene geometry and spatial relationships.\nTool 3: This tool provides a pre-trained Mask2Former model for instance segmentation using a Swin-based tiny architecture, trained on the COCO dataset. It enables automatic detection and segmentation of individual objects in images by predicting masks and corresponding class labels, supporting a unified approach to image segmentation. It is useful for applications that require identifying and isolating specific objects within images.\nTool 4: A YOLOv8-based object detection model designed to identify forklifts and people in images, allowing users to detect and localize these objects from either local files or image URLs with high accuracy.\nTool 5: Deformable DETR is an object detection model with a ResNet-50 backbone, designed for end-to-end detection of objects in images. It can be used to identify and locate multiple objects within an image by processing it through a transformer-based architecture, and is trained on the COCO 2017 dataset for robust detection performance. This model is accessible via Hugging Face Transformers and is suitable for applications in computer vision that require accurate and efficient object recognition.\nTool 6: MaskFormer with a Swin-Tiny backbone, trained on COCO panoptic segmentation, performs instance and panoptic segmentation on images. It processes an input image to predict per-pixel masks and class labels, enabling detailed object and scene understanding, and is useful for tasks requiring precise segmentation of multiple objects within a single image.\nTool 7: GLPN-KITTI is a Hugging Face Transformers model for monocular depth estimation, using Global-Local Path Networks trained on the KITTI dataset. It predicts depth maps from single images, allowing applications in 3D scene understanding, robotics, and computer vision tasks that require distance or spatial information from 2D images.\nTool 8: `fcakyon/yolov5s-v7.0` is an object detection model based on YOLOv5, designed to identify and locate objects in images. It outputs bounding boxes, confidence scores, and object categories, making it useful for applications that require automated visual recognition, such as image analysis, surveillance, or content tagging.\nTool 9: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to detect objects in images by providing one or more text queries. It leverages a CLIP-based multi-modal backbone with a Vision Transformer to extract visual features and a language model to encode text, enabling detection without task-specific training. Trained on publicly available image-caption datasets and fine-tuned on object detection benchmarks like COCO and OpenImages, OWL-ViT can identify and localize objects in images based on descriptive text prompts.\nTool 10: A SegFormer model fine-tuned for semantic segmentation on the CityScapes dataset at 1024×1024 resolution, designed to accurately label each pixel in an image with its corresponding class using a transformer-based architecture. It is used to perform automated scene understanding in urban environments, such as identifying roads, vehicles, pedestrians, and other objects.\nTool 11: A pre-trained MobileNet V2 model for image classification that can efficiently recognize objects in images. Designed for low-latency and low-power applications, it is suitable for use on mobile devices and resource-constrained environments, making it useful for tasks such as object recognition, feature extraction, and building custom computer vision solutions.\nTool 12: This tool provides a pre-trained Vision Transformer (ViT) model for image classification, capable of recognizing a wide range of objects by analyzing images at a 224×224 resolution. It leverages a model fine-tuned on ImageNet to predict object classes from input images and is used to automatically identify and label visual content in computer vision applications.\nTool 13: This tool performs zero-shot image classification using the CLIP ConvNeXt-XXLarge model, trained on the LAION-2B dataset. It leverages the Hugging Face framework and achieves between 79.1% and 79.4% top-1 accuracy on ImageNet-1k. The model classifies images based on provided class names without needing prior training on the specific categories.\nTool 14: A transformer-based image segmentation model that performs end-to-end panoptic segmentation on images, identifying both objects and background regions. Trained on the COCO 2017 dataset, it can generate pixel-level segmentation maps and object masks, making it useful for detailed scene understanding and computer vision tasks that require distinguishing individual instances as well as overall scene structure.\nTool 15: This tool uses the Swin Transformer v2 model, pre-trained on ImageNet-1k, to classify images by predicting their object categories. It processes input images using hierarchical patch-based feature extraction and local self-attention, enabling efficient and accurate image classification. The model is designed for use in computer vision tasks where identifying or labeling objects in images is needed.\nTool 16: The \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\" model is designed for zero-shot image classification using the CLIP ConvNeXt architecture, trained on subsets of the LAION-5B dataset. It allows you to classify images without needing retraining, by providing a list of possible class names. The model achieves between 70.8% and 71.7% top-1 accuracy on the ImageNet-1k dataset. It can be easily used via the Hugging Face pipeline API and is suitable for tasks like image classification and retrieval.\nTool 17: BLIP-VQA-Base is a vision-language model designed for visual question answering, allowing users to input an image and a related question to receive accurate answers. It leverages a pretrained transformer-based architecture to interpret visual content and language together, making it useful for tasks that require understanding and reasoning about images, such as counting objects, identifying scenes, or extracting visual details from complex pictures.\nTool 18: OWL-ViT is a zero-shot, text-conditioned object detection model that allows users to identify objects in images by providing one or more text queries. It leverages a vision transformer to extract visual features and a language model to encode text, enabling detection of objects without task-specific training. This tool is useful for flexible object recognition across diverse datasets, such as COCO and OpenImages, where pre-defined object categories may not cover all desired queries.\nTool 19: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv5-based model for detecting license plates in images, trained on a custom dataset to achieve high accuracy. It allows users to identify and localize license plates efficiently, making it useful for applications in vehicle monitoring, parking management, and traffic analysis.\", \"parameters\": {\"conf\": {\"type\": \"number\", \"description\": \"The confidence threshold for detecting objects. Predictions with confidence below this value will be ignored.\", \"required\": true, \"default\": 0.25, \"example\": 0.3}, \"iou\": {\"type\": \"number\", \"description\": \"The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS) to filter overlapping boxes.\", \"required\": true, \"default\": 0.45, \"example\": 0.5}, \"agnostic\": {\"type\": \"boolean\", \"description\": \"Whether to perform class-agnostic NMS, ignoring class labels when suppressing overlapping detections.\", \"required\": true, \"default\": false, \"example\": true}, \"multi_label\": {\"type\": \"boolean\", \"description\": \"If True, allows each detected box to have multiple class labels.\", \"required\": true, \"default\": false, \"example\": true}, \"max_det\": {\"type\": \"number\", \"description\": \"The maximum number of detections allowed per image.\", \"required\": true, \"default\": 1000, \"example\": 500}, \"img\": {\"type\": \"string\", \"description\": \"The input image for detection. Can be a URL or local file path.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"size\": {\"type\": \"number\", \"description\": \"The size to which the input image will be resized before detection.\", \"required\": true, \"default\": 640, \"example\": 512}, \"augment\": {\"type\": \"boolean\", \"description\": \"If True, applies data augmentation during inference for potentially improved results.\", \"required\": true, \"default\": true, \"example\": false}}, \"response\": \"The tool returns an object detection result specific to license plates in an input image. The response is structured to provide both the raw detection data and optional visualizations. Its main components are:\\n\\n1. **pred**  \\n   - A list (or tensor) containing the detection predictions for each input image.  \\n   - For a single image, it is typically a 2D array where each row corresponds to one detected object.  \\n   - Each row contains:  \\n     - **Bounding box coordinates** (usually the first four elements) specifying the location of the detected license plate in the image, often in `[x1, y1, x2, y2]` format.  \\n     - **Confidence score** (usually the fifth element) indicating the model\\u2019s certainty for that detection.  \\n     - **Class index** (usually the sixth element) representing the detected object\\u2019s category; here it corresponds to the license plate class.\\n\\n2. **boxes**  \\n   - Extracted from `pred`, this array contains only the bounding box coordinates of all detected objects.\\n\\n3. **scores**  \\n   - Extracted from `pred`, this array contains the confidence scores for each detected object.\\n\\n4. **categories**  \\n   - Extracted from `pred`, this array contains the class indices for each detection.\\n\\n5. **Visualization methods**  \\n   - **results.show()**: produces an annotated image showing bounding boxes around detected license plates.  \\n   - **results.save(save_dir=...)**: saves the annotated images to a specified directory.\\n\\n6. **Additional metadata** (implicit)  \\n   - Number of detections returned is limited by `max_det`.  \\n   - Detections are filtered based on `conf` (confidence threshold) and optionally adjusted using `iou` for non-max suppression.  \\n   - The response may include augmented results if `augment=True` is used during inference.\\n\\nOverall, the response provides both the structured detection data (coordinates, confidence, class) and convenience functions for visualization and saving, tailored for license plate detection in images.\", \"examples\": [{\"user_intent\": \"Detect license plates in a car image I took on the highway.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/highway_car.jpg\", \"conf\": 0.3, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 500, \"size\": 640, \"augment\": true}}}, {\"user_intent\": \"Scan multiple parking lot images for license plates and save the results.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/parking_lot_1.jpg\", \"conf\": 0.25, \"iou\": 0.5, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"size\": 640, \"augment\": false}}}, {\"user_intent\": \"Quickly check license plates in a low-resolution surveillance image.\", \"tool_call\": {\"name\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"arguments\": {\"img\": \"https://example.com/surveillance_frame.jpg\", \"conf\": 0.2, \"iou\": 0.4, \"agnostic\": true, \"multi_label\": false, \"max_det\": 200, \"size\": 320, \"augment\": true}}}]}\n\n# User Query\nCalculate the distance between the objects in an image for an autonomous vehicle driving in a parking lot.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to estimate the depth and spatial relationships between objects in an image of a parking lot for an autonomous vehicle.",
                    "tool_description": "Perform depth estimation on an image to calculate per-pixel distances, allowing for the measurement of object distances in a scene.",
                    "expected_response": "A depth map providing the per-pixel distance to each object in the image, allowing for accurate distance measurement between objects."
                }
            ],
            "extracted_arguments": {
                "img": "The input image for depth estimation, typically a URL or local file path.",
                "size": "The size to which the input image will be resized before depth estimation, affecting resolution and processing efficiency."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_135",
        "query": "I have written a small story in English about the adventures of a superhero who saves the day from evil villains. I would like to translate it into French so that my friends in France can read it too.",
        "instruction": "Given a `translation` task, retrieve tools that handle text-to-text conversion to translate written content from one language to another, specifically from English to French, by processing textual input and delivering the translated output.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Text-to-Text Generation",
                    "api_call": "pipeline('translation')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "optimum.onnxruntime"
                    ],
                    "example_code": "from transformers import AutoTokenizer, pipeline\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\nresults = translator(My name is Eustache and I have a pet raccoon)\nprint(results)",
                    "performance": {
                        "dataset": "c4",
                        "accuracy": "N/A"
                    },
                    "description": "T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.",
                    "id": "huggingface_api_918",
                    "name": "optimum/t5-small"
                },
                "id": "gorilla_huggingface_tool_534",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 1: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 2: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 3: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.",
            "Tool 4: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 5: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.",
            "Tool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.",
            "Tool 7: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.",
            "Tool 8: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 9: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.",
            "Tool 10: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.",
            "Tool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 14: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 15: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.",
            "Tool 16: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 17: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 19: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair."
        ],
        "tool_view": {
            "description": "The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text in French that needs to be translated into English.",
                    "required": true,
                    "example": "Bonjour, comment ça va?"
                }
            },
            "response": "The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\n\nHere’s a description of the expected response:\n\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\n  \nIn principle, the response will consist of the following:\n\n1. **Translated Text**: A string containing the English translation of the input French text.\n\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\n\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool’s design or configuration.",
            "examples": [
                {
                    "user_intent": "Translate this French sentence to English: 'Je voudrais réserver une table pour deux ce soir.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Je voudrais réserver une table pour deux ce soir."
                        }
                    }
                },
                {
                    "user_intent": "I need the English version of this French paragraph about climate change: 'Le réchauffement climatique est une menace sérieuse pour notre planète.'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Le réchauffement climatique est une menace sérieuse pour notre planète."
                        }
                    }
                },
                {
                    "user_intent": "Convert this casual French message into English: 'Salut! On se voit demain au café?'",
                    "tool_call": {
                        "name": "opus-mt-fr-en",
                        "arguments": {
                            "text": "Salut! On se voit demain au café?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 1: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 2: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 3: Cadet-Tiny is a lightweight conversational model designed for edge inference, optimized to run on devices with limited resources, such as a Raspberry Pi with 2GB of RAM. Built on the t5-small model from Google and trained on the SODA dataset, it offers a compact alternative to larger models like Cosmo-3B, with only about 2% of the size. This tool enables efficient, real-time dialogue generation, making it ideal for small-scale conversational applications.\nTool 4: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 5: A Swedish-to-English translation model built with Hugging Face Transformers, trained on the OPUS dataset using the transformer-align architecture, and preprocessed with normalization and SentencePiece. It is designed to accurately translate Swedish text into English, achieving strong performance on benchmarks like Tatoeba.\nTool 6: A compact English-to-German translation model built with the Marian framework in Hugging Face Transformers, designed to quickly and efficiently translate English text into German.\nTool 7: This tool provides a pre-trained Tacotron2 model for converting text into speech using SpeechBrain. It generates spectrograms from input text, which can then be transformed into audio waveforms with a vocoder like HiFIGAN, enabling easy creation of natural-sounding speech from written text.\nTool 8: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 9: This tool predicts and restores punctuation in English, German, French, and Italian text, making it useful for improving the readability of transcribed spoken language. Trained on the Europarl dataset, it can accurately insert common punctuation marks such as periods, commas, question marks, dashes, and colons, enabling cleaner, more natural text output from unpunctuated input.\nTool 10: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 11: `distilbert-base-multilingual-cased-ner-hrl` is a multilingual Named Entity Recognition (NER) model based on a fine-tuned DistilBERT base, designed to identify persons, organizations, and locations in text across 10 high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It can be used to automatically extract structured information from unstructured text in multiple languages, supporting tasks like information retrieval, content analysis, and multilingual text processing.\nTool 12: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 13: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 14: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 15: Helsinki-NLP/opus-mt-fi-en is a machine translation model designed to translate Finnish text into English. Built on the Hugging Face Transformers framework and trained on the OPUS dataset, it enables accurate and efficient sequence-to-sequence translation, making it suitable for applications that require converting Finnish content into English while maintaining fluency and meaning.\nTool 16: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 17: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 18: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 19: CodeBERTa-small-v1 is a Transformer-based model designed for masked language modeling on source code. Trained on the CodeSearchNet dataset, it supports languages including Go, Java, JavaScript, PHP, Python, and Ruby, and can predict missing tokens in code snippets, enabling tasks like code completion and repair.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text in French that needs to be translated into English.\", \"required\": true, \"example\": \"Bonjour, comment \\u00e7a va?\"}}, \"response\": \"The tool is a machine translation model that translates text from French to English. The response from this tool would typically contain the translated text corresponding to the input text provided.\\n\\nHere\\u2019s a description of the expected response:\\n\\n- **Translated Text**: This is the primary component of the response. It will be a string containing the English translation of the French input text provided to the model.\\n  \\nIn principle, the response will consist of the following:\\n\\n1. **Translated Text**: A string containing the English translation of the input French text.\\n\\nThe content will be directly derived from the model's output and is intended to provide a meaningful English equivalent for the French input text. The tool will output the translation in response to a given input phrase or sentence in French.\\n\\nNo other metadata (such as confidence scores or intermediate values) is expected to be included in the response unless further specified by the tool\\u2019s design or configuration.\", \"examples\": [{\"user_intent\": \"Translate this French sentence to English: 'Je voudrais r\\u00e9server une table pour deux ce soir.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Je voudrais r\\u00e9server une table pour deux ce soir.\"}}}, {\"user_intent\": \"I need the English version of this French paragraph about climate change: 'Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Le r\\u00e9chauffement climatique est une menace s\\u00e9rieuse pour notre plan\\u00e8te.\"}}}, {\"user_intent\": \"Convert this casual French message into English: 'Salut! On se voit demain au caf\\u00e9?'\", \"tool_call\": {\"name\": \"opus-mt-fr-en\", \"arguments\": {\"text\": \"Salut! On se voit demain au caf\\u00e9?\"}}}]}\n\n# User Query\nI have written a small story in English about the adventures of a superhero who saves the day from evil villains. I would like to translate it into French so that my friends in France can read it too.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate my English story about a superhero into French.",
                    "tool_description": "Use a machine translation model to convert English text into French while preserving meaning, style, and context.",
                    "expected_response": "A string containing the French translation of the English story."
                }
            ],
            "extracted_arguments": {
                "text": "The input story written in English that needs to be translated into French."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_73",
        "query": "We need to create a dialogue in Russian for our educational app. It should cover a general greeting and asking about the users' well-being.",
        "instruction": "Given a `conversational dialogue creation` task, retrieve tools that generate and process dialogue content in the specified language (Russian) for applications by utilizing pre-trained language models designed to produce coherent and contextually relevant exchanges.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Conversational",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "tinkoff-ai/ruDialoGPT-medium"
                    },
                    "python_environment_requirements": [
                        "torch",
                        "transformers"
                    ],
                    "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\nmodel = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\ninputs = tokenizer('@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@', return_tensors='pt')\ngenerated_token_ids = model.generate(\n **inputs,\n top_k=10,\n top_p=0.95,\n num_beams=3,\n num_return_sequences=3,\n do_sample=True,\n no_repeat_ngram_size=2,\n temperature=1.2,\n repetition_penalty=1.2,\n length_penalty=1.0,\n eos_token_id=50257,\n max_new_tokens=40\n)\ncontext_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\ncontext_with_response",
                    "performance": {
                        "dataset": "Private Validation Set",
                        "sensibleness": 0.78,
                        "specificity": 0.69,
                        "SSA": 0.735
                    },
                    "description": "This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents. The model was trained with context size 3.",
                    "id": "huggingface_api_599",
                    "name": "tinkoff-ai/ruDialoGPT-medium"
                },
                "id": "gorilla_huggingface_tool_594",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "Tool 1: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.",
            "Tool 4: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.",
            "Tool 5: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 6: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.",
            "Tool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 8: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 9: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 10: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.",
            "Tool 11: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 12: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.",
            "Tool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.",
            "Tool 14: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.",
            "Tool 15: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 17: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.",
            "Tool 18: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.",
            "Tool 19: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information."
        ],
        "tool_view": {
            "description": "A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.",
            "parameters": {
                "num_return_sequences": {
                    "type": "number",
                    "description": "The number of different text sequences the model should generate for a single input prompt.",
                    "required": true,
                    "default": 1,
                    "example": 3
                },
                "max_length": {
                    "type": "number",
                    "description": "The maximum number of tokens to generate in the output sequence.",
                    "required": true,
                    "default": 512,
                    "example": 256
                },
                "no_repeat_ngram_size": {
                    "type": "number",
                    "description": "Prevents the model from repeating any n-grams of this size in the generated text.",
                    "required": true,
                    "default": 3,
                    "example": 2
                },
                "do_sample": {
                    "type": "boolean",
                    "description": "Whether to use sampling; if false, the model will use greedy decoding.",
                    "required": true,
                    "default": true,
                    "example": false
                },
                "top_k": {
                    "type": "number",
                    "description": "The number of highest probability vocabulary tokens to keep for top-k-filtering during sampling.",
                    "required": true,
                    "default": 50,
                    "example": 30
                },
                "top_p": {
                    "type": "number",
                    "description": "The cumulative probability threshold for nucleus (top-p) sampling. Tokens are considered until the cumulative probability exceeds this value.",
                    "required": true,
                    "default": 0.9,
                    "example": 0.8
                },
                "temperature": {
                    "type": "number",
                    "description": "Controls the randomness of predictions by scaling the logits before applying softmax. Higher values produce more random outputs.",
                    "required": true,
                    "default": 0.6,
                    "example": 1.0
                },
                "mask_token_id": {
                    "type": "string",
                    "description": "The token ID used to represent masked tokens in the input text, typically provided by the tokenizer.",
                    "required": true,
                    "default": "tokenizer.mask_token_id",
                    "example": "tokenizer.mask_token_id"
                },
                "eos_token_id": {
                    "type": "string",
                    "description": "The token ID representing the end-of-sequence marker, as defined by the tokenizer.",
                    "required": true,
                    "default": "tokenizer.eos_token_id",
                    "example": "tokenizer.eos_token_id"
                },
                "unk_token_id": {
                    "type": "string",
                    "description": "The token ID used for unknown tokens that the model cannot recognize.",
                    "required": true,
                    "default": "tokenizer.unk_token_id",
                    "example": "tokenizer.unk_token_id"
                },
                "pad_token_id": {
                    "type": "string",
                    "description": "The token ID used for padding sequences to a fixed length.",
                    "required": true,
                    "default": "tokenizer.pad_token_id",
                    "example": "tokenizer.pad_token_id"
                },
                "device": {
                    "type": "string",
                    "description": "Specifies the hardware device to run the model on, such as 'cpu' or 'cuda'.",
                    "required": true,
                    "default": "cpu",
                    "example": "cuda"
                }
            },
            "response": "The tool described performs text generation using a pre-trained and fine-tuned conversational model, specifically a version of DialoGPT for the Russian language, trained on Russian forums and Telegram chat data.\n\n### Expected Response Structure\n\nThe response from this tool is the result of text generation by the model. The key elements of the response would include:\n\n1. **Generated Text (or Sequences):**\n   - The tool will return one or more text sequences generated by the model. This output is based on the model’s inference using the input provided to it (e.g., a prompt or conversation history).\n   - The number of returned sequences is specified by the `num_return_sequences` parameter (default is 1). This field will contain the generated text(s) in string format.\n\n2. **Completion Quality Considerations:**\n   - The model uses parameters such as `max_length`, `top_k`, `top_p`, `temperature`, and `no_repeat_ngram_size` to influence the style, coherence, and variability of the output. These parameters control aspects like the length of the response, randomness, and how much repetition is allowed.\n   - Based on the provided settings, the generated text will reflect:\n     - A reasonable maximum length (`max_length`).\n     - Sampling behavior (influenced by `temperature`, `top_k`, and `top_p` settings).\n     - Avoidance of repetition at the n-gram level (`no_repeat_ngram_size`).\n\n3. **Tokenization Information:**\n   - The response may include tokenized representations if the underlying framework or function generates any internal data related to tokenization. However, typically, this is abstracted and only the generated output (in string form) is returned.\n\n4. **Metadata:**\n   - If any metadata or additional information about the generation process is included, it might consist of:\n     - **Model Details:** The specific version of the model used, which in this case is the `ruDialoGpt3-medium-finetuned-telegram` variant fine-tuned on Telegram data.\n     - **Parameters Used:** Details of the specific configuration parameters like `device` and model settings might be referenced, though this is more likely to be internal unless explicitly requested.\n\n### Summary:\nThe tool's response is primarily a string of text (or multiple strings) representing the model's generated output, based on the conversational or prompt context provided. This text will adhere to the length, randomness, and coherence characteristics specified in the parameters. Additional metadata about the model or process is typically not included unless explicitly requested, and any token-level data is abstracted for general use cases.",
            "examples": [
                {
                    "user_intent": "I want the bot to continue my Telegram conversation in Russian after I send 'Привет, как дела?'",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "Привет, как дела?"
                        }
                    }
                },
                {
                    "user_intent": "Generate a friendly Russian response to someone complaining about a long day at work.",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "У меня был ужасно долгий день на работе."
                        }
                    }
                },
                {
                    "user_intent": "Simulate a casual chat in Russian about weekend plans with the Telegram bot.",
                    "tool_call": {
                        "name": "ruDialoGpt3-medium-finetuned-telegram",
                        "arguments": {
                            "num_return_sequences": 1,
                            "max_length": 512,
                            "no_repeat_ngram_size": 3,
                            "do_sample": true,
                            "top_k": 50,
                            "top_p": 0.9,
                            "temperature": 0.6,
                            "mask_token_id": "tokenizer.mask_token_id",
                            "eos_token_id": "tokenizer.eos_token_id",
                            "unk_token_id": "tokenizer.unk_token_id",
                            "pad_token_id": "tokenizer.pad_token_id",
                            "device": "cpu",
                            "prompt": "Какие у тебя планы на выходные?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\nTool 1: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 3: BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It can engage in multi-turn conversations, offering dynamic responses that include asking and answering questions, as well as displaying knowledge, empathy, and personality. Trained on the blended_skill_talk dataset, it is accessible through the Hugging Face Transformers library, making it ideal for building conversational agents.\nTool 4: `tinkoff-ai/ruDialoGPT-medium` is a Russian-language conversational AI model designed to generate human-like dialogue. Trained on a large corpus of conversational data, it can produce contextually relevant responses and be used to build generative chatbots or dialogue systems. The model supports multi-turn interactions with a context window of three turns, enabling coherent and context-aware conversation generation.\nTool 5: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 6: LEGAL-BERT is a lightweight BERT model designed for legal natural language processing tasks, offering efficient performance for legal research, computational law, and legal technology applications. It is a smaller, faster version of BERT-BASE, pre-trained on legal data, and delivers comparable accuracy to larger models while being about four times faster and requiring less computational resources.\nTool 7: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 8: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 9: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 10: Generate images from text descriptions using the Linaqruf/anything-v3.0 model, enabling users to create visual content directly from written prompts.\nTool 11: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 12: Pygmalion 6B is a conversational text-generation model based on GPT-J-6B, fine-tuned on diverse dialogue data to simulate realistic character interactions. It is designed to generate responses in a dialogue context, allowing users to role-play characters or engage in interactive conversations.\nTool 13: The \"michellejieli/emotion_text_classifier\" is a transformer-based sentiment analysis model fine-tuned on dialogue from the TV show *Friends* to classify emotions in text. It predicts six Ekman emotions—anger, disgust, fear, joy, sadness, and surprise—as well as a neutral class. This model is useful for analyzing emotions in text data, such as movie or TV show scripts, and is implemented using Hugging Face Transformers.\nTool 14: A text-to-video model that generates videos from textual descriptions, trained on the OpenAssistant/oasst1 dataset, enabling users to create visual content directly from text prompts.\nTool 15: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 16: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 17: This tool is a fine-tuned version of the LayoutLM model designed for question answering on invoices and other documents. It can handle both general comprehension tasks, trained on datasets like SQuAD2.0 and DocVQA, as well as specialized tasks involving invoices. Unlike traditional QA models that predict consecutive token sequences, it can also predict longer, non-consecutive sequences thanks to an additional classifier head. It uses the Hugging Face Transformers framework and requires the \"transformers\" Python library.\nTool 18: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\nTool 19: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A Russian-language conversational AI model based on DialoGPT, fine-tuned on Telegram chat data and Russian forum text, designed to generate coherent and contextually relevant responses in dialogue settings.\", \"parameters\": {\"num_return_sequences\": {\"type\": \"number\", \"description\": \"The number of different text sequences the model should generate for a single input prompt.\", \"required\": true, \"default\": 1, \"example\": 3}, \"max_length\": {\"type\": \"number\", \"description\": \"The maximum number of tokens to generate in the output sequence.\", \"required\": true, \"default\": 512, \"example\": 256}, \"no_repeat_ngram_size\": {\"type\": \"number\", \"description\": \"Prevents the model from repeating any n-grams of this size in the generated text.\", \"required\": true, \"default\": 3, \"example\": 2}, \"do_sample\": {\"type\": \"boolean\", \"description\": \"Whether to use sampling; if false, the model will use greedy decoding.\", \"required\": true, \"default\": true, \"example\": false}, \"top_k\": {\"type\": \"number\", \"description\": \"The number of highest probability vocabulary tokens to keep for top-k-filtering during sampling.\", \"required\": true, \"default\": 50, \"example\": 30}, \"top_p\": {\"type\": \"number\", \"description\": \"The cumulative probability threshold for nucleus (top-p) sampling. Tokens are considered until the cumulative probability exceeds this value.\", \"required\": true, \"default\": 0.9, \"example\": 0.8}, \"temperature\": {\"type\": \"number\", \"description\": \"Controls the randomness of predictions by scaling the logits before applying softmax. Higher values produce more random outputs.\", \"required\": true, \"default\": 0.6, \"example\": 1.0}, \"mask_token_id\": {\"type\": \"string\", \"description\": \"The token ID used to represent masked tokens in the input text, typically provided by the tokenizer.\", \"required\": true, \"default\": \"tokenizer.mask_token_id\", \"example\": \"tokenizer.mask_token_id\"}, \"eos_token_id\": {\"type\": \"string\", \"description\": \"The token ID representing the end-of-sequence marker, as defined by the tokenizer.\", \"required\": true, \"default\": \"tokenizer.eos_token_id\", \"example\": \"tokenizer.eos_token_id\"}, \"unk_token_id\": {\"type\": \"string\", \"description\": \"The token ID used for unknown tokens that the model cannot recognize.\", \"required\": true, \"default\": \"tokenizer.unk_token_id\", \"example\": \"tokenizer.unk_token_id\"}, \"pad_token_id\": {\"type\": \"string\", \"description\": \"The token ID used for padding sequences to a fixed length.\", \"required\": true, \"default\": \"tokenizer.pad_token_id\", \"example\": \"tokenizer.pad_token_id\"}, \"device\": {\"type\": \"string\", \"description\": \"Specifies the hardware device to run the model on, such as 'cpu' or 'cuda'.\", \"required\": true, \"default\": \"cpu\", \"example\": \"cuda\"}}, \"response\": \"The tool described performs text generation using a pre-trained and fine-tuned conversational model, specifically a version of DialoGPT for the Russian language, trained on Russian forums and Telegram chat data.\\n\\n### Expected Response Structure\\n\\nThe response from this tool is the result of text generation by the model. The key elements of the response would include:\\n\\n1. **Generated Text (or Sequences):**\\n   - The tool will return one or more text sequences generated by the model. This output is based on the model\\u2019s inference using the input provided to it (e.g., a prompt or conversation history).\\n   - The number of returned sequences is specified by the `num_return_sequences` parameter (default is 1). This field will contain the generated text(s) in string format.\\n\\n2. **Completion Quality Considerations:**\\n   - The model uses parameters such as `max_length`, `top_k`, `top_p`, `temperature`, and `no_repeat_ngram_size` to influence the style, coherence, and variability of the output. These parameters control aspects like the length of the response, randomness, and how much repetition is allowed.\\n   - Based on the provided settings, the generated text will reflect:\\n     - A reasonable maximum length (`max_length`).\\n     - Sampling behavior (influenced by `temperature`, `top_k`, and `top_p` settings).\\n     - Avoidance of repetition at the n-gram level (`no_repeat_ngram_size`).\\n\\n3. **Tokenization Information:**\\n   - The response may include tokenized representations if the underlying framework or function generates any internal data related to tokenization. However, typically, this is abstracted and only the generated output (in string form) is returned.\\n\\n4. **Metadata:**\\n   - If any metadata or additional information about the generation process is included, it might consist of:\\n     - **Model Details:** The specific version of the model used, which in this case is the `ruDialoGpt3-medium-finetuned-telegram` variant fine-tuned on Telegram data.\\n     - **Parameters Used:** Details of the specific configuration parameters like `device` and model settings might be referenced, though this is more likely to be internal unless explicitly requested.\\n\\n### Summary:\\nThe tool's response is primarily a string of text (or multiple strings) representing the model's generated output, based on the conversational or prompt context provided. This text will adhere to the length, randomness, and coherence characteristics specified in the parameters. Additional metadata about the model or process is typically not included unless explicitly requested, and any token-level data is abstracted for general use cases.\", \"examples\": [{\"user_intent\": \"I want the bot to continue my Telegram conversation in Russian after I send '\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?'\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?\"}}}, {\"user_intent\": \"Generate a friendly Russian response to someone complaining about a long day at work.\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u0423 \\u043c\\u0435\\u043d\\u044f \\u0431\\u044b\\u043b \\u0443\\u0436\\u0430\\u0441\\u043d\\u043e \\u0434\\u043e\\u043b\\u0433\\u0438\\u0439 \\u0434\\u0435\\u043d\\u044c \\u043d\\u0430 \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435.\"}}}, {\"user_intent\": \"Simulate a casual chat in Russian about weekend plans with the Telegram bot.\", \"tool_call\": {\"name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\", \"prompt\": \"\\u041a\\u0430\\u043a\\u0438\\u0435 \\u0443 \\u0442\\u0435\\u0431\\u044f \\u043f\\u043b\\u0430\\u043d\\u044b \\u043d\\u0430 \\u0432\\u044b\\u0445\\u043e\\u0434\\u043d\\u044b\\u0435?\"}}}]}\n\n# User Query\nWe need to create a dialogue in Russian for our educational app. It should cover a general greeting and asking about the users' well-being.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a Russian-language conversation for an educational app including greetings and asking about well-being.",
                    "tool_description": "Use a Russian-language conversational AI model fine-tuned on dialogue data to produce contextually coherent and relevant multi-turn conversational text.",
                    "expected_response": "One or more generated Russian text sequences simulating a friendly educational dialogue with greetings and wellness inquiries."
                }
            ],
            "extracted_arguments": {
                "conversation_topic": "The subject of the dialogue, in this case general greetings and checking well-being, as a text description.",
                "language": "The language in which the conversation should be generated, specified as a string (e.g., Russian).",
                "context_type": "The intended context or use case for the conversation, such as an educational app dialogue."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_234",
        "query": "We need a solution for creating Polish subtitles for YouTube videos in Spanish. The AI should provide the translation.",
        "instruction": "Given a `subtitle translation` task, retrieve tools that facilitate language translation by processing video language inputs and generating translated subtitles in the target language, aligning with the task's requirement for multilingual support.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Hugging Face",
                    "functionality": "Text2Text Generation",
                    "api_call": "MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')",
                    "api_arguments": {
                        "pretrained_model_name_or_path": "facebook/mbart-large-50-many-to-many-mmt"
                    },
                    "python_environment_requirements": {
                        "transformers": "4.0.0"
                    },
                    "example_code": "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi model = MBartForConditionalGeneration.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer = MBart50TokenizerFast.from_pretrained(facebook/mbart-large-50-many-to-many-mmt)\ntokenizer.src_lang = hi_IN\nencoded_hi = tokenizer(article_hi, return_tensors=pt)\ngenerated_tokens = model.generate(\n **encoded_hi,\n forced_bos_token_id=tokenizer.lang_code_to_id[fr_XX]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)",
                    "performance": {
                        "dataset": "Multilingual Translation",
                        "accuracy": "Not specified"
                    },
                    "description": "mBART-50 many-to-many multilingual machine translation model can translate directly between any pair of 50 languages. It was introduced in the Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.",
                    "id": "huggingface_api_661",
                    "name": "facebook/mbart-large-50-many-to-many-mmt"
                },
                "id": "gorilla_huggingface_tool_656",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 1: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.",
            "Tool 2: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.",
            "Tool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.",
            "Tool 4: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.",
            "Tool 5: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.",
            "Tool 6: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.",
            "Tool 7: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.",
            "Tool 8: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.",
            "Tool 9: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.",
            "Tool 10: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.",
            "Tool 11: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.",
            "Tool 12: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.",
            "Tool 13: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 14: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 15: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 17: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.",
            "Tool 18: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation."
        ],
        "tool_view": {
            "description": "A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "parameters": {},
            "response": "The tool performs token classification using a fine-tuned model based on the XLM-RoBERTa architecture, specifically designed for punctuation prediction across twelve different languages. The expected response from this tool would include the following components:\n\n1. **Predicted Token Classes**: Each token in the input text is assigned a class label, indicating its predicted punctuation. This could include tokens such as words or punctuation marks, with labels like \"PUNCT\" for punctuation or \"NO_PUNCT\" for non-punctuation tokens.\n\n2. **Token Information**: The response will likely include the tokenized version of the input text. For each token, the corresponding prediction will be associated with the token.\n\n3. **Language Information**: Since the model supports multiple languages, the response could potentially indicate which language is being processed for each token prediction, ensuring compatibility with any multilingual inputs.\n\n4. **Confidence Scores**: The response might include confidence scores (e.g., probabilities or logits) indicating the model's certainty in its predictions for each token class. This helps assess the reliability of the punctuation assignment.\n\nThe output is primarily intended to indicate the punctuation placement in a sequence of tokens, with each token classified as either part of punctuation or not, based on the model’s predictions.",
            "examples": [
                {
                    "user_intent": "I have a paragraph in English without punctuation. Can you automatically add the correct punctuation marks?",
                    "tool_call": {
                        "name": "kredor/punctuate-all",
                        "arguments": {
                            "text": "hello my name is john i love programming and reading books"
                        }
                    }
                },
                {
                    "user_intent": "Please punctuate this French sentence properly so it reads naturally.",
                    "tool_call": {
                        "name": "kredor/punctuate-all",
                        "arguments": {
                            "text": "bonjour je m'appelle claire et je suis ravie de vous rencontrer"
                        }
                    }
                },
                {
                    "user_intent": "I need the correct punctuation added to a mixed-language document containing Spanish and Italian text.",
                    "tool_call": {
                        "name": "kredor/punctuate-all",
                        "arguments": {
                            "text": "hola me llamo luca e sono felice di essere qui con vosotros"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 1: This tool provides a German-to-English translation model, trained on the OPUS dataset and implemented using the Hugging Face Transformers library. It allows for automatic translation of text from German (de) to English (en), offering a practical solution for language translation tasks.\nTool 2: This tool provides English-to-Spanish translation using the Hugging Face Transformers library. It leverages the Helsinki-NLP Marian model trained on the OPUS dataset to convert English text into Spanish and is useful for applications that require automated language translation.\nTool 3: A French-to-Spanish translation model built with Hugging Face Transformers, trained on the OPUS dataset. It uses a transformer-based architecture with normalization and SentencePiece preprocessing to accurately translate French text into Spanish, making it suitable for natural language translation tasks.\nTool 4: A Hugging Face diffusion-based model that generates videos from English text prompts, producing video content that visually matches the provided description. It is used for creating text-driven videos through multi-stage text-to-video synthesis.\nTool 5: A neural machine translation model that translates text from English to Portuguese, part of the OPUS-MT project designed to provide accessible and widely available multilingual translation models.\nTool 6: A speech-to-speech translation tool that converts Romanian audio into English, enabling real-time or recorded audio translation using a model developed by Facebook AI.\nTool 7: Helsinki-NLP/opus-mt-es-en is a machine translation model that converts Spanish text into English using the Hugging Face Transformers library. It leverages the Marian framework and is trained on the OPUS dataset, achieving reliable translation quality across standard Spanish-English benchmarks, making it suitable for automated multilingual text processing and cross-lingual applications.\nTool 8: The Helsinki-NLP/opus-mt-zh-en is a Chinese-to-English translation model developed by the University of Helsinki's Language Technology Research Group. It is based on the Marian NMT framework and trained on the OPUS dataset, offering a reliable solution for text translation. The model can be accessed via Hugging Face Transformers and provides solid translation performance, with an accuracy of 36.1 BLEU and 0.548 chr-F.\nTool 9: The \"textless_sm_sl_es\" is a Fairseq model designed for audio-to-audio speech-to-speech translation, enabling the conversion of spoken language from Slovenian to Spanish.\nTool 10: BlenderBot-1B-distill is a smaller, faster version of the BlenderBot-1B model, designed for engaging in open-domain conversations, answering questions, and displaying knowledge, empathy, and personality. It is optimized for use in applications where conversational AI is required, offering a more accessible solution compared to the original larger model.\nTool 11: A German-to-Spanish translation tool that uses a transformer-based model trained on the OPUS dataset, providing accurate translations with preprocessing that includes normalization and SentencePiece tokenization.\nTool 12: A Hugging Face transformer model for translating text between Catalan and Spanish, trained on the OPUS dataset with preprocessing using normalization and SentencePiece, offering high-quality translation performance as measured by BLEU and chr-F scores.\nTool 13: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 14: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 15: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 16: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 17: An end-to-end Vietnamese automatic speech recognition tool based on wav2vec 2.0, pre-trained on 13,000 hours of unlabeled Vietnamese YouTube audio and fine-tuned on 250 hours of labeled VLSP ASR data, designed to convert 16 kHz Vietnamese speech recordings into text.\nTool 18: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 19: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\", \"parameters\": {}, \"response\": \"The tool performs token classification using a fine-tuned model based on the XLM-RoBERTa architecture, specifically designed for punctuation prediction across twelve different languages. The expected response from this tool would include the following components:\\n\\n1. **Predicted Token Classes**: Each token in the input text is assigned a class label, indicating its predicted punctuation. This could include tokens such as words or punctuation marks, with labels like \\\"PUNCT\\\" for punctuation or \\\"NO_PUNCT\\\" for non-punctuation tokens.\\n\\n2. **Token Information**: The response will likely include the tokenized version of the input text. For each token, the corresponding prediction will be associated with the token.\\n\\n3. **Language Information**: Since the model supports multiple languages, the response could potentially indicate which language is being processed for each token prediction, ensuring compatibility with any multilingual inputs.\\n\\n4. **Confidence Scores**: The response might include confidence scores (e.g., probabilities or logits) indicating the model's certainty in its predictions for each token class. This helps assess the reliability of the punctuation assignment.\\n\\nThe output is primarily intended to indicate the punctuation placement in a sequence of tokens, with each token classified as either part of punctuation or not, based on the model\\u2019s predictions.\", \"examples\": [{\"user_intent\": \"I have a paragraph in English without punctuation. Can you automatically add the correct punctuation marks?\", \"tool_call\": {\"name\": \"kredor/punctuate-all\", \"arguments\": {\"text\": \"hello my name is john i love programming and reading books\"}}}, {\"user_intent\": \"Please punctuate this French sentence properly so it reads naturally.\", \"tool_call\": {\"name\": \"kredor/punctuate-all\", \"arguments\": {\"text\": \"bonjour je m'appelle claire et je suis ravie de vous rencontrer\"}}}, {\"user_intent\": \"I need the correct punctuation added to a mixed-language document containing Spanish and Italian text.\", \"tool_call\": {\"name\": \"kredor/punctuate-all\", \"arguments\": {\"text\": \"hola me llamo luca e sono felice di essere qui con vosotros\"}}}]}\n\n# User Query\nWe need a solution for creating Polish subtitles for YouTube videos in Spanish. The AI should provide the translation.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate the Spanish video transcript into Polish.",
                    "tool_description": "Use a machine translation model to convert text from Spanish into Polish, ensuring the translation is suitable for subtitles.",
                    "expected_response": "A text string representing the Spanish content translated into Polish."
                },
                {
                    "user_intent": "Generate properly punctuated Polish subtitles for the translated text.",
                    "tool_description": "Apply a multilingual punctuation prediction model to the translated Polish text to ensure accurate punctuation for subtitle readability.",
                    "expected_response": "The translated Polish text with correct punctuation added for subtitle use."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the original video content (Spanish).",
                "target_language": "The language for the subtitles (Polish).",
                "video_transcript": "The textual content of the YouTube video to be translated and punctuated."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_329",
        "query": "Generate a classical image by using Diffusion Model",
        "instruction": "Given an `image generation` task, retrieve tools that utilize diffusion models to create classical images by processing relevant input parameters and returning an image based on the predefined model and dataset.",
        "labels": [
            {
                "doc": {
                    "domain": "Computer Vision Unconditional Image Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Unconditional Image Generation",
                    "api_call": "DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')",
                    "api_arguments": "",
                    "python_environment_requirements": "diffusers",
                    "example_code": "from diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\nimage = pipeline().images[0]\nimage",
                    "performance": {
                        "dataset": "https://huggingface.co/datasets/huggan/wikiart",
                        "accuracy": "Not provided"
                    },
                    "description": "This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.",
                    "id": "huggingface_api_297",
                    "name": "johnowhitaker/sd-class-wikiart-from-bedrooms"
                },
                "id": "gorilla_huggingface_tool_293",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.",
            "Tool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.",
            "Tool 2: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.",
            "Tool 3: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.",
            "Tool 4: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.",
            "Tool 5: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.",
            "Tool 6: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.",
            "Tool 7: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.",
            "Tool 8: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.",
            "Tool 9: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.",
            "Tool 10: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.",
            "Tool 11: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.",
            "Tool 12: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.",
            "Tool 13: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.",
            "Tool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.",
            "Tool 15: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.",
            "Tool 16: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.",
            "Tool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.",
            "Tool 18: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.",
            "Tool 19: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions."
        ],
        "tool_view": {
            "description": "Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.",
            "parameters": {},
            "response": "The tool returns a single generated image as its primary output. The response is structured as an object containing an attribute that holds the generated image data, typically in an image object format compatible with Python imaging libraries (e.g., PIL Image). Specifically:  \n\n- **images**: A list of generated images. In this tool, the list usually contains a single image resulting from the unconditional generation process. Each element represents an image object that can be displayed, saved, or further processed.  \n\nNo additional metadata, labels, or performance metrics are included in the response by default. The response is purely the visual content produced by the diffusion model.",
            "examples": [
                {
                    "user_intent": "Generate a random, high-quality image of the universe using a pre-trained diffusion model.",
                    "tool_call": {
                        "name": "ocariz/universe_1400",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "I want to create an abstract cosmic image for a science presentation.",
                    "tool_call": {
                        "name": "ocariz/universe_1400",
                        "arguments": {}
                    }
                },
                {
                    "user_intent": "Produce a visually stunning space scene for a digital art project without specifying any details.",
                    "tool_call": {
                        "name": "ocariz/universe_1400",
                        "arguments": {}
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\nTool 1: Generate custom Minecraft skins using a diffusion-based unconditional image generation model, allowing users to create unique skin designs without any input prompts.\nTool 2: Generate images of cute butterflies using a diffusion-based model for unconditional image generation, allowing users to create butterfly images without providing any input conditions.\nTool 3: Generate high-quality, high-resolution images of human faces using a Latent Diffusion Model trained on the CelebA-HQ dataset, enabling efficient and realistic image synthesis without conditioning on specific inputs.\nTool 4: Stable Diffusion v2-base is a diffusion-based text-to-image generation model that creates and modifies images from textual prompts. Trained on a subset of the LAION-5B dataset and using a fixed OpenCLIP-ViT/H text encoder, it enables users to generate high-quality images from descriptive text for research and creative experimentation.\nTool 5: This tool utilizes Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality images based on a pretrained model. It allows for image synthesis using diffusion models, inspired by nonequilibrium thermodynamics, and is implemented with the Hugging Face Transformers framework. This tool is ideal for creating images from scratch or enhancing image quality, leveraging pretrained models like \"google/ddpm-ema-celebahq-256\" to produce realistic visuals.\nTool 6: This tool is a diffusion-based image generation model that creates images unconditionally, trained on the WikiArt dataset. It can generate artwork-style images starting from a pretrained bedroom-themed diffusion model and is designed for use with the Hugging Face Diffusers library.\nTool 7: A diffusion-based image generation model that creates Minecraft skin images unconditionally, allowing users to generate unique character skins without any input prompts.\nTool 8: The \"lllyasviel/sd-controlnet-scribble\" model is a neural network designed to control diffusion models by adding extra conditions, specifically using scribble images. It is used in combination with Stable Diffusion to generate images based on text and scribble input, offering enhanced control over the output. This model is ideal for tasks requiring detailed image manipulation based on simple sketch inputs.\nTool 9: The Apocalypse-19/shoe-generator is a diffusion model designed for unconditional image generation of shoes. It is trained on a custom dataset at a 128x128 resolution and can generate images without requiring specific input conditions. The model is implemented using Hugging Face Transformers and the Diffusers library.\nTool 10: This tool uses the waifu-diffusion model, a text-to-image latent diffusion model fine-tuned on high-quality anime images, to generate anime-style images from textual prompts, enabling users to create detailed and stylistically consistent anime visuals based on descriptive input.\nTool 11: The \"lllyasviel/sd-controlnet-normal\" tool is a neural network model designed to control diffusion models by incorporating additional conditions, specifically for Normal Map Estimation. It can be used in combination with Stable Diffusion to generate normal maps from input images, enabling more detailed and controlled image transformations. This tool is part of the ControlNet framework and is utilized for advanced image-to-image tasks, particularly in computer vision applications.\nTool 12: This tool generates images of cute butterflies using a diffusion model trained for 200 epochs, providing unconditional image generation capabilities. It is implemented with Hugging Face Transformers and requires the \"diffusers\" library in the Python environment. The model is accessed via the DDPMPipeline, and it can generate images without any input conditions.\nTool 13: ControlNet v1.1 is a diffusion-based text-to-image generation model that enhances pretrained large diffusion models by allowing additional input conditions, specifically soft edge maps. It is designed to work with Stable Diffusion pipelines to guide image generation using structured inputs, enabling more precise control over the output while translating textual prompts into detailed images.\nTool 14: Generate images of cute butterflies using an unconditional diffusion model, allowing users to create butterfly visuals without providing any specific input conditions.\nTool 15: Stable Diffusion v2-1 is a text-to-image generation model that creates and modifies images from English text prompts. Designed for research purposes, it uses a diffusion-based approach trained on a subset of the LAION-5B dataset, enabling users to generate high-quality, creative visuals such as photorealistic or artistic scenes based on descriptive input.\nTool 16: This tool provides high-quality unconditional image generation using Denoising Diffusion Probabilistic Models (DDPM). It leverages a pretrained model from Hugging Face to generate realistic 256×256 bedroom images, producing outputs with strong perceptual quality as measured by metrics like Inception score and FID. The model is useful for tasks that require photorealistic image synthesis without conditioning inputs, and it can be easily integrated into Python workflows using the `diffusers` library.\nTool 17: The Stable Diffusion x4 upscaler is a text-to-image latent diffusion model designed to generate and enhance images based on text prompts. It can upscale low-resolution images while optionally applying controlled noise to refine details, making it useful for producing higher-quality visuals from smaller inputs. Trained on high-resolution images with English captions, it supports creative image generation and modification within a multimodal workflow.\nTool 18: This tool provides an unconditional image generation model based on Denoising Diffusion Probabilistic Models (DDPM), allowing the creation of high-quality synthetic images. It is implemented with Hugging Face’s Transformers framework and can generate images from random noise using various diffusion schedulers. The model achieves strong performance on standard benchmarks, including an Inception score of 9.46 and an FID score of 3.17 on the CIFAR10 dataset, making it suitable for research or creative applications in generative computer vision.\nTool 19: Generate high-quality, unconditional images of animals using a diffusion model trained on the AFHQ dataset, allowing users to create visually appealing images without any input conditions.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Generate images of the universe without any input conditions using a diffusion model trained for 1400 epochs, allowing for creative and diverse unconditional image synthesis.\", \"parameters\": {}, \"response\": \"The tool returns a single generated image as its primary output. The response is structured as an object containing an attribute that holds the generated image data, typically in an image object format compatible with Python imaging libraries (e.g., PIL Image). Specifically:  \\n\\n- **images**: A list of generated images. In this tool, the list usually contains a single image resulting from the unconditional generation process. Each element represents an image object that can be displayed, saved, or further processed.  \\n\\nNo additional metadata, labels, or performance metrics are included in the response by default. The response is purely the visual content produced by the diffusion model.\", \"examples\": [{\"user_intent\": \"Generate a random, high-quality image of the universe using a pre-trained diffusion model.\", \"tool_call\": {\"name\": \"ocariz/universe_1400\", \"arguments\": {}}}, {\"user_intent\": \"I want to create an abstract cosmic image for a science presentation.\", \"tool_call\": {\"name\": \"ocariz/universe_1400\", \"arguments\": {}}}, {\"user_intent\": \"Produce a visually stunning space scene for a digital art project without specifying any details.\", \"tool_call\": {\"name\": \"ocariz/universe_1400\", \"arguments\": {}}}]}\n\n# User Query\nGenerate a classical image by using Diffusion Model\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Generate a high-quality classical-style image using a diffusion model.",
                    "tool_description": "Use a pretrained diffusion-based image generation model to create an image in a classical art style without requiring specific input prompts.",
                    "expected_response": "An object containing the generated image data, typically as a single image or a list with one image object suitable for display or further processing."
                }
            ],
            "extracted_arguments": {
                "art_style": "The target artistic style for the generated image, represented as a string or descriptive label."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_468",
        "query": "A researcher needs information about how to use the Whisper ASR model to transcribe and analyze the sentiment of an audio file.",
        "instruction": "Given an `audio transcription and sentiment analysis` task, retrieve tools that combine Automatic Speech Recognition (ASR) functionalities with sentiment analysis capabilities by processing audio inputs to accurately transcribe and analyze sentiment from spoken content, in alignment with the query's requirements and utilizing models such as Whisper ASR.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Automatic Speech Recognition",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Automatic Speech Recognition and Speech Translation",
                    "api_call": "WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                    "api_arguments": {
                        "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                    },
                    "python_environment_requirements": [
                        "transformers",
                        "datasets"
                    ],
                    "example_code": [
                        "from transformers import WhisperProcessor, WhisperForConditionalGeneration",
                        "from datasets import load_dataset",
                        "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')",
                        "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')",
                        "model.config.forced_decoder_ids = None",
                        "ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')",
                        "sample = ds[0]['audio']",
                        "input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features",
                        "predicted_ids = model.generate(input_features)",
                        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
                    ],
                    "performance": {
                        "dataset": "LibriSpeech test-clean",
                        "accuracy": 3.0003583080317573
                    },
                    "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.",
                    "id": "huggingface_api_757",
                    "name": "openai/whisper-large-v2"
                },
                "id": "gorilla_huggingface_tool_752",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "Tool 1: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.",
            "Tool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.",
            "Tool 3: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.",
            "Tool 4: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.",
            "Tool 5: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.",
            "Tool 6: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.",
            "Tool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.",
            "Tool 8: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.",
            "Tool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.",
            "Tool 10: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 11: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.",
            "Tool 12: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.",
            "Tool 13: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 14: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.",
            "Tool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 16: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 17: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.",
            "Tool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.",
            "Tool 19: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio."
        ],
        "tool_view": {
            "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.",
            "parameters": {
                "forced_decoder_ids": {
                    "type": "string",
                    "description": "Specifies the decoder prompt IDs used by the model to enforce a particular language or task during generation. Typically obtained from `WhisperProcessor.get_decoder_prompt_ids` with the desired language and task.",
                    "required": true,
                    "example": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                }
            },
            "response": "The tool produces a response that represents the transcription or translation of input audio into text. The typical structure and semantic components of the response are as follows:\n\n1. **Transcribed Text**  \n   - This is the main output of the tool. It is a string (or a list of strings if batch decoding is used) containing the spoken content extracted from the input audio.  \n   - The text may be in the same language as the audio (for ASR) or in a target language (for speech translation), depending on the configuration of the `forced_decoder_ids` or decoder prompt.\n\n2. **Batch Structure (Optional)**  \n   - If multiple audio samples are processed together, the response is structured as a list of transcriptions, where each element corresponds to one audio sample.\n\n3. **Decoded Tokens (Optional, Internal)**  \n   - Internally, the model generates token IDs (`predicted_ids`) which are then decoded into human-readable text. While not always exposed in the final output, these token sequences represent the intermediate model output before decoding.\n\n4. **Special Tokens Handling**  \n   - The processor’s `batch_decode` method typically removes special tokens (like padding or start/end markers) to produce clean text.\n\n5. **Associated Metadata (Optional)**  \n   - Depending on the implementation, the response could also include auxiliary information such as confidence scores, timestamps for words or segments, or language detection results. However, the standard usage described focuses primarily on the textual transcription.\n\nIn essence, the response is primarily a textual representation of the audio content, formatted either as a single string or a list of strings for multiple inputs, optionally accompanied by decoding-related metadata.",
            "examples": [
                {
                    "user_intent": "Transcribe a short English podcast clip into text.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "model_name": "openai/whisper-large-v2",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                        }
                    }
                },
                {
                    "user_intent": "Convert a French interview audio file into English text using Whisper.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "model_name": "openai/whisper-large-v2",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='translate')"
                        }
                    }
                },
                {
                    "user_intent": "Quickly transcribe a batch of audio recordings from a meeting into readable text.",
                    "tool_call": {
                        "name": "WhisperForConditionalGeneration.from_pretrained",
                        "arguments": {
                            "model_name": "openai/whisper-large-v2",
                            "forced_decoder_ids": "WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\nTool 1: Whisper is a pre-trained automatic speech recognition (ASR) and speech translation model developed by OpenAI. Based on a Transformer architecture, it can transcribe and translate audio in multiple languages. Trained on 680,000 hours of labeled data, it excels in generalizing across various datasets and domains without requiring fine-tuning. The tool can be used for accurate transcription tasks and supports integration within Python environments using libraries like Hugging Face Transformers and Datasets.\nTool 2: This tool uses a fine-tuned version of the Wav2Vec2 model for sentiment classification of Spanish audio, trained on the MESD dataset. It can be used to analyze the underlying sentiment in speech, offering high accuracy for this specific task.\nTool 3: A lightweight pre-trained model for automatic speech recognition that transcribes English audio into text. Whisper-tiny.en leverages the Hugging Face Transformers framework and is designed for fast, general-purpose ASR without fine-tuning, making it suitable for converting speech from diverse datasets into accurate text.\nTool 4: The Data2Vec-Audio-Base-960h model by Facebook is a pre-trained Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of 16kHz sampled Librispeech data. It is designed to transcribe audio files and delivers competitive performance on major speech recognition benchmarks. The model leverages the Data2Vec framework, which applies the same learning method across speech, natural language processing, and computer vision tasks.\nTool 5: Whisper-large is a pre-trained model for automatic speech recognition (ASR) and speech translation, capable of converting spoken audio into text and translating speech across languages. Trained on 680,000 hours of labeled data, it generalizes well across diverse datasets and domains, enabling robust transcription and translation without additional fine-tuning.\nTool 6: FinBERT is a pre-trained NLP model designed to analyze the sentiment of financial text. Built by further training BERT on a large financial corpus, it is fine-tuned specifically for financial sentiment classification, enabling accurate detection of positive, negative, or neutral sentiment in financial documents and communications.\nTool 7: Donut is a model that uses a vision encoder (Swin Transformer) and a text decoder (BART) to generate text from images. It processes an image by encoding it into a tensor of embeddings, and then the decoder generates text based on these embeddings. This model is useful for tasks where visual input needs to be translated into textual descriptions or information.\nTool 8: This tool performs speech enhancement using the SpeechBrain framework, specifically with the pre-trained \"mtl-mimic-voicebank\" model. It enhances audio quality by improving speech clarity and robustness, which can be useful for applications like automatic speech recognition (ASR) training. The model's performance includes metrics such as PESQ, COVL, and WER, demonstrating its effectiveness in enhancing speech signals. To use it, users load the model and apply it to audio files to generate enhanced outputs.\nTool 9: This is a tabular regression model, trained with AutoTrain, designed to predict carbon emissions. It uses Joblib for model loading and requires a Python environment with JSON, Joblib, and Pandas. The model can be loaded using `joblib.load('model.joblib')` and makes predictions based on data processed from a CSV file. The example code provided demonstrates how to load the model, configure features, and apply it to a dataset.\nTool 10: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 11: This tool is a Hugging Face Transformers model fine-tuned for language identification in audio recordings. It uses a version of OpenAI's Whisper-medium trained on the FLEURS subset of the google/xtreme_s dataset, enabling accurate classification of spoken languages in audio data.\nTool 12: A tool that uses the Whisper large-v2 model via CTranslate2 to perform automatic speech recognition on audio files, enabling transcription of speech in multiple languages with segment-level timing, suitable for projects that require efficient and accurate audio-to-text conversion.\nTool 13: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 14: A pretrained speech recognition model from Facebook that uses Wav2Vec2 to transcribe 16 kHz audio into text. It is fine-tuned on 960 hours of Librispeech data and is designed for accurate automatic speech recognition of clean and noisy speech.\nTool 15: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 16: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 17: This tool performs speech enhancement by denoising audio using a pretrained SepFormer model implemented in SpeechBrain. It is designed to improve the clarity of speech in recordings with background noise or reverberation and is pretrained on the WHAM! dataset. Users can input an audio file to obtain an enhanced version of the speech, making it useful for applications in audio preprocessing, transcription, and voice communication.\nTool 18: Whisper is a pre-trained model for automatic speech recognition and speech translation, capable of transcribing and translating audio across diverse datasets and domains without additional fine-tuning. It is designed to convert spoken language into text with high accuracy, making it useful for transcription, multilingual speech processing, and real-time audio understanding.\nTool 19: The \"facebook/hubert-large-ls960-ft\" is an Automatic Speech Recognition (ASR) model fine-tuned on 960 hours of Librispeech data with 16kHz sampled speech audio. Built on the HuBERT (Hidden-Unit BERT) framework, it uses self-supervised speech representation learning to achieve high accuracy in transcribing speech. The model performs similarly or better than state-of-the-art systems like wav2vec 2.0 on various ASR benchmarks, including Librispeech and Libri-light. It is ideal for converting speech into text, leveraging Hugging Face Transformers and the Wav2Vec2Processor for processing input audio.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation that can transcribe and translate spoken audio into text. Trained on 680,000 hours of labeled data, it offers strong generalization across diverse datasets and domains, making it suitable for applications that require accurate transcription or multilingual speech understanding without additional fine-tuning.\", \"parameters\": {\"forced_decoder_ids\": {\"type\": \"string\", \"description\": \"Specifies the decoder prompt IDs used by the model to enforce a particular language or task during generation. Typically obtained from `WhisperProcessor.get_decoder_prompt_ids` with the desired language and task.\", \"required\": true, \"example\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}, \"response\": \"The tool produces a response that represents the transcription or translation of input audio into text. The typical structure and semantic components of the response are as follows:\\n\\n1. **Transcribed Text**  \\n   - This is the main output of the tool. It is a string (or a list of strings if batch decoding is used) containing the spoken content extracted from the input audio.  \\n   - The text may be in the same language as the audio (for ASR) or in a target language (for speech translation), depending on the configuration of the `forced_decoder_ids` or decoder prompt.\\n\\n2. **Batch Structure (Optional)**  \\n   - If multiple audio samples are processed together, the response is structured as a list of transcriptions, where each element corresponds to one audio sample.\\n\\n3. **Decoded Tokens (Optional, Internal)**  \\n   - Internally, the model generates token IDs (`predicted_ids`) which are then decoded into human-readable text. While not always exposed in the final output, these token sequences represent the intermediate model output before decoding.\\n\\n4. **Special Tokens Handling**  \\n   - The processor\\u2019s `batch_decode` method typically removes special tokens (like padding or start/end markers) to produce clean text.\\n\\n5. **Associated Metadata (Optional)**  \\n   - Depending on the implementation, the response could also include auxiliary information such as confidence scores, timestamps for words or segments, or language detection results. However, the standard usage described focuses primarily on the textual transcription.\\n\\nIn essence, the response is primarily a textual representation of the audio content, formatted either as a single string or a list of strings for multiple inputs, optionally accompanied by decoding-related metadata.\", \"examples\": [{\"user_intent\": \"Transcribe a short English podcast clip into text.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"model_name\": \"openai/whisper-large-v2\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}}, {\"user_intent\": \"Convert a French interview audio file into English text using Whisper.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"model_name\": \"openai/whisper-large-v2\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='translate')\"}}}, {\"user_intent\": \"Quickly transcribe a batch of audio recordings from a meeting into readable text.\", \"tool_call\": {\"name\": \"WhisperForConditionalGeneration.from_pretrained\", \"arguments\": {\"model_name\": \"openai/whisper-large-v2\", \"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}}}]}\n\n# User Query\nA researcher needs information about how to use the Whisper ASR model to transcribe and analyze the sentiment of an audio file.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Transcribe an audio file into text using the Whisper ASR model.",
                    "tool_description": "Use a pre-trained Whisper automatic speech recognition model to convert spoken audio into textual transcription, supporting multiple languages and requiring decoder prompt configuration.",
                    "expected_response": "A text string or list of strings representing the transcribed content of the audio, optionally including segment timestamps or decoding metadata."
                },
                {
                    "user_intent": "Analyze the sentiment of the transcribed text.",
                    "tool_description": "Apply a text-based sentiment analysis model to the transcribed text to classify it as positive, negative, or neutral, providing insight into the emotional tone of the spoken content.",
                    "expected_response": "A sentiment label or score indicating the emotional polarity of the input text."
                }
            ],
            "extracted_arguments": {
                "audio_file": "The audio input to be transcribed, typically in a standard audio format like WAV or MP3.",
                "forced_decoder_ids": "Decoder prompt IDs specifying the language and task configuration for the Whisper model during transcription.",
                "sentiment_model": "The pre-trained text sentiment analysis model to apply on the transcribed content."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_452",
        "query": "We are an online platform that offers various Chinese language courses for students. Now we need to autoplay a fill-in-the-blank video game for our users.",
        "instruction": "Given a `video game automation` task, retrieve tools that facilitate the integration and execution of interactive multimedia elements in educational platforms, specifically those capable of autogenerating and managing fill-in-the-blank style content for user engagement.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Fill-Mask",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Masked Language Modeling",
                    "api_call": "AutoModelForMaskedLM.from_pretrained('bert-base-chinese')",
                    "api_arguments": {
                        "pretrained_model_name": "bert-base-chinese"
                    },
                    "python_environment_requirements": {
                        "transformers": "from transformers import AutoTokenizer, AutoModelForMaskedLM"
                    },
                    "example_code": "tokenizer = AutoTokenizer.from_pretrained(bert-base-chinese)\nmodel = AutoModelForMaskedLM.from_pretrained(bert-base-chinese)",
                    "performance": {
                        "dataset": "[More Information Needed]",
                        "accuracy": "[More Information Needed]"
                    },
                    "description": "This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper). It can be used for masked language modeling.",
                    "id": "huggingface_api_667",
                    "name": "bert-base-chinese"
                },
                "id": "gorilla_huggingface_tool_662",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "Tool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.",
            "Tool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.",
            "Tool 3: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.",
            "Tool 4: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 5: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.",
            "Tool 6: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.",
            "Tool 7: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.",
            "Tool 8: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 9: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 10: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.",
            "Tool 11: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 12: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 13: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.",
            "Tool 14: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.",
            "Tool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.",
            "Tool 16: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.",
            "Tool 17: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.",
            "Tool 18: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.",
            "Tool 19: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research."
        ],
        "tool_view": {
            "description": "Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.",
            "parameters": {
                "model": {
                    "type": "string",
                    "description": "The name or identifier of the pre-trained model to use for the fill-mask task. In this case, it specifies the 'roberta-large' model.",
                    "required": true,
                    "example": "roberta-large"
                }
            },
            "response": "The tool performs masked language modeling using the \"roberta-large\" model from the Hugging Face Transformers library. When invoked, it fills in the missing word (or token) in a given sentence or text where a mask token (denoted as `<mask>`) is present.\n\n### Expected Response Structure:\n\n1. **Predicted Token(s)**:\n   - The response contains one or more possible tokens (words or subwords) that can fill in the mask in the provided text.\n   - Each token is associated with a probability score, indicating the model's confidence in the prediction.\n\n2. **Confidence Score(s)**:\n   - The confidence score(s) reflect the likelihood that each predicted token is correct. These scores are typically in the form of floating-point numbers between 0 and 1.\n   - Higher scores indicate stronger predictions by the model.\n\n3. **Contextual Information (Optional)**:\n   - The response might include additional contextual information depending on the specific implementation, such as the original masked input or the top N most likely predictions.\n   - This helps the user understand how the model arrived at its prediction and the range of possible completions.\n\n### General Purpose:\n- The tool's response aims to provide one or more possible valid completions for the text with a mask, based on the context provided to the model. The completions reflect the model's understanding of the language and its training data.\n\n",
            "examples": [
                {
                    "user_intent": "I want to predict the missing word in this sentence: 'The weather today is <mask>.'",
                    "tool_call": {
                        "name": "roberta-large",
                        "arguments": {
                            "input_text": "The weather today is <mask>."
                        }
                    }
                },
                {
                    "user_intent": "Can you help me fill in the blank in 'She loves to drink <mask> every morning.'?",
                    "tool_call": {
                        "name": "roberta-large",
                        "arguments": {
                            "input_text": "She loves to drink <mask> every morning."
                        }
                    }
                },
                {
                    "user_intent": "Predict the missing word in this sentence from a story: 'Once upon a time, there was a <mask> who lived in a forest.'",
                    "tool_call": {
                        "name": "roberta-large",
                        "arguments": {
                            "input_text": "Once upon a time, there was a <mask> who lived in a forest."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\nTool 1: This tool uses the DistilBERT model to perform masked language modeling, allowing users to predict missing words in a sentence. It provides a fast and lightweight alternative to BERT for tasks like fill-in-the-blank text completion and can be used in natural language processing workflows to analyze, interpret, or generate text based on context.\nTool 2: This tool provides a pretrained BERT model for Brazilian Portuguese that can perform fill-in-the-blank (masked word) predictions in text. It leverages the Hugging Face Transformers framework to enable users to complete sentences by predicting missing words, supporting applications in language understanding and NLP tasks such as text analysis, contextual word prediction, and linguistic research.\nTool 3: Chinese CLIP is a zero-shot image classification model that uses a large-scale dataset of approximately 200 million Chinese image-text pairs. It combines a Vision Transformer (ViT-L/14@336px) for image encoding and a RoBERTa-wwm-base model for text encoding, enabling classification of images based on textual descriptions without the need for task-specific training. This model is useful for applying zero-shot learning to image-text matching tasks in various domains.\nTool 4: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 5: A natural language processing tool that uses the DeBERTa model to predict masked words in text, enabling tasks like sentence completion and language understanding. It improves on BERT and RoBERTa with disentangled attention and an enhanced mask decoder, achieving strong performance on benchmarks such as SQuAD and MNLI, making it useful for advanced fill-in-the-blank and comprehension applications.\nTool 6: This tool provides a pre-trained model for part-of-speech tagging in traditional Chinese text, using the BERT-based model \"ckiplab/bert-base-chinese-pos.\" It is designed to classify tokens in a sentence into their respective parts of speech, making it useful for various natural language processing tasks. The model can be easily integrated using the Transformers framework.\nTool 7: GroNLP/bert-base-dutch-cased (BERTje) is a Dutch pre-trained BERT model designed for fill-in-the-blank (masked language) tasks, allowing prediction of missing words in Dutch text. It is used in natural language processing applications that require understanding or completing Dutch sentences, and has demonstrated high accuracy on benchmarks like CoNLL-2002, SoNaR-1, and spaCy UD LassySmall.\nTool 8: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 9: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 10: The `bert-base-chinese` model is a pre-trained masked language model for Chinese, designed to predict missing words in a sentence by masking random word pieces, as described in the original BERT paper. It is useful for tasks such as language understanding and completion in Chinese text.\nTool 11: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 12: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 13: Provides masked language modeling using the multilingual XLM-RoBERTa-Large model, allowing users to predict and fill in missing words in text across 100 languages, useful for understanding context or preparing data for downstream NLP tasks.\nTool 14: A YOLOv8 model specialized for detecting planes in images, trained on the keremberke/plane-detection dataset, capable of identifying planes with high accuracy using either local or online images.\nTool 15: X-CLIP is a video-language model designed for extracting features from videos to enable tasks such as zero-shot, few-shot, or fully supervised video classification and video-text retrieval. Trained contrastively on video-text pairs, it provides a way to understand and relate video content to language, making it useful for general video understanding and analysis.\nTool 16: JosephusCheung/GuanacoVQA is a multilingual visual question answering model that supports English, Chinese, Japanese, and German, allowing users to ask questions about images and receive answers. Built on the MiniGPT-4 framework and leveraging the Guanaco 7B language model, it integrates vision and language understanding for multimodal reasoning.\nTool 17: VideoMAE is a video classification model based on Masked Autoencoders (MAE) and designed to process video data by treating it as a sequence of fixed-size image patches. It leverages a Vision Transformer (ViT)-like architecture with a decoder to predict pixel values for masked patches. This model learns to represent videos in a way that can be used for various downstream tasks, such as classification, by pre-training on video data. It is particularly useful for extracting features from video content to aid in tasks like action recognition.\nTool 18: The MCG-NJU/videomae-base-short is a pre-trained video classification model based on the VideoMAE architecture, an extension of Masked Autoencoders for video. It processes videos as sequences of fixed-size patches, using a Vision Transformer (ViT) structure with a decoder to predict masked pixel values. The model is pre-trained on video data to learn representations that can be used for various downstream tasks, such as classification. It requires the Hugging Face Transformers library and can be applied to video data for extracting useful features.\nTool 19: A pre-trained Chinese Text-to-Speech model using the ESPnet framework that converts written Chinese text into natural-sounding speech. It enables users to generate audio output from text input for applications such as voice assistants, audiobooks, or speech synthesis research.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"Provides a fill-in-the-blank (masked language modeling) capability using the RoBERTa-large transformer model, allowing users to predict missing words in text for tasks like language understanding, text completion, or data analysis.\", \"parameters\": {\"model\": {\"type\": \"string\", \"description\": \"The name or identifier of the pre-trained model to use for the fill-mask task. In this case, it specifies the 'roberta-large' model.\", \"required\": true, \"example\": \"roberta-large\"}}, \"response\": \"The tool performs masked language modeling using the \\\"roberta-large\\\" model from the Hugging Face Transformers library. When invoked, it fills in the missing word (or token) in a given sentence or text where a mask token (denoted as `<mask>`) is present.\\n\\n### Expected Response Structure:\\n\\n1. **Predicted Token(s)**:\\n   - The response contains one or more possible tokens (words or subwords) that can fill in the mask in the provided text.\\n   - Each token is associated with a probability score, indicating the model's confidence in the prediction.\\n\\n2. **Confidence Score(s)**:\\n   - The confidence score(s) reflect the likelihood that each predicted token is correct. These scores are typically in the form of floating-point numbers between 0 and 1.\\n   - Higher scores indicate stronger predictions by the model.\\n\\n3. **Contextual Information (Optional)**:\\n   - The response might include additional contextual information depending on the specific implementation, such as the original masked input or the top N most likely predictions.\\n   - This helps the user understand how the model arrived at its prediction and the range of possible completions.\\n\\n### General Purpose:\\n- The tool's response aims to provide one or more possible valid completions for the text with a mask, based on the context provided to the model. The completions reflect the model's understanding of the language and its training data.\\n\\n\", \"examples\": [{\"user_intent\": \"I want to predict the missing word in this sentence: 'The weather today is <mask>.'\", \"tool_call\": {\"name\": \"roberta-large\", \"arguments\": {\"input_text\": \"The weather today is <mask>.\"}}}, {\"user_intent\": \"Can you help me fill in the blank in 'She loves to drink <mask> every morning.'?\", \"tool_call\": {\"name\": \"roberta-large\", \"arguments\": {\"input_text\": \"She loves to drink <mask> every morning.\"}}}, {\"user_intent\": \"Predict the missing word in this sentence from a story: 'Once upon a time, there was a <mask> who lived in a forest.'\", \"tool_call\": {\"name\": \"roberta-large\", \"arguments\": {\"input_text\": \"Once upon a time, there was a <mask> who lived in a forest.\"}}}]}\n\n# User Query\nWe are an online platform that offers various Chinese language courses for students. Now we need to autoplay a fill-in-the-blank video game for our users.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Automatically generate missing words for a Chinese fill-in-the-blank exercise.",
                    "tool_description": "Use a pre-trained masked language model for Chinese text to predict and fill in missing words, enabling automatic completion of sentences for educational purposes.",
                    "expected_response": "A list of predicted tokens for each masked position, each with a confidence score, optionally including contextual information about the sentence."
                },
                {
                    "user_intent": "Autoplay a video game that presents Chinese fill-in-the-blank challenges using the generated predictions.",
                    "tool_description": "Integrate the predicted words into a video game interface to automatically progress through fill-in-the-blank challenges for learners.",
                    "expected_response": "Don't need response."
                }
            ],
            "extracted_arguments": {
                "input_text": "The Chinese sentence(s) containing masked tokens representing the blanks to be filled.",
                "model": "The identifier of the pre-trained Chinese masked language model to use for prediction."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_202",
        "query": "I want to build a personal assistant to check the logical relationship between two sentences, especially when I have a lot of texts in English.",
        "instruction": "Given a `logical relationship evaluation` task, retrieve tools that utilize natural language processing techniques to assess and determine the relationship between two sentences by processing textual inputs and delivering classification outcomes such as contradiction, entailment, or neutral.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Zero-Shot Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Natural Language Inference",
                    "api_call": "AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-MiniLM2-L6-H768')",
                    "api_arguments": [
                        "sentence1",
                        "sentence2"
                    ],
                    "python_environment_requirements": [
                        "sentence_transformers",
                        "transformers"
                    ],
                    "example_code": "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-MiniLM2-L6-H768')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])",
                    "performance": {
                        "dataset": "SNLI and MultiNLI",
                        "accuracy": "See SBERT.net - Pretrained Cross-Encoder for evaluation results"
                    },
                    "description": "This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.",
                    "id": "huggingface_api_507",
                    "name": "cross-encoder/nli-MiniLM2-L6-H768"
                },
                "id": "gorilla_huggingface_tool_503",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "Tool 1: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 2: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 3: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.",
            "Tool 4: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.",
            "Tool 5: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.",
            "Tool 6: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.",
            "Tool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.",
            "Tool 8: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.",
            "Tool 9: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.",
            "Tool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 11: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 12: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 13: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli is a multilingual model for zero-shot text classification, capable of assigning labels to text in over 100 languages without task-specific training. It leverages natural language inference (NLI) by evaluating the relationship between input sequences and candidate labels, enabling flexible classification across diverse domains. The model is pre-trained on the CC100 multilingual corpus and fine-tuned on the XNLI and MNLI datasets, making it suitable for accurate cross-lingual classification tasks.",
            "Tool 14: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.",
            "Tool 15: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.",
            "Tool 16: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.",
            "Tool 17: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 18: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.",
            "Tool 19: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets."
        ],
        "tool_view": {
            "description": "This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.",
            "parameters": {
                "text1": {
                    "type": "string",
                    "description": "The first text input that will be compared to the second text to determine the logical relationship.",
                    "required": true,
                    "example": "The sky is blue."
                },
                "text2": {
                    "type": "string",
                    "description": "The second text input that will be compared to the first text to determine the logical relationship.",
                    "required": true,
                    "example": "The ocean is vast."
                }
            },
            "response": "The tool returns a classification of the logical relationship between two input texts, producing a probability distribution over three possible categories: **entailment**, **contradiction**, and **neutral**. The response is typically structured as a mapping from each category label to a corresponding numeric score representing the model’s confidence (often a probability value between 0 and 1).  \n\nSpecifically, the response contains:\n\n- **entailment**: A numeric value indicating the likelihood that the first text logically entails the second text.  \n- **contradiction**: A numeric value indicating the likelihood that the first text contradicts the second text.  \n- **neutral**: A numeric value indicating the likelihood that there is no clear logical relationship between the texts.  \n\nThese values are usually normalized so that their sum equals 1, representing a probability distribution over the three classes. The response allows downstream applications to interpret which logical relationship the model predicts most strongly between the two input texts.",
            "examples": [
                {
                    "user_intent": "Check if the statement 'The cat is sleeping on the mat.' logically follows from 'A cat is lying down inside the house.'",
                    "tool_call": {
                        "name": "cointegrated/rubert-base-cased-nli-threeway",
                        "arguments": {
                            "text1": "The cat is sleeping on the mat.",
                            "text2": "A cat is lying down inside the house."
                        }
                    }
                },
                {
                    "user_intent": "Determine if 'Russia and Ukraine signed a peace treaty.' contradicts 'The two countries are still in conflict.'",
                    "tool_call": {
                        "name": "cointegrated/rubert-base-cased-nli-threeway",
                        "arguments": {
                            "text1": "Russia and Ukraine signed a peace treaty.",
                            "text2": "The two countries are still in conflict."
                        }
                    }
                },
                {
                    "user_intent": "Assess whether 'It is raining heavily today.' is neutral compared to 'People are carrying umbrellas outside.'",
                    "tool_call": {
                        "name": "cointegrated/rubert-base-cased-nli-threeway",
                        "arguments": {
                            "text1": "It is raining heavily today.",
                            "text2": "People are carrying umbrellas outside."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\nTool 1: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 2: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 3: This tool provides a transformer-based model for translating Italian text into English, using the Helsinki-NLP/opus-mt-it-en model trained on the OPUS dataset. It can be used for accurate and efficient translation between these two languages.\nTool 4: A Cross-Encoder model for natural language inference that performs zero-shot classification by evaluating the relationship between a pair of sentences. Trained on the SNLI and MultiNLI datasets, it outputs scores for contradiction, entailment, and neutral, allowing users to classify text against candidate labels without task-specific training.\nTool 5: A pre-trained Hugging Face Transformers model that classifies images as either dogs or food, achieving high accuracy on the sasha/dog-food dataset, suitable for quickly distinguishing between these two categories in computer vision tasks.\nTool 6: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.\nTool 7: A Hugging Face Transformers model for detecting speech segments in audio files, allowing users to identify when voice activity occurs within recordings. It is implemented using pyannote.audio and can process audio inputs to distinguish between speech and non-speech regions.\nTool 8: A zero-shot natural language inference model based on DeBERTa-v3-xsmall that predicts whether a given hypothesis is entailed by or contradicts a premise. It is trained on multiple NLI datasets, enabling it to perform text classification without task-specific training, making it useful for quickly assessing the relationship between any premise-hypothesis pair.\nTool 9: A PyTorch-based machine translation tool that uses the NLLB-200 model to translate individual sentences between 200 languages, designed for general-purpose text and research in machine translation, particularly for low-resource languages.\nTool 10: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 11: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 12: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 13: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli is a multilingual model for zero-shot text classification, capable of assigning labels to text in over 100 languages without task-specific training. It leverages natural language inference (NLI) by evaluating the relationship between input sequences and candidate labels, enabling flexible classification across diverse domains. The model is pre-trained on the CC100 multilingual corpus and fine-tuned on the XNLI and MNLI datasets, making it suitable for accurate cross-lingual classification tasks.\nTool 14: A Hugging Face Transformers model that translates text from English to Arabic using a transformer architecture. It is trained on the Tatoeba dataset and can be used to convert English sentences into Arabic for applications requiring multilingual support or cross-lingual communication.\nTool 15: This tool performs binary sentiment analysis on English text using the SiEBERT model, a fine-tuned RoBERTa-large checkpoint. It predicts whether input text expresses positive or negative sentiment and is designed to generalize across diverse text types, including reviews, tweets, and other user-generated content. Evaluated on 15 datasets from various domains, it achieves high accuracy, making it suitable for reliably analyzing sentiment in different English-language texts.\nTool 16: A speech-to-speech translation tool that converts spoken Hokkien audio into English, using a two-pass UnitY decoder trained on TED, drama, and TAT domain data. It leverages Fairseq and Facebook’s Unit HiFiGAN vocoder to generate high-quality synthesized speech, enabling end-to-end translation from input audio to output spoken translation.\nTool 17: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 18: Detects voice activity in audio recordings, specifically designed for Indian languages, allowing users to identify when speech is present or absent.\nTool 19: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a Russian-language natural language inference model that predicts the logical relationship between two short texts, classifying them as entailment, contradiction, or neutral. It is used for zero-shot text classification and semantic reasoning tasks, enabling applications such as automated consistency checking, contradiction detection, and textual entailment analysis in Russian.\", \"parameters\": {\"text1\": {\"type\": \"string\", \"description\": \"The first text input that will be compared to the second text to determine the logical relationship.\", \"required\": true, \"example\": \"The sky is blue.\"}, \"text2\": {\"type\": \"string\", \"description\": \"The second text input that will be compared to the first text to determine the logical relationship.\", \"required\": true, \"example\": \"The ocean is vast.\"}}, \"response\": \"The tool returns a classification of the logical relationship between two input texts, producing a probability distribution over three possible categories: **entailment**, **contradiction**, and **neutral**. The response is typically structured as a mapping from each category label to a corresponding numeric score representing the model\\u2019s confidence (often a probability value between 0 and 1).  \\n\\nSpecifically, the response contains:\\n\\n- **entailment**: A numeric value indicating the likelihood that the first text logically entails the second text.  \\n- **contradiction**: A numeric value indicating the likelihood that the first text contradicts the second text.  \\n- **neutral**: A numeric value indicating the likelihood that there is no clear logical relationship between the texts.  \\n\\nThese values are usually normalized so that their sum equals 1, representing a probability distribution over the three classes. The response allows downstream applications to interpret which logical relationship the model predicts most strongly between the two input texts.\", \"examples\": [{\"user_intent\": \"Check if the statement 'The cat is sleeping on the mat.' logically follows from 'A cat is lying down inside the house.'\", \"tool_call\": {\"name\": \"cointegrated/rubert-base-cased-nli-threeway\", \"arguments\": {\"text1\": \"The cat is sleeping on the mat.\", \"text2\": \"A cat is lying down inside the house.\"}}}, {\"user_intent\": \"Determine if 'Russia and Ukraine signed a peace treaty.' contradicts 'The two countries are still in conflict.'\", \"tool_call\": {\"name\": \"cointegrated/rubert-base-cased-nli-threeway\", \"arguments\": {\"text1\": \"Russia and Ukraine signed a peace treaty.\", \"text2\": \"The two countries are still in conflict.\"}}}, {\"user_intent\": \"Assess whether 'It is raining heavily today.' is neutral compared to 'People are carrying umbrellas outside.'\", \"tool_call\": {\"name\": \"cointegrated/rubert-base-cased-nli-threeway\", \"arguments\": {\"text1\": \"It is raining heavily today.\", \"text2\": \"People are carrying umbrellas outside.\"}}}]}\n\n# User Query\nI want to build a personal assistant to check the logical relationship between two sentences, especially when I have a lot of texts in English.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze the logical relationship between two English sentences to see if one entails, contradicts, or is neutral with respect to the other.",
                    "tool_description": "Use a natural language inference (NLI) model to classify the relationship between two English text inputs as entailment, contradiction, or neutral, enabling semantic reasoning and zero-shot text classification.",
                    "expected_response": "A probability distribution over the three categories: entailment, contradiction, and neutral, indicating the model's confidence for each relationship."
                }
            ],
            "extracted_arguments": {
                "text1": "The first English sentence whose logical relationship to the second sentence will be evaluated.",
                "text2": "The second English sentence that will be compared against the first sentence to determine the semantic relationship."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_255",
        "query": "We are a medical company providing health FAQs. We need to answer customers' questions accurately.",
        "instruction": "Given a `question answering` task, retrieve tools that are capable of providing precise and accurate answers to health-related FAQs by processing contextual information and questions. These tools should leverage natural language processing models fine-tuned on medical datasets to ensure the quality and relevance of the responses.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Question Answering",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Question Answering",
                    "api_call": "pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')",
                    "api_arguments": null,
                    "python_environment_requirements": [
                        "transformers",
                        "sentencepiece"
                    ],
                    "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})",
                    "performance": {
                        "dataset": "SQuAD2.0 Dev",
                        "accuracy": {
                            "exact": 84.33420365535248,
                            "f1": 87.49354241889522
                        }
                    },
                    "description": "BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.",
                    "id": "huggingface_api_463",
                    "name": "sultan/BioM-ELECTRA-Large-SQuAD2"
                },
                "id": "gorilla_huggingface_tool_459",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "Tool 1: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 2: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.",
            "Tool 3: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 5: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.",
            "Tool 6: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.",
            "Tool 7: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 11: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.",
            "Tool 13: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 14: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.",
            "Tool 15: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 16: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.",
            "Tool 18: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.",
            "Tool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries."
        ],
        "tool_view": {
            "description": "A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.",
            "parameters": {
                "answer": {
                    "type": "string",
                    "description": "The answer text based on which the model will generate a corresponding question.",
                    "required": true,
                    "example": "Paris"
                },
                "context": {
                    "type": "string",
                    "description": "The context or passage in which the answer is located, providing background for question generation.",
                    "required": true,
                    "example": "Paris is the capital city of France, known for its art, culture, and landmarks like the Eiffel Tower."
                },
                "max_length": {
                    "type": "integer",
                    "description": "The maximum length of the generated question in tokens. If not specified, defaults to 64.",
                    "required": false,
                    "default": 64,
                    "example": 32
                }
            },
            "response": "The tool is designed for question generation using a fine-tuned version of Google's T5 model. The response format is not explicitly defined in the raw tool documentation, but based on the functionality and typical outputs for question generation tasks, a reasonable and typical response would include the following components:\n\n### Structure and Components of the Response:\n\n1. **Generated Question**:\n   - **Field Name**: `\"question\"`\n   - **Type**: String\n   - **Description**: The question generated by the model based on the provided `answer` and `context`. The question is intended to be relevant and logically derived from the input answer and context.\n\n2. **Confidence or Score (optional)**:\n   - **Field Name**: `\"confidence_score\"` (or a similar term depending on implementation)\n   - **Type**: Float (optional)\n   - **Description**: A numerical score indicating the model's confidence in the generated question. This field may not be included in all responses, depending on the implementation.\n\n3. **Metadata (optional)**:\n   - **Field Name**: `\"metadata\"`\n   - **Type**: Object (optional)\n   - **Description**: Additional metadata related to the generation process. This could include things like the model version, processing time, or other relevant information about the generation process.\n\n### Expected Information in the Response:\n- The **generated question** will be a string that is directly related to the `answer` and `context` provided as inputs.\n- The response may include additional optional fields, such as a **confidence score** to indicate the model’s certainty about the quality or relevance of the generated question.\n- **Metadata** may be included for tracking purposes, though this would depend on the specific implementation of the tool.\n\nThis response structure ensures that the output directly supports the task of question generation, which is the core functionality of the tool.",
            "examples": [
                {
                    "user_intent": "Generate a question for the answer 'Paris' given the context about French cities.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Paris",
                            "context": "France has many beautiful cities, but Paris is known as the capital and a major cultural center.",
                            "max_length": 50
                        }
                    }
                },
                {
                    "user_intent": "Create a question for the answer 'Albert Einstein' in the context of famous scientists.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Albert Einstein",
                            "context": "Among the most influential physicists of the 20th century was Albert Einstein, who developed the theory of relativity.",
                            "max_length": 64
                        }
                    }
                },
                {
                    "user_intent": "Formulate a question for the answer 'Photosynthesis' based on plant biology information.",
                    "tool_call": {
                        "name": "mrm8488/t5-base-finetuned-question-generation-ap",
                        "arguments": {
                            "answer": "Photosynthesis",
                            "context": "Plants convert sunlight into chemical energy through a process called photosynthesis, which is vital for life on Earth."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\nTool 1: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 2: This tool provides a question-answering model based on Roberta Base, specifically trained on the SQuAD task, allowing it to accurately answer questions from text. It uses the Hugging Face Transformers framework and is compatible with the latest version of the `transformers` library. The model has demonstrated strong performance, with high accuracy on datasets like SQuADv1 and MoviesQA.\nTool 3: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 4: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 5: GanjinZero/UMLSBert_ENG is a Hugging Face Transformers model for extracting knowledge-infused embeddings of English medical terms, designed to support cross-lingual term normalization in medical applications.\nTool 6: A Visual Question Answering model based on the ViLT architecture, fine-tuned on the VQASI dataset to answer questions by analyzing the content of input images.\nTool 7: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 8: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 9: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 10: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 11: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 12: A Korean table question answering model, fine-tuned on the KorWikiTQ dataset, designed to answer natural language questions based on tabular data using the Hugging Face Transformers framework.\nTool 13: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 14: A TAPAS small model fine-tuned for Sequential Question Answering on tables, designed to answer questions by interpreting table structure using relative position embeddings for improved cell-level reasoning.\nTool 15: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 16: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 17: A natural language processing tool that uses the TAPAS medium model fine-tuned on WikiTable Questions to answer questions about tabular data. It allows users to input a table and a query, and returns precise answers by interpreting the table’s content, making it useful for extracting structured information from tables in a straightforward, automated way.\nTool 18: This tool uses the TAPAS transformer model to answer natural language questions based on tabular data, enabling users to extract information and insights directly from tables by interpreting both the question and the table’s structure.\nTool 19: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A natural language processing tool that uses a T5 model fine-tuned on SQuAD v1.1 to generate questions by providing an answer and its context, enabling automated question generation from text.\", \"parameters\": {\"answer\": {\"type\": \"string\", \"description\": \"The answer text based on which the model will generate a corresponding question.\", \"required\": true, \"example\": \"Paris\"}, \"context\": {\"type\": \"string\", \"description\": \"The context or passage in which the answer is located, providing background for question generation.\", \"required\": true, \"example\": \"Paris is the capital city of France, known for its art, culture, and landmarks like the Eiffel Tower.\"}, \"max_length\": {\"type\": \"integer\", \"description\": \"The maximum length of the generated question in tokens. If not specified, defaults to 64.\", \"required\": false, \"default\": 64, \"example\": 32}}, \"response\": \"The tool is designed for question generation using a fine-tuned version of Google's T5 model. The response format is not explicitly defined in the raw tool documentation, but based on the functionality and typical outputs for question generation tasks, a reasonable and typical response would include the following components:\\n\\n### Structure and Components of the Response:\\n\\n1. **Generated Question**:\\n   - **Field Name**: `\\\"question\\\"`\\n   - **Type**: String\\n   - **Description**: The question generated by the model based on the provided `answer` and `context`. The question is intended to be relevant and logically derived from the input answer and context.\\n\\n2. **Confidence or Score (optional)**:\\n   - **Field Name**: `\\\"confidence_score\\\"` (or a similar term depending on implementation)\\n   - **Type**: Float (optional)\\n   - **Description**: A numerical score indicating the model's confidence in the generated question. This field may not be included in all responses, depending on the implementation.\\n\\n3. **Metadata (optional)**:\\n   - **Field Name**: `\\\"metadata\\\"`\\n   - **Type**: Object (optional)\\n   - **Description**: Additional metadata related to the generation process. This could include things like the model version, processing time, or other relevant information about the generation process.\\n\\n### Expected Information in the Response:\\n- The **generated question** will be a string that is directly related to the `answer` and `context` provided as inputs.\\n- The response may include additional optional fields, such as a **confidence score** to indicate the model\\u2019s certainty about the quality or relevance of the generated question.\\n- **Metadata** may be included for tracking purposes, though this would depend on the specific implementation of the tool.\\n\\nThis response structure ensures that the output directly supports the task of question generation, which is the core functionality of the tool.\", \"examples\": [{\"user_intent\": \"Generate a question for the answer 'Paris' given the context about French cities.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Paris\", \"context\": \"France has many beautiful cities, but Paris is known as the capital and a major cultural center.\", \"max_length\": 50}}}, {\"user_intent\": \"Create a question for the answer 'Albert Einstein' in the context of famous scientists.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Albert Einstein\", \"context\": \"Among the most influential physicists of the 20th century was Albert Einstein, who developed the theory of relativity.\", \"max_length\": 64}}}, {\"user_intent\": \"Formulate a question for the answer 'Photosynthesis' based on plant biology information.\", \"tool_call\": {\"name\": \"mrm8488/t5-base-finetuned-question-generation-ap\", \"arguments\": {\"answer\": \"Photosynthesis\", \"context\": \"Plants convert sunlight into chemical energy through a process called photosynthesis, which is vital for life on Earth.\"}}}]}\n\n# User Query\nWe are a medical company providing health FAQs. We need to answer customers' questions accurately.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Answer customer health-related questions based on medical knowledge.",
                    "tool_description": "Use a question-answering model trained on medical or general text to extract precise answers from provided context or knowledge sources.",
                    "expected_response": "A textual answer addressing the customer's question accurately, possibly including confidence or supporting context."
                },
                {
                    "user_intent": "Generate new questions for the health FAQ from existing medical content.",
                    "tool_description": "Use a question generation model to create relevant questions from provided answers and medical context to expand the FAQ knowledge base.",
                    "expected_response": "A generated question string related to the provided answer and context, optionally including confidence or metadata."
                }
            ],
            "extracted_arguments": {
                "customer_question": "The natural language text of the user's health-related question.",
                "medical_context": "Relevant medical text or knowledge source from which to derive the answer.",
                "faq_answer": "The answer text from which to generate new questions for the FAQ.",
                "max_question_length": "Optional numeric limit for the length of generated questions in tokens."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_335",
        "query": "We have an online journal application that requires users to upload documents in which we need to automatically answer the questions related to the uploaded document.",
        "instruction": "Given a `document question answering` task, retrieve tools that process document inputs to automatically generate answers to questions based on the content of the uploaded document.",
        "labels": [
            {
                "doc": {
                    "domain": "Multimodal Document Question Answer",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')",
                    "api_arguments": "",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "",
                        "accuracy": ""
                    },
                    "description": "A Document Question Answering model based on LayoutXLM.",
                    "id": "huggingface_api_120",
                    "name": "CZ_DVQA_layoutxlm-base"
                },
                "id": "gorilla_huggingface_tool_119",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "Tool 1: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.",
            "Tool 2: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.",
            "Tool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.",
            "Tool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.",
            "Tool 5: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 7: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.",
            "Tool 8: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.",
            "Tool 9: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.",
            "Tool 10: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.",
            "Tool 11: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.",
            "Tool 12: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.",
            "Tool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.",
            "Tool 14: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 15: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.",
            "Tool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 17: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.",
            "Tool 18: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents."
        ],
        "tool_view": {
            "description": "A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The file path to the image that contains the document to be processed for question answering.",
                    "required": true,
                    "example": "path/to/image"
                },
                "question": {
                    "type": "string",
                    "description": "The question that you want to ask regarding the content of the image.",
                    "required": true,
                    "example": "What is the name of the company in the document?"
                }
            },
            "response": "The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\n\nThe response would typically include:\n\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\n\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\n\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\n\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.",
            "examples": [
                {
                    "user_intent": "I have a scanned contract and I want to know the effective date mentioned in it.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/contract_scan_01.png",
                            "question": "What is the effective date of this contract?"
                        }
                    }
                },
                {
                    "user_intent": "Check this invoice image and tell me the total amount due.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/invoice_march2025.png",
                            "question": "What is the total amount to be paid?"
                        }
                    }
                },
                {
                    "user_intent": "I need to extract the name of the company from this PDF screenshot.",
                    "tool_call": {
                        "name": "frizwankhan/entity-linking-model-final",
                        "arguments": {
                            "image": "documents/company_profile_screenshot.png",
                            "question": "What is the name of the company in this document?"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\nTool 1: This tool uses a document question-answering model fine-tuned on the DocVQA dataset with LayoutLMv2 to answer questions based on document content, making it useful for extracting information from documents in a structured format.\nTool 2: A tool that uses the LayoutLM architecture to answer questions based on the content and layout of documents, enabling automated extraction of information from structured and unstructured document data.\nTool 3: This tool uses a LayoutLMv2 model for document question answering, allowing users to input a question and context, and receive an answer based on the provided document. It is designed for tasks where understanding and extracting information from documents is required.\nTool 4: This tool provides a fine-tuned model for document question answering, based on the LayoutLMv2 architecture. It uses the Hugging Face Transformers framework and is specifically designed to answer questions about documents by processing both text and image inputs.\nTool 5: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 6: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 7: A document question-answering tool that uses the Donut model, combining a vision encoder and text decoder, to extract answers directly from document images without OCR. It allows users to provide an image of a document along with a question, and generates a text answer by interpreting the visual content, making it useful for automated information retrieval from scanned or photographed documents.\nTool 8: A fine-tuned LayoutLMv2 model designed for document question answering, enabling users to provide a question and a document context to extract accurate answers from structured or scanned documents.\nTool 9: This tool provides a pretrained LayoutLMv3 model fine-tuned for answering questions on multi-page documents. It processes document images along with associated text, layout information, and bounding boxes to extract precise answers, making it suitable for multimodal document question-answering tasks such as form understanding, reports, and scanned document analysis.\nTool 10: A LayoutLM-based model that performs question answering on documents, allowing users to provide a text question and a document context to extract accurate answers.\nTool 11: A compact LayoutLMv3 model designed for document question answering, allowing users to extract answers from images of documents using the Hugging Face Transformers framework.\nTool 12: A VisualBERT model designed for multimodal visual question answering, enabling users to answer questions about images by combining visual and textual information.\nTool 13: A fine-tuned version of LayoutLMv2 designed for document question answering, enabling users to extract answers from complex, multimodal documents by leveraging both text and layout information.\nTool 14: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 15: A BERT-based TAPAS model fine-tuned on WikiTable Questions (WTQ) that allows users to answer natural language questions about structured tabular data, enabling automated table question answering using the Hugging Face Transformers framework.\nTool 16: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 17: A compact TAPAS model designed for table question answering, trained on the WikiTableQuestions dataset, allowing users to answer questions based on structured tabular data using the Hugging Face Transformers framework.\nTool 18: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 19: A Document Question Answering model based on LayoutXLM that processes multimodal documents to extract answers from text and layout information, useful for understanding and querying structured documents.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multimodal document question-answering tool based on LayoutLMv2 that can answer questions about the content of images, enabling users to extract information directly from documents using a pretrained transformer model.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The file path to the image that contains the document to be processed for question answering.\", \"required\": true, \"example\": \"path/to/image\"}, \"question\": {\"type\": \"string\", \"description\": \"The question that you want to ask regarding the content of the image.\", \"required\": true, \"example\": \"What is the name of the company in the document?\"}}, \"response\": \"The response from the tool is expected to contain the answer to the question posed based on the input image, leveraging the model `frizwankhan/entity-linking-model-final`. This is a question-answering model designed to process both text and image inputs (in this case, a document image).\\n\\nThe response would typically include:\\n\\n1. **Answer**: A direct textual answer to the user's question, based on the content extracted or understood from the provided image. This is the primary output.\\n\\n2. **Context**: The specific part of the document or image that supports the answer, such as the text surrounding the answer, a reference to a particular section, or other relevant information that justifies the answer.\\n\\n3. **Confidence score (optional)**: A numerical or probabilistic indication of the model's confidence in the accuracy of the answer. This could help indicate how reliable or uncertain the model is about its response.\\n\\nThe output will be structured to present the most relevant answer along with contextual clues that support it, potentially with an associated confidence level or ranking.\", \"examples\": [{\"user_intent\": \"I have a scanned contract and I want to know the effective date mentioned in it.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/contract_scan_01.png\", \"question\": \"What is the effective date of this contract?\"}}}, {\"user_intent\": \"Check this invoice image and tell me the total amount due.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/invoice_march2025.png\", \"question\": \"What is the total amount to be paid?\"}}}, {\"user_intent\": \"I need to extract the name of the company from this PDF screenshot.\", \"tool_call\": {\"name\": \"frizwankhan/entity-linking-model-final\", \"arguments\": {\"image\": \"documents/company_profile_screenshot.png\", \"question\": \"What is the name of the company in this document?\"}}}]}\n\n# User Query\nWe have an online journal application that requires users to upload documents in which we need to automatically answer the questions related to the uploaded document.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract answers from documents uploaded by users based on specific questions.",
                    "tool_description": "A multimodal document question-answering tool that processes uploaded documents and answers user-provided questions by leveraging both text and image content.",
                    "expected_response": "An answer to the specific question based on the document, along with contextual information supporting the answer, and optionally a confidence score."
                }
            ],
            "extracted_arguments": {
                "image": "The file path to the image or document that contains the content to be processed for question answering.",
                "question": "The specific question the user wants to ask regarding the content of the uploaded document."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_499",
        "query": "We need to analyze the user's text for extracting entities and improve our virtual assistant interaction.",
        "instruction": "Given an `entity extraction` task, retrieve tools that analyze text to identify and extract entities, enhancing interactions with virtual assistants by processing user inputs for improved natural language understanding.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Token Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Entity Extraction",
                    "api_call": "AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577')",
                    "api_arguments": {
                        "inputs": "I love AutoTrain"
                    },
                    "python_environment_requirements": {
                        "transformers": "AutoModelForTokenClassification",
                        "tokenizer": "AutoTokenizer"
                    },
                    "example_code": "from transformers import AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\noutputs = model(**inputs)",
                    "performance": {
                        "dataset": "ismail-lucifer011/autotrain-data-name_all",
                        "accuracy": 0.9989316041363876
                    },
                    "description": "This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.",
                    "id": "huggingface_api_413",
                    "name": "904029577"
                },
                "id": "gorilla_huggingface_tool_409",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 3: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.",
            "Tool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.",
            "Tool 5: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.",
            "Tool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.",
            "Tool 8: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.",
            "Tool 9: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.",
            "Tool 10: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.",
            "Tool 11: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.",
            "Tool 12: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.",
            "Tool 13: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.",
            "Tool 14: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 15: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.",
            "Tool 16: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.",
            "Tool 17: A high-accuracy Token Classification model built on DistilBERT and trained with AutoTrain for entity extraction, capable of identifying and labeling entities in text inputs for natural language processing tasks.",
            "Tool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.",
            "Tool 19: A BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification, enabling the identification of entities or labels at the token level in text using Hugging Face Transformers."
        ],
        "tool_view": {
            "description": "A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "parameters": {
                "question": {
                    "type": "string",
                    "description": "The natural language question you want the model to answer based on the provided context.",
                    "required": true,
                    "example": "What is the capital of Germany?"
                },
                "context": {
                    "type": "string",
                    "description": "The text or passage that contains the information needed to answer the question.",
                    "required": true,
                    "example": "Berlin is the capital of Germany."
                }
            },
            "response": "The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \n\n1. **`answer`** – A string representing the model’s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \n\n2. **`score`** – A numerical confidence score indicating the model’s certainty about the predicted answer. Higher values correspond to higher confidence.  \n\n3. **`start`** – An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \n\n4. **`end`** – An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \n\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.",
            "examples": [
                {
                    "user_intent": "What's the main ingredient in a Caesar salad according to this recipe?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "What is the main ingredient in a Caesar salad?",
                            "context": "A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing."
                        }
                    }
                },
                {
                    "user_intent": "Who wrote the novel 'Pride and Prejudice'?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "Who is the author of 'Pride and Prejudice'?",
                            "context": "'Pride and Prejudice' is a classic novel written by Jane Austen in 1813."
                        }
                    }
                },
                {
                    "user_intent": "When did the Apollo 11 mission land on the moon?",
                    "tool_call": {
                        "name": "deepset/roberta-large-squad2",
                        "arguments": {
                            "question": "When did Apollo 11 land on the moon?",
                            "context": "Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 3: Dizex/InstaFoodRoBERTa-NER is a RoBERTa-based model fine-tuned for Named Entity Recognition to identify food-related entities in informal text, such as social media posts. It is designed to detect mentions of food items in short, casual content and can be easily used with the Hugging Face Transformers library for extracting food entities from text. The model was trained on a dataset of English Instagram posts and achieves high accuracy, making it suitable for applications in social media analysis, food tracking, or content tagging.\nTool 4: A SegFormer-based image segmentation model fine-tuned for clothing detection, allowing users to identify and segment clothing regions in images for applications like fashion analysis or virtual try-on.\nTool 5: The `dslim/bert-base-NER` model is a fine-tuned BERT-based model designed for Named Entity Recognition in English text. It can identify four types of entities—persons, locations, organizations, and miscellaneous terms—and delivers state-of-the-art performance on the CoNLL-2003 dataset, making it suitable for extracting structured information from unstructured text using the Hugging Face Transformers framework.\nTool 6: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 7: This tool is a fast English named entity recognition (NER) model that identifies four types of entities—person names, locations, organizations, and miscellaneous names—in text. Built on Flair embeddings with an LSTM-CRF architecture, it can be used to automatically detect and label entities in sentences, making it useful for text analysis, information extraction, and NLP applications requiring entity recognition.\nTool 8: The `dslim/bert-large-NER` is a fine-tuned BERT model designed for Named Entity Recognition (NER). It identifies and classifies four entity types: locations (LOC), organizations (ORG), persons (PER), and miscellaneous entities (MISC). This model achieves state-of-the-art performance with high accuracy, precision, and recall, making it ideal for extracting structured information from text. It can be easily used with the Hugging Face Transformers library by loading the pre-trained model and tokenizer.\nTool 9: The \"flair/ner-english\" tool is a Named Entity Recognition (NER) model designed for identifying four types of entities in English text: person names (PER), locations (LOC), organizations (ORG), and other types (MISC). It uses Flair embeddings and an LSTM-CRF architecture, and achieves an accuracy of 93.06% on the CoNLL-2003 dataset. This model is ideal for extracting structured information, such as names of people, places, and organizations, from unstructured text. It is implemented using the Hugging Face Transformers framework and requires the Flair library for usage.\nTool 10: A fine-tuned text-to-text generation model based on Flan-T5-Large designed to correct and improve grammar in text, making it useful for refining written content and producing grammatically polished output.\nTool 11: DialoGPT-small is a pretrained conversational AI model designed to generate human-like responses in multi-turn dialogues. Trained on millions of Reddit discussion threads, it can continue conversations naturally, making it suitable for chatbots, virtual assistants, and other applications requiring coherent and context-aware dialogue.\nTool 12: A multilingual Named Entity Recognition (NER) model based on mBERT, fine-tuned on the WikiNEuRal dataset to recognize entities in nine languages (de, en, es, fr, it, nl, pl, pt, ru). It can be used with the Hugging Face Transformers pipeline to automatically identify and classify named entities in text, achieving strong performance for multilingual NER tasks.\nTool 13: A pretrained BERT model for Named Entity Recognition (NER) that identifies and extracts entities such as person names, locations, and organizations from uncased text, useful for analyzing and structuring unstructured text data.\nTool 14: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 15: Dreamlike Photoreal 2.0 is a photorealistic text-to-image model based on Stable Diffusion 1.5, created by dreamlike.art. It generates high-quality, photorealistic images from text prompts, making it ideal for creating detailed visuals based on user descriptions.\nTool 16: Perform zero-shot text classification using the MobileBERT model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, enabling classification of text into user-defined categories without prior task-specific training.\nTool 17: A high-accuracy Token Classification model built on DistilBERT and trained with AutoTrain for entity extraction, capable of identifying and labeling entities in text inputs for natural language processing tasks.\nTool 18: This tool provides a multilingual Named Entity Recognition (NER) model based on a fine-tuned mBERT base, capable of identifying person, organization, and location entities in ten high-resource languages, including Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese, and Chinese. It is used to automatically detect and classify entities in text for natural language processing tasks across multiple languages.\nTool 19: A BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification, enabling the identification of entities or labels at the token level in text using Hugging Face Transformers.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\", \"parameters\": {\"question\": {\"type\": \"string\", \"description\": \"The natural language question you want the model to answer based on the provided context.\", \"required\": true, \"example\": \"What is the capital of Germany?\"}, \"context\": {\"type\": \"string\", \"description\": \"The text or passage that contains the information needed to answer the question.\", \"required\": true, \"example\": \"Berlin is the capital of Germany.\"}}, \"response\": \"The tool returns a structured answer to a question based on the provided context. The typical response is a dictionary containing the following components:  \\n\\n1. **`answer`** \\u2013 A string representing the model\\u2019s predicted answer extracted from the context. This is the text snippet the model considers the most likely answer to the given question.  \\n\\n2. **`score`** \\u2013 A numerical confidence score indicating the model\\u2019s certainty about the predicted answer. Higher values correspond to higher confidence.  \\n\\n3. **`start`** \\u2013 An integer representing the starting character index of the predicted answer within the provided context. This allows locating the exact position of the answer in the original text.  \\n\\n4. **`end`** \\u2013 An integer representing the ending character index of the predicted answer within the context. Together with `start`, it defines the span of text identified as the answer.  \\n\\nOverall, the response provides both the textual answer and metadata describing its location and confidence, enabling precise extraction and interpretation within the given context.\", \"examples\": [{\"user_intent\": \"What's the main ingredient in a Caesar salad according to this recipe?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"What is the main ingredient in a Caesar salad?\", \"context\": \"A Caesar salad typically includes romaine lettuce, croutons, Parmesan cheese, and Caesar dressing.\"}}}, {\"user_intent\": \"Who wrote the novel 'Pride and Prejudice'?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"Who is the author of 'Pride and Prejudice'?\", \"context\": \"'Pride and Prejudice' is a classic novel written by Jane Austen in 1813.\"}}}, {\"user_intent\": \"When did the Apollo 11 mission land on the moon?\", \"tool_call\": {\"name\": \"deepset/roberta-large-squad2\", \"arguments\": {\"question\": \"When did Apollo 11 land on the moon?\", \"context\": \"Apollo 11 was the first manned mission to land on the Moon. The lunar module touched down on July 20, 1969.\"}}}]}\n\n# User Query\nWe need to analyze the user's text for extracting entities and improve our virtual assistant interaction.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to extract named entities from the user's text.",
                    "tool_description": "Apply a Named Entity Recognition (NER) model to identify and classify entities such as persons, organizations, locations, and miscellaneous items in the provided text.",
                    "expected_response": "A structured list of detected entities with their types and positions within the text."
                },
                {
                    "user_intent": "I want to enhance the virtual assistant's responses based on user input.",
                    "tool_description": "Use a conversational AI model to generate context-aware, coherent responses to user input, improving interaction quality and engagement.",
                    "expected_response": "A generated text response from the virtual assistant suitable for continuing the conversation naturally."
                }
            ],
            "extracted_arguments": {
                "user_text": "The input text from the user that needs entity extraction and analysis.",
                "conversation_context": "Optional context or prior dialogue to inform the virtual assistant's response generation."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_161",
        "query": "I am an astronaut in space, writing my diary every day. I need a summary of my diary before sharing it with my family.",
        "instruction": "Given a `text summarization` task, retrieve tools that process lengthy text inputs to generate concise summaries, ensuring the output maintains the essence and key points of the original diary entries.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Text2Text Generation",
                    "api_call": "LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')",
                    "api_arguments": "input_text",
                    "python_environment_requirements": "transformers",
                    "example_code": "",
                    "performance": {
                        "dataset": "arxiv",
                        "accuracy": "2109.02492"
                    },
                    "description": "DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.",
                    "id": "huggingface_api_642",
                    "name": "DialogLED-base-16384"
                },
                "id": "gorilla_huggingface_tool_637",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "Tool 1: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.",
            "Tool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.",
            "Tool 3: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.",
            "Tool 4: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.",
            "Tool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.",
            "Tool 6: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.",
            "Tool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 8: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.",
            "Tool 9: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.",
            "Tool 10: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.",
            "Tool 11: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.",
            "Tool 12: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.",
            "Tool 13: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.",
            "Tool 14: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.",
            "Tool 15: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.",
            "Tool 16: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.",
            "Tool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 18: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.",
            "Tool 19: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation."
        ],
        "tool_view": {
            "description": "The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.",
            "parameters": {
                "images": {
                    "type": "array",
                    "description": "A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.",
                    "required": true,
                    "example": [
                        [
                            [
                                [
                                    0.1,
                                    0.2
                                ],
                                [
                                    0.3,
                                    0.4
                                ]
                            ],
                            [
                                [
                                    0.5,
                                    0.6
                                ],
                                [
                                    0.7,
                                    0.8
                                ]
                            ],
                            [
                                [
                                    0.9,
                                    1.0
                                ],
                                [
                                    1.1,
                                    1.2
                                ]
                            ]
                        ]
                    ]
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool’s response is an object representing the output of a video classification model. Its structure and components are as follows:\n\n1. **`logits`**  \n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\n\n2. **`predicted_class_idx`** (derived from `logits`)  \n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \n   - It represents the model’s most likely prediction for that video.\n\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \n   - This is a human-readable string corresponding to `predicted_class_idx`.  \n   - It maps the integer class index to the semantic label defined in the model’s configuration (e.g., “picking something up” or “throwing an object”).\n\n4. **Optional additional fields**  \n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \n\nIn summary, the primary semantic content of the response is the model’s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.",
            "examples": [
                {
                    "user_intent": "Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Process a set of random images as a video input and classify it using TimeSformer.",
                    "tool_call": {
                        "name": "facebook/timesformer-base-finetuned-ssv2",
                        "arguments": {
                            "images": "list(np.random.randn(8, 3, 224, 224))",
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\nTool 1: DistilGPT2 is a compact, efficient English-language model derived from GPT-2 using knowledge distillation. With 82 million parameters, it offers faster and lighter text generation while maintaining high-quality outputs, making it suitable for applications like writing assistance, creative writing, and entertainment.\nTool 2: Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus, fine-tuned on multiple Chinese datasets covering news, social media, and educational content. It generates concise summaries from Chinese text inputs, enabling efficient content understanding and information extraction.\nTool 3: Provides a baseline regression model for predicting tips using the tips9y0jvt5q dataset, implemented with Ridge regression via the dabl library. It is intended to quickly establish a reference performance for tabular regression tasks before applying more advanced training methods.\nTool 4: This tool provides abstractive summarization for Russian text using a T5-based model fine-tuned on multiple datasets. It generates concise, coherent summaries and allows control over summary length either by specifying an approximate word count or a compression ratio, making it useful for condensing long documents while preserving essential information.\nTool 5: A conversational AI tool that uses the `hyunwoongko/blenderbot-9B` model to engage in open-domain, multi-turn dialogue. It is designed to provide human-like conversation by blending skills such as asking and answering questions, sharing knowledge, showing empathy, and maintaining engaging interactions. This tool is suitable for building chatbots that require natural, context-aware responses and can be used in applications needing interactive and realistic conversational agents.\nTool 6: Salesforce/codegen-2B-multi is an autoregressive language model designed for program synthesis, enabling the generation of code from natural language prompts. Pre-trained on a multi-language dataset, it can handle tasks involving various programming languages. This model, part of the CodeGen family, has 2 billion parameters and is suitable for generating code snippets based on textual input, making it useful for automating coding tasks and enhancing programming workflows.\nTool 7: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 8: GPT-2 Large is a 774-million-parameter transformer-based language model developed by OpenAI, designed for generating coherent English text. It can produce multiple continuations of a given prompt, making it useful for tasks such as story generation, dialogue completion, or creative writing.\nTool 9: This is a sentence-transformers model that maps sentences and paragraphs to a 512-dimensional dense vector space, enabling tasks like clustering and semantic search.\nTool 10: A pre-trained TimeSformer model for classifying videos into one of 400 categories from the Kinetics-400 dataset, enabling automated video understanding using space-time attention.\nTool 11: A Hugging Face Transformers model that converts sentences and paragraphs into 768-dimensional dense vectors, enabling semantic similarity comparisons and semantic search by capturing the meaning of text in a vector space.\nTool 12: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\nTool 13: GPT-2 is a pretrained language model that generates coherent English text by predicting the next word in a sequence, trained on a large corpus of unlabeled data using a self-supervised approach. It can produce text continuations of a given prompt and supports controlling output length and generating multiple sequences, making it useful for creative writing, content generation, and experimenting with natural language generation.\nTool 14: This tool provides a baseline regression model trained on the outhimar_64 dataset to predict the \"Close\" value of tabular data. Built with scikit-learn and preprocessed using the dabl library, it offers a quick starting point for regression tasks, allowing users to evaluate performance before applying more advanced models. The model can be easily loaded and used with training data, delivering high baseline accuracy and serving as a reference for further model improvement.\nTool 15: OpenAI-GPT is a transformer-based language model designed for generating coherent and contextually relevant text. It uses a unidirectional, causal architecture trained on large text corpora to capture long-range dependencies, making it suitable for tasks such as text completion, creative writing, and language modeling. The model can produce multiple text sequences of a specified length, enabling diverse output generation.\nTool 16: The sentence-transformers/all-distilroberta-v1 model maps sentences and paragraphs to a 768-dimensional vector space, enabling tasks such as clustering and semantic search. It uses the Hugging Face Transformers framework and is accessed through the SentenceTransformer API.\nTool 17: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 18: FLAN-T5 small is a fine-tuned T5 language model designed for research in natural language processing, capable of performing zero-shot and few-shot tasks such as reasoning, translation, and question answering. It has been trained on over 1,000 diverse tasks across multiple languages and achieves strong benchmark performance, including 75.2% accuracy on five-shot MMLU. The model is intended for experimental and research purposes and should be evaluated for safety and fairness before any real-world application.\nTool 19: The \"sentence-transformers/LaBSE\" tool is a PyTorch implementation of the LaBSE model, designed to map sentences from 109 languages into a shared vector space. It is used to compute sentence embeddings, making it useful for tasks such as sentence similarity and multilingual text representation.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"The TimeSformer model, pre-trained on the Something Something v2 dataset, is designed for video classification tasks. It leverages space-time attention mechanisms to process video inputs, as introduced in the paper \\\"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\\\". This model can be easily integrated using the Hugging Face Transformers framework and is fine-tuned for video classification, making it suitable for tasks requiring understanding of dynamic visual content over time.\", \"parameters\": {\"images\": {\"type\": \"array\", \"description\": \"A list or sequence of video frames represented as arrays, where each frame is typically a 3D array (channels, height, width). This input is used by the model for video classification.\", \"required\": true, \"example\": [[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]]]}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors the processor should return. Common values include 'pt' for PyTorch tensors or 'tf' for TensorFlow tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool\\u2019s response is an object representing the output of a video classification model. Its structure and components are as follows:\\n\\n1. **`logits`**  \\n   - This is a tensor containing the raw, unnormalized scores produced by the model for each class in the classification task.  \\n   - The shape typically corresponds to `[batch_size, num_classes]`, where `batch_size` is the number of input video samples and `num_classes` is the total number of target classes the model can predict.  \\n   - Higher values indicate stronger model confidence for the corresponding class, but they are not probabilities.\\n\\n2. **`predicted_class_idx`** (derived from `logits`)  \\n   - This is the integer index of the class with the highest score in `logits` for a given input video.  \\n   - It represents the model\\u2019s most likely prediction for that video.\\n\\n3. **`predicted_class_label`** (derived using `model.config.id2label`)  \\n   - This is a human-readable string corresponding to `predicted_class_idx`.  \\n   - It maps the integer class index to the semantic label defined in the model\\u2019s configuration (e.g., \\u201cpicking something up\\u201d or \\u201cthrowing an object\\u201d).\\n\\n4. **Optional additional fields**  \\n   - Depending on the framework version and configuration, the response object may also include intermediate outputs such as attention weights or hidden states, but these are typically optional and not returned by default.  \\n\\nIn summary, the primary semantic content of the response is the model\\u2019s **raw class scores (`logits`)** and the **predicted class** for the input video, with a mapping to a human-readable label for interpretation.\", \"examples\": [{\"user_intent\": \"Classify a video using the TimeSformer model pre-trained on the Something Something v2 dataset.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to run video classification on a sample video input with the TimeSformer model and get the predicted class.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Process a set of random images as a video input and classify it using TimeSformer.\", \"tool_call\": {\"name\": \"facebook/timesformer-base-finetuned-ssv2\", \"arguments\": {\"images\": \"list(np.random.randn(8, 3, 224, 224))\", \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nI am an astronaut in space, writing my diary every day. I need a summary of my diary before sharing it with my family.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to summarize the content of my daily space diary entries.",
                    "tool_description": "Generate a concise and coherent abstractive summary from a set of diary text entries, preserving key events and important details.",
                    "expected_response": "A textual summary capturing the main points and highlights of the diary entries in a condensed form."
                }
            ],
            "extracted_arguments": {
                "diary_text": "The full text content of the astronaut's daily diary entries, provided as a string or collection of strings."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_367",
        "query": "We need to analyze customer reviews and find out how well our new product is doing in the market.",
        "instruction": "Given a `sentiment analysis` task, retrieve tools that analyze text inputs, particularly customer reviews, to evaluate the sentiment and determine the product's market performance.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Sentiment Analysis",
                    "api_call": "pipeline('sentiment-analysis')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "result = sentiment_pipeline('I love this product!')",
                    "performance": {
                        "dataset": [
                            {
                                "language": "English",
                                "accuracy": {
                                    "exact": "67%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Dutch",
                                "accuracy": {
                                    "exact": "57%",
                                    "off-by-1": "93%"
                                }
                            },
                            {
                                "language": "German",
                                "accuracy": {
                                    "exact": "61%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "French",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "94%"
                                }
                            },
                            {
                                "language": "Italian",
                                "accuracy": {
                                    "exact": "59%",
                                    "off-by-1": "95%"
                                }
                            },
                            {
                                "language": "Spanish",
                                "accuracy": {
                                    "exact": "58%",
                                    "off-by-1": "95%"
                                }
                            }
                        ]
                    },
                    "description": "This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
                    "id": "huggingface_api_914",
                    "name": "bert-base-multilingual-uncased-sentiment"
                },
                "id": "gorilla_huggingface_tool_379",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 1: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.",
            "Tool 2: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.",
            "Tool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.",
            "Tool 4: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.",
            "Tool 5: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.",
            "Tool 6: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.",
            "Tool 7: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.",
            "Tool 8: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.",
            "Tool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.",
            "Tool 10: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.",
            "Tool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.",
            "Tool 13: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.",
            "Tool 14: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.",
            "Tool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 16: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 17: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.",
            "Tool 18: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.",
            "Tool 19: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics."
        ],
        "tool_view": {
            "description": "A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.",
                    "required": true,
                    "example": "I love this product!"
                }
            },
            "response": "The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\n\n1. **label** – A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \"1 star\" through \"5 stars\"). This reflects the overall sentiment expressed in the input text.\n\n2. **score** – A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\n\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool’s description.",
            "examples": [
                {
                    "user_intent": "I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "This phone is amazing, totally worth the price!"
                        }
                    }
                },
                {
                    "user_intent": "Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "Dit product is echt slecht, ik ben teleurgesteld."
                        }
                    }
                },
                {
                    "user_intent": "Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'",
                    "tool_call": {
                        "name": "bert-base-multilingual-uncased-sentiment",
                        "arguments": {
                            "text": "C'est un excellent produit, je le recommande vivement!"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 1: This tool is a text-classification model built with Hugging Face Transformers, designed to evaluate the adequacy of paraphrased utterances. It is part of the Parrot paraphraser framework and is used to support natural language understanding (NLU) model training by assessing how well generated paraphrases preserve the meaning of the original text.\nTool 2: FashionCLIP is a CLIP-based model designed for zero-shot image classification in the fashion domain. Using a pre-trained OpenAI ViT-B/32 checkpoint, it has been fine-tuned on a large fashion dataset to generate product representations that can be applied to new datasets and tasks without additional training. This model is particularly useful for classifying fashion images based on textual descriptions, making it highly adaptable for a variety of fashion-related tasks.\nTool 3: This tool provides a German-language sentiment classification model built on the BERT architecture, designed to analyze texts from diverse sources such as social media posts and reviews. It allows users to input German texts and predict their sentiment, making it useful for understanding opinions, feedback, or emotional tone in written content. The model was trained on over 1.8 million samples across multiple domains, achieving high accuracy on a range of German datasets.\nTool 4: GODEL-v1.1 is a large-scale, Transformer-based model designed for goal-directed conversational response generation. It can produce dialog responses grounded in external knowledge or context, enabling more accurate and context-aware interactions. The model is pre-trained on a combination of multi-turn Reddit discussions and instruction- or knowledge-grounded dialogs, making it suitable for fine-tuning on new dialog tasks with limited task-specific data. It is used to generate coherent, informed responses in scenarios where conversations need to incorporate external information beyond the immediate dialog.\nTool 5: A Hugging Face Transformers model fine-tuned for sentiment analysis on stock-related comments, classifying them as \"Bullish\" or \"Bearish\" based on 3.2 million StockTwits posts, enabling users to infer market sentiment from textual data with high accuracy.\nTool 6: A fine-tuned DistilBERT model for sentiment analysis on movie reviews, providing accurate classification of text as positive or negative, achieving 92.8% accuracy on the IMDB dataset.\nTool 7: A binary classification model trained on IMDb movie reviews that predicts whether a review expresses positive or negative sentiment, useful for analyzing and categorizing textual sentiment in tabular data.\nTool 8: A fine-tuned BERT-based model for sentiment analysis on restaurant reviews from Yelp, designed to classify text as positive or negative, providing accurate text classification for natural language processing tasks.\nTool 9: COSMO-XL is a conversational AI model designed to generate natural, human-like dialogue across a variety of topics and settings. It can take a situation description and role instructions to produce contextually appropriate responses, making it suitable for both in-domain and out-of-domain chitchat. Trained on the SODA and ProsocialDialog datasets, COSMO-XL is particularly effective at modeling realistic interactions and flexible conversation behaviors.\nTool 10: This tool uses a pretrained sentence transformer model to generate dense vector embeddings that capture the semantic meaning of input sentences. It is designed for tasks such as sentence similarity comparison, clustering, and information retrieval, allowing applications to measure how closely sentences relate in meaning.\nTool 11: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 12: A tabular classification model trained with Scikit-learn to predict recidivism using the COMPAS dataset. It uses the imodels.FIGSClassifier and can be loaded via Hugging Face to make predictions on new tabular data.\nTool 13: GreeneryScenery/SheepsControlV3 is an image-to-image model that generates new images based on an input image and optional text guidance, allowing users to transform or enhance images according to specified visual or textual cues.\nTool 14: FinBERT (finbert-tone) is a BERT-based model fine-tuned for financial sentiment analysis, capable of classifying text as positive, negative, or neutral. Trained on 10,000 manually annotated sentences from analyst reports and pre-trained on corporate reports, earnings call transcripts, and analyst communications, it delivers high accuracy in detecting the tone of financial text, making it useful for analyzing investor communications, market commentary, and other finance-related documents.\nTool 15: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 16: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 17: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\nTool 18: A tabular regression model trained with AutoTrain to predict carbon emissions from structured data, using selected features from a dataset. It is designed for loading with Joblib and processing input data with pandas, allowing users to generate emission predictions from new tabular datasets.\nTool 19: A fine-tuned version of Long T5 designed to generate concise summaries of long texts, producing SparkNotes-style summaries for books, academic papers, or narrative content. It is suitable for condensing large documents while retaining key information and generalizes well across diverse topics.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The input text for which the sentiment analysis will be performed. This should be a product review or any text snippet whose sentiment needs to be evaluated.\", \"required\": true, \"example\": \"I love this product!\"}}, \"response\": \"The tool returns a sentiment analysis result for a given text input, specifically predicting a rating in the form of stars (from 1 to 5) for product reviews. The typical response structure would be an object or dictionary containing the following components:\\n\\n1. **label** \\u2013 A string or categorical value representing the predicted sentiment category. In this case, it corresponds to a star rating (e.g., \\\"1 star\\\" through \\\"5 stars\\\"). This reflects the overall sentiment expressed in the input text.\\n\\n2. **score** \\u2013 A numerical value representing the confidence or probability associated with the predicted label. This is usually a floating-point number between 0 and 1, indicating how certain the model is about its prediction.\\n\\nOverall, the response is a structured output that links a textual input to a predicted sentiment category along with a confidence score, allowing downstream applications to interpret or threshold the prediction. The response is designed to support multiple languages as specified in the tool\\u2019s description.\", \"examples\": [{\"user_intent\": \"I want to know the sentiment of this product review: 'This phone is amazing, totally worth the price!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"This phone is amazing, totally worth the price!\"}}}, {\"user_intent\": \"Analyze the sentiment of a review in Dutch: 'Dit product is echt slecht, ik ben teleurgesteld.'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"Dit product is echt slecht, ik ben teleurgesteld.\"}}}, {\"user_intent\": \"Check the sentiment of this French review: 'C'est un excellent produit, je le recommande vivement!'\", \"tool_call\": {\"name\": \"bert-base-multilingual-uncased-sentiment\", \"arguments\": {\"text\": \"C'est un excellent produit, je le recommande vivement!\"}}}]}\n\n# User Query\nWe need to analyze customer reviews and find out how well our new product is doing in the market.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to analyze customer reviews to assess the market reception of a product.",
                    "tool_description": "Analyze customer reviews using sentiment analysis to determine the overall reception of a product in the market.",
                    "expected_response": "A sentiment analysis result for each review, with predicted star ratings (1 to 5) reflecting customer feedback on the product."
                }
            ],
            "extracted_arguments": {
                "text": "The text of the customer review that will be analyzed for sentiment."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_448",
        "query": "Design a chatbot for a school website, which can answer queries related to admissions, classes, teachers, and extracurriculars.",
        "instruction": "Given a `chatbot design` task, retrieve tools that specialize in conversational interfaces, which process textual inputs related to specific topics and provide informative and empathetic responses to user queries on a school website context.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Text2Text Generation",
                    "framework": "Hugging Face",
                    "functionality": "Conversational",
                    "api_call": "BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')",
                    "api_arguments": [
                        "message"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "Input a message to start chatting with facebook/blenderbot_small-90M.",
                    "performance": {
                        "dataset": "blended_skill_talk",
                        "accuracy": "Not provided"
                    },
                    "description": "Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.",
                    "id": "huggingface_api_573",
                    "name": "facebook/blenderbot_small-90M"
                },
                "id": "gorilla_huggingface_tool_569",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.",
            "Tool 1: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.",
            "Tool 2: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.",
            "Tool 3: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.",
            "Tool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.",
            "Tool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.",
            "Tool 6: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.",
            "Tool 7: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.",
            "Tool 8: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.",
            "Tool 9: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.",
            "Tool 10: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.",
            "Tool 11: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.",
            "Tool 12: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.",
            "Tool 13: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.",
            "Tool 14: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.",
            "Tool 15: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 16: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 17: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.",
            "Tool 18: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.",
            "Tool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables."
        ],
        "tool_view": {
            "description": "This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.",
            "parameters": {
                "sentences": {
                    "type": "array",
                    "description": "A list of sentences or paragraphs that need to be transformed into sentence embeddings.",
                    "required": true,
                    "example": [
                        "This is an example sentence",
                        "Each sentence is converted"
                    ]
                }
            },
            "response": "The tool produces a numerical embedding representation for each input sentence. The response is structured as a collection (typically a list or array) of dense vectors, where:  \n\n- **Each element in the collection** corresponds to one input sentence.  \n- **Each vector** is a fixed-length numeric array representing the semantic content of the sentence in a high-dimensional space (384 dimensions for this specific model).  \n- **Vector values** are floating-point numbers capturing the semantic features such that sentences with similar meanings have embeddings that are close in vector space according to metrics like cosine similarity.  \n\nThis response can be directly used for tasks such as sentence similarity comparison, clustering, semantic search, or other downstream NLP applications that operate on vector representations of text.",
            "examples": [
                {
                    "user_intent": "I want to compare the similarity between these two sentences: 'The weather is lovely today' and 'It's a beautiful day outside.'",
                    "tool_call": {
                        "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "arguments": {
                            "sentences": [
                                "The weather is lovely today",
                                "It's a beautiful day outside."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Generate embeddings for a list of customer reviews so I can cluster them based on meaning.",
                    "tool_call": {
                        "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "arguments": {
                            "sentences": [
                                "The product arrived late but in good condition.",
                                "Excellent customer service and fast shipping!",
                                "I am not satisfied with the quality of this item."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Encode several multilingual sentences to find semantically similar sentences across languages.",
                    "tool_call": {
                        "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "arguments": {
                            "sentences": [
                                "I love reading books in my free time.",
                                "Me encanta leer libros en mi tiempo libre.",
                                "J'aime lire des livres pendant mon temps libre."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool uses the \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.\nTool 1: TAPEX is a transformer-based model fine-tuned to answer natural language questions over structured tables. It leverages a BART encoder-decoder architecture and pre-training on synthetic SQL queries to enable table reasoning, allowing users to input a table and a question and receive an accurate, context-aware answer.\nTool 2: A Visual Question Answering tool that uses a fine-tuned transformer model to answer questions about images in Polish, enabling users to extract information from visual content through natural language queries.\nTool 3: A zero-shot image classification tool based on OpenCLIP that can categorize images into specified classes without needing any training data for those categories.\nTool 4: Zixtrauce/BDBot4Epoch is a conversational AI model based on the GPT-2 architecture that generates text responses, enabling interactive chatbot-like conversations.\nTool 5: ShuanBot is a conversational chatbot based on the GPT-2 architecture that generates human-like responses in a chat setting, enabling natural and engaging interactions.\nTool 6: This tool provides a transformer-based model, TAPAS, fine-tuned on the WikiSQL dataset, designed to answer questions about tabular data. It enables natural language queries over tables by interpreting both the question and the table structure, making it useful for applications that require extracting or reasoning over information stored in tabular formats.\nTool 7: Stable Diffusion is a text-to-image generation model that produces high-quality, photo-realistic images from textual prompts. Fine-tuned for creative and research purposes, it can be used to generate artwork, design concepts, educational visuals, or explore generative modeling. This model allows users to convert descriptive text into visual content efficiently, supporting experimentation and creative workflows.\nTool 8: OmniTab is a table-based question-answering model that uses Hugging Face Transformers to answer natural language queries over structured data in tables. It is designed to process a table and a user query, generating accurate answers directly from the table content, making it useful for extracting information from tabular datasets in an automated and efficient way.\nTool 9: A zero-shot image classification model based on OpenCLIP that can categorize images into predefined classes without requiring additional training, enabling flexible image labeling across diverse categories.\nTool 10: This tool is a TAPAS-based model fine-tuned for Sequential Question Answering, enabling users to answer questions about tabular data in a conversational context. It is pretrained on a large English corpus from Wikipedia and leverages the Hugging Face Transformers framework to process tables and queries, making it suitable for natural language understanding tasks involving structured data.\nTool 11: BlenderBot-400M-distill is a compact, conversational AI model designed for open-domain chatbot interactions. Trained on the Blended Skill Talk dataset, it generates context-aware text responses and can be used via the Transformers library with PyTorch, TensorFlow, or JAX, enabling developers to build interactive dialogue systems.\nTool 12: This tool is a table-based question-answering model that uses a text-to-text Transformer architecture to answer natural language queries over structured tabular data. It is designed to take a table and a user query as input and generate accurate, context-aware answers, enabling automated extraction of information from tables for tasks such as data analysis, reporting, or knowledge retrieval.\nTool 13: BlenderBot is a conversational chatbot model designed to engage in natural, dynamic dialogue by asking and answering questions, demonstrating knowledge, empathy, and personality, and responding appropriately to different conversational contexts.\nTool 14: TAPEX (Table Pre-training via Execution) is a transformer-based model designed for table question answering, enabling models to perform reasoning over tabular data. It pre-trains a neural SQL executor on a synthetic corpus of automatically generated SQL queries, allowing it to understand and answer questions about tables by leveraging both a bidirectional encoder and an autoregressive decoder.\nTool 15: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 16: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 17: A text classification model built with Hugging Face Transformers that distinguishes between question queries and statement queries, useful for automatically identifying the type of user input in natural language processing applications.\nTool 18: Stable Diffusion v2 is a text-to-image generation model that creates and modifies images based on text prompts. Using a pretrained text encoder, it is primarily designed for research, enabling safe deployment of models with potential for generating harmful content, as well as exploring limitations, biases, and applications in art and design.\nTool 19: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool uses the \\\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\\" model to convert sentences and paragraphs into 384-dimensional vector embeddings, which can be applied to tasks such as clustering and semantic search. It is based on the Hugging Face Transformers framework and is useful for comparing sentence similarity or finding related content in large datasets.\", \"parameters\": {\"sentences\": {\"type\": \"array\", \"description\": \"A list of sentences or paragraphs that need to be transformed into sentence embeddings.\", \"required\": true, \"example\": [\"This is an example sentence\", \"Each sentence is converted\"]}}, \"response\": \"The tool produces a numerical embedding representation for each input sentence. The response is structured as a collection (typically a list or array) of dense vectors, where:  \\n\\n- **Each element in the collection** corresponds to one input sentence.  \\n- **Each vector** is a fixed-length numeric array representing the semantic content of the sentence in a high-dimensional space (384 dimensions for this specific model).  \\n- **Vector values** are floating-point numbers capturing the semantic features such that sentences with similar meanings have embeddings that are close in vector space according to metrics like cosine similarity.  \\n\\nThis response can be directly used for tasks such as sentence similarity comparison, clustering, semantic search, or other downstream NLP applications that operate on vector representations of text.\", \"examples\": [{\"user_intent\": \"I want to compare the similarity between these two sentences: 'The weather is lovely today' and 'It's a beautiful day outside.'\", \"tool_call\": {\"name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"arguments\": {\"sentences\": [\"The weather is lovely today\", \"It's a beautiful day outside.\"]}}}, {\"user_intent\": \"Generate embeddings for a list of customer reviews so I can cluster them based on meaning.\", \"tool_call\": {\"name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"arguments\": {\"sentences\": [\"The product arrived late but in good condition.\", \"Excellent customer service and fast shipping!\", \"I am not satisfied with the quality of this item.\"]}}}, {\"user_intent\": \"Encode several multilingual sentences to find semantically similar sentences across languages.\", \"tool_call\": {\"name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"arguments\": {\"sentences\": [\"I love reading books in my free time.\", \"Me encanta leer libros en mi tiempo libre.\", \"J'aime lire des livres pendant mon temps libre.\"]}}}]}\n\n# User Query\nDesign a chatbot for a school website, which can answer queries related to admissions, classes, teachers, and extracurriculars.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to create a chatbot that can answer questions about a school's admissions, classes, teachers, and extracurricular activities.",
                    "tool_description": "Develop a conversational AI chatbot capable of understanding and responding to natural language queries related to school operations, including admissions, class schedules, teacher information, and extracurricular programs.",
                    "expected_response": "A text-based interactive chatbot that can process user queries and return contextually accurate answers relevant to the school's information."
                }
            ],
            "extracted_arguments": {
                "school_website_context": "The specific informational content and sections of the school website that the chatbot will reference.",
                "query_topics": "A list of subject areas the chatbot should handle, including admissions, classes, teachers, and extracurricular activities."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_290",
        "query": "Create a smart speaker that can recognize voice commands such as \"Turn on the lights,\" \"Play music,\" or \"Set a timer.\"",
        "instruction": "Given a `smart speaker development` task, retrieve tools that enable the creation of devices capable of recognizing and processing voice commands by utilizing audio input for command classification and execution.",
        "labels": [
            {
                "doc": {
                    "domain": "Audio Audio Classification",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Transformers",
                    "api_call": "AutoModelForAudioClassification.from_pretrained('MIT/ast-finetuned-speech-commands-v2')",
                    "api_arguments": "audio file",
                    "python_environment_requirements": "transformers library",
                    "example_code": "result = audio_classifier('path/to/audio/file.wav')",
                    "performance": {
                        "dataset": "Speech Commands v2",
                        "accuracy": "98.120"
                    },
                    "description": "Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.",
                    "id": "huggingface_api_831",
                    "name": "ast-finetuned-speech-commands-v2"
                },
                "id": "gorilla_huggingface_tool_826",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "Tool 1: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.",
            "Tool 2: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.",
            "Tool 3: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.",
            "Tool 4: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.",
            "Tool 5: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.",
            "Tool 6: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.",
            "Tool 7: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.",
            "Tool 8: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 9: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.",
            "Tool 10: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.",
            "Tool 11: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.",
            "Tool 12: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.",
            "Tool 13: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.",
            "Tool 14: A fine-tuned Audio Spectrogram Transformer (AST) model for classifying audio commands, which converts audio into spectrogram images and applies a Vision Transformer to achieve high-accuracy results on the Speech Commands v2 dataset. It is used to perform audio classification tasks efficiently using the Hugging Face Transformers framework.",
            "Tool 15: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.",
            "Tool 16: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.",
            "Tool 17: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.",
            "Tool 18: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.",
            "Tool 19: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1."
        ],
        "tool_view": {
            "description": "A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.",
            "parameters": {
                "audio": {
                    "type": "array",
                    "description": "An array representing the input speech waveform that will be used for speech-to-speech conversion.",
                    "required": true,
                    "example": [
                        0.0,
                        0.1,
                        -0.05,
                        0.02
                    ]
                },
                "sampling_rate": {
                    "type": "number",
                    "description": "The sampling rate (in Hz) of the input audio waveform, which ensures the audio is processed correctly by the model.",
                    "required": true,
                    "example": 16000
                },
                "return_tensors": {
                    "type": "string",
                    "description": "Specifies the type of tensors to return after processing the audio. Typically 'pt' for PyTorch tensors.",
                    "required": true,
                    "example": "pt"
                }
            },
            "response": "The tool does not explicitly define the response format. Based on the functionality described, we can infer a plausible response structure for the tool's operation.\n\nWhen the tool is invoked, it processes input speech data using the SpeechT5 model, and the expected response would most likely be the transformed speech output. Here’s a description of the response structure:\n\n1. **Transformed Speech Output**:  \n   The response will likely contain the generated transformed speech in the form of an audio waveform. This audio data would be represented as a numerical array, typically using a tensor format (e.g., PyTorch tensor). The array represents the audio signal generated by the model after processing the input speech with a speaker embedding and vocoder.\n\n   - **Audio Data**: This would be the generated speech waveform, possibly in the form of a 1D array of audio samples.\n   - **Sampling Rate**: The response may include the sampling rate at which the audio was generated, which is typically required for further audio processing or saving the file.\n\n2. **Optional Metadata**:  \n   Depending on the system design, the response could also include metadata related to the transformation, such as:\n   - **Speaker Embeddings**: The embedding used for voice conversion might be included as part of the response for traceability or further processing.\n   - **Additional Parameters**: Any relevant details like the model's specific configuration used during the transformation, or a reference to the vocoder model, could be returned.\n\nThus, the expected response will consist primarily of the generated speech audio data (as a numerical array or tensor) and possibly include additional metadata such as sampling rate and speaker embeddings.",
            "examples": [
                {
                    "user_intent": "Convert my recorded English speech to sound like a different speaker's voice.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "my_recording.wav",
                            "sampling_rate": 16000,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "I want to modify the voice in this audiobook sample to match a target speaker.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "audiobook_sample.wav",
                            "sampling_rate": 22050,
                            "return_tensors": "pt"
                        }
                    }
                },
                {
                    "user_intent": "Transform a short speech clip to mimic another person's voice for testing a voice assistant.",
                    "tool_call": {
                        "name": "SpeechT5ForSpeechToSpeech.from_pretrained",
                        "arguments": {
                            "audio": "test_clip.wav",
                            "sampling_rate": 16000,
                            "return_tensors": "pt"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker’s voice into another speaker’s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\nTool 1: This tool is a speaker diarization pipeline built with pyannote.audio that automatically detects voice activity, identifies speaker changes, and handles overlapped speech in audio recordings. It allows fully automatic processing without manual intervention and can be customized with parameters such as the number of speakers and segmentation settings, making it useful for analyzing multi-speaker audio and generating precise speaker timelines.\nTool 2: This tool uses a Wav2Vec2-based model to perform speaker identification on 16kHz speech audio. Built for the SUPERB benchmark, it classifies audio samples by speaker, enabling tasks like voice recognition and speaker verification, and achieves strong performance on datasets such as VoxCeleb1.\nTool 3: This tool uses a fine-tuned version of the `facebook/hubert-base-ls960` model for audio classification tasks, specifically designed to identify speech commands. It achieves a high accuracy of 0.9973 on the evaluation set and can process audio files or browser-recorded inputs. The tool is built on the Hugging Face Transformers framework and requires specific Python libraries for use.\nTool 4: This tool uses the Wav2Vec2-Base model for keyword spotting (KS) in the SUPERB benchmark, specifically designed to detect preregistered keywords in speech audio. Trained on the Speech Commands dataset, the model classifies audio utterances into a predefined set of words. It is implemented using Hugging Face Transformers and can be easily integrated into Python environments with libraries like Torch, Transformers, and Torchaudio.\nTool 5: PersonaGPT is an open-domain conversational agent that generates personalized responses based on a bot's persona profile and incorporates specific turn-level goals, such as discussing work or asking about favorite music. It builds on the DialoGPT-medium model and the GPT-2 architecture, enabling more engaging and context-aware dialogues. The model is designed to handle personalized interactions by decoding input persona facts and adjusting responses accordingly.\nTool 6: A Pyannote Segmentation model fine-tuned on CHiME-7 DASR Challenge data, used for voice activity detection and speaker diarization in audio recordings.\nTool 7: A pre-trained Fairseq transformer-based text-to-speech model that generates a single-speaker female voice in Simplified Chinese, trained on Common Voice v7 and fine-tuned on CSS10, designed to convert written text into natural-sounding speech.\nTool 8: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 9: This tool provides a pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework. It enables accurate analysis of audio for detecting speech activity, identifying overlapping speech, and segmenting speakers, making it useful for tasks such as transcription, speaker diarization, and audio event detection.\nTool 10: This tool provides a pre-trained transformer-based text-to-speech model from Fairseq, designed to convert text into speech using a French, single-speaker male voice. It is based on the \"facebook/tts_transformer-fr-cv7_css10\" model, which was fine-tuned on the CSS10 dataset and pre-trained on Common Voice v7. The tool can be used to generate audio from French text using Fairseq's API in a Python environment.\nTool 11: A trained reinforcement learning model of a Poca agent designed to play SoccerTwos, built using the Unity ML-Agents Library, which can be loaded and run to train or evaluate gameplay.\nTool 12: This tool performs voice activity detection (VAD) on audio recordings using a Hugging Face Transformers pipeline, identifying segments of speech versus silence. It is designed for tasks such as speaker segmentation and speech analysis, allowing users to detect when speech starts and ends with configurable onset, offset, and minimum duration parameters. The model, trained on datasets like AMI Mix-Headset, DIHARD3, and VoxConverse, provides accurate speech boundary detection and is useful for audio processing, transcription, and speaker diarization workflows.\nTool 13: This tool uses a fine-tuned Hugging Face Transformers model to perform audio classification, specifically recognizing speech commands. It is highly accurate, achieving 99.79% accuracy on a benchmark speech commands dataset, making it suitable for applications that require reliable automated speech recognition and command detection.\nTool 14: A fine-tuned Audio Spectrogram Transformer (AST) model for classifying audio commands, which converts audio into spectrogram images and applies a Vision Transformer to achieve high-accuracy results on the Speech Commands v2 dataset. It is used to perform audio classification tasks efficiently using the Hugging Face Transformers framework.\nTool 15: A Unity ML-Agents model that allows users to train and play SoccerTwos using a pre-trained poca agent, enabling experimentation with reinforcement learning in a soccer simulation environment.\nTool 16: DialoGPT-large is a state-of-the-art pretrained dialogue generation model designed for multi-turn conversations. Trained on 147 million Reddit discussion threads, it can generate human-like responses in conversational settings, producing replies that are comparable in quality to human responses in single-turn interactions. It is commonly used for building chatbots and conversational agents that require natural, context-aware dialogue generation.\nTool 17: This tool performs speaker identification on 16 kHz speech audio using the Hubert-Large model from Hugging Face Transformers. It classifies each audio utterance to determine the speaker’s identity, making it suitable for multi-class speaker recognition tasks such as those in the SUPERB benchmark.\nTool 18: This tool is a transformer-based text-to-speech model for generating Arabic speech from text, using a male voice. It is trained on the Common Voice v7 dataset and can be accessed via the Fairseq framework. The model is loaded from the Hugging Face Hub and can convert Arabic text into audio output.\nTool 19: This tool provides a pretrained SpeechBrain model for speaker verification, enabling users to extract speaker embeddings from audio using a TDNN-based encoder trained on Voxceleb1 and Voxceleb2 data. It allows efficient verification of speaker identity in audio recordings and achieves high accuracy on benchmark datasets such as Voxceleb1.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A speech-to-speech voice conversion tool based on the SpeechT5 model, designed to transform an input speaker\\u2019s voice into another speaker\\u2019s voice while preserving linguistic content. It uses a shared encoder-decoder architecture with speech- and text-specific modules to enhance modeling capabilities, and is suitable for tasks like voice conversion and speech synthesis across different speakers.\", \"parameters\": {\"audio\": {\"type\": \"array\", \"description\": \"An array representing the input speech waveform that will be used for speech-to-speech conversion.\", \"required\": true, \"example\": [0.0, 0.1, -0.05, 0.02]}, \"sampling_rate\": {\"type\": \"number\", \"description\": \"The sampling rate (in Hz) of the input audio waveform, which ensures the audio is processed correctly by the model.\", \"required\": true, \"example\": 16000}, \"return_tensors\": {\"type\": \"string\", \"description\": \"Specifies the type of tensors to return after processing the audio. Typically 'pt' for PyTorch tensors.\", \"required\": true, \"example\": \"pt\"}}, \"response\": \"The tool does not explicitly define the response format. Based on the functionality described, we can infer a plausible response structure for the tool's operation.\\n\\nWhen the tool is invoked, it processes input speech data using the SpeechT5 model, and the expected response would most likely be the transformed speech output. Here\\u2019s a description of the response structure:\\n\\n1. **Transformed Speech Output**:  \\n   The response will likely contain the generated transformed speech in the form of an audio waveform. This audio data would be represented as a numerical array, typically using a tensor format (e.g., PyTorch tensor). The array represents the audio signal generated by the model after processing the input speech with a speaker embedding and vocoder.\\n\\n   - **Audio Data**: This would be the generated speech waveform, possibly in the form of a 1D array of audio samples.\\n   - **Sampling Rate**: The response may include the sampling rate at which the audio was generated, which is typically required for further audio processing or saving the file.\\n\\n2. **Optional Metadata**:  \\n   Depending on the system design, the response could also include metadata related to the transformation, such as:\\n   - **Speaker Embeddings**: The embedding used for voice conversion might be included as part of the response for traceability or further processing.\\n   - **Additional Parameters**: Any relevant details like the model's specific configuration used during the transformation, or a reference to the vocoder model, could be returned.\\n\\nThus, the expected response will consist primarily of the generated speech audio data (as a numerical array or tensor) and possibly include additional metadata such as sampling rate and speaker embeddings.\", \"examples\": [{\"user_intent\": \"Convert my recorded English speech to sound like a different speaker's voice.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"my_recording.wav\", \"sampling_rate\": 16000, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"I want to modify the voice in this audiobook sample to match a target speaker.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"audiobook_sample.wav\", \"sampling_rate\": 22050, \"return_tensors\": \"pt\"}}}, {\"user_intent\": \"Transform a short speech clip to mimic another person's voice for testing a voice assistant.\", \"tool_call\": {\"name\": \"SpeechT5ForSpeechToSpeech.from_pretrained\", \"arguments\": {\"audio\": \"test_clip.wav\", \"sampling_rate\": 16000, \"return_tensors\": \"pt\"}}}]}\n\n# User Query\nCreate a smart speaker that can recognize voice commands such as \"Turn on the lights,\" \"Play music,\" or \"Set a timer.\"\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to detect when a person is speaking in an audio stream to isolate voice segments.",
                    "tool_description": "Perform voice activity detection (VAD) on audio recordings to identify segments containing speech versus silence, enabling downstream audio processing for command recognition.",
                    "expected_response": "A list of time-stamped segments indicating speech presence and duration."
                },
                {
                    "user_intent": "Identify who is speaking to differentiate between multiple users if needed.",
                    "tool_description": "Perform speaker identification or verification on detected speech segments to classify or verify the speaker's identity for personalized interactions.",
                    "expected_response": "A mapping of speech segments to speaker identities or embeddings."
                },
                {
                    "user_intent": "Recognize specific voice commands from the user's speech.",
                    "tool_description": "Classify audio segments into predefined command categories using a speech command recognition model, enabling the smart speaker to respond to commands like turning on lights, playing music, or setting timers.",
                    "expected_response": "The recognized command label for each speech segment along with confidence scores."
                },
                {
                    "user_intent": "Convert text responses into spoken audio for feedback from the smart speaker.",
                    "tool_description": "Use a text-to-speech model to generate audio output from text responses, allowing the smart speaker to communicate back to the user naturally.",
                    "expected_response": "An audio waveform representing the spoken response."
                }
            ],
            "extracted_arguments": {
                "audio_input": "The raw audio waveform from the user's microphone, represented as an array of samples.",
                "sampling_rate": "The sampling rate of the input audio in Hz, required for correct audio processing.",
                "return_tensors": "The desired format for returned tensors from models, such as PyTorch tensors.",
                "command_set": "A predefined list of voice commands that the smart speaker should recognize, represented as text labels.",
                "speaker_identity": "Optional parameter representing the user’s identity for personalization or multi-user differentiation.",
                "response_text": "Text content to be converted into speech for the smart speaker's verbal output."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_482",
        "query": "Analyze a set of sentences to find the most similar pairs.",
        "instruction": "Given a `sentence similarity analysis` task, retrieve tools that process sets of sentences to extract features and identify the most similar pairs by utilizing models capable of mapping sentences into dense vector spaces for semantic comparisons.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Sentence Similarity",
                    "framework": "Hugging Face Transformers",
                    "functionality": "Feature Extraction",
                    "api_call": "SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')",
                    "api_arguments": [
                        "sentences"
                    ],
                    "python_environment_requirements": "pip install -U sentence-transformers",
                    "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)",
                    "performance": {
                        "dataset": "https://seb.sbert.net",
                        "accuracy": "Not provided"
                    },
                    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
                    "id": "huggingface_api_22",
                    "name": "sentence-transformers/distilbert-base-nli-mean-tokens"
                },
                "id": "gorilla_huggingface_tool_22",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "Tool 1: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.",
            "Tool 2: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.",
            "Tool 3: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.",
            "Tool 4: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.",
            "Tool 5: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.",
            "Tool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.",
            "Tool 7: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.",
            "Tool 8: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.",
            "Tool 9: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.",
            "Tool 10: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.",
            "Tool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.",
            "Tool 12: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.",
            "Tool 13: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.",
            "Tool 14: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.",
            "Tool 15: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.",
            "Tool 16: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.",
            "Tool 17: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.",
            "Tool 18: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.",
            "Tool 19: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently."
        ],
        "tool_view": {
            "description": "This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.",
            "parameters": {
                "query": {
                    "type": "string",
                    "description": "A single input sentence or question for which semantic similarity scores are computed against the provided documents.",
                    "required": true,
                    "example": "How many people live in London?"
                },
                "docs": {
                    "type": "array",
                    "description": "A list of sentences or paragraphs representing the documents or texts to compare semantically against the query.",
                    "required": true,
                    "example": [
                        "Around 9 Million people live in London",
                        "London is known for its financial district"
                    ]
                }
            },
            "response": "The tool produces a set of similarity scores between a given query sentence and a collection of candidate documents or sentences. The typical structure of the response includes the following components:\n\n1. **Query embedding**: A high-dimensional vector representing the semantic meaning of the input query. This vector is used as the reference for similarity comparisons.\n\n2. **Document embeddings**: A list of high-dimensional vectors, each representing the semantic meaning of a corresponding document or candidate sentence in the input collection.\n\n3. **Similarity scores**: A numeric value for each document indicating the semantic similarity between the query embedding and that document’s embedding. These scores are typically computed using a dot product or cosine similarity.\n\n4. **Ranked document-score pairs**: An ordered list of tuples or objects where each entry contains:\n   - **Document text**: The original text of the document or candidate sentence.\n   - **Similarity score**: The corresponding numeric similarity value.\n   \n   The list is usually sorted in descending order of similarity, so that the most semantically relevant documents appear first.\n\nOverall, the response is a mapping of each input document to a similarity score with respect to the query, enabling semantic search, ranking, or retrieval tasks. The embeddings themselves may also be optionally returned if further downstream processing is needed.",
            "examples": [
                {
                    "user_intent": "I want to find which product descriptions best match the query 'lightweight running shoes for men'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "lightweight running shoes for men",
                            "docs": [
                                "Men's ultralight running sneakers with breathable mesh",
                                "Comfortable leather hiking boots for long trails",
                                "Lightweight sports shoes for daily jogging and training"
                            ]
                        }
                    }
                },
                {
                    "user_intent": "Compare these two customer support answers to see which one better addresses 'How do I reset my password?'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "How do I reset my password?",
                            "docs": [
                                "Click on 'Forgot Password' and follow the instructions to reset your password.",
                                "To change your profile picture, go to settings and upload a new image."
                            ]
                        }
                    }
                },
                {
                    "user_intent": "I need to rank news headlines according to their relevance to the question 'Who won the Nobel Peace Prize in 2023?'.",
                    "tool_call": {
                        "name": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                        "arguments": {
                            "query": "Who won the Nobel Peace Prize in 2023?",
                            "docs": [
                                "The Nobel Peace Prize 2023 was awarded to Narges Mohammadi for her human rights activism.",
                                "Stock markets surged today as tech companies released quarterly earnings.",
                                "Scientists discovered a new species of frog in the Amazon rainforest."
                            ]
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\nTool 1: A cross-encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs based on their semantic relationship.\nTool 2: A Transformer-based language model, DistilRoBERTa-base provides fill-in-the-blank predictions for text by identifying the most likely words to replace masked tokens. It is a smaller, faster version of RoBERTa, designed for efficient NLP tasks where predicting missing words or completing sentences is needed.\nTool 3: A cross-encoder model for natural language inference that evaluates a pair of sentences and outputs scores for contradiction, entailment, and neutral relationships, enabling zero-shot classification of sentence pairs without task-specific training.\nTool 4: A Cross-Encoder model for natural language inference that evaluates pairs of sentences and outputs scores for contradiction, entailment, and neutral relationships. It is designed for zero-shot classification using the Hugging Face Transformers framework and is trained on the SNLI and MultiNLI datasets, making it suitable for assessing semantic relationships between sentences without task-specific fine-tuning.\nTool 5: A T5 model fine-tuned on the CommonGen dataset for generative commonsense reasoning, designed to create coherent sentences that describe everyday scenarios from a given set of concepts. It is used to test and generate text demonstrating a machine’s ability to reason about common situations using specified words.\nTool 6: A Hugging Face Transformers tool that uses the RoBERTa-base model for masked language modeling, allowing users to predict and fill in missing words in text. It is designed for natural language understanding tasks and can help analyze or complete sentences based on context.\nTool 7: A sentence-transformers model that encodes sentences and paragraphs into 768-dimensional vectors for semantic search, enabling comparison of text similarity. It is trained on 215 million question–answer pairs from diverse sources, making it suitable for retrieving or ranking relevant documents based on a query.\nTool 8: This tool is a sentence-transformers model that encodes sentences and paragraphs into 768-dimensional dense vectors for semantic similarity and search. It allows users to compare a query against a list of documents by computing similarity scores, enabling tasks like question answering, information retrieval, and ranking relevant text. The model is trained on 215 million diverse question–answer pairs to capture rich semantic relationships between sentences.\nTool 9: A pre-trained cross-encoder model that performs natural language inference on sentence pairs, predicting whether the relationship between two sentences is entailment, contradiction, or neutral. It is designed for zero-shot classification and can be used to evaluate semantic relationships without task-specific training, making it useful for NLP applications that require understanding sentence-level meaning.\nTool 10: This tool provides a high-accuracy English part-of-speech tagging model using Flair, capable of labeling words with universal POS tags like NOUN, VERB, ADJ, and others. It leverages Flair embeddings and an LSTM-CRF architecture to analyze sentences, making it useful for tasks in natural language processing that require syntactic understanding, such as text analysis, parsing, or information extraction.\nTool 11: This tool is a German-language named entity recognition (NER) model from Flair that identifies four types of entities—person names, locations, organizations, and miscellaneous names—within text. Built on Flair embeddings with an LSTM-CRF architecture, it can analyze sentences to automatically label and extract entities, making it useful for tasks like information extraction, text analysis, and NLP pipelines requiring structured recognition of proper nouns in German.\nTool 12: The \"google/pegasus-newsroom\" model, based on PEGASUS, is designed for abstractive text summarization tasks. It is pre-trained on the C4 and HugeNews datasets and can generate concise summaries from input text. This model is accessed via the Hugging Face Transformers pipeline for summarization and is suitable for summarizing news articles and similar content.\nTool 13: Generate images in an analog photographic style from text prompts using the wavymulder/Analog-Diffusion model, which is trained on a diverse set of analog photographs and accessible through the Hugging Face Transformers library.\nTool 14: This model classifies emotions in English text, identifying Ekman's six basic emotions (anger, disgust, fear, joy, sadness, surprise) along with a neutral category. It is a fine-tuned version of DistilRoBERTa-base and is designed to predict emotional tones in text, providing scores for each emotion. The model is suitable for applications in sentiment analysis, user feedback interpretation, and similar tasks.\nTool 15: This tool enables the generation of image variations using Stable Diffusion, fine-tuned to accept CLIP image embeddings instead of text embeddings. It allows for the creation of image variations similar to those produced by DALL·E 2. The tool is based on the \"lambdalabs/sd-image-variations-diffusers\" model from Hugging Face, and is useful for applications that require generating different versions of a given image.\nTool 16: The tool uses Google’s PEGASUS model via Hugging Face Transformers to perform abstractive text summarization, generating concise summaries by identifying and synthesizing the most important content from input text. It is designed for tasks requiring high-quality, human-like summaries across diverse datasets.\nTool 17: A compact audio classification model built on wav2vec2, designed to analyze audio input and categorize it into predefined classes using the Hugging Face Transformers framework.\nTool 18: A pretrained Hubert-based model for classifying emotions in local audio files, enabling users to analyze speech and detect emotional content using Hugging Face Transformers.\nTool 19: Graphormer is a pretrained graph Transformer model designed for graph classification and graph representation tasks, such as molecular modeling, enabling users to analyze and classify graph-structured data efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"This tool is a sentence-transformers model that encodes sentences and paragraphs into 384-dimensional vectors to measure semantic similarity, making it useful for tasks like semantic search and question-answer retrieval. It is trained on 215 million question-answer pairs from diverse sources and allows comparison of a query against multiple documents to identify the most relevant matches.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"A single input sentence or question for which semantic similarity scores are computed against the provided documents.\", \"required\": true, \"example\": \"How many people live in London?\"}, \"docs\": {\"type\": \"array\", \"description\": \"A list of sentences or paragraphs representing the documents or texts to compare semantically against the query.\", \"required\": true, \"example\": [\"Around 9 Million people live in London\", \"London is known for its financial district\"]}}, \"response\": \"The tool produces a set of similarity scores between a given query sentence and a collection of candidate documents or sentences. The typical structure of the response includes the following components:\\n\\n1. **Query embedding**: A high-dimensional vector representing the semantic meaning of the input query. This vector is used as the reference for similarity comparisons.\\n\\n2. **Document embeddings**: A list of high-dimensional vectors, each representing the semantic meaning of a corresponding document or candidate sentence in the input collection.\\n\\n3. **Similarity scores**: A numeric value for each document indicating the semantic similarity between the query embedding and that document\\u2019s embedding. These scores are typically computed using a dot product or cosine similarity.\\n\\n4. **Ranked document-score pairs**: An ordered list of tuples or objects where each entry contains:\\n   - **Document text**: The original text of the document or candidate sentence.\\n   - **Similarity score**: The corresponding numeric similarity value.\\n   \\n   The list is usually sorted in descending order of similarity, so that the most semantically relevant documents appear first.\\n\\nOverall, the response is a mapping of each input document to a similarity score with respect to the query, enabling semantic search, ranking, or retrieval tasks. The embeddings themselves may also be optionally returned if further downstream processing is needed.\", \"examples\": [{\"user_intent\": \"I want to find which product descriptions best match the query 'lightweight running shoes for men'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"lightweight running shoes for men\", \"docs\": [\"Men's ultralight running sneakers with breathable mesh\", \"Comfortable leather hiking boots for long trails\", \"Lightweight sports shoes for daily jogging and training\"]}}}, {\"user_intent\": \"Compare these two customer support answers to see which one better addresses 'How do I reset my password?'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"How do I reset my password?\", \"docs\": [\"Click on 'Forgot Password' and follow the instructions to reset your password.\", \"To change your profile picture, go to settings and upload a new image.\"]}}}, {\"user_intent\": \"I need to rank news headlines according to their relevance to the question 'Who won the Nobel Peace Prize in 2023?'.\", \"tool_call\": {\"name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"arguments\": {\"query\": \"Who won the Nobel Peace Prize in 2023?\", \"docs\": [\"The Nobel Peace Prize 2023 was awarded to Narges Mohammadi for her human rights activism.\", \"Stock markets surged today as tech companies released quarterly earnings.\", \"Scientists discovered a new species of frog in the Amazon rainforest.\"]}}}]}\n\n# User Query\nAnalyze a set of sentences to find the most similar pairs.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "I want to find which sentences in a set are most semantically similar to each other.",
                    "tool_description": "Encode each sentence into a high-dimensional vector using a sentence-transformers model, then compute similarity scores between all sentence pairs to identify the most semantically similar pairs.",
                    "expected_response": "A list of sentence pairs along with their corresponding similarity scores, ranked from most to least similar."
                }
            ],
            "extracted_arguments": {
                "sentences": "A list of sentences or short paragraphs to be compared for semantic similarity."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_467",
        "query": "We are a sports analysis company that processes data from spreadsheets with game statistics. We need to identify the player who has scored the maximum goals in a given match.",
        "instruction": "Given a `data analysis` task, retrieve tools that process spreadsheet data to identify key statistics such as the player with the maximum goals in a specific match by analyzing game statistics and providing clear outcomes based on the query's requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Table Question Answering",
                    "framework": "Transformers",
                    "functionality": "Table Question Answering",
                    "api_call": "TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')",
                    "api_arguments": [
                        "question",
                        "table"
                    ],
                    "python_environment_requirements": [
                        "transformers"
                    ],
                    "example_code": "https://huggingface.co/google/tapas-large-finetuned-sqa",
                    "performance": {
                        "dataset": "msr_sqa",
                        "accuracy": 0.7289
                    },
                    "description": "TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).",
                    "id": "huggingface_api_450",
                    "name": "google/tapas-large-finetuned-sqa"
                },
                "id": "gorilla_huggingface_tool_446",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "Tool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 2: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.",
            "Tool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.",
            "Tool 4: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.",
            "Tool 5: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.",
            "Tool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.",
            "Tool 7: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.",
            "Tool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.",
            "Tool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.",
            "Tool 10: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.",
            "Tool 11: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.",
            "Tool 12: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.",
            "Tool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.",
            "Tool 14: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.",
            "Tool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.",
            "Tool 16: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.",
            "Tool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.",
            "Tool 18: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.",
            "Tool 19: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis."
        ],
        "tool_view": {
            "description": "A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.",
            "parameters": {
                "image": {
                    "type": "string",
                    "description": "The URL or local file path of the image on which to perform object detection.",
                    "required": true,
                    "example": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"
                }
            },
            "response": "The tool returns an object detection result for a given input image, identifying CS:GO players and their positions. The typical response structure includes the following components:\n\n1. **Results List**: The output is usually a list of prediction results, where each element corresponds to one input image (even if only one image is processed).  \n\n2. **Boxes**: For each detected object, the response includes a bounding box specifying the location of the object in the image. A bounding box typically contains:\n   - **Coordinates**: The position of the box, often in the format `[x_min, y_min, x_max, y_max]` or `[x, y, width, height]`.\n   - **Confidence Score**: A numeric value representing the model’s confidence in the detection.\n   - **Class Label**: The predicted category of the object, which for this model is one of `['ct', 'cthead', 't', 'thead']`.\n\n3. **Additional Metadata (Optional)**: Depending on the implementation, the response may also include:\n   - **Segmentation masks** if supported.\n   - **Keypoints** if the model provides them.\n   - **Other model-specific attributes**, such as detection indices or tracking IDs.\n\n4. **Rendering Support**: Although not part of the raw prediction, a separate rendering utility (`render_result`) can take the results and overlay bounding boxes on the original image for visualization.\n\nOverall, the response provides structured information to locate and classify CS:GO players in the input image, including position, type, and confidence for each detected player.",
            "examples": [
                {
                    "user_intent": "Detect all CS:GO players in this match screenshot.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                        "arguments": {
                            "image": "https://example.com/csgo_match_screenshot1.jpg"
                        }
                    }
                },
                {
                    "user_intent": "Analyze a local image to identify CT and T players for training purposes.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                        "arguments": {
                            "image": "/home/user/images/csgo_round5.png"
                        }
                    }
                },
                {
                    "user_intent": "Quickly check if any headshots are visible in this CS:GO demo frame.",
                    "tool_call": {
                        "name": "YOLO('keremberke/yolov8n-csgo-player-detection')",
                        "arguments": {
                            "image": "https://csgo-demos.com/frame_1024.png"
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\nTool 1: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 2: The \"videomae-base-finetuned-ucf101\" tool is a model fine-tuned on the UCF101 dataset for video action recognition, utilizing the VideoMAE framework from Hugging Face Transformers. It is used to classify actions in video clips by processing and extracting features with the VideoMAE model. This tool helps identify action labels from video inputs, making it suitable for tasks involving video content analysis, such as activity recognition in sports, surveillance, or entertainment.\nTool 3: OmniTab is a table-based question answering model designed for answering queries based on tabular data. It uses the `neulab/omnitab-large` model, built on the BART architecture and pretrained on both natural and synthetic data. This model processes a table (in the form of a Pandas DataFrame) and a natural language query to generate an answer, making it suitable for applications where questions need to be answered from structured data tables.\nTool 4: A YOLOv8 model specialized for detecting heads in American football, capable of identifying helmets, blurred or partially visible helmets, challenging helmet angles, and sideline helmets. It is used to automatically locate and classify player heads in images, supporting analysis or monitoring tasks in football visual data.\nTool 5: This tool provides a pretrained model for detecting both bordered and borderless tables in document images using the Hugging Face Transformers framework. It processes images to identify table locations and outputs their bounding boxes with confidence scores, making it useful for automated document analysis, data extraction, and table structure recognition.\nTool 6: A pre-trained RoBERTa model designed for question answering, capable of extracting answers from a given context based on input questions. It is useful for natural language understanding tasks where precise answers need to be derived from text.\nTool 7: A YOLOv8 model for detecting in-game objects in Valorant, including enemies, teammates, dropped spikes, and planted spikes. It is trained on a custom Valorant dataset and provides high-accuracy object detection suitable for analyzing game scenes and automating in-game object recognition.\nTool 8: This tool provides a TAPAS-based model fine-tuned on the WikiTable Questions dataset, enabling question answering over tabular data. It leverages a pretrained language model to interpret tables and generate answers to natural-language questions, making it useful for extracting information from structured data like spreadsheets or HTML tables.\nTool 9: A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, detecting labels such as 'ct', 'cthead', 't', and 'thead'. It is used to automatically locate and classify players within game screenshots or video frames with high accuracy.\nTool 10: OmniTab is a table-based question-answering model designed to answer natural language queries using structured tabular data. Built on the BART architecture and pretrained on a combination of natural and synthetic data, it allows users to provide a table and a query as input and generates accurate answers, making it useful for extracting information from spreadsheets or databases in a few-shot learning setting.\nTool 11: The \"google/tapas-base-finetuned-sqa\" model is a transformer-based model fine-tuned for Table Question Answering. It is designed to answer questions related to tabular data, such as querying values from a table in a conversational context. The model is based on TAPAS, a BERT-like architecture, and has been fine-tuned using Sequential Question Answering (SQA) on data from sources like Wikipedia. This tool can be used in natural language processing tasks where users ask questions about tables, providing accurate answers based on the given data.\nTool 12: The \"ahotrod/electra_large_discriminator_squad2_512\" is a fine-tuned ELECTRA language model designed for question answering tasks. It uses the SQuAD2.0 dataset to provide accurate answers by analyzing a given context in response to a specific question. This model is useful for extracting relevant information from text, with high performance, achieving an 87.1% exact match and 90% F1 score on the SQuAD2.0 benchmark.\nTool 13: LLaMA-13B is a transformer-based, auto-regressive language model designed for research in natural language understanding, question answering, and reading comprehension. It can generate coherent text given a prompt and has been trained on diverse sources, including web data, GitHub, Wikipedia, and books across 20 languages. The model has been evaluated on multiple benchmarks, demonstrating strong performance on tasks such as BoolQ, PIQA, HellaSwag, and WinoGrande, making it suitable for advanced NLP text generation experiments and research applications.\nTool 14: A multilingual XLM-RoBERTa large model for extractive question answering that can identify answers within a given context across multiple languages. It is trained on the SQuAD 2.0 dataset and is suitable for applications requiring precise answer extraction from text in different languages.\nTool 15: This tool provides an automatic speaker diarization pipeline using the pyannote.audio framework, enabling the identification and segmentation of speakers within an audio file. It processes audio inputs and outputs speaker-labeled segments in RTTM format, supporting scenarios where the number of speakers is specified or bounded by minimum and maximum limits, making it useful for analyzing multi-speaker recordings such as meetings, interviews, or broadcasts.\nTool 16: The RoBERTa base OpenAI Detector is a text-classification model designed to identify whether a given text was generated by GPT-2. Fine-tuned on GPT-2 outputs, it provides high-accuracy detection, making it useful for distinguishing AI-generated content from human-written text.\nTool 17: Provides an extractive question-answering model based on RoBERTa, trained on the SQuAD 2.0 dataset, that can identify answers to questions within a given text. It is compatible with the Transformers library and can be used to build NLP applications that require precise retrieval of information from documents or passages.\nTool 18: This tool performs zero-shot classification in Spanish using a fine-tuned BERT model, specifically designed with the Spanish portion of the XNLI dataset. It allows users to classify text into predefined categories without needing labeled training data. By leveraging Hugging Face's Transformers framework, the model can categorize text into labels such as culture, society, economics, health, and sports.\nTool 19: A Vision Transformer (ViT) model trained with the DINO self-supervised method, used to extract visual features from images for computer vision tasks. It processes images to produce rich embeddings that can be used for downstream applications such as classification, retrieval, or similarity analysis.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A YOLOv8-based object detection model designed to identify Counter-Strike: Global Offensive (CS:GO) players in images, distinguishing between player types such as 'ct', 'cthead', 't', and 'thead'. It can be used to automatically detect and localize CS:GO players in visual data for analysis, gaming applications, or computer vision research.\", \"parameters\": {\"image\": {\"type\": \"string\", \"description\": \"The URL or local file path of the image on which to perform object detection.\", \"required\": true, \"example\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}}, \"response\": \"The tool returns an object detection result for a given input image, identifying CS:GO players and their positions. The typical response structure includes the following components:\\n\\n1. **Results List**: The output is usually a list of prediction results, where each element corresponds to one input image (even if only one image is processed).  \\n\\n2. **Boxes**: For each detected object, the response includes a bounding box specifying the location of the object in the image. A bounding box typically contains:\\n   - **Coordinates**: The position of the box, often in the format `[x_min, y_min, x_max, y_max]` or `[x, y, width, height]`.\\n   - **Confidence Score**: A numeric value representing the model\\u2019s confidence in the detection.\\n   - **Class Label**: The predicted category of the object, which for this model is one of `['ct', 'cthead', 't', 'thead']`.\\n\\n3. **Additional Metadata (Optional)**: Depending on the implementation, the response may also include:\\n   - **Segmentation masks** if supported.\\n   - **Keypoints** if the model provides them.\\n   - **Other model-specific attributes**, such as detection indices or tracking IDs.\\n\\n4. **Rendering Support**: Although not part of the raw prediction, a separate rendering utility (`render_result`) can take the results and overlay bounding boxes on the original image for visualization.\\n\\nOverall, the response provides structured information to locate and classify CS:GO players in the input image, including position, type, and confidence for each detected player.\", \"examples\": [{\"user_intent\": \"Detect all CS:GO players in this match screenshot.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-csgo-player-detection')\", \"arguments\": {\"image\": \"https://example.com/csgo_match_screenshot1.jpg\"}}}, {\"user_intent\": \"Analyze a local image to identify CT and T players for training purposes.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-csgo-player-detection')\", \"arguments\": {\"image\": \"/home/user/images/csgo_round5.png\"}}}, {\"user_intent\": \"Quickly check if any headshots are visible in this CS:GO demo frame.\", \"tool_call\": {\"name\": \"YOLO('keremberke/yolov8n-csgo-player-detection')\", \"arguments\": {\"image\": \"https://csgo-demos.com/frame_1024.png\"}}}]}\n\n# User Query\nWe are a sports analysis company that processes data from spreadsheets with game statistics. We need to identify the player who has scored the maximum goals in a given match.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Identify the player with the highest number of goals from a match statistics spreadsheet.",
                    "tool_description": "Process structured tabular data to determine the row corresponding to the maximum value in the 'goals' column and return the associated player's information.",
                    "expected_response": "The name or identifier of the player with the maximum goals and the corresponding goal count."
                }
            ],
            "extracted_arguments": {
                "match_statistics_table": "A structured data table containing player statistics for a specific match, including at least a 'player name' column and a 'goals' column."
            }
        }
    },
    {
        "id": "gorilla_huggingface_query_108",
        "query": "The company has just received a document written in French, and they need it translated into English.",
        "instruction": "Given a `translation` task, retrieve tools that perform language translation by processing text inputs in French and generating accurate English translations, aligning with the query's linguistic and content requirements.",
        "labels": [
            {
                "doc": {
                    "domain": "Natural Language Processing Translation",
                    "framework": "Transformers",
                    "functionality": "Translation",
                    "api_call": "pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')",
                    "api_arguments": [
                        "text"
                    ],
                    "python_environment_requirements": [
                        "transformers",
                        "torch"
                    ],
                    "example_code": "translation_pipeline('Bonjour, comment ça va?')",
                    "performance": {
                        "dataset": "opus",
                        "accuracy": {
                            "BLEU": {
                                "newsdiscussdev2015-enfr.fr.en": 33.1,
                                "newsdiscusstest2015-enfr.fr.en": 38.7,
                                "newssyscomb2009.fr.en": 30.3,
                                "news-test2008.fr.en": 26.2,
                                "newstest2009.fr.en": 30.2,
                                "newstest2010.fr.en": 32.2,
                                "newstest2011.fr.en": 33.0,
                                "newstest2012.fr.en": 32.8,
                                "newstest2013.fr.en": 33.9,
                                "newstest2014-fren.fr.en": 37.8,
                                "Tatoeba.fr.en": 57.5
                            }
                        }
                    },
                    "description": "Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.",
                    "id": "huggingface_api_524",
                    "name": "opus-mt-fr-en"
                },
                "id": "gorilla_huggingface_tool_520",
                "relevance": 1
            }
        ],
        "category": "code",
        "retrieved_tool_descriptions": [
            "Tool 0: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "Tool 1: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.",
            "Tool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.",
            "Tool 3: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.",
            "Tool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.",
            "Tool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.",
            "Tool 6: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.",
            "Tool 7: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.",
            "Tool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.",
            "Tool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.",
            "Tool 10: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.",
            "Tool 11: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.",
            "Tool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.",
            "Tool 13: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.",
            "Tool 14: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.",
            "Tool 15: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.",
            "Tool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.",
            "Tool 17: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.",
            "Tool 18: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.",
            "Tool 19: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently."
        ],
        "tool_view": {
            "description": "A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.",
            "parameters": {
                "text": {
                    "type": "string",
                    "description": "The text input that will be converted into speech by the model.",
                    "required": true,
                    "example": "Hello, how are you?"
                }
            },
            "response": "The tool produces an audio waveform corresponding to the input text provided. The response is typically a structured object containing at least the following components:  \n\n- **audio_data**: The synthesized speech signal, often represented as a raw waveform array or a buffer in a standard audio format (e.g., PCM, WAV). This is the primary output of the text-to-speech conversion.  \n- **sampling_rate**: The sample rate of the generated audio, indicating how many samples per second are present in the waveform. This is necessary to correctly play back or process the audio.  \n- **metadata (optional)**: Additional information about the generated speech, which may include:  \n  - **duration**: Length of the audio in seconds.  \n  - **language**: Detected or specified language used for synthesis.  \n  - **voice or speaker_id**: Identifier for the voice characteristics if multiple voices are supported.  \n  - **model_info**: Reference to the model or configuration used for synthesis.  \n\nIn essence, the response encapsulates all data necessary to play, store, or further process the generated speech from the input text.",
            "examples": [
                {
                    "user_intent": "Convert this English paragraph into speech so I can listen to it: 'Hello, welcome to our virtual assistant demo.'",
                    "tool_call": {
                        "name": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10",
                        "arguments": {
                            "text": "Hello, welcome to our virtual assistant demo."
                        }
                    }
                },
                {
                    "user_intent": "I want to generate Spanish audio for the sentence: '¿Cómo estás hoy?'",
                    "tool_call": {
                        "name": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10",
                        "arguments": {
                            "text": "¿Cómo estás hoy?"
                        }
                    }
                },
                {
                    "user_intent": "Produce French speech output for: 'Ceci est un exemple de synthèse vocale.'",
                    "tool_call": {
                        "name": "facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10",
                        "arguments": {
                            "text": "Ceci est un exemple de synthèse vocale."
                        }
                    }
                }
            ]
        },
        "rewritten_messages": [
            {
                "role": "system",
                "content": "You are an expert Query Rewriting Agent for a Tool Retrieval System. Your goal is to decompose a user request into a structured multi-field retrieval query that aligns with our tool documentation schema.\n\n# Target Tool Schema\nThe tools you are retrieving are structured as follows:\n- **description**: Formal technical summary of the tool's function.\n- **parameters**: Definitions of required inputs (names, types, roles).\n- **response**: Description of the data returned (entities, status, structure).\n- **examples**: List of `user_intent` strings showing how real users phrase the request.\n\n# Input Context\nYou will be provided with:\n1. **User Query**: The raw user request.\n2. **Partial Tool Descriptions in the Toolset**: A list of potentially relevant tool descriptions retrieved via keyword search. Use these to align your vocabulary with the available toolset.\n3. **Tool View**: A human-friendly flattened view of the tool. This is provided to help you better understand the tool schema.\n\n\n# Your Output Task\nGenerate a JSON object with exactly two top-level fields:\n\n1. **`tool_needs`** (List[Dict]): \n   - Decompose the request into one or more sequential actions. Each dictionary must contain:\n   - `user_intent` (str): A natural language phrasing of this step, optimized to match the **examples** field.\n   - `tool_description` (str): A formal, technical description of the action, optimized to match the **description** field.\n   - `expected_response` (str): An abstract description of the expected data or outcome, optimized to match the **response** field. If no data is returned, write \"Don't need response.\".\n\n2. **`extracted_arguments`** (Dict[str, str]):\n   - A single dictionary containing all parameters extracted from the query.\n   - **Key**: A logical parameter name.\n   - **Value**: An **abstract description** of the parameter's semantic role and data type.\n   - **Constraint**: NEVER include concrete values from the query (e.g., do not write \"Alice\" or \"123\"). Instead, write \"The target username\" or \"Numeric ID\".\n\n# Response Format\nReturn ONLY the JSON object. Do not add markdown formatting or explanations.\n\n# Example\n\nQuery: \"Find a flight to London and book it for John Doe.\"\n\nOutput:\n{\n    \"tool_needs\": [\n        {\n            \"user_intent\": \"I want to search for available flights to a specific city.\",\n            \"tool_description\": \"Retrieve a list of flight itineraries based on destination and passenger criteria.\",\n            \"expected_response\": \"A collection of flight objects containing flight IDs, prices, and schedules.\"\n        },\n        {\n            \"user_intent\": \"Book a ticket for the selected flight using passenger details.\",\n            \"tool_description\": \"Book a ticket for the selected flight using passenger details.\",\n            \"expected_response\": \"A confirmation object including a booking reference number and status.\"\n        }\n    ],\n    \"extracted_arguments\": {\n        \"destination\": \"The name of the arrival city or airport code.\",\n        \"passenger_name\": \"The full name of the person traveling.\",\n        \"flight_id\": \"A unique identifier for the selected flight itinerary.\"\n    }\n}\n"
            },
            {
                "role": "user",
                "content": "# Partial Tools in the Toolset\nThe following tool descriptions were roughly matched via keyword search. \n\nTool 0: A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\nTool 1: A Fairseq-based speech-to-speech translation tool that converts spoken audio in one language (English, Spanish, French, or Italian) into speech in another, using a model trained on multilingual datasets like mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli. It processes 16 kHz mono audio, generating natural-sounding translated speech through integrated vocoder technology, making it suitable for applications requiring real-time or high-quality multilingual audio translation.\nTool 2: A Russian-language dialogue summarization model based on MBart, fine-tuned to generate concise summaries of conversational text. It is designed to transform longer dialogues into readable summaries and has been trained on the SamSum dataset translated into Russian, making it useful for applications like chat summarization, including integration with messaging platforms.\nTool 3: The `opus-mt-fr-en` tool is a machine translation model designed to translate text from French to English. It uses the Marian NMT framework and is trained on the OPUS dataset. This model can be accessed through the `pipeline('translation_fr_to_en')` API call and is typically used in Natural Language Processing (NLP) tasks that require accurate French-to-English translation.\nTool 4: A fine-tuned multi-modal LayoutLM model designed for question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive information. It has been trained on a proprietary invoice dataset as well as SQuAD2.0 and DocVQA, enabling it to handle general document comprehension and accurately answer questions from structured and unstructured document content.\nTool 5: A speech-to-speech translation model developed by Facebook that converts English spoken input into French spoken output, enabling seamless voice translation between the two languages.\nTool 6: A speech-to-speech translation tool that converts English audio into Hokkien using Fairseq’s UnitY two-pass decoder. It is trained on TED and audiobook data and supports end-to-end processing from input audio to synthesized translated speech, enabling applications in multilingual communication and real-time translation.\nTool 7: FastSpeech 2 English text-to-speech model from Fairseq that generates high-quality speech from text using 200 male and female voices, trained on the Common Voice v4 dataset, suitable for converting written text into natural-sounding audio.\nTool 8: A multilingual sentiment analysis tool based on the BERT architecture, fine-tuned to predict star ratings (1 to 5) for product reviews in English, Dutch, German, French, Spanish, and Italian, enabling automated evaluation of customer feedback across multiple languages.\nTool 9: A token classification model built with Hugging Face Transformers for extracting entities, specifically company names, from text. Trained using AutoTrain on the ismail-lucifer011/autotrain-data-company_all dataset with high accuracy, it leverages a distilBERT-based architecture to identify and extract relevant entities efficiently.\nTool 10: A fine-tuned XLM-RoBERTa model for predicting punctuation in text across twelve languages, including English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian, enabling accurate multilingual token-level punctuation annotation.\nTool 11: Helsinki-NLP/opus-mt-en-fr is a Hugging Face translation model that converts English text into French. It leverages a transformer-based architecture with SentencePiece tokenization and normalization, trained on the OPUS dataset to provide accurate and reliable translations for a variety of text domains.\nTool 12: A transformer-based model for translating multiple Romance languages, including French, Spanish, Portuguese, Italian, and Romanian, into English. Trained on the OPUS dataset, it uses normalization and SentencePiece for preprocessing and delivers high-quality translations, making it suitable for multilingual text translation tasks.\nTool 13: CQI_Visual_Question_Answer_PT_v0 is a multimodal document question-answering tool that processes images and PDFs to extract information and answer questions in Portuguese and English, making it useful for quickly retrieving specific data from invoices, statements, and other structured documents.\nTool 14: This tool is a T5-based model designed to automatically correct errors in sentences, supporting English, German, and French text. It can be used to repair or improve the grammatical and structural accuracy of input sentences, making it useful for text cleaning, proofreading, or enhancing natural language inputs in NLP applications.\nTool 15: A French-language zero-shot text classification tool based on the Camembert-base model fine-tuned on the French portion of the XNLI dataset, allowing users to classify text into arbitrary categories without task-specific training. It is particularly useful for labeling French text with custom categories using natural-language hypotheses.\nTool 16: A fine-tuned version of LayoutLMv2 designed for document question answering, allowing users to provide a document context and a question to extract accurate answers from the document’s text and layout information.\nTool 17: CamemBERT is a French language model based on RoBERTa that can predict missing words in a sentence using the Fill-Mask task. It is useful for tasks such as text completion, language understanding, and contextual word prediction in French.\nTool 18: This is a text-to-speech model trained on multiple datasets, including mtedx, covost2, europarl_st, and voxpopuli, and supports English, Spanish, French, and Italian languages. It is built using the Fairseq framework and can be accessed via the API call `unit.TTS.from_pretrained('facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10')`. The model requires the `huggingface_hub` and `fairseq` libraries in a Python environment.\nTool 19: Salesforce/codegen-350M-multi is an autoregressive language model designed for program synthesis, capable of generating executable code from English prompts and completing partially-written code. It supports multiple programming languages and can process natural language and code to predict likely continuations, making it useful for automating code generation and assisting developers in writing code efficiently.\n\n# Tool View\nIt's important to note that this is just a tool example; ; it doesn't mean you need to generate tools exactly as shown.\n\n{\"description\": \"A multilingual text-to-speech model that converts written text into natural-sounding speech in English, French, Spanish, and Italian, trained on diverse datasets including MTEDx, CoVoST2, Europarl-ST, and VoxPopuli. It is designed for high-quality speech synthesis across multiple languages and is licensed under CC BY-NC 4.0.\", \"parameters\": {\"text\": {\"type\": \"string\", \"description\": \"The text input that will be converted into speech by the model.\", \"required\": true, \"example\": \"Hello, how are you?\"}}, \"response\": \"The tool produces an audio waveform corresponding to the input text provided. The response is typically a structured object containing at least the following components:  \\n\\n- **audio_data**: The synthesized speech signal, often represented as a raw waveform array or a buffer in a standard audio format (e.g., PCM, WAV). This is the primary output of the text-to-speech conversion.  \\n- **sampling_rate**: The sample rate of the generated audio, indicating how many samples per second are present in the waveform. This is necessary to correctly play back or process the audio.  \\n- **metadata (optional)**: Additional information about the generated speech, which may include:  \\n  - **duration**: Length of the audio in seconds.  \\n  - **language**: Detected or specified language used for synthesis.  \\n  - **voice or speaker_id**: Identifier for the voice characteristics if multiple voices are supported.  \\n  - **model_info**: Reference to the model or configuration used for synthesis.  \\n\\nIn essence, the response encapsulates all data necessary to play, store, or further process the generated speech from the input text.\", \"examples\": [{\"user_intent\": \"Convert this English paragraph into speech so I can listen to it: 'Hello, welcome to our virtual assistant demo.'\", \"tool_call\": {\"name\": \"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10\", \"arguments\": {\"text\": \"Hello, welcome to our virtual assistant demo.\"}}}, {\"user_intent\": \"I want to generate Spanish audio for the sentence: '\\u00bfC\\u00f3mo est\\u00e1s hoy?'\", \"tool_call\": {\"name\": \"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10\", \"arguments\": {\"text\": \"\\u00bfC\\u00f3mo est\\u00e1s hoy?\"}}}, {\"user_intent\": \"Produce French speech output for: 'Ceci est un exemple de synth\\u00e8se vocale.'\", \"tool_call\": {\"name\": \"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_fr_css10\", \"arguments\": {\"text\": \"Ceci est un exemple de synth\\u00e8se vocale.\"}}}]}\n\n# User Query\nThe company has just received a document written in French, and they need it translated into English.\n\n# Task\nReturn **only** the JSON object. Do not include markdown formatting or explanations."
            }
        ],
        "rewritten_query": {
            "tool_needs": [
                {
                    "user_intent": "Translate a French document into English.",
                    "tool_description": "A machine translation model that converts text written in French into English, using a transformer-based neural architecture trained on multilingual parallel corpora.",
                    "expected_response": "The translated text in English corresponding to the input French document."
                }
            ],
            "extracted_arguments": {
                "source_language": "The language of the input text, in this case French.",
                "target_language": "The language into which the text should be translated, in this case English.",
                "text_content": "The full textual content of the document that needs to be translated."
            }
        }
    }
]